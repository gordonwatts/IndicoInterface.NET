{"count": 1, "additionalInfo": {}, "_type": "HTTPAPIResult", "url": "https:\/\/indico.cern.ch\/export\/event\/304944.json?nc=yes&detail=subcontributions", "results": [{"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "02:00:00"}, "endDate": {"date": "2015-04-17", "tz": "Europe\/Zurich", "time": "09:00:00"}, "creator": {"_type": "Avatar", "emailHash": "a6558e0be6a85bc95cec59bfdb8cb816", "affiliation": "High Energy Accelerator Research Organization (JP)", "_fossil": "conferenceChairMetadata", "fullName": "NAKAMURA, Tomoaki", "id": "40454"}, "hasAnyProtection": false, "roomFullname": null, "modificationDate": {"date": "2015-04-27", "tz": "UTC", "time": "05:04:03.191118"}, "timezone": "Asia\/Tokyo", "contributions": [{"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "344", "speakers": [{"_type": "ContributionParticipation", "emailHash": "d009a8085f34aded6e7e6453fb148bfe", "affiliation": "H", "_fossil": "contributionParticipationMetadata", "fullName": "KIM, Sung Hun", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "a9db592454003edc4ee459a7c7da7af6", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PIA, Maria Grazia", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "9c9fe1a2c3e791f3085355d47c5e172b", "affiliation": "Hanyang University", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. KIM, Chan Hyeong", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "66e689fbf5a150f1ebfc800be407dacd", "affiliation": "INFN - Genova (Italy)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SARACCO, Paolo", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "d009a8085f34aded6e7e6453fb148bfe", "affiliation": "H", "_fossil": "contributionParticipationMetadata", "fullName": "KIM, Sung Hun", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "a9db592454003edc4ee459a7c7da7af6", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PIA, Maria Grazia", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "e837410be9bc73cdde4789f1829f156e", "affiliation": "H", "_fossil": "contributionParticipationMetadata", "fullName": "HAN, Mincheol", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "39cf31a786e22ae82c648331bc5efd23", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "HOFF, Gabriela", "id": "5"}], "title": "Experimental quantification of Geant4 PhysicsList recommendations: methods and results", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T10:44:27.683870+00:00", "description": "", "title": "poster_physlist_rosa.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/344\/attachments\/578445\/796569\/poster_physlist_rosa.pdf", "filename": "poster_physlist_rosa.pdf", "content_type": "application\/pdf", "type": "file", "id": 796569, "size": 331807}], "title": "Poster", "default_folder": false, "id": 578445, "description": ""}], "_type": "Contribution", "description": "Geant4 recommends a set of PhysicsLists and related classes (Builders, PhysicsConstructors) to its user community to facilitate the use of Geant4 functionality despite its intrinsic physics complexity. Relative limited documentation is available in the literature regarding Geant4 physics configuration tools, especially concerning the quantification of their accuracy, their computational performance and the stability of their results.\r\n\r\nWe present the results of a comprehensive experimental quantification project  concerning recommendations related to multiple scattering, which are implemented in the \"constructors\" package of Geant4 PhysicsLists. We show how these findings affect the simulation of energy deposition, both regarding the absolute value of the energy deposited in a volume and its spatial pattern, as well as other observables specifically pertinent to the treatment of multiple scattering.\r\n\r\nIn-depth analysis was carried out to quantify not only the behaviour of the recommended PhysicsConstructor classes of Geant4 physics_list package, but also of individual settings embedded in them, and their contribution to the overall simulation outcome. We report an extensive assessment of the accuracy, of the computational performance and of alternative settings of the physics configuration options subject to evaluation: all these aspects are quantified with respect to a wide collection of experimental test cases with rigorous statistical methods.  This analysis is documented over several Geant4 versions to quantify the stability of the results.\r\n\r\nThe results of this project provide objective guidelines for the improvement of Geant4 physics models, of their recommended use and of experimental applications of Geant4. We discuss how the methodology developed for this assessment could be extended to other Constructors, Builders and complete PhysicsLists released within the Geant4 Toolkit.", "track": "Track2: Offline software ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578445", "resources": [{"_type": "LocalFile", "name": "poster_physlist_rosa.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/344\/attachments\/578445\/796569\/poster_physlist_rosa.pdf", "fileName": "poster_physlist_rosa.pdf", "_fossil": "localFileMetadata", "id": "796569", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/344", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:45:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "345", "speakers": [{"_type": "ContributionParticipation", "emailHash": "91b6e0458cfc38c6e2b8701f5a7b9a24", "affiliation": "Universita & INFN, Milano-Bicocca (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "MASCHERONI, Marco", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "91b6e0458cfc38c6e2b8701f5a7b9a24", "affiliation": "Universita & INFN, Milano-Bicocca (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "MASCHERONI, Marco", "id": "0"}], "title": "CMS data distributed analysis with CRAB3", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T16:23:49.792367+00:00", "description": "", "title": "CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/345\/attachments\/578446\/796570\/CHEP2015.pdf", "filename": "CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796570, "size": 373898}], "title": "Slides", "default_folder": false, "id": 578446, "description": ""}], "_type": "Contribution", "description": "The CMS Remote Analysis Builder (CRAB) provides the service for managing analysis tasks isolating users from the technical details of the distributed Grid infrastructure. Throughout the LHC Run 1, CRAB has been successfully employed by an average 350 distinct users every week executing about 200,000 jobs per day.\r\n\r\nIn order to face the new challenges posed by the LHC Run 2, CRAB has been significantly re-factored. The pieces of the new system are a lightweight client, a central server exposing a REST interface accepting user requests, a number of servers dealing with user analysis tasks and submitting jobs to the CMS resource provisioning system, and a central service to asynchronously move user data from the execution node to the desired storage location. The new system improves the robustness, scalability and sustainability of the service.\r\n\r\nThis contribution will give an overview of the new system, reporting the status, experience and lessons learnt from the commissioning phase and the production rollout for the initial data taking. It will address all aspects of the project from development to operations and support.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578446", "resources": [{"_type": "LocalFile", "name": "CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/345\/attachments\/578446\/796570\/CHEP2015.pdf", "fileName": "CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796570", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "d854ed989b482b2f04dd5615b5a9f643", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "BELFORTE, Stefano", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "35fe3c2d6b40455311e03c0675b71596", "affiliation": "University of Malaya (MY)", "_fossil": "contributionParticipationMetadata", "fullName": "YUSLI, Mohd Nizam Bin", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "3ef11477becf35256032b913ffa168f1", "affiliation": "University of Notre Dame (US)", "_fossil": "contributionParticipationMetadata", "fullName": "WOLF, Matthias", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "3c0927d8d883c0c7337a19519dcd5670", "affiliation": "University of Notre Dame (US)", "_fossil": "contributionParticipationMetadata", "fullName": "WOODARD, Anna Elizabeth", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "e82c46a3a0d49dd555c888d113eace3a", "affiliation": "University of Nebraska (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BOCKELMAN, Brian Paul", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "bc7330245b15761981349f5fa0fed77c", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "CIANGOTTINI, Diego", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "65fd5d0f517c5ad7a467486ead92360b", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. VAANDERING, Eric", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "d4502e02b88247265f7423d8fafc6ba3", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "RIAHI, Hassen", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "d566d9c238d24452797875574a57cae6", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "TANASIJCZUK, Andres Jorge", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "0e407565f4dca7350556665ec4ad8847", "affiliation": "Centro de Investigaciones Energ. Medioambientales y Tecn. - (ES", "_fossil": "contributionParticipationMetadata", "fullName": "HERNANDEZ CALAMA, Jose", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "0e16821a7ef57745e150ed35cc005ab4", "affiliation": "Vilnius University (LT)", "_fossil": "contributionParticipationMetadata", "fullName": "BALCAS, Justas", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "1d75705660b8422955f3c72fda785208", "affiliation": "Bulgarian Academy of Sciences (BG)", "_fossil": "contributionParticipationMetadata", "fullName": "KONSTANTINOV, Preslav Borislavov", "id": "12"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/345", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:15:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "346", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ed96c4767feeb06950d16695aa31278b", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CURRIE, Robert Andrew", "id": "8"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "7ba575b05769913d1239745e15dafee3", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "BAUER, Daniela", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "cd17961312c904e3bb4b71723c8e87ac", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. COLLING, David", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "ed96c4767feeb06950d16695aa31278b", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CURRIE, Robert Andrew", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "b4e7a4ea1d679a57a1467fc9ae74c443", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "FAYER, Simon", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "6cccef6d6cebb7a770526f8954e4483f", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "HUFFMAN, Adam", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "a11b77fc54a5a3fd19cf040a3c41e953", "affiliation": "Imperial College London", "_fossil": "contributionParticipationMetadata", "fullName": "MARTYNIAK, Janusz", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "6794b20ff1a79867a414c13ad66a52f2", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "RAND, Duncan", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "7d89f79a43b5cfacaafd400ecdaea334", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. RICHARDS, Alexander John", "id": "7"}], "title": "The GridPP DIRAC project - Implementation of a multi-VO DIRAC service", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T01:52:28.862968+00:00", "description": "", "title": "The_GridPP_DIRAC_project_v2.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/346\/attachments\/578447\/796571\/The_GridPP_DIRAC_project_v2.pdf", "filename": "The_GridPP_DIRAC_project_v2.pdf", "content_type": "application\/pdf", "type": "file", "id": 796571, "size": 794022}], "title": "Slides", "default_folder": false, "id": 578447, "description": ""}], "_type": "Contribution", "description": "The DIRAC INTERWARE system was originally developed within the LHCb VO as a common interface to access distributed resources, i.e. grids, clouds and local batch systems. It has been used successfully in this context by the LHCb VO for a number of years. In April 2013 the GridPP consortium in the UK decided to offer a DIRAC service to a number of small VOs. The majority of these had been previously accessing grid resources using the EMI-WMS system and some of them were also using the EMI-LFC for their data management. DIRAC provides integrated workload and data management systems, although these can also be used independently if required.   \r\nIt was decided that the most maintainable way to deploy the DIRAC server was with one instance shared between all supported VOs rather than a separate one for each VO. While this deployment model was not without precedent, this approach greatly deviates from the model for which DIRAC was originally designed. We set-up a test instance and while using this it was found that a number of core features needed to be improved so that they could be managed within the scope of a VO rather than server-wide. The main features that needed this adaptation were the access controls, the automatic configuration of users & resources and the data management functions.   \r\nThe DIRAC client toolset is fully-featured and contains analogues to most of the lcg client commands. For the VOs which already have wrappers around the lcg tools, just adapting these would be a possibility however this would still leave every VO with their own solution to maintain. New VOs don\u2019t want to spend valuable computing effort to develop custom wrappers for their users based on these tools; a better solution is to try and standardise on a set of common tools. This reduces both the maintenance effort for any single user as bugs get fixed for everyone at once while also minimising the number of different configurations the central experts need to be familiar with. The primary tool we have investigated for this role is Ganga, particularly with its DIRAC backend module.   \r\nWe report on how we resolved these issues to provide a production grade multi-VO DIRAC service, including where appropriate the details of changes applied to the DIRAC and user interface code bases.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578447", "resources": [{"_type": "LocalFile", "name": "The_GridPP_DIRAC_project_v2.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/346\/attachments\/578447\/796571\/The_GridPP_DIRAC_project_v2.pdf", "fileName": "The_GridPP_DIRAC_project_v2.pdf", "_fossil": "localFileMetadata", "id": "796571", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/346", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "347", "speakers": [{"_type": "ContributionParticipation", "emailHash": "14147f94caca4f2319ce8897d3f819f4", "affiliation": "University of Pavia - INFN Section of Pavia", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. AURORA, Tamborini", "id": "4"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "05908f97e76b07857fb25637f2d84ae4", "affiliation": "University of Pavia", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. EDOARDO, Farina", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "785a2f1dc34053d63bc7d057c74bbc62", "affiliation": "Loma Linda University Health", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PIERLUIGI, Piersimoni", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "ec034df58127a5f2935ef5048daa9675", "affiliation": "University of Pavia - INFN Section of Pavia", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CRISTINA, Riccardi", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "4d332e07d9efd2872becbbd92c804270", "affiliation": "University of Pavia - INFN Section of Pavia", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. ADELE, Rimoldi", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "14147f94caca4f2319ce8897d3f819f4", "affiliation": "University of Pavia - INFN Section of Pavia", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. AURORA, Tamborini", "id": "4"}], "title": "Geant4 simulation for a study of a possible use of carbon ion pencil beams for the treatment of ocular melanomas with the active scanning system at CNAO.", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T00:10:17.265662+00:00", "description": "", "title": "Tamborini_ID347.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/347\/attachments\/578448\/796572\/Tamborini_ID347.pdf", "filename": "Tamborini_ID347.pdf", "content_type": "application\/pdf", "type": "file", "id": 796572, "size": 1316373}], "title": "Poster", "default_folder": false, "id": 578448, "description": ""}], "_type": "Contribution", "description": "**Purpose**\r\n\r\nThe aim of this work is a study of a possible use of carbon ion pencil beams (delivered with active scanning modality) for the treatment of ocular melanomas at the National Centre for Oncological Hadrontherapy (CNAO). The promising aspect of carbon ions radiotherapy for the treatment of this disease lies in its superior relative radiobiological effectiveness (RBE).\r\nThe Monte Carlo Geant4 toolkit is used to simulate the complete CNAO extraction beamline, with the active and passive components along it.\r\nA human eye modeled detector, including a realistic target tumor volume, is used as target. Cross check with previous studies at CNAO using protons allows comparisons on possible benefits on using such a technique with respect to proton beams. Experimental data on proton and carbon ion beams transverse distributions are used to validate the simulation.\r\n\r\n**Description**\r\n\r\nThe ocular melanoma is nowadays the most common early intraocular tumor in adulthood, a malignant tumor which tends to grow both inside the bulb, invading and disrupting the intraocular tissues and outside it, infiltrating the sclera and orbital tissues.\r\nSeveral possible modalities are available to treat the disease (conventional radiotherapy, brachytherapy, protontherapy); thanks to the physical properties of hadrons penetrating matter, protons or heavy ions treatments can improve the visual prognosis, because the energy is delivered to the target tissue, with very little exposure of surrounding healthy tissues.\r\nIn the clinical practice of eye protontherapy, the currently worldwide accepted technique for dose delivery is represented by the passive scattering modality.\r\nHowever, in the present configuration at CNAO the dose delivery system adopted and only available to treat patients, is represented by the full active scanning modality and a commercial general-purpose (not specifically dedicated to the eye's treatment), image-based treatment planning system (TPS) is used.\r\n\r\n**Methods**\r\n\r\nThe CNAO synchrotron is able to produce carbon ion beams with a FWHM ranging from a minimum of \r\n4 \u00b1 0.04 mm  to a maximum of 10 \u00b1 0.1 mm at the standard target center (isocenter), inversely depending on the beam energy. For protons, the FWHM ranges from  7.0 \u00b1 0.7 mm to 22.3 \u00b1 0.2 mm.\r\nThe CNAO beam's energy ranges from 120 MeV\/u to 400 MeV\/u, for carbon ions and from \r\n63 MeV to 250 MeV for protons, with an uncertainty of 0.05%.\r\nThe final part of the CNAO transport beamline, with all its elements, is simulated using the Geant4 toolkit. Projectile charged particles, accelerated in the CNAO synchrotron ring, travel in a long extraction vacuum beam pipe (about 6 m), crossing the magnetic field generated by two orthogonal deflecting magnets. The vacuum beam pipe is sealed by a carbon fiber exit window, after which the accelerated beam reaches the treatment room and travels for several centimeters in air. In the treatment room the beam crosses a fixed structure called nozzle, inside of which two beam-monitoring chambers are embedded to measure in real-time the beam fluence and position. \r\nThe standard target center (isocenter) is located at a distance of 64 cm from the downstream edge of the nozzle.\r\nAs a first step of the simulation, a detailed description of the human eye with its internal components is used to build an eye-detector; a realistic tumor is also included inside, near the optic nerve.  The eye is placed in a water box simulating the human brain, protruding from it as well as in a real human head. Each eye's element is made sensitive in Geant4 simulation to evaluate the dose deposition in each of the biological structures of the eye.\r\nSubsequently, an image of an eye from a Computed Tomography (CT) is implemented as a realistic detector, by importing in Geant4 the CT data. The image's information is stored in a series of DICOM (Digital Imaging and Communication in Medicine) files containing the image of a specific CT slice. The DICOM-detector is built by superimposing 10 contiguous CT files.\r\n\r\n**Results**\r\n\r\nBefore the eye-detector irradiation a validation of the Geant4 simulation with CNAO experimental data is carried out with both carbon ions and protons.\r\nImportant beam parameters such as the transverse FWHM and scanned radiation field 's uniformity are tested within the simulation and compared with experimental measurements at CNAO Centre.\r\nThe physical processes involved in secondary particles generation by carbon ions and protons in the eye-detector are reproduced to take into account the additional dose to the primary beam given to irradiated eye's tissues.\r\nA study of beam shaping is carried out to produce a uniform 3D dose distribution (shaped on the tumor) by the use of a spread out Bragg peak. The eye-detector is then irradiated through a two dimensional transverse beam scan at different depths. In the use case the eye-detector is rotated of an angle of 40\u00b0 in the vertical direction, in order to misalign the tumor from healthy tissues in front of it. The treatment uniformity on the tumor in the eye-detector is tested. \r\nFor a more quantitative description of the deposited dose in the eye-detector and for the evaluation of the ratio between the dose deposited in the tumor and the other eye components, proton and carbon DVHs (Dose Volume Histograms) are compared. A high statistics simulated sample is used to minimize statistical errors. \r\nIn the simulation a new particle generation method is developed in order to reproduce the experimental treatment plan by importing the DICOM RT-PLAN file, which contains all the information on the irradiation geometries and sequences (treatment plan parameters).\r\n\r\n**Conclusions**\r\n\r\nEven further validations must be done, the good results so far obtained by this work point out and confirm the possibility of using carbon ions delivered with active scanning beams to treat the ocular melanoma.", "track": "Track2: Offline software ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578448", "resources": [{"_type": "LocalFile", "name": "Tamborini_ID347.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/347\/attachments\/578448\/796572\/Tamborini_ID347.pdf", "fileName": "Tamborini_ID347.pdf", "_fossil": "localFileMetadata", "id": "796572", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "777b073d6eea735caf588e5b5df6398d", "affiliation": "CNAO Foundation", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MARIO, Ciocca", "id": "5"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/347", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "340", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6794b20ff1a79867a414c13ad66a52f2", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "RAND, Duncan", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "5d169b25201b37d77b32a7bb333ee04e", "affiliation": "University of London (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "LLOYD, Steve", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "6794b20ff1a79867a414c13ad66a52f2", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "RAND, Duncan", "id": "1"}], "title": "Testing WAN access to storage over IPv4 and IPv6 using multiple transfer protocols", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-30T06:32:26.174297+00:00", "description": "", "title": "chep-2015-poster-v3.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/340\/attachments\/578449\/796573\/chep-2015-poster-v3.pdf", "filename": "chep-2015-poster-v3.pdf", "content_type": "application\/pdf", "type": "file", "id": 796573, "size": 220223}], "title": "Slides", "default_folder": false, "id": 578449, "description": ""}], "_type": "Contribution", "description": "In the lead up to Run 2 of the LHC the WLCG grid middleware, storage access protocols and LHC computing models are in a state of flux. The LCG utilities and SRM middleware are being phased out, IPv6 is being rolled out across the WLCG and LHC experiments are making increasing use of xrootd federated access to storage elements over the WAN. However, both client and server software and WLCG sites vary in their readiness for IPv6.\r\n\r\nSites within GridPP will need to enable IPv6, support it on relevant software and ensure everything is working correctly. Comprehensive monitoring will help and we report on work to extend existing monitoring of WAN transfer capability that GridPP has been doing for a number of years. WAN access from storage elements to worker nodes over both IPv4 and IPv6 is systematically checked. The gsiftp protocol is checked with *lcg-cp* and *gfal-copy* clients, https with *curl* and xrootd with *xrdcp*. Future work will include job submission and data handling tests using the GridPP DIRAC service.\r\n\r\nResults can be viewed at http:\/\/pprc.qmul.ac.uk\/~lloyd\/gridpp\/nettest_v6.html", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578449", "resources": [{"_type": "LocalFile", "name": "chep-2015-poster-v3.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/340\/attachments\/578449\/796573\/chep-2015-poster-v3.pdf", "fileName": "chep-2015-poster-v3.pdf", "_fossil": "localFileMetadata", "id": "796573", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/340", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "341", "speakers": [{"_type": "ContributionParticipation", "emailHash": "7619cbf29b7dbd4004fe9d055e7b0f7b", "affiliation": "INFN Bari", "_fossil": "contributionParticipationMetadata", "fullName": "ELIA, Domenico", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "0d64274f04e0339d6a0b2b8b22f284f4", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "MINIELLO, Giorgia", "id": "4"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c61b3a2666d34ab30569ee2a4534e133", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "COLELLA, Domenico", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "c3a9d60000380ace3b532f9f854d7620", "affiliation": "INFN-Bari", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. DONVITO, Giacinto", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "7619cbf29b7dbd4004fe9d055e7b0f7b", "affiliation": "INFN Bari", "_fossil": "contributionParticipationMetadata", "fullName": "ELIA, Domenico", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "0f04fff4aec652dc5b99a7b878e20a27", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. FRANCO, Antonio", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "0d64274f04e0339d6a0b2b8b22f284f4", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "MINIELLO, Giorgia", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "8abff1240906172c2d9fd8d2a913ceeb", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "MAGGI, Giorgio", "id": "5"}], "title": "Local storage federation through XRootD architecture for interactive distributed analysis", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T06:59:58.833662+00:00", "description": "", "title": "Poster-EliaMiniello_CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/341\/attachments\/578450\/796574\/Poster-EliaMiniello_CHEP2015.pdf", "filename": "Poster-EliaMiniello_CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796574, "size": 590263}], "title": "Poster", "default_folder": false, "id": 578450, "description": ""}], "_type": "Contribution", "description": "A cloud-based Virtual Analysis Facility (VAF) for the ALICE experiment at the LHC has been developed in Bari. Similar facilities are currently running in other Italian sites with the aim to create a federation of interoperating farms able to provide their computing resources for interactive distributed analysis. The facility consists in a PROOF cluster of virtual machines dynamically deployed by the Openstack cloud infrastructure built in Bari for the PRISMA project. \r\nThe use of cloud technology, along with elastic provisioning of computing resources as an alternative to the grid for running data intensive analyses, is the main challenge of the facility. One of the crucial aspects of the user-driven analysis execution is the data access. A local storage facility has the disadvantage that the stored data can be accessed by PROOF only locally, i.e. from within the single VAF. To overcome such a limitation a federated infrastructure, which provides full access to all the data belonging to the federation independently from the site where they are stored, has been set up. The federation architecture exploits both cloud computing (block storage, posix storage and object storage) and XRootD technologies, in order to provide a dynamic, easy-to-use and well performing solution for data handling. It allows the users to store the files and efficiently retrieve the data, since it implements a dynamic distributed cache among many datacenters in Italy connected to one another through the high-bandwidth national network. \r\nIn this contribution we will show the technical solution, based on CEPH, GlusterFS or Swift together with XRootD, which has been implemented to build this distributed storage infrastructure. This solution will be able either to store data for long time periods or just cache data locally, so that they can be processed by the PROOF facilities at any given site. Details on the architecture implementation and the achieved performance in the first prototype tests will be also discussed.\r\n \r\nThe present work is supported by the Istituto Nazionale di Fisica Nucleare (INFN) of Italy and is partially funded under contract 20108T4XTM of Programmi di Ricerca Scientifica di Rilevante Interesse Nazionale (STOA-LHC PRIN, Italy).", "track": "Track3: Data store and access ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578450", "resources": [{"_type": "LocalFile", "name": "Poster-EliaMiniello_CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/341\/attachments\/578450\/796574\/Poster-EliaMiniello_CHEP2015.pdf", "fileName": "Poster-EliaMiniello_CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796574", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/341", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "342", "speakers": [{"_type": "ContributionParticipation", "emailHash": "017f6964531992962b690dc19924f69c", "affiliation": "Polish Academy of Sciences (PL)", "_fossil": "contributionParticipationMetadata", "fullName": "GRZYMKOWSKI, Rafal Zbigniew", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "017f6964531992962b690dc19924f69c", "affiliation": "Polish Academy of Sciences (PL)", "_fossil": "contributionParticipationMetadata", "fullName": "GRZYMKOWSKI, Rafal Zbigniew", "id": "2"}], "title": "Belle II public and private clouds management in VMDIRAC system.", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T15:28:33.437535+00:00", "description": "", "title": "CHEP2015_POSTER_Grzymkowski_Rafal.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/342\/attachments\/578451\/796575\/CHEP2015_POSTER_Grzymkowski_Rafal.pdf", "filename": "CHEP2015_POSTER_Grzymkowski_Rafal.pdf", "content_type": "application\/pdf", "type": "file", "id": 796575, "size": 6482312}], "title": "Poster", "default_folder": false, "id": 578451, "description": ""}], "_type": "Contribution", "description": "The role of cloud computing technology in the distributed computing for HEP experiments grows rapidly. Some experiments (Atlas, BES-III, LHCb,\u2026) already exploits private and public cloud resources for data processing. Future experiments such as Belle II or upgraded LHC experiments will largely rely on the availability of cloud resources and therefore their computing models have to be adjusted to the specific features of cloud environment, in particular to the on-demand computing paradigm.\r\n\r\nBelle II experiment at SuperKEKB will start physics run in 2017. Belle II computing requirements are challenging. The data size at the level of hundred PB is expected after several years of operation, around 2020.\r\nThe baseline solution selected for distributed processing is the DIRAC system. DIRAC can handle variety of computing resources including Grids, Clouds and independent clusters. Cloud resources can be connected by VMDIRAC module through public interfaces. In particular the mechanism of dynamic activation of new virtual machines with reserved job slots for new tasks in case of an increasing demand for computing resources is introduced.\r\n\r\nThis work is focused on VMDIRAC interaction with public (Amazon EC2) and private (CC1) cloud. The solution applied by Belle II experiment and the experience from Monte Carlo production campaigns will be presented. Updated computation costs for different use cases will be shown.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578451", "resources": [{"_type": "LocalFile", "name": "CHEP2015_POSTER_Grzymkowski_Rafal.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/342\/attachments\/578451\/796575\/CHEP2015_POSTER_Grzymkowski_Rafal.pdf", "fileName": "CHEP2015_POSTER_Grzymkowski_Rafal.pdf", "_fossil": "localFileMetadata", "id": "796575", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "77129490a06056d8761c3ede7dea7289", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "HARA, Takanori", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/342", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "343", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ce4ab29b5fc6bd2e5d0cc264ac0a86c2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. COUET, Olivier", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ce4ab29b5fc6bd2e5d0cc264ac0a86c2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. COUET, Olivier", "id": "0"}], "title": "Base ROOT reference guide on Doxygen", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The ROOT reference guide is part of the code. Classes description, methods usage, examples etc...\r\nare all embedded with the code itself. Doxygen is the reference model allowing to extract the \r\ndocumentation from such self described system. The ROOT documentation requires the development\r\nof specific tools (scripts) in the Doxygen context. The proposed project is these tools writing.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/343", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:15:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "348", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a9db592454003edc4ee459a7c7da7af6", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PIA, Maria Grazia", "id": "8"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "3d5b88914882de0089aa59efc1e4a533", "affiliation": "Hanyang University (KR)", "_fossil": "contributionParticipationMetadata", "fullName": "CHOI, Chansoo", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "e837410be9bc73cdde4789f1829f156e", "affiliation": "H", "_fossil": "contributionParticipationMetadata", "fullName": "HAN, Mincheol", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "a0df065b2d4d04b79f76bcb72168e722", "affiliation": "European XFEL GmbH", "_fossil": "contributionParticipationMetadata", "fullName": "HAUF, Steffen", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "39cf31a786e22ae82c648331bc5efd23", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "HOFF, Gabriela", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "9c9fe1a2c3e791f3085355d47c5e172b", "affiliation": "Hanyang University", "_fossil": "contributionParticipationMetadata", "fullName": "KIM, Chan Hyeong", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "164f18aa38885d12da44660e6dedee8f", "affiliation": "Hanyang University", "_fossil": "contributionParticipationMetadata", "fullName": "KIM, Han Sung", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "d009a8085f34aded6e7e6453fb148bfe", "affiliation": "H", "_fossil": "contributionParticipationMetadata", "fullName": "KIM, Sung Hun", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "fa8af741d60af63f23be551f6b1782f4", "affiliation": "European XFEL GmbH", "_fossil": "contributionParticipationMetadata", "fullName": "KUSTER, Markus", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "a9db592454003edc4ee459a7c7da7af6", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PIA, Maria Grazia", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "66e689fbf5a150f1ebfc800be407dacd", "affiliation": "INFN - Genova (Italy)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SARACCO, Paolo", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "7feb349f17063dfa43936218b80cbde5", "affiliation": "MPE Garching", "_fossil": "contributionParticipationMetadata", "fullName": "WEIDENSPOINTNER, Georg", "id": "10"}], "title": "Testable physics by design", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:24:53.491704+00:00", "description": "", "title": "testablephysics.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/348\/attachments\/578452\/796576\/testablephysics.pdf", "filename": "testablephysics.pdf", "content_type": "application\/pdf", "type": "file", "id": 796576, "size": 2816759}, {"_type": "attachment", "modified_dt": "2015-04-13T00:24:53.491704+00:00", "description": "", "title": "testablephysics.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/348\/attachments\/578452\/796577\/testablephysics.pptx", "filename": "testablephysics.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796577, "size": 3590516}], "title": "Slides", "default_folder": false, "id": 578452, "description": ""}], "_type": "Contribution", "description": "Testable physics by design\r\n\r\nThe validation of physics calculations requires the capability to thoroughly test them. The difficulty of exposing parts of the software to adequate testing can be the source of incorrect physics functionality, which in turn may generate hard to identify systematic effects in physics observables produced by the experiments.\r\n\r\nStarting from real-life examples encountered in the course of an extensive validation effort of Geant4 physics, we show how software design choices may affect the ability to test basic aspects of physics functionality and to monitor their evolution in the course of software lifecycle. We document cases where inaccurate physics behaviour was hidden by software design that prevented its testability, and illustrate how improved physics functionality could be achieved by improving the transparency of software design.\r\n\r\nExploiting the experience with Geant4 as a playground for investigation, we discuss methods to enhance the testability of existing software systems by means of refactoring techniques, the identification of inflection points and their coverage. We discuss how these techniques differ from, but can be combined with, the traditional practice of monitoring some physics observables with regression testing. \r\n\r\nWe also present guidelines to introduce testability into the software design since the early phases of the software development, and to preserve it in the course of the product lifecycle. This issue is especially relevant in the context of ongoing R&D on future simulation systems.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578452", "resources": [{"_type": "LocalFile", "name": "testablephysics.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/348\/attachments\/578452\/796576\/testablephysics.pdf", "fileName": "testablephysics.pdf", "_fossil": "localFileMetadata", "id": "796576", "_deprecated": true}, {"_type": "LocalFile", "name": "testablephysics.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/348\/attachments\/578452\/796577\/testablephysics.pptx", "fileName": "testablephysics.pptx", "_fossil": "localFileMetadata", "id": "796577", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/348", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": null, "keywords": [], "id": "349", "speakers": [{"_type": "ContributionParticipation", "emailHash": "572f6f06cb2be973d28b9d129a0a91fd", "affiliation": "University of Edinburgh (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "BHIMJI, Wahid", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "572f6f06cb2be973d28b9d129a0a91fd", "affiliation": "University of Edinburgh (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "BHIMJI, Wahid", "id": "0"}], "title": "Fast and deep machine learning for HEP", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The extraction of rare physics signals in the HEP community has for some time benefited from machine learning approaches. However these have not widely made use of recent progress in algorithms from the \u2018data science\u2019 community, and can also hampered by considerable computation times. Efficient use of such techniques has the potential for considerable impact both in the HEP community and in industry.\r\n\r\nWe apply various machine learning approaches to real HEP analyses including the areas of \u2018jet substructure\u2019 and searches for new particles. We compare the use of the current methods of supervised neural nets and boosted decision trees with \u2018deep learning\u2019: multi-layer, neural nets. We implement our own many-core implementations and compare with existing libraries, as well as widely used current packages such as TMVA and python scikit-learn. We also provide performance measurements on a range of GPU and many-core devices.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "d2a9dd1d5ffb677268a37316407c3cd4", "affiliation": "University of Edinburgh (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "BRISTOW, Timothy Michael", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "d2481eb1f11b08f40d4e1ef3cdb920dc", "affiliation": "University of Edinburgh (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "WASHBROOK, Andrew John", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/contribution\/349", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:15:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "298", "speakers": [{"_type": "ContributionParticipation", "emailHash": "999374fec7057ef5475cac0d50d640f0", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. PETERS, Andreas Joachim", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "0210065dafb782e4d922a706dbed482e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SINDRILARU, Elvin Alin", "id": "0"}], "title": "Archiving tools for EOS", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T09:05:09.990971+00:00", "description": "", "title": "EOS-Archiving.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/298\/attachments\/578453\/796578\/EOS-Archiving.pdf", "filename": "EOS-Archiving.pdf", "content_type": "application\/pdf", "type": "file", "id": 796578, "size": 1608544}], "title": "Slides", "default_folder": false, "id": 578453, "description": ""}], "_type": "Contribution", "description": "Archiving data to tape is a critical operation for any storage system, especially for the EOS system at CERN which holds production data from all major LHC experiments. Each collaboration has an allocated quota it can use at any given time therefore, a mechanism for archiving \"stale\" data is needed so that storage space is reclaimed for online analysis operations.\r\n\r\nThe archiving tool that we propose for EOS aims to provide a robust interface for moving data between EOS and the tape storage system while enforcing data integrity and verification.\r\n\r\nThe archiving infrastructure is written in Python and is fully based on the XRootD framework. All data transfers are done using the third-party copy mechanism which ensures point-to-point communication between the source and destination, thus providing maximum aggregate throughput. Using ZMQ message-passing paradigm and a process-based approach enabled us to archive optimal utilisation of the resources and a stateless architecture which can easily be tuned during operation.\r\n\r\nIn conclusion, we make a comparative analysis between archiving a data set in a \"managed\" way using the archiving tool and the old plain copy method, highlighting the speed-up gain, the data integrity checks performed and the behaviour of the system in different failure scenarios. We expect this tool to considerably improve the data movement work-flow at CERN in both directions between disk and tape.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578453", "resources": [{"_type": "LocalFile", "name": "EOS-Archiving.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/298\/attachments\/578453\/796578\/EOS-Archiving.pdf", "fileName": "EOS-Archiving.pdf", "_fossil": "localFileMetadata", "id": "796578", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "999374fec7057ef5475cac0d50d640f0", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. PETERS, Andreas Joachim", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "cee8f9bdf304d0dba45ae82a7cb9ae5e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DUELLMANN, Dirk", "id": "2"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/298", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:15:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "296", "speakers": [{"_type": "ContributionParticipation", "emailHash": "999374fec7057ef5475cac0d50d640f0", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. PETERS, Andreas Joachim", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "999374fec7057ef5475cac0d50d640f0", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. PETERS, Andreas Joachim", "id": "0"}], "title": "EOS as the present and future solution for data storage at CERN", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T06:16:44.722261+00:00", "description": "", "title": "EOS-CHEP-Generic.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/296\/attachments\/578454\/796579\/EOS-CHEP-Generic.pdf", "filename": "EOS-CHEP-Generic.pdf", "content_type": "application\/pdf", "type": "file", "id": 796579, "size": 2424106}], "title": "Slides", "default_folder": false, "id": 578454, "description": ""}], "_type": "Contribution", "description": "EOS is an open source distributed disk storage system in production since 2011 at CERN. Development focus has been on low-latency analysis use cases for LHC and non-LHC experiments and life-cycle management using JBOD hardware for multi PB storage installations. The EOS design implies a split of hot and cold storage and introduced a change of the traditional HSM functionality based workflows at CERN.\r\n\r\nThe 2015 deployment brings storage at CERN to a new scale and foresees to breach 100 PB of disk storage in a distributed environment using tens of thousands of (heterogeneous) hard drives. EOS has brought to CERN major improvements compared to past storage solutions by allowing quick changes in the quality of services of storage pools. This allows the data centre to quickly meet the changing performance and reliability requirements of the LHC experiments with minimal data movements and dynamic reconfigurations. For example, the software stack has met the specific needs of the dual computing centre set-up required by CERN and allowed the fast design of new workflows accommodating the separation of long-term tape archive and disk storage required for the LHC Run II.\r\n\r\nThe talk will give a high-level state of the art overview of EOS with respect to Run II, introduce new tools and use cases and set the new roadmap for the next storage solutions to come.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578454", "resources": [{"_type": "LocalFile", "name": "EOS-CHEP-Generic.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/296\/attachments\/578454\/796579\/EOS-CHEP-Generic.pdf", "fileName": "EOS-CHEP-Generic.pdf", "_fossil": "localFileMetadata", "id": "796579", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "cee8f9bdf304d0dba45ae82a7cb9ae5e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DUELLMANN, Dirk", "id": "1"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/296", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:15:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "297", "speakers": [{"_type": "ContributionParticipation", "emailHash": "999374fec7057ef5475cac0d50d640f0", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. PETERS, Andreas Joachim", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "999374fec7057ef5475cac0d50d640f0", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. PETERS, Andreas Joachim", "id": "0"}], "title": "Integrating CEPH in EOS", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T06:21:42.560008+00:00", "description": "", "title": "EOS-CHEP-RD.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/297\/attachments\/578455\/796580\/EOS-CHEP-RD.pdf", "filename": "EOS-CHEP-RD.pdf", "content_type": "application\/pdf", "type": "file", "id": 796580, "size": 2384770}], "title": "Slides", "default_folder": false, "id": 578455, "description": ""}], "_type": "Contribution", "description": "The EOS storage software was designed to cover CERN disk-only storage use cases in the medium-term trading scalability against latency. To cover and prepare for long-term requirements the CERN IT data and storage services group (DSS) is actively conducting R&D and open source contributions to experiment with a next generation storage software  based on CEPH.\r\n\r\nCEPH provides a scale-out object storage system (RADOS) and additionally various optional high-level services like S3 gateway, RADOS block devices and a POSIX compliant file system (CephFS). The acquisition of CEPH by Redhat underlines the promising role of CEPH as the open source storage platform of the future.\r\n\r\nCERN IT is running a CEPH service in the context of OpenStack on a moderate scale of 1 PB replicated storage. Building a 100+PB storage system based on CEPH will require software and hardware tuning. It is of capital importance to demonstrate the feasibility and possibly iron out bottlenecks and blocking issues beforehand.\r\n\r\nMain idea behind this R&D is to leverage and contribute to existing building blocks in the CEPH storage stack and implement a few CERN specific requirements in a thin customizable storage layer.\r\n\r\n\r\nThe presentation will introduce various open source developments & contributions, their applicability and first performance figures of a next generation storage platform aka EOS Diamond:\r\n\r\n* radosFS - a RADOS based client API providing a scale-out meta data catalog and pseudo-hierarchical storage view with optimized(POSIX-light) directory and file access, parallel meta data queries, modification time propagation, file  striping ...\r\n\r\n* Erasure Codes - local reconstruction and reed solomon codes for storage cost reduction and improved reliability\r\n\r\n* a multi-protocol CEPH Overlay Service based on the XRootD framework providing data localization, multi-user policies, strong authentication, WebDAV\/XRootD access ...\r\n\r\n\r\n* a read-write Federated Storage Cloud platform based on storage virtualization and infrastructure aware scheduling", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578455", "resources": [{"_type": "LocalFile", "name": "EOS-CHEP-RD.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/297\/attachments\/578455\/796580\/EOS-CHEP-RD.pdf", "fileName": "EOS-CHEP-RD.pdf", "_fossil": "localFileMetadata", "id": "796580", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "2e0b405e221caf0be13f1c886622ff89", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "PEREIRA ROCHA, Joaquim Manuel", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "526ebd66173cf4a22d191ac12c5edab8", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "VAN DER STER, Dan", "id": "2"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/297", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:30:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "294", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e106556bcded279b116df4aee2de6c31", "affiliation": "University of Victoria (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SOBIE, Randy", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e106556bcded279b116df4aee2de6c31", "affiliation": "University of Victoria (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SOBIE, Randy", "id": "0"}], "title": "Utilizing cloud computing resources for BelleII", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T17:49:53.807658+00:00", "description": "", "title": "Sobie-BelleII-CHEP.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/294\/attachments\/578456\/796581\/Sobie-BelleII-CHEP.pdf", "filename": "Sobie-BelleII-CHEP.pdf", "content_type": "application\/pdf", "type": "file", "id": 796581, "size": 2666492}], "title": "Slides", "default_folder": false, "id": 578456, "description": ""}], "_type": "Contribution", "description": "The BelleII experiment is developing a global computing system for the simulation of MC data prior its collecting real collision data in the next few years. The system utilizes the grid middleware used in the WLCG and uses the DIRAC workload manager. We describe how IaaS cloud resources are being integrated into the BelleII production computing system in Australia and Canada. The IaaS resources include HEP as well as opportunistic and commercial clouds.\r\n\r\nIn Canada, the cloud resources are managed by a DIRAC installation at the University of Victoria, which acts as a slave to the DIRAC instance at KEK. A workload management service running on the DIRAC server at the University of Victoria submits pilot jobs to an HTCondor queue dedicated to the distributed cloud system. The CloudScheduler VM provisioning service boots the VMs based on the HTCondor queue.   The distributed cloud uses resources in Europe and North America including Amazon EC2. \r\n\r\nAustralia provides it's contribution to the Belle II Distributed Computing solution via CPU resources supplied by the NeCTAR Open Stack cloud. The Australian solution employs a Dynamic Torque Batch system with cloud-based worker nodes as a backend to an EGI CREAM-CE. The DIRAC interware sees this a conventional grid cluster with no further configuration or tuning required. The worker nodes employ an SL6 operating system configured via puppet and with the Belle II application software provided via CVMFS. At the time of writing, the Australian system efficiently supplies 350 worker nodes to the Belle II Distributed Computing solution.\r\n\r\nAll the clouds have been successfully used in BelleII MC production campaigns, producing a substantial fraction of the simulated data samples.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578456", "resources": [{"_type": "LocalFile", "name": "Sobie-BelleII-CHEP.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/294\/attachments\/578456\/796581\/Sobie-BelleII-CHEP.pdf", "fileName": "Sobie-BelleII-CHEP.pdf", "_fossil": "localFileMetadata", "id": "796581", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "31a2e16ea0419e091f98907026634fb9", "affiliation": "University of Melbourne (AU)", "_fossil": "contributionParticipationMetadata", "fullName": "SEVIOR, Martin", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "77129490a06056d8761c3ede7dea7289", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "HARA, Takanori", "id": "2"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/294", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "295", "speakers": [{"_type": "ContributionParticipation", "emailHash": "66278dc4d76ec0e88c859ea10b5f3964", "affiliation": "National Superconducting Cyclotron Laboratory, Michigan State University", "_fossil": "contributionParticipationMetadata", "fullName": "KUCHERA, Michelle", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "66278dc4d76ec0e88c859ea10b5f3964", "affiliation": "National Superconducting Cyclotron Laboratory, Michigan State University", "_fossil": "contributionParticipationMetadata", "fullName": "KUCHERA, Michelle", "id": "0"}], "title": "Updating the LISE\\textsuperscript{++} software and future upgrade plans", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-08T19:52:30.662477+00:00", "description": "", "title": "Kuchera_CHEP2015_FINAL.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/295\/attachments\/578457\/796582\/Kuchera_CHEP2015_FINAL.pdf", "filename": "Kuchera_CHEP2015_FINAL.pdf", "content_type": "application\/pdf", "type": "file", "id": 796582, "size": 1444398}, {"_type": "attachment", "modified_dt": "2015-04-08T19:52:30.662477+00:00", "description": "", "title": "Kuchera_CHEP2015_FINAL.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/295\/attachments\/578457\/796583\/Kuchera_CHEP2015_FINAL.pptx", "filename": "Kuchera_CHEP2015_FINAL.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796583, "size": 2690350}], "title": "Poster", "default_folder": false, "id": 578457, "description": ""}], "_type": "Contribution", "description": "Production of new isotopes is one of the opportunities at the intensity frontier of nuclear physics. The associated science ranges from tests of the Standard Model to exploration of the origin and evolution of the chemical elements in the universe. Leading facilities in this effort are  RIBF at RIKEN, TRIUMF in Canada, and ISOLDE at CERN. New large scale facilities under development at the nuclear intensity frontier include FAIR in Europe, FRIB in the United States, and others in countries including China, France, Korea, and Italy.\r\n\r\nThis talk will describe capabilities and future upgrade plans for the isotope production and simulation software that is used at many of these facilities, namely LISE$\\textsuperscript{++}$ [1].  For reference of its wide-scale use, the LISE$\\textsuperscript{++}$ website had approximately 3000 unique visitors in 2013, with Japan, USA, Germany, France, and China as the top five countries in terms of visitors.\r\n\r\nLISE$\\textsuperscript{++}$ is software used to predict beam intensity and purity of rare isotope beams produced in-flight by magnetic and electric separators. The primary use of LISE$\\textsuperscript{++}$ at most facilities is to predict and identify the composition of Radioactive Nuclear Beams [1]. Intensity and purity of a desired beam can be predicted, along with the separator magnet settings. Included in the LISE$\\textsuperscript{++}$ package are models of isotope production mechanisms, ion optical transport through magnetic and electric systems, and ion interactions in matter. The suite includes a full set of utilities for simulation of experiments.\r\n\r\nThe talk will highlight the process and methods of updating the software while retaining the computational integrity of the code. To accommodate the diversity of our users, we extend the software from Windows to a cross platform application. In addition, the C++ standard will be updated from Borland to C++11. The calculations of beam transport and isotope production are becoming more computationally intense with the new large scale facilities. For example, the 90 m long FRIB separator will have around fifty magnetic elements and ten points of beam interactions with matter. In order to perform the calculations in acceptable time, numerical optimization and parallel methods are applied. Computational improvements as well as the process of updating this large code will be discussed.\r\n\r\n[1] LISE++: Radioactive beam production with in-flight separators. O.B. Tarasov, D. Bazin", "track": "Track2: Offline software ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578457", "resources": [{"_type": "LocalFile", "name": "Kuchera_CHEP2015_FINAL.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/295\/attachments\/578457\/796582\/Kuchera_CHEP2015_FINAL.pdf", "fileName": "Kuchera_CHEP2015_FINAL.pdf", "_fossil": "localFileMetadata", "id": "796582", "_deprecated": true}, {"_type": "LocalFile", "name": "Kuchera_CHEP2015_FINAL.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/295\/attachments\/578457\/796583\/Kuchera_CHEP2015_FINAL.pptx", "fileName": "Kuchera_CHEP2015_FINAL.pptx", "_fossil": "localFileMetadata", "id": "796583", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "961756dcaa20ebac11d0b4145377d5fd", "affiliation": "National Superconducting Cyclotron Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "TARASOV, Oleg", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "898ce4af0f62ba0857b36355aa025107", "affiliation": "National Superconducting Cyclotron Laboratory, Michigan State University", "_fossil": "contributionParticipationMetadata", "fullName": "SHERRILL, Bradley", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "759c00dabf68d9c0bee75b15c480f92e", "affiliation": "National Superconducting Cyclotron Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "BAZIN, Daniel", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/295", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "292", "speakers": [{"_type": "ContributionParticipation", "emailHash": "818c88f50c6ae2b9ebe7de416071983a", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FRANZONI, Giovanni", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "818c88f50c6ae2b9ebe7de416071983a", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FRANZONI, Giovanni", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "4daea3ac567431a3fc62a5ca910a9e74", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CERMINARA, Gianluca", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c5fff1e296ec574187817b412c5fd077", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PFEIFFER, Andreas", "id": "2"}], "title": "USER AND GROUP STORAGE MANAGEMENT AT THE CMS CERN T2 CENTRE", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T02:04:04.752977+00:00", "description": "", "title": "poster.T2.v8.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/292\/attachments\/578458\/796584\/poster.T2.v8.pdf", "filename": "poster.T2.v8.pdf", "content_type": "application\/pdf", "type": "file", "id": 796584, "size": 676032}], "title": "Slides", "default_folder": false, "id": 578458, "description": ""}], "_type": "Contribution", "description": "A wide range of detector commissioning, calibration and data analysis tasks is carried out by members of the Compact Muon Solenoid (CMS) collaboration using dedicated storage resources available at the CMS CERN Tier-2 centre.\r\n\r\nRelying on the functionalities of the EOS disk-only storage technology, the optimal exploitation of the CMS user\/group resources has required the introduction of policies for data access management, data protection, cleanup campaigns based on access pattern, and long term tape archival.\r\n\r\nThe resource management has been organised around the definition of working groups and the delegation to an identified responsible of each group composition.\r\n\r\nIn this poster contribution we illustrate the user\/group storage management, and the development and operational experience at the CMS CERN Tier-2 centre in the 2012-2015 period.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578458", "resources": [{"_type": "LocalFile", "name": "poster.T2.v8.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/292\/attachments\/578458\/796584\/poster.T2.v8.pdf", "fileName": "poster.T2.v8.pdf", "_fossil": "localFileMetadata", "id": "796584", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/292", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "293", "speakers": [{"_type": "ContributionParticipation", "emailHash": "336e636659f12d5fe7aede25fd33f666", "affiliation": "University of Virginia", "_fossil": "contributionParticipationMetadata", "fullName": "GROUP, Robert", "id": "4"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "16fd38df9511193598ce9d4ef81431ca", "affiliation": "Indian University, Bloomington, Indiana, USA", "_fossil": "contributionParticipationMetadata", "fullName": "DAVIES, Gavin", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "364fe3816cfe6229b10e07f8d88b993a", "affiliation": "Fermi National Accelerator Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "REBEL, Brian", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "a03131261466f88a56cf5516acacef47", "affiliation": "University of Minnesota, Twin Cities, Minnesota, USA", "_fossil": "contributionParticipationMetadata", "fullName": "ZIRNSTEIN, Jan", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "ac15508921e4559339d39bb85b3af6d0", "affiliation": "University Of Minnesota, Twin Cities, MN, USA", "_fossil": "contributionParticipationMetadata", "fullName": "SACHDEV, Kanika", "id": "3"}], "title": "Software Management for the NOvA Experiment", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-05T10:55:37.509021+00:00", "description": "", "title": "novasoft_chep.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/293\/attachments\/578459\/796585\/novasoft_chep.pdf", "filename": "novasoft_chep.pdf", "content_type": "application\/pdf", "type": "file", "id": 796585, "size": 830544}], "title": "Slides", "default_folder": false, "id": 578459, "description": ""}], "_type": "Contribution", "description": "The NOvA software (NOvASoft) is written in C++ and built on the Fermilab Computing Division's ART framework that uses ROOT analysis software. NOvASoft makes use of more than 50 external software packages, is developed by more than 50 developers and is used by more than 100 physicists from over 30 universities and laboratories in 3 continents. The software builds are handled by Fermilab's custom version of Software Release Tools (SRT), a UNIX based software management system for large, collaborative projects that is used by several experiments at Fermilab. The system provides software version control with SVN configured in a client-server mode and is based on the code originally developed by the BaBar collaboration. In this paper, we present efforts towards distributing the NOvA software via the CERN VMFS distributed file system. We will also describe our recent work to use CMake build system and Jenkins, the open source continuous integration system, for NOvASoft.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578459", "resources": [{"_type": "LocalFile", "name": "novasoft_chep.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/293\/attachments\/578459\/796585\/novasoft_chep.pdf", "fileName": "novasoft_chep.pdf", "_fossil": "localFileMetadata", "id": "796585", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "336e636659f12d5fe7aede25fd33f666", "affiliation": "University of Virginia", "_fossil": "contributionParticipationMetadata", "fullName": "GROUP, Robert", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/293", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "290", "speakers": [{"_type": "ContributionParticipation", "emailHash": "84f3f721370dba12b47a611b1a36081e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SAILER, Andre", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "84f3f721370dba12b47a611b1a36081e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SAILER, Andre", "id": "1"}], "title": "Integration of DD4hep in the Linear Collider Software Framework", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T08:47:37.044769+00:00", "description": "", "title": "150413_Chep_DD4hepAndLC_Print.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/290\/attachments\/578460\/796586\/150413_Chep_DD4hepAndLC_Print.pdf", "filename": "150413_Chep_DD4hepAndLC_Print.pdf", "content_type": "application\/pdf", "type": "file", "id": 796586, "size": 6156615}], "title": "Slides", "default_folder": false, "id": 578460, "description": ""}], "_type": "Contribution", "description": "The DD4hep detector description toolkit offers a flexible and easy to use solution for the consistent and complete description of particle physics detectors in one single system. It provides software components addressing visualisation, simulation, reconstruction and analysis of high energy physics data.\r\nThe Linear Collider community has adopted DD4hep early on in the development phase and actively participated in the design of  the toolkit. The CLICdp and ILD detector working groups have reimplemented their simulation models in DD4hep, thereby ensuring a well defined hierarchy of the model, an accurate material description and a flexible scaling behaviour for the purpose of detector optimisation.\r\nIn parallel, an interface for reconstruction offering a high level view on the detector geometry has been developed.\r\nIt is based on surfaces assigned to detector components, which provide access to all relevant material properties and allow for fast and efficient navigation during track and calorimeter reconstruction.\r\nIn this talk we give a brief introduction to DD4hep, then focus on the description of the new simulation models and the reconstruction interface and its application to track reconstruction and particle flow at the ILC and CLIC.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578460", "resources": [{"_type": "LocalFile", "name": "150413_Chep_DD4hepAndLC_Print.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/290\/attachments\/578460\/796586\/150413_Chep_DD4hepAndLC_Print.pdf", "fileName": "150413_Chep_DD4hepAndLC_Print.pdf", "_fossil": "localFileMetadata", "id": "796586", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "dcb99157c4dd3dfa639bfc1133a3f993", "affiliation": "Jozef Stefan Institute (SI)", "_fossil": "contributionParticipationMetadata", "fullName": "PETRIC, Marko", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "84e347808f71c25608b52dc4bd77c2ad", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FRANK, Markus", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "c64e884c3f4101f740093ee4acbd85ec", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "LU, Shaojun", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "b5e94dc57d9ec82cc9dddec9b8706ed1", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "NIKIFOROU, Nikiforos", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "f5dcc35395831a55f6722d12d4272492", "affiliation": "CERN\/DESY", "_fossil": "contributionParticipationMetadata", "fullName": "GAEDE, Frank-Dieter", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/290", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:15:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:30:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "291", "speakers": [{"_type": "ContributionParticipation", "emailHash": "d76e9fcef7ed3cc5fefce881a0b613a4", "affiliation": "Colorado State University", "_fossil": "contributionParticipationMetadata", "fullName": "PAPADOPOULOS, Christos", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "5a75ef474988f25e48c1b8d0cfe973e4", "affiliation": "Colorado State University", "_fossil": "contributionParticipationMetadata", "fullName": "SHANNIGRAHI, Susmit", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "29739c7205cb42e874b61f7aaab0ec7b", "affiliation": "California Institute of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BARCZYK, Artur Jerzy", "id": "1"}], "title": "Named Data Networking in Climate Research and HEP Applications", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T08:32:31.190924+00:00", "description": "", "title": "chep.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/291\/attachments\/578461\/796587\/chep.pptx", "filename": "chep.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796587, "size": 3449805}], "title": "Slides", "default_folder": false, "id": 578461, "description": ""}], "_type": "Contribution", "description": "Introduction\r\n------------\r\nThe Computing Models of the LHC experiments continue to evolve from\r\nthe simple hierarchical MONARC model towards more agile models where\r\ndata is exchanged among many Tier2 and Tier3 sites, relying on both\r\nstrategic data placement, and an increased use of remote\r\naccess with caching through CMS's AAA and ATLAS' FAX projects, for example.\r\nThe challenges presented by expanding needs for CPU, storage and network capacity\r\nhave pointed the way towards future more agile pervasive models that make\r\nbest use of highly distributed heterogeneous resources.  \r\n\r\nIn this paper, we explore the use of **Named Data Networking (NDN)** [1],\r\na new Internet architecture focusing on content rather than the location of the\r\ndata collections. As NDN has shown considerable promise in another data intensive field,\r\nClimate Science, we discuss the similarities\r\nand differences between the Climate and HEP use cases, along with specific\r\nissues HEP faces and will face during LHC Run2 and beyond, which NDN\r\ncould address.  \r\n\r\nNDN\r\n---------------------\r\n\r\nNDN, an instance of Information Centric Networking (ICN), is a new Internet\r\narchitecture which focuses on the content of the data collections themselves,\r\nrather than on where the data resides. The end host addresses are\r\nreplaced with content names, which, similar to URLs,\r\nare hierarchical, unique and human readable. Thus, NDN imposes minimal \r\nstructure on applications, which can choose their own naming schemes.\r\nThe hierarchical structure of NDN names has several advantages:\r\n\r\n 1. it is an intuitive, common organizational structure (e.g., file systems, URLs, etc.),\r\n 2. it is scalable (similar to hierarchical IP addresses), and\r\n 3. coupled with longest prefix matching, it allows for data discovery and enumeration.\r\n\r\nNDN has a wide range of potential benefits such as in-network\r\ncontent caching with request deduplication to reduce congestion and improve delivery\r\nspeed, simpler application configuration, and security built into the\r\nnetwork at the data level.\r\n\r\nThe NDN concepts, structure and initial applications have been developed\r\nthrough an NSF Future Internet Architecture project in its second round of funding,\r\ninvolving eight universities.\r\nNDN has attracted significant interest from industry, including Cisco, Intel,\r\nAlcatel, Huawei, and Panasonic, and involves many of these companies\r\nthrough an industry consortium.\r\n\r\n\r\nNDN and Climate Applications\r\n----------------------------\r\n\r\nWe have successfully begun to test NDN in the climate application domain [2].\r\nTo handle the various naming schemes used in climate applications, we\r\nhave designed and implemented translators that take existing names with\r\narbitrary structure (produced by climate models, or home-grown) and\r\ntranslated them into NDN-compliant names. Depending on the original name\r\nstructure, the translation can be fairly direct (e.g., data that complies\r\nwith the \"Data Reference Syntax\" from the Coupled Model Intercomparison Project),\r\nor complex (from home-grown naming schemes that require the analysis of\r\nmetadata embedded in the dataset or even user feedback in order to construct\r\nproper NDN names).\r\n\r\nWe have deployed a dedicated 6-node testbed for climate applications that\r\nreaches locations such as the Atmospheric and Computer Science Departments at\r\nColorado State University, LBNL and NWCS. The testbed is connected via\r\n10G links by ESnet and is composed of high-end machines each with 40 core CPUs,\r\n128GB RAM and 48TB diskspace. The machines cumulatively host over\r\n50TB of climate data and are used for research, experimentation and development\r\nof climate applications.\r\n\r\nNDN Support for HEP Applications\r\n------------------------------------------------------\r\n\r\nSeveral features of NDN can be beneficial to the HEP computing use case.\r\nData sources publish new\r\ncontent to the network following an agreed upon naming scheme. Data delivery\r\nis always performed in a pull mode, driven by the consumer issuing interest\r\npackets. Intermediate nodes in the network dynamically cache data based on\r\ncontent popularity, ready to satisfy subsequent interests directly from\r\nthe cache, thus lowering the load on servers with popular content.\r\nCombining this with the pull-mode results in a multicast-like data delivery,\r\npossibly optimizing both the network utilisation as well as server load.\r\n\r\nThe use of multiple data sources simultaneously, as well as the\r\nnative use of multiple paths between client and data source, provide for\r\nrobust failover in case of network segment, node, or end-site failure.\r\n\r\nAll these are active research areas today. Caching as well as forwarding\r\nstrategies, naming schemes, multi-sourcing and multi-path forwarding need\r\nto be investigated not only from the network but also the application perspective.\r\n\r\nHEP experiments using the World-wide LHC Computing Grid (WLCG)\r\nhave  well-developed, hierarchical naming schemes in use, which already fit the NDN\r\napproach well. We take this logical file name structure as a starting\r\npoint for investigating the benefits of using NDN as the data distribution and\r\naccess network for HEP data processing. For this, we use the testbed described above.\r\nWe further target simultaneous optimization of storage and bandwidth resource utilization\r\nthrough dynamic caching using the VIP framework in [3].\r\nFor the scalability study, we complement the testbed with the use of a simulation\r\nenvironment with a representative topology including network nodes and end-sites.\r\n\r\nSummary\r\n-------\r\n\r\nIn this paper, we study data access over an NDN testbed developed\r\nfor Climate research. We study the behaviour using HEP-like data structures based\r\non the CMS naming scheme, showing data publishing, discovery and retrieval in\r\nan NDN network. We demonstrate the benefits of caching, speeding up data delivery\r\nin multi-job access from a single source, with jobs executing at multiple sites.\r\nWe also show the results of the simulation studies of remote data access over\r\nan NDN network demonstrating the scalability of the system.\r\n\r\nReferences\r\n----------\r\n1. V. Jacobson, et al.; \"Networking Named Content\", 2009\r\n2. C. Olschanowsky, et al., \"Supporting Climate Research using Named Data Networking\", LANMAN, 2014\r\n3. E. Yeh, et al.; \"VIP: A Framework for Joint Dynamic Forwarding and Caching in Named Data Networks\",  Proc. ACM Conf. on Information-Centric Networking, 2014", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578461", "resources": [{"_type": "LocalFile", "name": "chep.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/291\/attachments\/578461\/796587\/chep.pptx", "fileName": "chep.pptx", "_fossil": "localFileMetadata", "id": "796587", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "d76e9fcef7ed3cc5fefce881a0b613a4", "affiliation": "Colorado State University", "_fossil": "contributionParticipationMetadata", "fullName": "PAPADOPOULOS, Christos", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "38f38c492f6ca658cfb28aabcf9a502c", "affiliation": "LAWRENCE BERKELEY NATIONAL LABORATORY", "_fossil": "contributionParticipationMetadata", "fullName": "SIM, Alex", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "1e8462d5329e0dc95b296b6351dd31d4", "affiliation": "ESNET", "_fossil": "contributionParticipationMetadata", "fullName": "MONGA, Inder", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "14de4875b3b66f00314a8f89838c1ffa", "affiliation": "California Institute of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "NEWMAN, Harvey", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "a042d6b43b8bfd624ccd2323e1c96aca", "affiliation": "LAWRENCE BERKELEY NATIONAL LABORATORY", "_fossil": "contributionParticipationMetadata", "fullName": "WU, John", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "e7cd3a79153f1dcf8df73bb07029b83f", "affiliation": "Northeastern University", "_fossil": "contributionParticipationMetadata", "fullName": "YEH, Edmund", "id": "7"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/291", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:45:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "199", "speakers": [{"_type": "ContributionParticipation", "emailHash": "35721fed37de20dfa93d4c5ffdac389c", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "HOEFT, Bruno Heinrich", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "35721fed37de20dfa93d4c5ffdac389c", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "HOEFT, Bruno Heinrich", "id": "0"}], "title": "100G Deployment@(DE-KIT)", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T03:09:33.456532+00:00", "description": "", "title": "100G_Deployment-(DE-KIT).v16.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/199\/attachments\/578462\/796588\/100G_Deployment-DE-KIT.v16.pdf", "filename": "100G_Deployment-DE-KIT.v16.pdf", "content_type": "application\/pdf", "type": "file", "id": 796588, "size": 1556108}, {"_type": "attachment", "modified_dt": "2015-04-16T03:09:33.456532+00:00", "description": "", "title": "100G_Deployment-(DE-KIT).v16.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/199\/attachments\/578462\/796589\/100G_Deployment-DE-KIT.v16.pptx", "filename": "100G_Deployment-DE-KIT.v16.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796589, "size": 2817774}], "title": "Slides", "default_folder": false, "id": 578462, "description": ""}], "_type": "Contribution", "description": "The Steinbuch Center for Computing (SCC) at Karlsruhe Institute of Technology (KIT) was involved quite early in 100G network technology. In 2010 already a first 100G wide area network testbed over a distance of approx.  450 km was deployed between the national research organizations KIT and FZ-J\u00fclich - initiated by DFN (the German NREN). Only three years later 2013, KIT joined the Caltech SC13 100G \"show floor\" initiative using the transatlantic ANA-100G link to transfer LHC data from a storage at DE-KIT (GridKa) in Europe to hard disks at the show floor of SC13 in Denver (USA). \r\nThe network infrastructure of KIT as well as of the German Tier-1 installation DE-KIT (GridKa) however, is still based on 10Gbps. As highlighted in the contribution \"Status and Trends in Networking at LHC Tier1 Facilities\" to CHEP 2012, proactive investment is required at the Tier-1 sites. Bandwidth requirements will grow beyond the capacities currently available and the required upgrades are expected to be performed in 2015. \r\nIn close cooperation with DFN KIT is driving the upgrade from 10G to 100G. The process is divided into several phases, due to upgrade costs and different requirements in varying parts of the network infrastructure. The first phase will add a 100G interface to combine the interface connecting DE-KIT to LHCONE, where the highest demand for increased bandwidth is currently predicted. LHCONE is a routed virtual private network, connecting several Tier-[123] centers of WLCG and Belle-2. In phase number two, a second 100G interface will provide 100G symmetric interfaces to LHCONE. In phase number three, several of the routing interlinks of the Tier-1 center (DE-KIT) will receive an upgrade to 100G. KIT itself is still based on 10G, yet this will be upgraded in the next phase with two symmetric 100G uplinks. In the last phase, the router interface at KIT will receive a 100G upgrade at the required locations.\r\nThe requirements of the different phases as well as the planned topology will be presented. Some of the obstacles we discovered during the deployment will be discussed and solutions or workarounds presented.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578462", "resources": [{"_type": "LocalFile", "name": "100G_Deployment-(DE-KIT).v16.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/199\/attachments\/578462\/796588\/100G_Deployment-DE-KIT.v16.pdf", "fileName": "100G_Deployment-DE-KIT.v16.pdf", "_fossil": "localFileMetadata", "id": "796588", "_deprecated": true}, {"_type": "LocalFile", "name": "100G_Deployment-(DE-KIT).v16.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/199\/attachments\/578462\/796589\/100G_Deployment-DE-KIT.v16.pptx", "fileName": "100G_Deployment-DE-KIT.v16.pptx", "_fossil": "localFileMetadata", "id": "796589", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "aaedaf0b6923da9472cbc33726464d1d", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "PETZOLD, Andreas", "id": "1"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/199", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "198", "speakers": [{"_type": "ContributionParticipation", "emailHash": "4c29a1c4588ad8d5213b37e9f13c9b41", "affiliation": "U. Virginia", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. MINA, Robert", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "4c29a1c4588ad8d5213b37e9f13c9b41", "affiliation": "U. Virginia", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. MINA, Robert", "id": "0"}], "title": "Implementation of an Upward-Going Muon Trigger for Indirect Dark Matter Searches with the NOvA Far Detector", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T23:26:19.061097+00:00", "description": "", "title": "UpMu_portrait_large.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/198\/attachments\/578463\/796590\/UpMu_portrait_large.pdf", "filename": "UpMu_portrait_large.pdf", "content_type": "application\/pdf", "type": "file", "id": 796590, "size": 9204400}], "title": "Slides", "default_folder": false, "id": 578463, "description": ""}], "_type": "Contribution", "description": "The NOvA collaboration has constructed a 14,000 ton, fine-grained, low-Z, total absorption tracking calorimeter at an off-axis angle to an upgraded NuMI neutrino beam.  This detector, with its excellent granularity and energy resolution, and relatively low-energy neutrino thresholds was designed to observe electron neutrino appearance in a muon neutrino beam but it also has unique capabilities suitable for more exotic efforts.     In fact, if an efficient upward-going muon trigger with sufficient cosmic ray background rejection can be demonstrated, NOvA will be capable of a competitive indirect dark matter search for low-mass WIMPs.  The cosmic ray muon rate at the NOvA far detector is about 100 kHz and provides the primary challenge for triggering and optimizing such a search analysis The status of the NOvA upward-going muon trigger and initial sensitivity estimates for indirect detection of WIMP dark matter will be presented.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578463", "resources": [{"_type": "LocalFile", "name": "UpMu_portrait_large.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/198\/attachments\/578463\/796590\/UpMu_portrait_large.pdf", "fileName": "UpMu_portrait_large.pdf", "_fossil": "localFileMetadata", "id": "796590", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "6ba1f95da55f3a0622e4785d12b5cc25", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "NORMAN, Andrew", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c0c44979895c4d91d6ef23fc49f5e258", "affiliation": "U", "_fossil": "contributionParticipationMetadata", "fullName": "FRANK, Martin", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "4c245be14588e71633a148c37e7cc7c7", "affiliation": "UVa", "_fossil": "contributionParticipationMetadata", "fullName": "OKSUZIAN, Iuri", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "3635e1dac08620eb1e22089dba1af3db", "affiliation": "U. Virginia", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. FRIES, Eric", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "336e636659f12d5fe7aede25fd33f666", "affiliation": "University of Virginia", "_fossil": "contributionParticipationMetadata", "fullName": "GROUP, Robert", "id": "5"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/198", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": null, "keywords": [], "id": "195", "speakers": [{"_type": "ContributionParticipation", "emailHash": "124fa52dc48726b258150d77190762fd", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KOVALSKYI, Dmytro", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "124fa52dc48726b258150d77190762fd", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KOVALSKYI, Dmytro", "id": "0"}], "title": "Realtime optimization of computing grid resources utilization for individual users and groups in CMS", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [], "_type": "Contribution", "description": "For Run 2 of the LHC, CMS physics output is expected to be increasingly computing resource limited. In addition, to exploring opportunistic, cloud, and HPC resources we are thus increasingly focusing on reducing inefficiencies in our use of pledged grid resources within WLCG.\r\n\r\nThe dominant source of inefficient use of pledged wall time within the WLCG is due to failed physics user jobs. Lowering this failure rate has proven to be a very difficult problem during Run 1 due to the diversity of user tasks leading to a changing pattern of a wide range of failure modes. In this talk we report first results on using data mining and predictive analytics to adjust scheduling decisions in CMS GlideinWMS in realtime in order to maximize physics output by minimizing wall-time wasted due to user level errors.\r\n\r\nWe use historic data from Run 1 operations to quantify the impact of this approach in principle, and show initial results of operating the algorithms developed in practice. Possible future refinements are also discussed.", "track": "Track5: Computing activities and Computing models", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "37a9d67c5498998436724740f824542b", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LETTS, James", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "404ffd25f1af8b96d12533b8dcfc88dd", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "WUERTHWEIN, Frank", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/contribution\/195", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "194", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e86323288b18387d48dc1af9718c14d6", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WILDISH, Tony", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e86323288b18387d48dc1af9718c14d6", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WILDISH, Tony", "id": "0"}], "title": "Understanding the CMS Tier-2 network traffic during Run-1", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T22:12:40.281283+00:00", "description": "", "title": "T2_Traffic_-_poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/194\/attachments\/578464\/796591\/T2_Traffic_-_poster.pdf", "filename": "T2_Traffic_-_poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796591, "size": 1872675}], "title": "Poster", "default_folder": false, "id": 578464, "description": ""}], "_type": "Contribution", "description": "At the beginning of Run-1 CMS was operating it's facilities according to the MONARC model, where data-transfers were strictly hierarchical in nature. Direct transfers between Tier-2 nodes was excluded, being perceived as operationally intensive and risky in an era where the network was expected to be a major source of errors. By the end of Run-1 wide-area networks were more capable and stable than originally anticipated. The original data-placement model was largely superceded, and traffic was allowed between Tier-2 nodes.\r\n\r\nTier-2 to Tier-2 traffic in 2012 already exceeded the amount of Tier-2 to Tier-1 traffic, so it clearly has the potential to become important in the future. Moreover, while Tier-2 to Tier-1 traffic is mostly upload of monte-carlo data, the Tier-2 to Tier-2 traffic represents data moved in direct response to requests from the physics analysis community. As such, problems or delays there are more likely to have a direct impact on the user community.\r\n\r\nTier-2 to Tier-2 traffic may also traverse parts of the WAN that are at the 'edge' of our network, with limited network capacity or reliability compared to, say, the Tier-0 to Tier-1 traffic which goes the over LHCOPN network.\r\n\r\nCMS is looking to exploit technologies that allow us to interact with the network fabric so that it can manage our traffic better for us, this we hope to achieve before the end of Run-2. Tier-2 to Tier-2 traffic would be the most interesting use-case for such traffic management, precisely because it is close to the users' analysis and far from the 'core' network infrastructure.\r\n\r\nAs such, a better understanding of our Tier-2 to Tier-2 traffic is important. Knowing the characteristics of our data-flows can help us place our data more intelligently. Knowing how widely the data moves can help us anticipate the requirements for network capacity, and inform the dynamic data placement algorithms we expect to have in place for Run-2.\r\n\r\nThis paper presents our analysis of the Tier-2 traffic during Run 1. We examine the geographical and temporal distribution of traffic, the transfer quality, and the correlation between data-movement and physics analysis groups. We conclude with recommendations for improving data-placement in Run-2 and suggestions for monitoring improvements to help us better understand the changes in this behaviour as our system evolves.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578464", "resources": [{"_type": "LocalFile", "name": "T2_Traffic_-_poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/194\/attachments\/578464\/796591\/T2_Traffic_-_poster.pdf", "fileName": "T2_Traffic_-_poster.pdf", "_fossil": "localFileMetadata", "id": "796591", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/194", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "197", "speakers": [{"_type": "ContributionParticipation", "emailHash": "503b1db30b53d3800d2ba234dfbd61f7", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Mrs. RATNIKOVA, Natalia", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "503b1db30b53d3800d2ba234dfbd61f7", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Mrs. RATNIKOVA, Natalia", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "e86323288b18387d48dc1af9718c14d6", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WILDISH, Tony", "id": "1"}], "title": "Comprehensive Monitoring for Heterogeneous Geographically Distributed Storage.", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "Storage capacity at CMS Tier-1 and Tier-2 sites reached over 100 Petabytes in 2014, and will be substantially increased during Run 2 data taking. The allocation of storage for the individual users analysis data, which is not accounted as a centrally managed storage space, will be increased to up to 40%. For comprehensive tracking and monitoring of the storage utilization across all participating sites, CMS developed a Space Monitoring system, which provides a central view of the geographically dispersed heterogeneous storage systems. The first prototype has been deployed at the pilot sites in summer 2014, and has been substantially reworked since then. In this presentation we discuss the functionality and our experience of system deployment and operation on the full CMS scale.", "track": "Track3: Data store and access ", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/197", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "196", "speakers": [{"_type": "ContributionParticipation", "emailHash": "124fa52dc48726b258150d77190762fd", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KOVALSKYI, Dmytro", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "124fa52dc48726b258150d77190762fd", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KOVALSKYI, Dmytro", "id": "0"}], "title": "Lightweight user and production job monitoring system for GlideinWMS", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "Glidemon is a lightweight monitoring system for GlideinWMS job submission infrastructure. It allows for basic information aggregation based on ClassAds in HTCondor environment of GlideinWMS. It can easily be adopted for a specific application running on top of GlideinWMS. In CMS it is used for user and production job monitoring managed by CRAB and WMAgent. We will review critical design choices that were made to deliver needed functionality keeping the system lightweight using the traditional relational database management systems.", "track": "Track5: Computing activities and Computing models", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/196", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:15:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "191", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e86323288b18387d48dc1af9718c14d6", "affiliation": "Princeton", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WILDISH, Tony", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e86323288b18387d48dc1af9718c14d6", "affiliation": "Princeton", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WILDISH, Tony", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "7f1dcf9fb960577c3ff458aaf521a15c", "affiliation": "California Institute of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LAPADATESCU, Vlad", "id": "1"}], "title": "Virtual Circuits in PhEDEx, an update from the ANSE project", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T22:09:21.283167+00:00", "description": "", "title": "ANSE-CHEP.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/191\/attachments\/578465\/796592\/ANSE-CHEP.pdf", "filename": "ANSE-CHEP.pdf", "content_type": "application\/pdf", "type": "file", "id": 796592, "size": 947746}, {"_type": "attachment", "modified_dt": "2015-04-12T22:09:21.283167+00:00", "description": "", "title": "ANSE-CHEP.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/191\/attachments\/578465\/796593\/ANSE-CHEP.pptx", "filename": "ANSE-CHEP.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796593, "size": 633950}], "title": "Slides", "default_folder": false, "id": 578465, "description": ""}], "_type": "Contribution", "description": "The ANSE project has been working with the CMS and ATLAS experiments to bring network awareness into their middleware stacks. For CMS, this means enabling control of virtual network circuits in PhEDEx, the CMS data-transfer management system. PhEDEx orchestrates the transfer of data around the CMS experiment to the tune of 1 PB per week spread over about 70 sites.\r\n\r\nThe goal of ANSE is to improve the overall working efficiency of the experiments, by enabling more deterministic time to completion for a designated set of data transfers, through the use of end-to-end dynamic virtual circuits with guaranteed bandwidth.\r\n\r\nANSE has enhanced PhEDEx, allowing it to create, use and destroy circuits according to it's own needs. PhEDEx can now decide if a circuit is worth creating based on its current workload and past transfer history, which allows circuits to be created only when they will be useful.\r\n\r\nThis paper reports on the progress made by ANSE in PhEDEx. We show how PhEDEx is now capable of using virtual circuits as a production-quality service, and describe how the mechanism it uses can be refactored for use in other software domains. We present first results of transfers between CMS sites using this mechanism, and report on the stability and performance of PhEDEx when using virtual circuits.\r\n\r\nThe ability to use dynamic virtual circuits for prioritised large-scale data transfers over shared global network infrastructures represents an important new capability and opens many possibilities. The experience we have gained with ANSE is being incorporated in an evolving picture of future LHC Computing Models, in which the network is considered as an explicit component.\r\n\r\nFinally, we describe the remaining work to be done by ANSE in PhEDEx, and discuss future directions for continued development.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578465", "resources": [{"_type": "LocalFile", "name": "ANSE-CHEP.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/191\/attachments\/578465\/796592\/ANSE-CHEP.pdf", "fileName": "ANSE-CHEP.pdf", "_fossil": "localFileMetadata", "id": "796592", "_deprecated": true}, {"_type": "LocalFile", "name": "ANSE-CHEP.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/191\/attachments\/578465\/796593\/ANSE-CHEP.pptx", "fileName": "ANSE-CHEP.pptx", "_fossil": "localFileMetadata", "id": "796593", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "14de4875b3b66f00314a8f89838c1ffa", "affiliation": "California Institute of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "NEWMAN, Harvey", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "29739c7205cb42e874b61f7aaab0ec7b", "affiliation": "California Institute of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BARCZYK, Artur Jerzy", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "98c30cd056776b299f3f8acc278de1b1", "affiliation": "California Institute of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "VOICU, Ramiro", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "d535bd06f52f9ee070ce340b4f9e93dd", "affiliation": "California Institute of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LEGRAND, Iosif-Charles", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "9c57dc2141cdf17ce309f98331fdbfac", "affiliation": "California Institute of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MUGHAL, Azher", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "d3e9959de1b23ae792c02e52481270cc", "affiliation": "Vanderbilt University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "SHELDON, Paul", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "c4bdbfc9ba5607d9d45276d4da82f758", "affiliation": "Vanderbilt University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MELO, Andrew Malone", "id": "8"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/191", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "190", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a6215b4b9fd5985eb6a95b24a590adcd", "affiliation": "INFN Italy", "_fossil": "contributionParticipationMetadata", "fullName": "AREZZINI, Silvia", "id": "8"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "5a6305fab0daeaacceff5539e9093825", "affiliation": "INFN-Pisa", "_fossil": "contributionParticipationMetadata", "fullName": "MAZZONI, Enrico", "id": "0"}], "title": "Docker experience at INFN-Pisa Grid Data Center", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-03-29T07:04:36.039534+00:00", "description": "", "title": "docker_pisa_chep_2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/190\/attachments\/578466\/796594\/docker_pisa_chep_2015.pdf", "filename": "docker_pisa_chep_2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796594, "size": 1663411}], "title": "Poster", "default_folder": false, "id": 578466, "description": ""}], "_type": "Contribution", "description": "A large scale computing center, when not dedicated to a single\/few users, has to face the problem of meeting ever changing user needs with respect to operating system version, architecture, availability of attached data volumes and logins. While clouds are a typical answer to these types of questions, they introduce resource problems like higher usage of RAM, difficulty to expose bare metal additional hardware, and are in general a few % slower. The Pisa computing center chose to use CHROOT long ago, in order to meet the needs of more than 50 user groups, with distinct needs. The CHROOTed system were started from a local tar image, preloaded on the host systems. We recently started investigating system distribution and setup via Docker, which does not cause any of the resource problems clouds do. We report here on our experience, and on the ease of its deployment on a 8000+ cores computing centers. Docker has also be used to deploy user specific services, like web portals, caching machines and web services. The data volume distribution is obtained via a 2+ PB GPFS system mounted on the host systems, which is attached to the CHROOTed machines whenever needed.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578466", "resources": [{"_type": "LocalFile", "name": "docker_pisa_chep_2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/190\/attachments\/578466\/796594\/docker_pisa_chep_2015.pdf", "fileName": "docker_pisa_chep_2015.pdf", "_fossil": "localFileMetadata", "id": "796594", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "ad27265638303547200b971ea77f7b31", "affiliation": "Sezione di Pisa (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "BOCCALI, Tommaso", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "3d234484309a9c5a5de4305a4d95ec81", "affiliation": "Universita degli Studi di Pisa-INFN, Sezione di Pisa", "_fossil": "contributionParticipationMetadata", "fullName": "CIAMPA, Alberto", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "a6215b4b9fd5985eb6a95b24a590adcd", "affiliation": "Univ. + INFN", "_fossil": "contributionParticipationMetadata", "fullName": "AREZZINI, Silvia", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "098e5bb9dca896cbb0d47fa090292492", "affiliation": "Sezione di Pisa (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "COSCETTI, Simone", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "a2d09fdb7770d9c7616858b213b2434d", "affiliation": "INFN-Pisa", "_fossil": "contributionParticipationMetadata", "fullName": "FABIANI, Dario", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "77adff23b18e3eb3eee03730a3853584", "affiliation": "INFN-Pisa", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CARUSO, Giuseppe", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "0d28138f3fc2216fb1d97810d9bd7e74", "affiliation": "University of Bologna", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. BONACORSI, Daniele", "id": "7"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/190", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "193", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e86323288b18387d48dc1af9718c14d6", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WILDISH, Tony", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e86323288b18387d48dc1af9718c14d6", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WILDISH, Tony", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "a2dbd3d2df26e3fbc0360f6b3594fd21", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CAMPANA, Simone", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c0af0a453fb7de772f32e585bcdcd2ba", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GARONNE, Vincent", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "a0dd1a3a7589f1e2a04cc967f2ae6c39", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DI GIROLAMO, Alessandro", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "ef2db26b320ecf7a9b75abbaef349287", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LASSNIG, Mario", "id": "4"}], "title": "Dimensions of Data Management: A taxonomy of data-transfer solutions in ATLAS and CMS", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T22:10:59.600357+00:00", "description": "", "title": "20150401_CHEP2015_poster_v05.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/193\/attachments\/578467\/796595\/20150401_CHEP2015_poster_v05.pdf", "filename": "20150401_CHEP2015_poster_v05.pdf", "content_type": "application\/pdf", "type": "file", "id": 796595, "size": 2960042}], "title": "Poster", "default_folder": false, "id": 578467, "description": ""}], "_type": "Contribution", "description": "We present an abstract view of data-transfer architectures in use in ATLAS and CMS. We use this to classify data-transfer tools not in terms of their technology, but in terms of their more basic features, such as the properties of the traffic they handle and the use-cases they serve.\r\n\r\nThis classification moves the focus from programming interfaces and technologies back into the original problem-space, which enables a number of things:\r\n\r\n- we can identify how existing tools can be re-factored according to common and technology-independent guidelines, which can facilitate re-use of code, both within and between experiments.\r\n\r\n- we can identify design patterns which work well for certain areas of the architectural phase-space. This can lead to greater robustness through re-use of good design and shallower learning curves for developers moving from project to project.\r\n\r\n- we can map new use-cases into the architectural phase-space independently of existing tools. This lets us see how components from existing tools could be used to solve new problems, which can lead to more rapid deployment of new solutions.\r\n\r\n- we provide a common language for expressing experiment needs to external providers, such as storage element developers, FTS3 developers, or WLCG and OSG.\r\n\r\nIn this paper we show how data-transfer tools from both ATLAS and CMS fit into this architecture. We give examples of where this architecture would have improved our tool development in the past. Finally, we present a few sample use-cases for the future, and discuss them in terms of this architecture.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578467", "resources": [{"_type": "LocalFile", "name": "20150401_CHEP2015_poster_v05.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/193\/attachments\/578467\/796595\/20150401_CHEP2015_poster_v05.pdf", "fileName": "20150401_CHEP2015_poster_v05.pdf", "_fossil": "localFileMetadata", "id": "796595", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/193", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:30:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:45:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "192", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e86323288b18387d48dc1af9718c14d6", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WILDISH, Tony", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e86323288b18387d48dc1af9718c14d6", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WILDISH, Tony", "id": "0"}], "title": "Bandwidth-sharing in LHCONE, an analysis of the problem", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T22:10:29.981406+00:00", "description": "", "title": "PSP-2.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/192\/attachments\/578468\/796596\/PSP-2.pdf", "filename": "PSP-2.pdf", "content_type": "application\/pdf", "type": "file", "id": 796596, "size": 736678}, {"_type": "attachment", "modified_dt": "2015-04-12T22:10:29.981406+00:00", "description": "", "title": "PSP-2.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/192\/attachments\/578468\/796597\/PSP-2.pptx", "filename": "PSP-2.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796597, "size": 992555}], "title": "Slides", "default_folder": false, "id": 578468, "description": ""}], "_type": "Contribution", "description": "The LHC experiments have traditionally regarded the network as an unreliable resource, one which was expected to be a major source of errors and inefficiency at the time their original computing models were derived. Now, however, the network is seen as much more capable and reliable. Data are routinely transferred with high efficiency and low latency to wherever computing or storage resources are available to use or manage them.\r\n\r\nAlthough there was sufficient network bandwidth for the experiments' needs during Run-1, they cannot rely on ever-increasing bandwidth as a solution to their data-transfer needs in the future. Sooner or later they need to consider the network as a finite resource that they interact with to manage their traffic, in much the same way as they manage their use of disk and CPU resources.\r\n\r\nThere are several possible ways for the experiments to integrate management of the network in their software stacks, such as the use of virtual circuits with hard bandwidth guarantees or soft real-time flow-control, with somewhat less firm guarantees. Abstractly, these can all be considered as the users (the experiments, or groups of users within the experiment) expressing a request for a given bandwidth between two points for a given duration of time. The network fabric then grants some allocation to each user, dependent on the sum of all requests and the sum of available resources, and attempts to ensure the requirements are met (either deterministically or statistically).\r\n\r\nAn unresolved question at this time is how to convert the users' requests into an allocation. Simply put, how do we decide what fraction of a network's bandwidth to allocate to each user when the sum of requests exceeds the available bandwidth? The usual problems of any resource-scheduling system arise here, namely how to ensure the resource is used efficiently and fairly, while still satisfying the needs of the users.\r\n\r\nSimply fixing quotas on network paths for each user is likely to lead to inefficient use of the network. If one user cannot use their quota for some reason, that bandwidth is lost. Likewise, there is no incentive for the user to be efficient within their quota, they have nothing to gain by using less than their allocation.\r\n\r\nAs with CPU farms, some sort of dynamic allocation is more likely to be useful. A promising approach for sharing bandwidth at LHCONE is the 'Progressive Second-Price auction', where users are given a budget and are required to bid from that budget for the specific resources they want to reserve. The auction allows users to effectively determine among themselves the degree of sharing they are willing to accept based on the priorities of their traffic and their global share, as represented by their total budget. The network then implements those allocations using whatever mix of technologies is appropriate or available.\r\n\r\nThis paper describes how the Progressive Second-Price auction works and presents a prototype simulation of the auction. Results from the simulation are shown, addressing practical questions such as how are budgets set, what strategy should users use to manage their budget, how and how often should the auction be run, and how do we ensure that the goals of fairness and efficiency are met.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578468", "resources": [{"_type": "LocalFile", "name": "PSP-2.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/192\/attachments\/578468\/796596\/PSP-2.pdf", "fileName": "PSP-2.pdf", "_fossil": "localFileMetadata", "id": "796596", "_deprecated": true}, {"_type": "LocalFile", "name": "PSP-2.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/192\/attachments\/578468\/796597\/PSP-2.pptx", "fileName": "PSP-2.pptx", "_fossil": "localFileMetadata", "id": "796597", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/192", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:15:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "270", "speakers": [{"_type": "ContributionParticipation", "emailHash": "3f27100a34dbefbeb2642ed0477f23f8", "affiliation": "Brunswick Technical University (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "NIEKE, Christian", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "cee8f9bdf304d0dba45ae82a7cb9ae5e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DUELLMANN, Dirk", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "3f27100a34dbefbeb2642ed0477f23f8", "affiliation": "Brunswick Technical University (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "NIEKE, Christian", "id": "1"}], "title": "Analysis of CERN Computing Infrastructure and Monitoring Data", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T12:47:13.185707+00:00", "description": "", "title": "Nieke_CHEP15.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/270\/attachments\/578469\/796598\/Nieke_CHEP15.pdf", "filename": "Nieke_CHEP15.pdf", "content_type": "application\/pdf", "type": "file", "id": 796598, "size": 791108}, {"_type": "attachment", "modified_dt": "2015-04-10T12:47:13.185707+00:00", "description": "", "title": "Nieke_CHEP15.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/270\/attachments\/578469\/796599\/Nieke_CHEP15.pptx", "filename": "Nieke_CHEP15.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796599, "size": 1769385}], "title": "Slides", "default_folder": false, "id": 578469, "description": ""}], "_type": "Contribution", "description": "Optimising a computing infrastructure on the scale of LHC requires a quantitative understanding of a complex network of many different resources and services. For this purpose the CERN IT department and the LHC experiments are collecting a large multitude of logs and performance probes, which are already successfully used for short-term analysis (e.g. operational dashboards) within each group.\u00a0\r\nThe IT analytics working group has been created with the goal to bring data sources from different services and on different abstraction levels together and to implement a suitable infrastructure for mid- to long-term statistical analysis. It further provides a forum for joint optimization across single service boundaries and the exchange of analysis methods and tools.\u00a0\r\nTo simplify access to the collected data, we implemented an automated repository for cleaned and aggregated data sources based on the Hadoop ecosystem. This contribution describes some of the challenges encountered, such as dealing with heterogeneous data formats, selecting an efficient storage format for map reduce and external access, and will describe the repository user interface.\u00a0\r\nUsing this infrastructure we were able to quantitatively analyse the relationship between CPU\/wall fraction, latency\/throughput constraints of network and disk and the effective job throughput.\u00a0\r\nIn this contribution we will first describe the design of the shared analysis infrastructure and then present a summary of first analysis results from the combined data sources.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578469", "resources": [{"_type": "LocalFile", "name": "Nieke_CHEP15.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/270\/attachments\/578469\/796598\/Nieke_CHEP15.pdf", "fileName": "Nieke_CHEP15.pdf", "_fossil": "localFileMetadata", "id": "796598", "_deprecated": true}, {"_type": "LocalFile", "name": "Nieke_CHEP15.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/270\/attachments\/578469\/796599\/Nieke_CHEP15.pptx", "fileName": "Nieke_CHEP15.pptx", "_fossil": "localFileMetadata", "id": "796599", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/270", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:15:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "271", "speakers": [{"_type": "ContributionParticipation", "emailHash": "9e7dd8f84b1524d686815744ec524fea", "affiliation": "University of Manchester (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "MCNAB, Andrew", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "9e7dd8f84b1524d686815744ec524fea", "affiliation": "University of Manchester (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "MCNAB, Andrew", "id": "0"}], "title": "Managing virtual machines with Vac and Vcycle", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T08:52:47.022565+00:00", "description": "", "title": "mcnab-vac-vcycle-13apr15.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/271\/attachments\/578470\/796600\/mcnab-vac-vcycle-13apr15.pdf", "filename": "mcnab-vac-vcycle-13apr15.pdf", "content_type": "application\/pdf", "type": "file", "id": 796600, "size": 764623}], "title": "Slides", "default_folder": false, "id": 578470, "description": ""}], "_type": "Contribution", "description": "We compare the Vac and Vcycle virtual machine lifecycle managers and our experiences in providing production job execution services for ATLAS, LHCb, and the GridPP VO at sites in the UK and at CERN. In both the Vac and Vcycle systems, the virtual machines are created outside of the experiment's job submission and pilot framework. In the case of Vac, a daemon runs on each physical host which manages a pool of virtual machines on that host, and a peer-to-peer UDP protocol is used to achieve the desired target shares between experiments across the site. In the case of Vcycle, a daemon manages a pool of virtual machines on an Infrastructure As A Service cloud system such as OpenStack, and has within itself enough information to create the types of virtual machines to achieve the desired target shares. Both systems allow unused shares for one experiment to temporarily taken up by other experiements with work to be done. The virtual machine lifecycle is managed with a minimum of information, gathered from the virtual machine creation mechanism (such as libirt or OpenStack) and using the proposed Machine\/Job Features API from WLCG. We demonstrate that the same virtualmachine designs can be used to run production jobs on Vac and Vcycle\/OpenStack sites for ATLAS, LHCb, and GridPP, and that these technologies allow sites to be operated in a reliable and robust way.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578470", "resources": [{"_type": "LocalFile", "name": "mcnab-vac-vcycle-13apr15.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/271\/attachments\/578470\/796600\/mcnab-vac-vcycle-13apr15.pdf", "fileName": "mcnab-vac-vcycle-13apr15.pdf", "_fossil": "localFileMetadata", "id": "796600", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "f10a491a558480b67d78d8826c40ea5e", "affiliation": "Lancaster University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LOVE, Peter", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "0912ed3f7fc8599ad4d34add50eb5564", "affiliation": "Particle Physics-University of Oxford-Unknown", "_fossil": "contributionParticipationMetadata", "fullName": "MAC MAHON, Ewan Christopher", "id": "2"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/271", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "272", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a62fab4312c26d8e93729920981115f8", "affiliation": "Max-Planck-Institut fuer Physik (Werner-Heisenberg-Institut) (D", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MAZZAFERRO, Luca", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "a62fab4312c26d8e93729920981115f8", "affiliation": "Max-Planck-Institut fuer Physik (Werner-Heisenberg-Institut) (D", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MAZZAFERRO, Luca", "id": "0"}], "title": "Integrating Puppet and Gitolite to provide a novel solution for scalable system management at the MPPMU Tier2 centre", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-08T08:43:28.007181+00:00", "description": "", "title": "PosterPuppet.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/272\/attachments\/578471\/796601\/PosterPuppet.pdf", "filename": "PosterPuppet.pdf", "content_type": "application\/pdf", "type": "file", "id": 796601, "size": 2956572}], "title": "Slides", "default_folder": false, "id": 578471, "description": ""}], "_type": "Contribution", "description": "In a grid computing infrastructure tasks such as continuous upgrades, services installations and software deployments are part of an admins daily work. In such an environment tools to help with the management, provisioning and monitoring of the deployed systems and services have become crucial.\r\n\r\nAs experiments such as the LHC increase in scale, the computing infrastructure also becomes larger and more complex. Moreover, today's admins increasingly work within teams that share responsibilities and tasks. Such a scaled up situation requires tools that not only simplify the workload on administrators but also enable them to work seamlessly in teams.\r\n\r\nIn this paper will be presented our experience from managing the Max Planck\r\nInstitute Tier2 using Puppet and Gitolite in a cooperative way to help the system\r\nadministrator in their daily work.\r\n\r\nIn addition to describing the Puppet-Gitolite system, best practices and customizations will also be shown.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578471", "resources": [{"_type": "LocalFile", "name": "PosterPuppet.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/272\/attachments\/578471\/796601\/PosterPuppet.pdf", "fileName": "PosterPuppet.pdf", "_fossil": "localFileMetadata", "id": "796601", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "85f863191188abed761d707cfc440a8a", "affiliation": "Max-Planck-Institut fuer Physik (Werner-Heisenberg-Institut) (D", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KLUTH, Stefan", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "9ed13e60b06efa4b6e282b75235ae690", "affiliation": "Rechenzentrum Garching (RZG) - Max Planck Society", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KENNEDY, John", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "bc899b9d9abc452cc0ca541f8c264b55", "affiliation": "Max-Planck-Institut fuer Physik (Werner-Heisenberg-Institut) (D", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. DELLE FRATTE, Cesare", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/272", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:45:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "273", "speakers": [{"_type": "ContributionParticipation", "emailHash": "c3a7219cddfc2426bb03a5df6bbbb909", "affiliation": "CC-IN2P3 - Centre de Calcul  (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "VERNET, Renaud", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c3a7219cddfc2426bb03a5df6bbbb909", "affiliation": "CC-IN2P3 - Centre de Calcul  (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "VERNET, Renaud", "id": "0"}], "title": "A Model for Forecasting Data Centre Infrastructure Costs", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T06:46:29.200680+00:00", "description": "", "title": "CHEP_VERNET_COSTMODEL_2.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/273\/attachments\/578472\/796602\/CHEP_VERNET_COSTMODEL_2.pdf", "filename": "CHEP_VERNET_COSTMODEL_2.pdf", "content_type": "application\/pdf", "type": "file", "id": 796602, "size": 1218587}], "title": "Slides", "default_folder": false, "id": 578472, "description": ""}], "_type": "Contribution", "description": "The computing needs in the HEP community are increasing steadily, but the current funding situation in many countries is tight. As a consequence experiments, data centres, and funding agencies have to rationalize resource usage and expenditures.\r\n\r\nCC-IN2P3 (Lyon, France) provides computing resources to many experiments including LHC, and is a major partner for astroparticle projects like LSST, CTA or Euclid.  The financial cost to accommodate all these experiments is substantial and has to be planned well in advance for funding and strategic reasons.\r\n\r\nIn that perspective, leveraging infrastructure expenses, electric power cost and hardware performance observed in our site over the last years, we have built a model that integrates these data and provides estimates of the investments that would be required to cater to the experiments for the mid-term future.\r\n\r\nWe present how our model is built and the expenditure forecast it produces, taking into account the experiment roadmaps. We also examine the resource growth predicted by our model over the next years assuming a flat-budget scenario.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578472", "resources": [{"_type": "LocalFile", "name": "CHEP_VERNET_COSTMODEL_2.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/273\/attachments\/578472\/796602\/CHEP_VERNET_COSTMODEL_2.pdf", "fileName": "CHEP_VERNET_COSTMODEL_2.pdf", "_fossil": "localFileMetadata", "id": "796602", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/273", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "275", "speakers": [{"_type": "ContributionParticipation", "emailHash": "7ba107605d3e1ba095adbdc4d750d6e3", "affiliation": "University College London", "_fossil": "contributionParticipationMetadata", "fullName": "MARTYNIUK, Alex Christopher", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "7ba107605d3e1ba095adbdc4d750d6e3", "affiliation": "University College London", "_fossil": "contributionParticipationMetadata", "fullName": "MARTYNIUK, Alex Christopher", "id": "1"}], "title": "The Database Driven ATLAS Trigger Configuration System", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T02:44:13.285200+00:00", "description": "", "title": "TriggerToolPoster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/275\/attachments\/578473\/796603\/TriggerToolPoster.pdf", "filename": "TriggerToolPoster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796603, "size": 11982326}], "title": "Slides", "default_folder": false, "id": 578473, "description": ""}], "_type": "Contribution", "description": "This contribution describes the trigger selection configuration system\r\nof the ATLAS low- and high-level trigger (HLT) and the upgrades it\r\nreceived in preparation for LHC Run 2.\r\n\r\nThe ATLAS trigger configuration system is responsible for applying the\r\nphysics selection parameters for the online data taking at both\r\ntrigger levels and the proper connection of the trigger lines across\r\nthose levels. Here the low-level trigger consists of the already\r\nexisting central trigger (CT) and the new Level-1 Topological trigger\r\n(L1Topo), which has been added for Run 2. In detail the tasks of the\r\nconfiguration system during the online data taking are\r\n\r\n* Application of the selection criteria, e.g. energy cuts, minimum\r\n  multiplicities, trigger object correlation, at the three trigger\r\n  components L1Topo, CT, and HLT\r\n\r\n* On-the-fly, e.g. rate-dependent, generation and application of\r\n  prescale factors to the CT and HLT to adjust the trigger rates to\r\n  the data taking conditions, such as falling luminosity or rate\r\n  spikes in the detector readout\r\n\r\n* Recording of the complete trigger configuration for any given point\r\n  in time, for later use by data analysts\r\n\r\nThe core of the trigger configuration system is an oracle database\r\n(TriggerDB), with a dedicated schema to reflect the L1Topo, CT, and\r\nHLT configuration needs. A java-based UI serves as the front-end to\r\nthe TriggerDB for the trigger experts to store new and modify existing\r\ntrigger configurations. C++-based database reader software exists for\r\nthe trigger clients to retrieve configurations from the database. Web\r\ninterfaces exist to display the information to a large group of ATLAS\r\nmembers.\r\n\r\nWith the vast amount of upgrades of the CT and HLT during the last two\r\nyears, and the addition of the L1Topo, substantial changes to the\r\ndatabase and software were necessary, which will be presented.\r\nTechnical problems, such as the low-latency distribution of the\r\nconfiguration across the HLT computing farm and the synchronous\r\napplication to the data will be addressed.  Also the propagation of\r\nthe trigger configuration database from the data-taking side to\r\nvarious ATLAS data reconstruction sites will be discussed, including a\r\nshort description how the same trigger configuration mechanism is\r\nbeing used for ATLAS Monte Carlo simulation. New features such an\r\nautomated luminosity tracking and prescale application system, which\r\noptimizes the ATLAS data taking efficiency, will also be shown.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578473", "resources": [{"_type": "LocalFile", "name": "TriggerToolPoster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/275\/attachments\/578473\/796603\/TriggerToolPoster.pdf", "fileName": "TriggerToolPoster.pdf", "_fossil": "localFileMetadata", "id": "796603", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "272c7ef50985d3ca7d85f3885f4e1e77", "affiliation": "Imperial College London", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. PANDURO VAZQUEZ, William", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "b599f4a9469b5ebe0ca1932a087a8722", "affiliation": "Royal Holloway, University of London", "_fossil": "contributionParticipationMetadata", "fullName": "FAUCCI GIANNELLI, Michele", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "becc84dd383bace2c39506db896f73b8", "affiliation": "McGill University (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "STOCKTON, Mark", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "41c23d0149638de858a148e60a426472", "affiliation": "University of Liverpool", "_fossil": "contributionParticipationMetadata", "fullName": "CHAVEZ, Carlos", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "1665102814567dd7e1b4fd132d6baf2c", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "STELZER, Joerg", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "c0a239ad7ae3e3a3cffd42b6bbcf1bee", "affiliation": "University of Wisconsin (US)", "_fossil": "contributionParticipationMetadata", "fullName": "WIEDENMANN, Werner", "id": "7"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/275", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:45:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "276", "speakers": [{"_type": "ContributionParticipation", "emailHash": "65149bdeea9f5c348fe88b7464fc0b79", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ROVERE, Marco", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "933f02c56e90be780752ddb3275e45d3", "affiliation": "INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "SGUAZZONI, Giacomo", "id": "0"}], "title": "CMS reconstruction improvements for the tracking in large pile-up events", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T03:48:36.607762+00:00", "description": "", "title": "CHEP2015_MR_Tracking.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/276\/attachments\/578474\/796604\/CHEP2015_MR_Tracking.pdf", "filename": "CHEP2015_MR_Tracking.pdf", "content_type": "application\/pdf", "type": "file", "id": 796604, "size": 2290812}], "title": "Slides", "default_folder": false, "id": 578474, "description": ""}], "_type": "Contribution", "description": "The CMS tracking code is organized in several levels, known as 'iterative steps', each optimized to reconstruct a class of particle trajectories, as the ones of particles originating from the primary vertex or displaced tracks from particles resulting from secondary vertices. Each iterative step consists of seeding, pattern recognition and fitting by a kalman filter, and a final filtering and cleaning. Each subsequent step works on hits not yet associated to a reconstructed particle trajectory.\r\n\r\nThe CMS tracking code is continuously evolving to make the reconstruction computing load compatible with the increasing instantaneous luminosity of LHC, resulting in a large number of primary vertices and tracks per bunch crossing.\r\n\r\nThe major upgrade put in place during the present LHC Long Shutdown will allow the tracking code to comply with the conditions expected during Run2 and the much larger pile-up. In particular, new algorithms that are intrinsically more robust in high occupancy conditions have been developed, iterations have been re-designed (including a new one, dedicated to specific physics objects), code optimizations have been deployed and new software techniques have been used. The speed improvement has been achieved without significant reduction in term of physics performance.\r\n\r\nThe methods and the results are presented and the prospects for future applications are discussed.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578474", "resources": [{"_type": "LocalFile", "name": "CHEP2015_MR_Tracking.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/276\/attachments\/578474\/796604\/CHEP2015_MR_Tracking.pdf", "fileName": "CHEP2015_MR_Tracking.pdf", "_fossil": "localFileMetadata", "id": "796604", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "65149bdeea9f5c348fe88b7464fc0b79", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ROVERE, Marco", "id": "1"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/276", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:15:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "277", "speakers": [{"_type": "ContributionParticipation", "emailHash": "959f53b1cd9e1a3573140173e1578241", "affiliation": "urn:Google", "_fossil": "contributionParticipationMetadata", "fullName": "ROCCO, Dominick", "id": "7"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "a521f1cb3659857d474a9a138c1e03f3", "affiliation": "Indiana University", "_fossil": "contributionParticipationMetadata", "fullName": "NINER, Evan", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "8da7c1009e44e760516f3af5a15891a7", "affiliation": "Indiana University", "_fossil": "contributionParticipationMetadata", "fullName": "BAIRD, Michael", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "ac15508921e4559339d39bb85b3af6d0", "affiliation": "University of Minnesota", "_fossil": "contributionParticipationMetadata", "fullName": "SACHDEV, Kanika", "id": "2"}], "title": "Event Reconstruction Techniques in NOvA", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T10:03:17.059645+00:00", "description": "", "title": "chep_reconstruction_2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/277\/attachments\/578475\/796605\/chep_reconstruction_2015.pdf", "filename": "chep_reconstruction_2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796605, "size": 3553172}], "title": "Slides", "default_folder": false, "id": 578475, "description": ""}], "_type": "Contribution", "description": "The NOvA experiment is a long baseline neutrino oscillation experiment utilizing the NuMI beam generated at Fermilab.  The experiment will measure the oscillations within a muon neutrino beam in a 300 ton Near Detector located underground at Fermilab and a functionally-identical 14 kiloton Far Detector placed 810 km away.  The detectors are liquid scintillator tracking calorimeters with a fine-grained cellular structure that provides a wealth of information for separating the different particle track and shower topologies.  Each detector has its own challenges with the Near Detector seeing multiple overlapping neutrino interactions in each event and the Far Detector having a large background of cosmic rays due to being located on the surface.  A series of pattern recognition techniques have been developed to go from event records, to spatially and temporally separating individual interactions, to vertexing and tracking, and particle identification.  This combination of methods to achieve the full event reconstruction will be presented.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578475", "resources": [{"_type": "LocalFile", "name": "chep_reconstruction_2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/277\/attachments\/578475\/796605\/chep_reconstruction_2015.pdf", "fileName": "chep_reconstruction_2015.pdf", "_fossil": "localFileMetadata", "id": "796605", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "df6f12602a0e37a3a6aafcc9eaab0c42", "affiliation": "University of Cincinnati", "_fossil": "contributionParticipationMetadata", "fullName": "AURISANO, Adam", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "959f53b1cd9e1a3573140173e1578241", "affiliation": "urn:Google", "_fossil": "contributionParticipationMetadata", "fullName": "ROCCO, Dominick", "id": "7"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/277", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "278", "speakers": [{"_type": "ContributionParticipation", "emailHash": "b4182e99b67ed31547235bab5d2bd367", "affiliation": "Department of Computer Science, University of Helsinki", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. OSMANI, Lirim", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "b4182e99b67ed31547235bab5d2bd367", "affiliation": "Department of Computer Science, University of Helsinki", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. OSMANI, Lirim", "id": "1"}], "title": "The DII-HEP OpenStack based CMS Data Analysis for secure cloud resources", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:37:58.903507+00:00", "description": "", "title": "okinawa.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/278\/attachments\/578476\/796606\/okinawa.pdf", "filename": "okinawa.pdf", "content_type": "application\/pdf", "type": "file", "id": 796606, "size": 3982605}], "title": "Poster", "default_folder": false, "id": 578476, "description": ""}], "_type": "Contribution", "description": "The topic of data storage and analysis on Cloud infrastructures has gained importance in recent years. The High Energy Physics community is interested in performing simulations and data analysis on public or private Cloud facilities. Currently the simulations and analysis are performed mostly on a computing and data Grid. The software and experience of operating on a Grid needs to be adapted for running on Cloud facilities. The approach of harnessing Grid and Cloud technologies ensures a steady and seamless transition towards new ways of working.\r\n\r\nIn order to fulfill this task, a virtual cluster has been constructed that is Cloud-based and Grid-enabled. For the Cloud part we use OpenStack and for the Grid software, that controls the execution of the physics jobs, we use components of the Advanced Resource Connector (ARC). This allows the end-users to submit the jobs with their preferred Grid or Cloud submission system and at the same time provides flexibility to maintain the infrastructure. The analysis software and libraries are installed via the CERN Virtual Machine file system. The cluster is monitored by running Site Availability Monitoring (SAM) jobs as well by using Graphite.\r\n\r\nOur solution uses a hybrid approach of combining elements of Cloud and Grid software components. To manage the virtual machines (VMs) dynamically in an elastic fashion, we are using the EMI authorization service (Argus) and the Execution  Environment Service (Argus-EES) with an OpenStack plugin that has been developed for Argus-EES. Our newly developed plugin for the Argus-EES can communicate with multiple OpenStack deployments to expand and shrink resources dynamically upon demand.\r\n\r\nThe Host Identity Protocol (HIP) has been designed for mobile networks and it provides a secure method for IP mobility and multi homing. HIP separates the end-point identifier and locator role for IP address which improves network agility of applications and the underlying virtual machines. Our solution leverages HIP for traffic management. This is useful for secure connections of hybrid Cloud resources.\r\n\r\nWe describe the state and current experience with a virtualized computing environment for CMS data analysis in the Datacenter Indirection Infrastructure for Secure High Energy Physics (DII-HEP) project.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578476", "resources": [{"_type": "LocalFile", "name": "okinawa.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/278\/attachments\/578476\/796606\/okinawa.pdf", "fileName": "okinawa.pdf", "_fossil": "localFileMetadata", "id": "796606", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "1a0330ec3e6a90529d72975c6cc98706", "affiliation": "Helsinki Institute of Physics (FI)", "_fossil": "contributionParticipationMetadata", "fullName": "LINDEN, Tomas", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "e9aa00736577a44a0f0b67bbd4378950", "affiliation": "Department of Computer Science, University of Helsinki", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. TARKOMA, Sasu", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "ff6fe0c6a328fda971944f080c755f5d", "affiliation": "University of Helsinki (FI)", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. EEROLA, Paula", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "d9db034eda7a094c5c1b5e4d4995bb66", "affiliation": "Ericsson Research", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KOMU, Miika", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "67aae68dea840259fcbb2893c48b0bff", "affiliation": "Helsinki Institute of Physics (FI)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. TOOR, Salman", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "1e829309a0797e340d9fbb63058ebb8d", "affiliation": "Helsinki Institute of Physics (FI)", "_fossil": "contributionParticipationMetadata", "fullName": "WHITE, John White", "id": "6"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/278", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:00:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "279", "speakers": [{"_type": "ContributionParticipation", "emailHash": "50a1799bdcddc4a0accf8871044dc30f", "affiliation": "KIT - Karlsruhe Institute of Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. HAUTH, Thomas", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "50a1799bdcddc4a0accf8871044dc30f", "affiliation": "KIT - Karlsruhe Institute of Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. HAUTH, Thomas", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "f17d164b042b783356167290b00244ef", "affiliation": "KIT - Karlsruhe Institute of Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. POLGART, Frank", "id": "1"}], "title": "Dynamic provisioning of local and remote compute resources with OpenStack", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T06:17:16.186356+00:00", "description": "", "title": "Hauth_CHEP2015_OpenStack.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/279\/attachments\/578477\/796607\/Hauth_CHEP2015_OpenStack.pdf", "filename": "Hauth_CHEP2015_OpenStack.pdf", "content_type": "application\/pdf", "type": "file", "id": 796607, "size": 2474783}], "title": "Slides", "default_folder": false, "id": 578477, "description": ""}], "_type": "Contribution", "description": "Modern high-energy physics experiments rely on the extensive usage of computing resources,\r\nboth for the reconstruction of measured events as well as for Monte Carlo simulation. The \r\nInstitut f\u00fcr Experimentelle Kernphysik (EKP) at KIT is participating in both the CMS and Belle\r\nexperiments with computing and storage resources. In the upcoming years, these requirements \r\nare expected to increase due to growing amount of recorded data and the rise in complexity\r\nof the simulated events. It is therefore essential to increase the available computing capabilities\r\nby tapping into all resource pools.\r\n\r\nAt the EKP institute, powerful desktop machines are available to users. Due to the multi-core\r\nnature of modern CPUs, vast amounts of CPU time are not utilized by common desktop usage patterns.\r\nOther important providers of compute capabilities are classical HPC data centers at Universities or\r\nnational research centers. Due to the shared nature of these installations, the standardized \r\nsoftware stack required by HEP applications cannot be installed.\r\n\r\nA viable way to overcome this constraint and offer a standardized software environment in a transparent manner\r\nis the usage of virtualization technologies. The OpenStack project has become a widely adopted solution \r\nto virtualize hardware and offer additional services like storage and virtual machine management.\r\n\r\nThis contribution will report on the incorporation of the institute's desktop machines into a private\r\nOpenStack cloud. The additional compute resources provisioned via the virtual machines have been used for \r\nMonte Carlo simulation and data analysis. Furthermore, a concept to integrate shared, remote HPC centers \r\ninto regular HEP job workflows will be presented. In this approach, local and remote resources \r\nare be merged to form a unfiorm, virtual compute cluster with a single point-of-entry for the user. Evaluations \r\nof the performance and stability of this setup and operational experiences will be discussed.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578477", "resources": [{"_type": "LocalFile", "name": "Hauth_CHEP2015_OpenStack.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/279\/attachments\/578477\/796607\/Hauth_CHEP2015_OpenStack.pdf", "fileName": "Hauth_CHEP2015_OpenStack.pdf", "_fossil": "localFileMetadata", "id": "796607", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "7aba38cfa2a99bc621b6076d46ea0ab9", "affiliation": "KIT - Karlsruhe Institute of Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. QUAST, G\u00fcnter", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "4535f96fcde7dd5d7c121710acfa24cd", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GIFFELS, Manuel", "id": "3"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/279", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:15:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "524", "speakers": [{"_type": "ContributionParticipation", "emailHash": "327917ea7e888458a1e8c94344912b8d", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GASPAR APARICIO, Ruben Domingo", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "327917ea7e888458a1e8c94344912b8d", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GASPAR APARICIO, Ruben Domingo", "id": "0"}], "title": "Experience in running relational databases on clustered storage", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T13:54:05.017568+00:00", "description": "", "title": "RDBMS_storage_rgaspar.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/524\/attachments\/578478\/796608\/RDBMS_storage_rgaspar.pdf", "filename": "RDBMS_storage_rgaspar.pdf", "content_type": "application\/pdf", "type": "file", "id": 796608, "size": 1433579}, {"_type": "attachment", "modified_dt": "2015-04-12T13:54:05.017568+00:00", "description": "", "title": "RDBMS_storage_rgaspar.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/524\/attachments\/578478\/796609\/RDBMS_storage_rgaspar.pptx", "filename": "RDBMS_storage_rgaspar.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796609, "size": 1175755}], "title": "Slides", "default_folder": false, "id": 578478, "description": ""}], "_type": "Contribution", "description": "CERN IT-DB group is migrating its storage platform, mainly NetApp NAS\u2019s running on 7-mode but also SAN arrays, to a set of NetApp C-mode clusters. The largest one is made of 14 controllers and it will hold a range of critical databases from administration to accelerators control or experiment control databases. This talk shows our setup: network, monitoring, use of features like transparent movement of file systems, flash pools (SSD + HDD storage pools), snapshots, etc. It will also show how these features are used on our infrastructure to support backup & recovery solutions with different database solutions: Oracle (11g and 12c multi tenancy), MySQL or PostgreSQL. Performance benchmarks and experience collected while running services on this platform will be also shared. It will be also covered the use of the cluster to provide iSCSI (block device) access for OpenStack windows virtual machines.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578478", "resources": [{"_type": "LocalFile", "name": "RDBMS_storage_rgaspar.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/524\/attachments\/578478\/796608\/RDBMS_storage_rgaspar.pdf", "fileName": "RDBMS_storage_rgaspar.pdf", "_fossil": "localFileMetadata", "id": "796608", "_deprecated": true}, {"_type": "LocalFile", "name": "RDBMS_storage_rgaspar.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/524\/attachments\/578478\/796609\/RDBMS_storage_rgaspar.pptx", "fileName": "RDBMS_storage_rgaspar.pptx", "_fossil": "localFileMetadata", "id": "796609", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "ad4ce0c9703641d367c0fd07e068af97", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "POTOCKY, Miroslav", "id": "1"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/524", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "525", "speakers": [{"_type": "ContributionParticipation", "emailHash": "327917ea7e888458a1e8c94344912b8d", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GASPAR APARICIO, Ruben Domingo", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "327917ea7e888458a1e8c94344912b8d", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GASPAR APARICIO, Ruben Domingo", "id": "0"}], "title": "DataBase on Demand : insight how to build your own DBaaS", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T16:35:04.972600+00:00", "description": "", "title": "DBoD_Chep2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/525\/attachments\/578479\/796610\/DBoD_Chep2015.pdf", "filename": "DBoD_Chep2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796610, "size": 2582799}], "title": "Slides", "default_folder": false, "id": 578479, "description": ""}], "_type": "Contribution", "description": "Inspired on different database as a service, DBaas, providers, the database group at CERN has developed a platform to allow CERN user community to run a database instance with database administrator privileges providing a full toolkit that allows the instance owner to perform backup\/ point in time recoveries, monitoring specific database metrics, start\/stop of the instance and uploading\/downloading specific logging or configuration files. With about 150 instances Oracle (11g and 12c), MySQL and PostgreSQL the platform has been designed and proofed to be flexible to run different RDBMS vendors and to scale up.\r\nInitially running on virtual machines, OracleVM, the instances are represented as objects in the management database toolset, making it independent of its physical representation. Nowadays instances run on physical servers together with virtual machines. A high availability solution has been implemented using Oracle cluster ware.\r\nThis talk explains how we have built this platform, different technologies involved, actual user interface, command execution based on a database queue, backups based on snapshots, and possible future evolution (Linux containers, storage replication, OpenStack, Puppet,\u2026).", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578479", "resources": [{"_type": "LocalFile", "name": "DBoD_Chep2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/525\/attachments\/578479\/796610\/DBoD_Chep2015.pdf", "fileName": "DBoD_Chep2015.pdf", "_fossil": "localFileMetadata", "id": "796610", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "73e51d5d39f11ce6110e45d4082a84b9", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "COTERILLO COZ, Ignacio", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "1f6f956fa9b278ea0769c69b16c59953", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. COLLADOS POLIDURA, David", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/525", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:15:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:30:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "526", "speakers": [{"_type": "ContributionParticipation", "emailHash": "b874041f5259e9d7290564f137bc14bf", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CORTI, Gloria", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "b874041f5259e9d7290564f137bc14bf", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CORTI, Gloria", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "c9f02a30fcdf251298934af7dec909c5", "affiliation": "University of Warwick (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "KREPS, Michal", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "1361efd0daf15bbf4cc3141a47fc97af", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHARPENTIER, Philippe", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "32f25df9044a9055f51527bb9fb36130", "affiliation": "Ruprecht-Karls-Universitaet Heidelberg (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "ZHELEZOV, Alexey", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "40e3c4499734552cbef1396b5c23c8e7", "affiliation": "Laboratoire de l'Accelerateur Lineaire (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "ROBBE, Patrick", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "85b213ea34548401be9e5fc4e9402b85", "affiliation": "University of Warwick (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "O'HANLON, Daniel Patrick", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "2d3197cea2de7705cbaf2df989cdf1da", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CLEMENCIC, Marco", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "218813bf33ab66728f320424e479d695", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CLOSIER, Joel", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "244c8becc3b603c6e5722f14640d8e65", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "COUTURIER, Ben", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "4538f10b9ce013c648a371b92a00d79e", "affiliation": "Institute for High Energy Physics  (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "ROMANOVSKIY, Vladimir", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "5ae4855aa9d3c6685bd9f450814f9af6", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "STAGNI, Federico", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "34fa6009bd34aced06fff1dd6d7fed34", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MATHE, Zoltan", "id": "11"}], "title": "How the Monte Carlo production of a wide variety of different samples if centrally handled in the LHCb experiment.", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T04:45:25.190295+00:00", "description": "", "title": "GCorti.MCProdInLHCb.CHEP2015.Apr14.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/526\/attachments\/578480\/796611\/GCorti.MCProdInLHCb.CHEP2015.Apr14.pdf", "filename": "GCorti.MCProdInLHCb.CHEP2015.Apr14.pdf", "content_type": "application\/pdf", "type": "file", "id": 796611, "size": 5653363}], "title": "Slides", "default_folder": false, "id": 578480, "description": ""}], "_type": "Contribution", "description": "In the LHCb experiment all massive processing of data is handled centrally.  In the case of simulated data a wide variety of different types of Monte Carlo (MC) events has to be produced, as each physics\u2019 analysis needs different sets of signal and background events. In order to cope with this large set of different types of MC events, of the order of several hundreds, a numerical event type identification code has been devised and is used throughout. A dedicated package contains all event type configurations files, automatically produced from this code, and is released independently from the simulation application. The deployment of the package on the distributed production system is handled centrally via the LHCb distribution tools and newly deployed event types are registered in the Bookkeeping catalogue. MC production requests are submitted via the LHCb production request system where a dedicated customization for MC data is in place. A specific request is made using predefined models centrally prepared to reproduce various data taking periods and selecting the event type from the Bookkeeping catalogue. After formal approval the requests are automatically forwarded to the LHCb Production team that carries them out. As the data are produced in remote sites they are automatically registered to the Bookkeeping catalogue where they can be found in folders specifying the event type, simulation conditions and processing chain. The various elements in the procedure, from writing a file for an event type to retrieving the sample produced, and the conventions established to allow their interplay will be described. The choices made have allowed to automate the MC production and for experts to concentrate on their specific tasks: while the configurations for each event type are prepared and validated by the physicists and simulation software experts, the MC samples are produced transparently on a world-wide distributed system the by LHCb production team.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578480", "resources": [{"_type": "LocalFile", "name": "GCorti.MCProdInLHCb.CHEP2015.Apr14.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/526\/attachments\/578480\/796611\/GCorti.MCProdInLHCb.CHEP2015.Apr14.pdf", "fileName": "GCorti.MCProdInLHCb.CHEP2015.Apr14.pdf", "_fossil": "localFileMetadata", "id": "796611", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/526", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:00:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "527", "speakers": [{"_type": "ContributionParticipation", "emailHash": "f6ab400e8c650428a24d05f6c8bef86d", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "NEUBERT, Sebastian", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "b4e6d4ec096ee67bd4c0531abbf34402", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "LINN, Christian Peter", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "c10fdf0d2ddefd23d451c9de740427fe", "affiliation": "ITEP Institute for Theoretical and Experimental Physics (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "USTYUZHANIN, Andrey", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "f6ab400e8c650428a24d05f6c8bef86d", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "NEUBERT, Sebastian", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "bbef8771fa0f5e9f0f935cc8ffc0d013", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "KARBACH, Till Moritz", "id": "3"}], "title": "Agile Research - Strengthening Reproducibility in Collaborative Data Analysis Projects", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T06:57:38.752746+00:00", "description": "", "title": "agile_research.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/527\/attachments\/578481\/796612\/agile_research.pdf", "filename": "agile_research.pdf", "content_type": "application\/pdf", "type": "file", "id": 796612, "size": 3452831}], "title": "Slides", "default_folder": false, "id": 578481, "description": ""}], "_type": "Contribution", "description": "Reproducibility of results is a fundamental quality of scientific research. However, as data analyses become more and more complex and research is increasingly carried out by larger and larger teams, it becomes a challenge to keep up this standard. The decomposition of complex problems into tasks that can be effectively distributed over a team in a reproducible manner becomes nontrivial.\r\nOvercoming these obstacles requires a shift in both management methodology as well as supporting technology. \r\nThe LHCb collaboration is experimenting with different methods and technologies to attack such challenges. In this talk we present a language and thinking framework for laying out data analysis projects. We show how this framework can be supported by specific tools and services that allow teams of researchers to achieve high quality results in a distributed environment. Those methodologies are based on so called agile development approaches that have been adopted very successfully in industry. We show how the approach has been adapted to HEP data analysis projects and report on experiences gathered on a pilot project.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578481", "resources": [{"_type": "LocalFile", "name": "agile_research.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/527\/attachments\/578481\/796612\/agile_research.pdf", "fileName": "agile_research.pdf", "_fossil": "localFileMetadata", "id": "796612", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/527", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "520", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6ba1f95da55f3a0622e4785d12b5cc25", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "NORMAN, Andrew", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6ba1f95da55f3a0622e4785d12b5cc25", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "NORMAN, Andrew", "id": "0"}], "title": "Timing in the NOvA detectors with atomic clock based time transfers between Fermilab, the Soudan mine and the Nova Far detector.", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T19:25:51.050093+00:00", "description": "", "title": "nova_chep2015_timing_poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/520\/attachments\/578482\/796613\/nova_chep2015_timing_poster.pdf", "filename": "nova_chep2015_timing_poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796613, "size": 55465082}], "title": "Slides", "default_folder": false, "id": 578482, "description": ""}], "_type": "Contribution", "description": "The NOvA experiment uses a GPS based timing system to both internally to synchronize the readout of the DAQ components and to establish an absolute \u201cwall clock\u201d reference which can be used to link the Fermilab accelerator complex with the neutrino flux that crosses the NOvA detectors.  We describe the methods that were used during the commissioning of the NOvA DAQ and Timing systems to establish the synchronization between the Fermilab beam and the NOvA far detector.  We present how high precision atomic clocks were trained and transported between the MINOS and NOvA detectors during a Northern Minnesota blizzard to validate the absolute time offsets of the experiments and make the first observation of beam neutrinos in the NOvA far detector.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578482", "resources": [{"_type": "LocalFile", "name": "nova_chep2015_timing_poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/520\/attachments\/578482\/796613\/nova_chep2015_timing_poster.pdf", "fileName": "nova_chep2015_timing_poster.pdf", "_fossil": "localFileMetadata", "id": "796613", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "a521f1cb3659857d474a9a138c1e03f3", "affiliation": "Indiana University", "_fossil": "contributionParticipationMetadata", "fullName": "NINER, Evan", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "698c361cfbc9ed204ca9f7b8fc5bb4cd", "affiliation": "Univ. of Minnesota Duluth", "_fossil": "contributionParticipationMetadata", "fullName": "HABIG, Alec", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/520", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "521", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6ea3743225ffb0457fc5cb31ebb0520e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "NAUMANN, Axel", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6ea3743225ffb0457fc5cb31ebb0520e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "NAUMANN, Axel", "id": "0"}], "title": "The ROOT 6 Runtime: Interpreter and Type Information", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "ROOT 6 can do all of C++ - thanks to its interpreter cling which in turn is based on the industrial strength compiler llvm \/ clang. With the transition from CINT to cling, all the runtime type information infrastructure (the well known \"dictionaries\") had to be adapted, too.\r\n\r\nThis contribution will explain how the binding of user and experiment classes to ROOT currently happens and how this binding is migrated to C++ modules. These same modules are also considered for inclusion in the upcoming C++ standards, where they are seen as a way to accelerate compilation time by factors. The presentation will showcase the advantages of using a compiler as an interpreter back-end and introduce the current areas of development for the interpreter.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "28b03fed334cb75a99ee0a92ecd47530", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "VASILEV, Vasil Georgiev", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "e71a4f9c822c9af7b8ab70c18eb4e640", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "PIPARO, Danilo", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "5b0402a9cd32d8915ceeb56718af64b6", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CANAL, Philippe", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/521", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:45:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "522", "speakers": [{"_type": "ContributionParticipation", "emailHash": "08c31f4b25f479cb22fb6293a61f1580", "affiliation": "urn:Facebook", "_fossil": "contributionParticipationMetadata", "fullName": "ORII, Asato", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "08c31f4b25f479cb22fb6293a61f1580", "affiliation": "urn:Facebook", "_fossil": "contributionParticipationMetadata", "fullName": "ORII, Asato", "id": "0"}], "title": "Development of New Data Acquisition System at Super-Kamiokande for Nearby Supernova Bursts", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T04:20:23.431809+00:00", "description": "", "title": "CHEP2015_orii.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/522\/attachments\/578483\/796614\/CHEP2015_orii.pdf", "filename": "CHEP2015_orii.pdf", "content_type": "application\/pdf", "type": "file", "id": 796614, "size": 1262668}, {"_type": "attachment", "modified_dt": "2015-04-13T04:20:23.431809+00:00", "description": "", "title": "CHEP2015_orii.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/522\/attachments\/578483\/796615\/CHEP2015_orii.pptx", "filename": "CHEP2015_orii.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796615, "size": 2273682}], "title": "Slides", "default_folder": false, "id": 578483, "description": ""}], "_type": "Contribution", "description": "Super-Kamiokande (SK), a 50-kiloton water Cherenkov detector, is one of\r\n the most sensitive neutrino detectors. SK is continuously collecting\r\n data as the neutrino observatory and can be used also for supernova \r\n observations by detecting supernova burst neutrinos.\r\n \r\n It is reported that Betelgeuse (640ly) is shrinking 15% in 15 years\r\n (C.H.townes et al. 2009) and this may be an indication of the supernova\r\n burst. Based on the Livermore model, the simulation study predicts \r\n 30MHz neutrino events observed in the SK detector for the neutrino \r\n burst from a supernova within a few hundreds of light years. \r\n The current SK data acquisition (DAQ) system can record only about \r\n first 20% of the events. To overcome this inefficiency, we developed \r\n a new DAQ system that records the number of hit PMTs so that we can \r\n store high-rate events and obtain a time profile of the number of \r\n neutrinos emitted at the supernova.\r\n This system uses the outputs from the number of hits from existing\r\n electronics modules as inputs and it is synchronized to the existing\r\n DAQ system. Therefore the data is easily checked the correlation to \r\n that from the existing electronics. The data is transferred to the \r\n computers with SiTCP, an implementation of TCP\/IP stack in FPGA without\r\n CPU. Part of the data are stored in the 4GB DDR2 memory before it \r\n transferred and this makes it possible to record detailed time structure\r\n of the superonva signal. The design and the production of the new \r\n modules were completed and we tested basic functions and the interference \r\n with the existing system. The firmware for the module is prepared \r\n and now being installed in SK. We will report the development and \r\n the status of the operation.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578483", "resources": [{"_type": "LocalFile", "name": "CHEP2015_orii.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/522\/attachments\/578483\/796614\/CHEP2015_orii.pdf", "fileName": "CHEP2015_orii.pdf", "_fossil": "localFileMetadata", "id": "796614", "_deprecated": true}, {"_type": "LocalFile", "name": "CHEP2015_orii.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/522\/attachments\/578483\/796615\/CHEP2015_orii.pptx", "fileName": "CHEP2015_orii.pptx", "_fossil": "localFileMetadata", "id": "796615", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "fc95ae1debb2abe6f06d4e112724e35d", "affiliation": "ICRR. University of Tokyo", "_fossil": "contributionParticipationMetadata", "fullName": "TOMURA, Tomonobu", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c9a63a9e9fe1bba43c13704760c8d39e", "affiliation": "University of Tokyo", "_fossil": "contributionParticipationMetadata", "fullName": "HAYATO, Yoshinari", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "80b639329ca995258f43de70a074a64a", "affiliation": "High Energy Accelerator Research Organization (KEK)", "_fossil": "contributionParticipationMetadata", "fullName": "IKENO, Masahiro", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "9133de9345b3d18834fdde04e7c3d5bf", "affiliation": "Department of Physics", "_fossil": "contributionParticipationMetadata", "fullName": "OBAYASHI, Yoshihisa", "id": "4"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/522", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "523", "speakers": [{"_type": "ContributionParticipation", "emailHash": "52ec300b2a63fdf7d0f86e212cf98126", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHIBANTE BARROSO, Vasco", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "52ec300b2a63fdf7d0f86e212cf98126", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHIBANTE BARROSO, Vasco", "id": "0"}], "title": "MAD \u2013 Monitoring ALICE Dataflow", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T07:45:52.782652+00:00", "description": "", "title": "MAD_poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/523\/attachments\/578484\/796616\/MAD_poster.pdf", "filename": "MAD_poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796616, "size": 4869242}], "title": "Slides", "default_folder": false, "id": 578484, "description": ""}], "_type": "Contribution", "description": "ALICE (A Large Ion Collider Experiment) is the heavy-ion detector designed to study the physics of strongly interacting matter and the quark-gluon plasma at the CERN Large Hadron Collider (LHC). Following a successful Run 1, which ended in February 2013, the ALICE data acquisition (DAQ) entered a consolidation phase to prepare for Run 2 which will start in the beginning of 2015. A new software tool has been developed by the data acquisition project to improve the monitoring of the experiment's dataflow, from the data readout in the DAQ farm up to its shipment to CERN's main computer centre. This software, called ALICE MAD (Monitoring ALICE Dataflow), uses the MonALISA framework as core module to gather, process, aggregate and distribute monitoring values from the different processes running in the distributed DAQ farm. Data are not only pulled from the data sources to MAD but can also be pushed by dedicated data collectors or the data source processes. A large set of monitored metrics (from the backpressure status on the readout links to event counters in each of the DAQ nodes and aggregated data rates for the whole data acquisition) is needed to provide a comprehensive view of the DAQ status. MAD also injects alarms in the Orthos alarm system whenever abnormal conditions are detected. The MAD web-based GUI uses WebSockets to provide dynamic and on-time status displays for the ALICE shift crew. Designed as a widget-based system, MAD supports an easy integration of new visualization blocks and also customization of the information displayed to the shift crew based on the ALICE activities.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578484", "resources": [{"_type": "LocalFile", "name": "MAD_poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/523\/attachments\/578484\/796616\/MAD_poster.pdf", "fileName": "MAD_poster.pdf", "_fossil": "localFileMetadata", "id": "796616", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "7b73f8fdf0c59c3804fe739200f59505", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "COSTA, Filippo", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "a03bb0b3b47a554f9eb69c2fca89dcb7", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GRIGORAS, Costin", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "ca50027eec50873b1ca5d70532ec74a3", "affiliation": "Warsaw University of Technology (PL)", "_fossil": "contributionParticipationMetadata", "fullName": "WEGRZYNEK, Adam", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/523", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "528", "speakers": [{"_type": "ContributionParticipation", "emailHash": "698c361cfbc9ed204ca9f7b8fc5bb4cd", "affiliation": "Univ. of Minnesota Duluth", "_fossil": "contributionParticipationMetadata", "fullName": "HABIG, Alec", "id": "10"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "bde30e28303c04b2e04d86bfdc281500", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SHANAHAN, Peter", "id": "0"}], "title": "Performance of the NOvA Data Acquisition System with the full 14 kT Far Detector", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T19:27:12.412380+00:00", "description": "", "title": "DAQ_Poster_CHEP_2015_v3.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/528\/attachments\/578485\/796617\/DAQ_Poster_CHEP_2015_v3.pdf", "filename": "DAQ_Poster_CHEP_2015_v3.pdf", "content_type": "application\/pdf", "type": "file", "id": 796617, "size": 3270499}], "title": "Slides", "default_folder": false, "id": 578485, "description": ""}], "_type": "Contribution", "description": "The NOvA experiment studies neutrino oscillations with 2 functionally\r\nidentical detectors separated by a baseline of 810km.  The 14 kT far\r\ndetector in Ash River, Minnesota, comprises 344,064 channels of liquid\r\nscintillator detection cells read out via wavelength-shifting fiber into\r\n32-channel Avalanche Photo Diodes (APD).  A custom designed Front End\r\nBoard (FEB) continuously digitizes and zero-supresses the output signals\r\nfrom each APD.  The smaller near detector located at Fermilab has 20,192\r\nchannels that are readout in an identical manner.   Both detectors are\r\ninternally synchronized by a GPS-based timing system with maintains a\r\nreadout-to-readout synchronization to better than 15.6 ns across the full\r\ndetector.  The timing system also provides a universal time base, which is\r\nused provide synchronization and correlation between the geographically\r\nseparated detectors and the Fermilab accelerator complex.\r\n\r\nThe NOvA Data Acquisition (DAQ) system for the far detector comprises \r\n168 powerPC-based custom computers for reading out and collating data from \r\nFEBs, a farm of 196 COTS linux nodes for buffering data for trigger decisions and \r\nevent building, another 10 for dedicated DAQ functions such as run control, data \r\nlogging, and DAQ system monitoring.  Data is transferred between the detector \r\nand the DAQ computing via a multiple bandwidth multilayer networks and fabric routing.\r\n\r\nThe performance of the recently completed DAQ on the full near and far\r\ndetectors will be reviewed.  The scaling characteristics of the network\r\ndata flow, event building systems, and DDS based message passing layers\r\nwill be covered in detail highlighting the computing and operational\r\nchallenges of bringing the full DAQ and readout system  online.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578485", "resources": [{"_type": "LocalFile", "name": "DAQ_Poster_CHEP_2015_v3.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/528\/attachments\/578485\/796617\/DAQ_Poster_CHEP_2015_v3.pdf", "fileName": "DAQ_Poster_CHEP_2015_v3.pdf", "_fossil": "localFileMetadata", "id": "796617", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "6ba1f95da55f3a0622e4785d12b5cc25", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "NORMAN, Andrew", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "177a78106eb711162ab9437b3a590d3c", "affiliation": "Argonne National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "PALEY, Jonathan", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "a521f1cb3659857d474a9a138c1e03f3", "affiliation": "Indiana University", "_fossil": "contributionParticipationMetadata", "fullName": "NINER, Evan", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "d56425ca2dac38f9c0bddaabc29723fe", "affiliation": "University of Minnesota", "_fossil": "contributionParticipationMetadata", "fullName": "KASAHARA, Susan", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "ad8b53362edcd51a323e1a262b2fbfb1", "affiliation": "Indiana University", "_fossil": "contributionParticipationMetadata", "fullName": "MESSIER, Mark", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "04b118f9278138e6412b5e7f9d70ddf5", "affiliation": "Wichita State University", "_fossil": "contributionParticipationMetadata", "fullName": "MEYER, Holger", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "0cd767b1f310fb96865d55bdb608993d", "affiliation": "Argonne National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "MAGILL, Steve", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "f8ab5ec7e82f0bbdcf9f8ac167f722a3", "affiliation": "Oxford\/T2K", "_fossil": "contributionParticipationMetadata", "fullName": "WALDRON, Abbey", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "328a57e48e662f1fecdbaba734fc9ddd", "affiliation": "Witchita State University", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. MUETHER, Mathew", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "698c361cfbc9ed204ca9f7b8fc5bb4cd", "affiliation": "Univ. of Minnesota Duluth", "_fossil": "contributionParticipationMetadata", "fullName": "HABIG, Alec", "id": "10"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/528", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "529", "speakers": [{"_type": "ContributionParticipation", "emailHash": "bde30e28303c04b2e04d86bfdc281500", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SHANAHAN, Peter", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "bde30e28303c04b2e04d86bfdc281500", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SHANAHAN, Peter", "id": "0"}], "title": "The NOvA Data Acquisition Error Handling System", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T19:29:20.157888+00:00", "description": "", "title": "CHEP_NOvA_DAQ_MessageAnalyzer.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/529\/attachments\/578486\/796618\/CHEP_NOvA_DAQ_MessageAnalyzer.pdf", "filename": "CHEP_NOvA_DAQ_MessageAnalyzer.pdf", "content_type": "application\/pdf", "type": "file", "id": 796618, "size": 2827151}], "title": "Slides", "default_folder": false, "id": 578486, "description": ""}], "_type": "Contribution", "description": "The NOvA experiment studies neutrino oscillations with 2 functionally identical detectors separated by a baseline of 810km.  The Data Acquisition (DAQ) system for the far detector in Ash River in Minnesota comprises more than 10,000 Front End Boards, and a cluster of 168 custom PPC-based, and 206 COTS x86 linux nodes performing a variety of functions.\r\n\r\nAn Error Handling system has been developed to facilitate operation of this expansive system, using status messages generated by DAQ applications, a dedicated \u201crule engine\u201d for analyzing status messages for specific or general patterns, and an Error Handler for taking pre-defined actions based on the outcome of the message analysis.\r\n\r\nThe performance of the Error Handling system in the context of production data-taking on the near and far NOvA detectors will be presented.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578486", "resources": [{"_type": "LocalFile", "name": "CHEP_NOvA_DAQ_MessageAnalyzer.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/529\/attachments\/578486\/796618\/CHEP_NOvA_DAQ_MessageAnalyzer.pdf", "fileName": "CHEP_NOvA_DAQ_MessageAnalyzer.pdf", "_fossil": "localFileMetadata", "id": "796618", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "1c91a48a2781df0da728041b919a031e", "affiliation": "Fermi National Accelerator Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "LU, Qiming", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "69898e3dbcb39fb1bcf97dfae8f4339e", "affiliation": "S", "_fossil": "contributionParticipationMetadata", "fullName": "WALDRON, Abbey", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "177a78106eb711162ab9437b3a590d3c", "affiliation": "Argonne National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "PALEY, Jonathan", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/529", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "449", "speakers": [{"_type": "ContributionParticipation", "emailHash": "fd06959b93c405340e9900abf962beb7", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "LAHIFF, Andrew David", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "fd06959b93c405340e9900abf962beb7", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "LAHIFF, Andrew David", "id": "0"}], "title": "Integrating grid and cloud resources at the RAL Tier-1", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T05:29:21.180423+00:00", "description": "", "title": "IntegratingGridAndCloudResourcesRAL.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/449\/attachments\/578487\/796619\/IntegratingGridAndCloudResourcesRAL.pdf", "filename": "IntegratingGridAndCloudResourcesRAL.pdf", "content_type": "application\/pdf", "type": "file", "id": 796619, "size": 1647824}], "title": "Slides", "default_folder": false, "id": 578487, "description": ""}], "_type": "Contribution", "description": "Today the primary method by which the LHC and other experiments run computing work at WLCG sites is grid job submission. Jobs are submitted to computing element middleware which in turn submits jobs to a batch system managing the local compute resources. With the increasing interest and usage of cloud technology, a new challenge facing sites which support multiple experiments in recent years is a need to provide both traditional grid as well as cloud interfaces, but without partitioning the underlying resources. When the batch system is busy but the cloud is idle, it should be possible for the unused cloud resources to be included in the batch system. Similarly, when the batch system is idle but the cloud is busy, the unused batch resources should be available for users within the cloud. At the RAL Tier-1 a cloud based on OpenNebula has been under development for some time and will made available to the LHC experiments and others, as well as being used internally by staff for activities such as testing and developement. Here we present our experience unifying the cloud with our production HTCondor batch system in a way that avoids static partitioning, ensures that resources are used efficiency and that allocations are respected.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578487", "resources": [{"_type": "LocalFile", "name": "IntegratingGridAndCloudResourcesRAL.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/449\/attachments\/578487\/796619\/IntegratingGridAndCloudResourcesRAL.pdf", "fileName": "IntegratingGridAndCloudResourcesRAL.pdf", "_fossil": "localFileMetadata", "id": "796619", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "2463c98197fe73772aa1cd10b34131aa", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "COLLIER, Ian Peter", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/449", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:15:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:30:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "448", "speakers": [{"_type": "ContributionParticipation", "emailHash": "579410e79e02e069dc197d8a22a258ae", "affiliation": "Fermi National Accelerator Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "MHASHILKAR, Parag", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "7d4d684e5f547cd039cf3d9777876bd6", "affiliation": "FERMI NATIONAL ACCELERATOR LABORATORY", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GARZOGLIO, Gabriele", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "5736aa29351bae70605b834f6af249c0", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "TIMM, Steven", "id": "6"}], "title": "Cloud services for the Fermilab scientific stakeholders", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T23:48:24.522928+00:00", "description": "", "title": "CloudServicesForTheFermilabScientificStakeholders-final.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/448\/attachments\/578488\/796620\/CloudServicesForTheFermilabScientificStakeholders-final.pdf", "filename": "CloudServicesForTheFermilabScientificStakeholders-final.pdf", "content_type": "application\/pdf", "type": "file", "id": 796620, "size": 3745490}], "title": "Slides", "default_folder": false, "id": 578488, "description": ""}], "_type": "Contribution", "description": "As part of the Fermilab\/KISTI cooperative research project, Fermilab has successfully run an experimental simulation workflow at scale on a federation of Amazon Web Services (AWS), FermiCloud, and local FermiGrid resources. We used the CernVM-FS (CVMFS) file system to deliver the application software. We established Squid caching servers in AWS as well, using the Shoal system to let each individual virtual machine find the closest squid server. We also developed an automatic virtual machine conversion system so that we could transition virtual machines made on FermiCloud to Amazon Web Services.\r\nWe used this system to successfully run a cosmic ray simulation of the NOvA detector at Fermilab, making use of both AWS spot pricing and network bandwidth discounts to minimize the cost. On FermiCloud we also were able to run the workflow at the scale of 1000 virtual machines, using a private network routable inside of Fermilab. We present the details of the technological improvements that were used to make this successfully.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578488", "resources": [{"_type": "LocalFile", "name": "CloudServicesForTheFermilabScientificStakeholders-final.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/448\/attachments\/578488\/796620\/CloudServicesForTheFermilabScientificStakeholders-final.pdf", "fileName": "CloudServicesForTheFermilabScientificStakeholders-final.pdf", "_fossil": "localFileMetadata", "id": "796620", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "579410e79e02e069dc197d8a22a258ae", "affiliation": "Fermi National Accelerator Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "MHASHILKAR, Parag", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "f00d7f043f9f6ef6c49645dc6e036b67", "affiliation": "FERMI NATIONAL ACCELERATOR LABORATORY", "_fossil": "contributionParticipationMetadata", "fullName": "BOYD, Joe", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "16dc86112d261a66015c3851a89f620b", "affiliation": "FERMI NATIONAL ACCELERATOR LABORATORY", "_fossil": "contributionParticipationMetadata", "fullName": "BERNABEU, Gerard", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "3994375d303e51f0bbdf250f60e635fc", "affiliation": "FERMI NATIONAL ACCELERATOR LABORATORY", "_fossil": "contributionParticipationMetadata", "fullName": "SHARMA, Neha", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "d6750f44178e4ea38656a3476da226ed", "affiliation": "FERMI NATIONAL ACCELERATOR LABORATORY", "_fossil": "contributionParticipationMetadata", "fullName": "PEREGONOW, Nicholas", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "710cfd71b2e4655a7f89f39a65f75c64", "affiliation": "KISTI Korea Institute of Science &amp;  Technology Information (KR)", "_fossil": "contributionParticipationMetadata", "fullName": "NOH, Seoyoung", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "f16642709b6f9b10cd8e839c0f72f999", "affiliation": "FERMI NATIONAL ACCELERATOR LABORATORY", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KIM, Hyunwoo", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "b7dcddf1081b23387055c8a24264dc65", "affiliation": "ILLINOIS INSTITUTE OF TECHNOLOGY", "_fossil": "contributionParticipationMetadata", "fullName": "PALUR, SANDEEP", "id": "9"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/448", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:45:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "443", "speakers": [{"_type": "ContributionParticipation", "emailHash": "771122777d10e8f8dc1c3aa7156922e8", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BLOMER, Jakob", "id": "8"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "5f7b6f0056d2d65a23659da9c0e4f79d", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DYKSTRA, Dave", "id": "9"}], "title": "Engineering the CernVM-FileSystem as a High Bandwidth Distributed Filesystem for Auxiliary Physics Data", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-06T19:54:57.280210+00:00", "description": "", "title": "CHEP15_Talk_CVMFSAuxData.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/443\/attachments\/578489\/796621\/CHEP15_Talk_CVMFSAuxData.pdf", "filename": "CHEP15_Talk_CVMFSAuxData.pdf", "content_type": "application\/pdf", "type": "file", "id": 796621, "size": 207863}], "title": "Slides", "default_folder": false, "id": 578489, "description": ""}], "_type": "Contribution", "description": "Fermilab has several physics experiments including NOvA, MicroBooNE, and\r\nthe Dark Energy Survey that have computing grid-based applications that\r\nneed to read from a shared set of data files. We call this type of data\r\nAuxiliary data to distinguish it from (a) Event data which tends to be\r\ndifferent for every job, and (b) Conditions data which tends to be the\r\nsame for each job in a batch of jobs. Conditions data also tends to be\r\nrelatively small per job (100s of Megabytes) where both Event data and\r\nAuxiliary data are larger per job (Gigabytes), but unlike Event data,\r\nAuxiliary data comes from a limited working set (10s to 100s of\r\nGigabytes) of shared files. Since there is some sharing of the\r\nAuxiliary data, it appeared at first that the CernVM-Filesystem (CVMFS)\r\ninfrastructure built for distributing software to the grid would also be\r\nthe best way to distribute Auxiliary data. However, because grid jobs\r\ntend to be started in large batches running the same code, the software\r\ndistribution use case of CVMFS has a very high cache hit ratio, so the\r\nbandwidth requirements on the per-site http proxy caches (squids) is\r\nquite low. As a result those proxy caches have been engineered with\r\nrelatively low bandwidth, and they are easily overwhelmed by Auxiliary\r\ndata with its relatively low cache hit ratio. A new approach was\r\nneeded. We are taking advantage of a CVMFS client feature called \"alien\r\ncache\" to cache data on site-local high-bandwidth data servers that were\r\nengineered for storing Event data. This site-shared cache replaces\r\ncaching CVMFS files on both the worker node local disks and on the\r\nsite-local squids. We have tested this alien cache with the dCache\r\nNFSv4.1 interface, Lustre, and Hadoop-FS-fuse, and found that they all\r\nperform well. In addition, we use high-bandwidth data servers at\r\ncentral sites to perform the CVMFS Stratum 1 function instead of the\r\nlow-bandwidth web servers deployed for the CVMFS software distribution\r\nfunction. We have tested this using the dCache http interface. As a\r\nresult, we have an end-to-end high-bandwidth widely distributed caching\r\nread-only filesystem, using existing client software already widely\r\ndeployed to grid worker nodes and existing file servers already widely\r\ninstalled at grid sites. Files are published in a central place and are\r\nsoon available on demand throughout the grid and cached locally on the\r\nsite with a convenient POSIX interface. This paper discusses the\r\ndetails of the architecture and reports performance measurements.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578489", "resources": [{"_type": "LocalFile", "name": "CHEP15_Talk_CVMFSAuxData.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/443\/attachments\/578489\/796621\/CHEP15_Talk_CVMFSAuxData.pdf", "fileName": "CHEP15_Talk_CVMFSAuxData.pdf", "_fossil": "localFileMetadata", "id": "796621", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "e82c46a3a0d49dd555c888d113eace3a", "affiliation": "University of Nebraska (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BOCKELMAN, Brian Paul", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "0c028d4c80034f0129fa2b0604484a73", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "LEVSHINA, Tanya", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "b7a3d367488441ffe67eff25c8fff69d", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "HERNER, Ken", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "495be4f5790553fa157ed05c614db307", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "SLYZ, Marko", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "771122777d10e8f8dc1c3aa7156922e8", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BLOMER, Jakob", "id": "10"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/443", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:45:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "442", "speakers": [{"_type": "ContributionParticipation", "emailHash": "1baeffa4c1ef9f748709ae060bf62bc1", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. JAYATILAKA, Bodhitha", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "1baeffa4c1ef9f748709ae060bf62bc1", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. JAYATILAKA, Bodhitha", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "0c028d4c80034f0129fa2b0604484a73", "affiliation": "FERMILAB", "_fossil": "contributionParticipationMetadata", "fullName": "Mrs. LEVSHINA, Tanya", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "e979084c43cacd73850f0111ea195fe1", "affiliation": "RENCI UNC Chapel Hill", "_fossil": "contributionParticipationMetadata", "fullName": "RYNGE, Mats", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "8d572bee1e6dfc9ecfdad6d55a42e799", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "SEHGAL, Chander", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "495be4f5790553fa157ed05c614db307", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "SLYZ, Marko", "id": "4"}], "title": "The OSG Open Facility: A Sharing Ecosystem Using Harvested Opportunistic Resources", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:04:36.462615+00:00", "description": "", "title": "chep-osg-jayatilaka.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/442\/attachments\/578490\/796622\/chep-osg-jayatilaka.pdf", "filename": "chep-osg-jayatilaka.pdf", "content_type": "application\/pdf", "type": "file", "id": 796622, "size": 4824734}], "title": "Slides", "default_folder": false, "id": 578490, "description": ""}], "_type": "Contribution", "description": "The Open Science Grid (OSG) ties together individual experiments' computing power, connecting their resources to create a large, robust computing grid; this computing infrastructure started primarily as a collection of sites associated with large HEP experiments such as ATLAS, CDF, CMS, and DZero. OSG has been funded by the Department of Energy Office of Science and National Science Foundation since 2006 to meet the US LHC community's computational needs. In the years since, the OSG has broadened its focus to also address the needs of other US researchers and increased delivery of Distributed High Throughput Computing (DHTC) to users from a wide variety of disciplines via the OSG Open Facility. Presently, the Open Facility delivers about 100 million computing wall hours per year to researchers who are not already associated with the owners of the computing sites; this is primarily accomplished by harvesting and organizing the temporarily unused capacity (i.e. opportunistic cycles) from the sites in the OSG. We present an overview of the infrastructure developed to accomplish this from flocking architecture to harvesting opportunistic resources to providing user support to a diverse set of researchers. We also present our experiences since becoming a high-throughput computing service provider as part of the NSF\u2019s XD program. Using these methods, OSG resource providers and scientists share computing hours with researchers in many other fields to enable their science, striving to make sure that these computing resources are used with maximal efficiency. We believe that expanded access to DHTC is an essential tool for scientific innovation and work continues in expanding this service.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578490", "resources": [{"_type": "LocalFile", "name": "chep-osg-jayatilaka.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/442\/attachments\/578490\/796622\/chep-osg-jayatilaka.pdf", "fileName": "chep-osg-jayatilaka.pdf", "_fossil": "localFileMetadata", "id": "796622", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/442", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "441", "speakers": [{"_type": "ContributionParticipation", "emailHash": "b4169d214f99022fe255ea33a34c4cda", "affiliation": "Universitaet Zuerich (CH)", "_fossil": "contributionParticipationMetadata", "fullName": "STORACI, Barbara", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "b4169d214f99022fe255ea33a34c4cda", "affiliation": "Universitaet Zuerich (CH)", "_fossil": "contributionParticipationMetadata", "fullName": "STORACI, Barbara", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "a78490ce03b219b30fa09fe02b8ca6e3", "affiliation": "University of Manchester (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "DUJANY, Giulio", "id": "2"}], "title": "Real-time alignment and calibration of the LHCb Detector in Run2", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-06T19:27:43.674463+00:00", "description": "", "title": "CHEP2015-onlineAlignAndCalib.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/441\/attachments\/578491\/796623\/CHEP2015-onlineAlignAndCalib.pdf", "filename": "CHEP2015-onlineAlignAndCalib.pdf", "content_type": "application\/pdf", "type": "file", "id": 796623, "size": 864737}], "title": "Slides", "default_folder": false, "id": 578491, "description": ""}], "_type": "Contribution", "description": "Stable, precise spatial alignment and PID calibration are necessary to achieve optimal detector performances.\r\nDuring Run2, LHCb will have a new real-time detector alignment and calibration to reach equivalent performances in the online and offline reconstruction. \r\nThis offers the opportunity to optimise the event selection by applying stronger constraints as well as hadronic particle identification at the trigger level.\r\nThe required computing time constraints are met thanks to a new dedicated framework using the multi-core farm infrastructure for the trigger.\r\nThe motivation for a real-time alignment and calibration of the LHCb detector is discussed from the operative and physics performance point of view.\r\nSpecific challenges of this configuration are discussed, as well as the designed framework and its performance.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578491", "resources": [{"_type": "LocalFile", "name": "CHEP2015-onlineAlignAndCalib.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/441\/attachments\/578491\/796623\/CHEP2015-onlineAlignAndCalib.pdf", "fileName": "CHEP2015-onlineAlignAndCalib.pdf", "_fossil": "localFileMetadata", "id": "796623", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/441", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "440", "speakers": [{"_type": "ContributionParticipation", "emailHash": "f10a491a558480b67d78d8826c40ea5e", "affiliation": "Lancaster University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LOVE, Peter", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "f10a491a558480b67d78d8826c40ea5e", "affiliation": "Lancaster University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LOVE, Peter", "id": "0"}], "title": "Subtlenoise: reducing cognitive load when monitoring distributed computing operations", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The operation of distributed computing systems requires comprehensive monitoring to ensure reliability and robustness. There are two components found in most monitoring systems: one being visually rich time-series graphs and another being notification systems for alerting operators under certain pre-defined conditions. In this paper the sonification of monitoring messages is explored using an architecture which fits easily within existing infrastructures based on mature opensource technologies such as ZeroMQ, Logstash, and Supercollider (a synth engine). Message attributes are mapped onto audio attributes based on broad classification of the message (continuous or discrete metrics) but keeping the audio stream subtle in nature. The benefits of audio-rendering are described in the context of distributed computing operations and may provide a less intrusive way to understand the operational health of these systems.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/440", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:15:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "447", "speakers": [{"_type": "ContributionParticipation", "emailHash": "fe1cfa859450437075f32ac7e6846b76", "affiliation": "Universita e INFN Torino (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "BAGNASCO, Stefano", "id": "5"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "aeeb887d8d8aff1186a2be5c4da190d9", "affiliation": "INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "PIANO, Stefano", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "64b7dcc505ab28db9fcbac74b1678ea3", "affiliation": "I.N.F.N. TORINO", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BAGNASCO, Stefano", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "7619cbf29b7dbd4004fe9d055e7b0f7b", "affiliation": "INFN Bari", "_fossil": "contributionParticipationMetadata", "fullName": "ELIA, Domenico", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "8711f3228c9b2233b9555d38146da383", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "LUPARELLO, Grazia", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "dade965cbbc7ace80c71c3d71ca3172b", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "VENARUZZO, Massimo", "id": "4"}], "title": "Interoperating Cloud-based Virtual Farms", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T23:17:01.437615+00:00", "description": "", "title": "InteroperatingCloudBasedCHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/447\/attachments\/578492\/796624\/InteroperatingCloudBasedCHEP2015.pdf", "filename": "InteroperatingCloudBasedCHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796624, "size": 1772630}], "title": "Slides", "default_folder": false, "id": 578492, "description": ""}], "_type": "Contribution", "description": "The present work aims at optimizing the use of computing resources available at the grid Italian Tier-2 sites of the ALICE experiment at CERN LHC by making them accessible to interactive distributed analysis, thanks to modern solutions based on cloud computing. The scalability and elasticity of the computing resources via dynamic (\u201con-demand\u201d) provisioning is essentially limited by the size of the computing site, reaching the theoretical optimum only in the asymptotic case of infinite resources. The main challenge of the project is to overcome this limitation by federating different sites through a distributed cloud facility. Storage capacities of the participating sites are seen as a single federated storage area, preventing from the need of mirroring data across them: high data access efficiency is guaranteed by location-aware analysis software and storage interfaces, in a transparent way from an end-user perspective. Moreover, the interactive analysis on the federated cloud reduces the execution time with respect to grid batch jobs. The tests of the investigated solutions for both cloud computing and distributed storage on wide area network will be presented.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578492", "resources": [{"_type": "LocalFile", "name": "InteroperatingCloudBasedCHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/447\/attachments\/578492\/796624\/InteroperatingCloudBasedCHEP2015.pdf", "fileName": "InteroperatingCloudBasedCHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796624", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/447", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "446", "speakers": [{"_type": "ContributionParticipation", "emailHash": "3e84f64e37bceead0577ddf1db41bb2a", "affiliation": "SLAC National Accelerator  Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GRAF, Norman Anthony", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "3e84f64e37bceead0577ddf1db41bb2a", "affiliation": "SLAC National Accelerator  Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GRAF, Norman Anthony", "id": "0"}], "title": "org.lcsim: A Java-based tracking toolkit", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "We describe a software toolkit for full event simulation and reconstruction in silicon tracking detectors. It features modular packages providing sophisticated simulations of the response of silicon detectors to the passage of charged particles. Sensor classes allow very detailed descriptions of charge carrier movement in silicon detectors: one can list the collecting, absorbing and reflecting regions, properties of silicon (doping, mobility, diffusion length, etc.), and electric and magnetic fields (including TCAD maps). After the charge carriers are generated and collected, the electronics simulation processes this into digital signals. We have defined an interface to specify how any such simulation should communicate with other parts of the package. Since details of signal processing are very sensor specific, it is anticipated that any sensor option will have its own class handling such processing, but we have implemented a number of readout technologies of interest to HEP detectors, such as CCDs and active pixel devices. Common to all the specific electronics simulation are the addition of electronics noise, propagation of the signal to readout, thresholding, and digitization of the signal. The final output is then a list of electronics channels with their corresponding ADC counts, and optionally the time for the signal, replicating the readout from a real detector. We also provide code for cluster finding, pattern recognition, track-finding and fitting, and analysis. The detector is defined by the same xml input files used for the Geant4 detector response simulation, ensuring that simulation and reconstruction geometries are always commensurate by construction. \r\nWe describe an easy-to-use software toolkit used to fully simulate all aspects of silicon trackers, from signal development in pixels and microstrips, through pattern recognition and track fitting, to analysis. Originally developed within the context of collider detector development for the ILC, it is being used by the Heavy Photon Search Detector, a fixed-target experiment at the Thomas Jefferson National Laboratory. We will describe its use and performance in a running experiment.", "track": "Track2: Offline software ", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/446", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:15:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "445", "speakers": [{"_type": "ContributionParticipation", "emailHash": "3e84f64e37bceead0577ddf1db41bb2a", "affiliation": "SLAC National Accelerator  Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GRAF, Norman Anthony", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "3e84f64e37bceead0577ddf1db41bb2a", "affiliation": "SLAC National Accelerator  Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GRAF, Norman Anthony", "id": "0"}], "title": "slic : A full-featured Geant4 simulation program", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T15:09:36.298153+00:00", "description": "", "title": "slic_150414_CHEP15_Graf.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/445\/attachments\/578493\/796625\/slic_150414_CHEP15_Graf.pdf", "filename": "slic_150414_CHEP15_Graf.pdf", "content_type": "application\/pdf", "type": "file", "id": 796625, "size": 1120002}], "title": "Slides", "default_folder": false, "id": 578493, "description": ""}], "_type": "Contribution", "description": "As the complexity and resolution of particle detectors increases, the need for detailed simulation of the experimental setup also increases. We have developed efficient and flexible tools for detailed physics and detector response simulations which build on the power of the Geant4 toolkit but free the end user from any C++ coding. Geant4 is the de facto high-energy physics standard for simulating the interaction of particles with fields and materials. However, the end user is required to write their own C++ program, and the learning curve for setting up the detector geometry and defining sensitive elements and readout can be quite daunting, especially for those without previous experience or not associated with large collaborations. We have developed the Geant4-based detector simulation program, slic, which employs generic IO formats as well as a textual detector description. Extending the pure geometric capabilities of GDML, LCDD enables fields, regions, sensitive detector readout elements, etc. to be fully described at runtime using an xml file. We also describe how more complex geometries, such as those from CAD programs, can be seamlessly incorporated into the xml files. We have defined generic \u201chits\u201d which can be used to model sophisticated tracking and calorimetry readouts, but the native Geant4 scoring functionality can also be used for simpler applications. Although developed within the context of HEP collider detectors, the program is completely flexible and can be used to simulate detectors in many different fields. We present a software toolkit and computing infrastructure which allows physicists to quickly and easily contribute to detector design by modeling detector elements without requiring either C++ coding expertise or experience with Geant4. Examples of its use designing collider detectors for the ILC and CLIC as well as its use in simulating the detector response and physics performance of a fixed-target experiment at the Thomas Jefferson National Laboratory will be presented.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578493", "resources": [{"_type": "LocalFile", "name": "slic_150414_CHEP15_Graf.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/445\/attachments\/578493\/796625\/slic_150414_CHEP15_Graf.pdf", "fileName": "slic_150414_CHEP15_Graf.pdf", "_fossil": "localFileMetadata", "id": "796625", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "8d425a4c31b17ef168797614c576d8a3", "affiliation": "SLAC National Accelerator Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "MCCORMICK, Jeremy", "id": "1"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/445", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:30:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:45:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "444", "speakers": [{"_type": "ContributionParticipation", "emailHash": "579410e79e02e069dc197d8a22a258ae", "affiliation": "Fermi National Accelerator Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "MHASHILKAR, Parag", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "384f25285a1799bb8e2b4ea136bfd945", "affiliation": "Fermi National Accelerator Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KIRBY, Michael", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "579410e79e02e069dc197d8a22a258ae", "affiliation": "Fermi National Accelerator Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "MHASHILKAR, Parag", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "0c028d4c80034f0129fa2b0604484a73", "affiliation": "FERMILAB", "_fossil": "contributionParticipationMetadata", "fullName": "Mrs. LEVSHINA, Tanya", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "7d4d684e5f547cd039cf3d9777876bd6", "affiliation": "FERMI NATIONAL ACCELERATOR LABORATORY", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GARZOGLIO, Gabriele", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "31043cac65bc783abb723e915f851724", "affiliation": "FERMILAB", "_fossil": "contributionParticipationMetadata", "fullName": "KREYMER, Arthur", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "d09b9a8ce6e474a0a1e82fb5b79b8bcb", "affiliation": "F", "_fossil": "contributionParticipationMetadata", "fullName": "BOX, Dennis", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "5f7b6f0056d2d65a23659da9c0e4f79d", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DYKSTRA, Dave", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "b7a3d367488441ffe67eff25c8fff69d", "affiliation": "SUNY Stony Brook", "_fossil": "contributionParticipationMetadata", "fullName": "HERNER, Ken", "id": "7"}], "title": "Advances in Distributed High Throughput Computing for the Fabric for Frontier Experiments Project at Fermilab", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T13:59:20.708735+00:00", "description": "", "title": "CHEP2015-FIFETalk-final.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/444\/attachments\/578494\/796626\/CHEP2015-FIFETalk-final.pdf", "filename": "CHEP2015-FIFETalk-final.pdf", "content_type": "application\/pdf", "type": "file", "id": 796626, "size": 1428006}], "title": "Slides", "default_folder": false, "id": 578494, "description": ""}], "_type": "Contribution", "description": "The FabrIc for Frontier Experiments (FIFE) program is an ambitious, major-impact initiative within the Fermilab Scientific Computing Division designed to lead the computing model development for Fermilab experiments and external projects. FIFE is a collaborative effort between physicists and computing professionals to provide computing solutions for experiments of varying scale, needs, and infrastructure from all areas of Particle Physics: from Neutrino to Collider to Astro-Particle Physics. The major focus of the FIFE project is a single, unified high throughput computing solution for all experiments by development, deployment, and integration of workload management system (WMS), data management layer, and database access layer working seamlessly at sites in Open Science Grid (OSG), commercial and community cloud providers (e.g. Amazon AWS and FermiCloud), and local computing farms. Two development areas where FIFE has made significant progress are a job submission system and reference data distribution. Jobsub is a redesigned scalable, reliable, and robust tiered jobs submission system that integrates with the GlideinWMS workload management system to run complex scientific workflows in Grids and clouds. Jobsub is responsible for functions such as site selection via GlideinWMS, managing credentials, and handling data transfers, with GlideinWMS responsible for provisioning computing resources. Through the development of Alien Cache for CVMFS, the FIFE program has considerably expanded the capabilities of CVMFS for reference data distribution. In addition to job submission and reference data distribution, the FIFE project has also made significant progress integrating services into experiment computing operations such as a flexible data transfer client and access to opportunistic resources on the Open Science Grid. The progress with current experiments and plans for expansion with additional projects will be discussed. FIFE has taken the leading role in defining the computing model for Fermilab experiments, aided in the design of experiments beyond Fermilab, and will continue to lead the future direction of high throughput computing for future physics experiments world wide.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578494", "resources": [{"_type": "LocalFile", "name": "CHEP2015-FIFETalk-final.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/444\/attachments\/578494\/796626\/CHEP2015-FIFETalk-final.pdf", "fileName": "CHEP2015-FIFETalk-final.pdf", "_fossil": "localFileMetadata", "id": "796626", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/444", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:30:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:45:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "108", "speakers": [{"_type": "ContributionParticipation", "emailHash": "84e347808f71c25608b52dc4bd77c2ad", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FRANK, Markus", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "84e347808f71c25608b52dc4bd77c2ad", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FRANK, Markus", "id": "2"}], "title": "The LHCb Data Aquisition and High Level Trigger Processing Architecture", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T09:23:46.039285+00:00", "description": "Slides Open Office format", "title": "LHCb_Online_overview_ID_108.odp", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/108\/attachments\/578496\/796628\/LHCb_Online_overview.odp", "filename": "LHCb_Online_overview.odp", "content_type": "application\/vnd.oasis.opendocument.presentation", "type": "file", "id": 796628, "size": 2303682}], "title": "Slides -- Open Office format", "default_folder": false, "id": 578496, "description": ""}, {"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T09:20:30.033824+00:00", "description": "Slides -- PDF format", "title": "LHCb_Online_overview_ID_108.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/108\/attachments\/578495\/796627\/LHCb_Online_overview.pdf", "filename": "LHCb_Online_overview.pdf", "content_type": "application\/pdf", "type": "file", "id": 796627, "size": 1597039}], "title": "Slides -- PDF", "default_folder": false, "id": 578495, "description": ""}], "_type": "Contribution", "description": "The LHCb experiment at the LHC accelerator at CERN collects collisions of particle \r\nbunches at 40 MHz. After a first level of hardware trigger with output of 1 MHz,\r\nthe physically interesting collisions are selected by running dedicated trigger \r\nalgorithms in the High Level Trigger (HLT) computing farm. This farm consists of \r\nup to roughly 25000 CPU cores in roughly 1600 physical nodes each equipped with \r\n2 TB of local storage space.<br>\r\nThis work describes the LHCb online system with an emphasis on the\r\ndevelopments implemented during during the current long shutdown (LS1). \r\nWe will elaborate the architecture to treble the available CPU power of the \r\nHLT farm and the technicalities to determine and verify precise calibration\r\nand alignment constants which are fed to the HLT event selection procedure. \r\nPrecise calibration and alignment constants are determined and verified in \r\na separate data acquisition activity as soon as data from particle collisions\r\nare delivered by the LHC collider. We will describe how the constants are fed \r\ninto a two stage HLT event selection facility using extensively the local \r\ndisk buffering capabilities on the worker nodes. With the installed disk \r\nbuffers, the installed CPU can be used during periods of up to ten days \r\nwithout beams. These periods in the past accounted to more than 70 % of \r\nthe total time.", "track": "Track1: Online computing ", "material": [{"_type": "Material", "title": "Slides -- Open Office format", "_fossil": "materialMetadata", "id": "578496", "resources": [{"_type": "LocalFile", "name": "LHCb_Online_overview_ID_108.odp", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/108\/attachments\/578496\/796628\/LHCb_Online_overview.odp", "fileName": "LHCb_Online_overview.odp", "_fossil": "localFileMetadata", "id": "796628", "_deprecated": true}], "_deprecated": true}, {"_type": "Material", "title": "Slides -- PDF", "_fossil": "materialMetadata", "id": "578495", "resources": [{"_type": "LocalFile", "name": "LHCb_Online_overview_ID_108.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/108\/attachments\/578495\/796627\/LHCb_Online_overview.pdf", "fileName": "LHCb_Online_overview.pdf", "_fossil": "localFileMetadata", "id": "796627", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "b17e687b3ad3b64726f8af4d26ec7a0d", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "JOST, Beat", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "9f655ee0d367b6896f4dd3a37228be6f", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "NEUFELD, Niko", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "cfb82256b7e6e29c354aef2d673dfaad", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GASPAR, Clara", "id": "3"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/108", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:30:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "109", "speakers": [{"_type": "ContributionParticipation", "emailHash": "fdf72c50739c48378b9f205d84cef90d", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MAGNONI, Luca", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "fdf72c50739c48378b9f205d84cef90d", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MAGNONI, Luca", "id": "0"}], "title": "Monitoring WLCG with lambda-architecture: a new scalable data store and analytics platform for monitoring at petabyte scale.", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:16:02.292516+00:00", "description": "", "title": "2015_CHEP_WDT.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/109\/attachments\/578497\/796629\/2015_CHEP_WDT.pdf", "filename": "2015_CHEP_WDT.pdf", "content_type": "application\/pdf", "type": "file", "id": 796629, "size": 1366306}, {"_type": "attachment", "modified_dt": "2015-04-13T00:16:02.292516+00:00", "description": "", "title": "2015_CHEP_WDT.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/109\/attachments\/578497\/796630\/2015_CHEP_WDT.pptx", "filename": "2015_CHEP_WDT.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796630, "size": 2674996}], "title": "Slides", "default_folder": false, "id": 578497, "description": ""}], "_type": "Contribution", "description": "Monitoring the WLCG infrastructure requires to gather and to analyze high volume of heterogeneous data (e.g. data transfers, job monitoring, site tests) coming from different services and experiment-specific frameworks to provide a uniform and flexible interface for scientists and sites. The current architecture, where relational database systems are used to store, to process and to serve monitoring data, has limitations in coping with the foreseen extension of the volume (e.g. higher LHC luminosity) and the variety (e.g. new data-transfer protocols and new resource-types, as cloud-computing) of WLCG monitoring events. This paper presents a new scalable data store and analytics platform designed by the Support for Distributed Computing (SDC) group, at the CERN IT department, which leverages on a stack of technology each one targeting specific aspects on big-scale distributed data-processing (commonly referred as lambda-architecture approach). Results on data processing on Hadoop for WLCG data transfers are presented, showing how the new architecture can easily analyze hundreds of millions of transfer logs in few minutes. Moreover, a comparison on data partitioning, compression and file format (e.g. CSV, AVRO) is presented, with particular attention on how the file structure impacts the overall MapReduce performance. In conclusion, the evolution of the current implementation, which focuses on data store and batch processing, towards a complete lambda-architecture is discussed, with consideration on candidate technology for the serving layer (e.g. ElasticSearch) and a description of a proof of concept implementation, based on Esper, for the real-time part which compensates for batch-processing latency and automates problem detection and failures.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578497", "resources": [{"_type": "LocalFile", "name": "2015_CHEP_WDT.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/109\/attachments\/578497\/796629\/2015_CHEP_WDT.pdf", "fileName": "2015_CHEP_WDT.pdf", "_fossil": "localFileMetadata", "id": "796629", "_deprecated": true}, {"_type": "LocalFile", "name": "2015_CHEP_WDT.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/109\/attachments\/578497\/796630\/2015_CHEP_WDT.pptx", "fileName": "2015_CHEP_WDT.pptx", "_fossil": "localFileMetadata", "id": "796630", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "dd2d05df8869eb91f302ce4046407f24", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ANDREEVA, Julia", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "50cc9f97da3f59afc0ca5bdda4933032", "affiliation": "Brunel University", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. SUTHAKAR, Uthayanath", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "eaade44dc93d15a83841cd2d3c3e1b5a", "affiliation": "Brunel University", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. KHAN, Akram", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "4df5de7fc4854e11b91e21b89f507ed8", "affiliation": "Brunel University", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. SMITH, David", "id": "4"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/109", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "102", "speakers": [{"_type": "ContributionParticipation", "emailHash": "22a1a3e2ab267299b1754df69296edaf", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "DAL PRA, Stefano", "id": "17"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6a3e34881b3c1179c63321967d015d71", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. DORIGO, Alvise", "id": "0"}], "title": "Implementation and use of an highly available and innovative IaaS solution: the Cloud Area Padovana", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-07T10:58:48.907592+00:00", "description": "", "title": "CLOUD_CHEP_2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/102\/attachments\/578498\/796631\/CLOUD_CHEP_2015.pdf", "filename": "CLOUD_CHEP_2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796631, "size": 1283872}], "title": "Poster", "default_folder": false, "id": 578498, "description": ""}], "_type": "Contribution", "description": "While in the business world the cloud paradigm is typically implemented purchasing resources and services from third party providers (e.g. Amazon), in the scientific environment there's usually the need of on-premises IaaS infrastructures which allow efficient usage of the hardware distributed among (and owned by) different scientific administrative domains. In addition, the requirement of open source adoption has leaded to the choice of products like OpenStack by many organizations.\r\n\r\nWe describe a use case of the Italian National Institute for Nuclear Physics (INFN) which resulted in the implementation of a unique Cloud service, called \"Cloud Area Padovana\", which encompasses resources spread in two different sites: the INFN Legnaro National Laboratories and the INFN Padova division.\r\nWe describe how this IaaS has been implemented, which technologies have been adopted, how services have been configured in high-availability (HA) mode.\r\n\r\nWe also discuss how identity and authorization management were implemented, adopting a widely accepted standard architecture based on SAML2 and OpenID: by leveraging the versatility of those standards the integration with authentication federations like IDEM was implemented. \r\n\r\nWe also discuss some other innovative developments, such as a pluggable scheduler, implemented as extension of the native OpenStack scheduler, which allows to allocate resources according to a fair-share based model and which provides a persistent queuing mechanism for handling user requests that can not be immediately served. \r\n\r\nTools, technologies, procedures used to install, configure, monitor, operate this Cloud service are also discussed.\r\n\r\nFinally we present some examples that show how this IaaS infrastructure is being used.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578498", "resources": [{"_type": "LocalFile", "name": "CLOUD_CHEP_2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/102\/attachments\/578498\/796631\/CLOUD_CHEP_2015.pdf", "fileName": "CLOUD_CHEP_2015.pdf", "_fossil": "localFileMetadata", "id": "796631", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "66f5612a209b211cc69329cdd189be94", "affiliation": "INFN, on leave of absence from \"Horia Hulubei\" National Institute for Physics and Nuclear Engineering (IFIN-HH)", "_fossil": "contributionParticipationMetadata", "fullName": "AIFTIMIEI, Doina Cristina", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "add96a383a739d73208d6aefa7430694", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "MICHELOTTO, Michele", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "80025860f8e9f499fddcf594bae0478e", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. SGARAVATTO, Massimo", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "dcdc6c3fc54ae64ab01d3428b39d80a4", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "TRALDI, Sergio", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "af8994c6e7ecac9e5ff886aa5a492b24", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. VERLATO, Marco", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "5385b4ff0a4c33c34a546e18ee4af04f", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "ZANGRANDO, Lisa", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "a10c24187b34f4f6db3612696496e9c5", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "FANZAGO, Federica", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "dade965cbbc7ace80c71c3d71ca3172b", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "VENARUZZO, Massimo", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "c601d542ec61b4913cc99f4ea43a5bce", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "ANDREETTO, Paolo", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "07c67c911ba77aada1e3241e8a8ac9dd", "affiliation": "INFN-PD", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BERTOCCO, sara", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "8141a52beae01684a52f9015b7b96390", "affiliation": "INFN Legnaro", "_fossil": "contributionParticipationMetadata", "fullName": "BIASOTTO, Massimo", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "43809912347f65178cae62d5aba8f6bc", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "CRESCENTE, Alberto", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "aeac1a71feccc2e7f8150a76a327ab51", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "COSTA, Fulvia", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "ddf3876cf8cfaef06be0159cdb9969ee", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "FANTINEL, Sergio", "id": "14"}, {"_type": "ContributionParticipation", "emailHash": "bdc22680105e112af4650f77c51b2c2b", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "FRIZZIERO, Eric", "id": "15"}, {"_type": "ContributionParticipation", "emailHash": "9bf297ed5f19508e7bc907d82ded343a", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "GULMINI, Michele", "id": "16"}, {"_type": "ContributionParticipation", "emailHash": "22a1a3e2ab267299b1754df69296edaf", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "DAL PRA, Stefano", "id": "17"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/102", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "103", "speakers": [{"_type": "ContributionParticipation", "emailHash": "55c4450776345424a4a88744e366a05a", "affiliation": "Austrian Academy of Sciences (AT)", "_fossil": "contributionParticipationMetadata", "fullName": "WALTENBERGER, Wolfgang", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "55c4450776345424a4a88744e366a05a", "affiliation": "Austrian Academy of Sciences (AT)", "_fossil": "contributionParticipationMetadata", "fullName": "WALTENBERGER, Wolfgang", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "6e850c18c3973e616499d48fd036d9f1", "affiliation": "Austrian Academy of Sciences (AT)", "_fossil": "contributionParticipationMetadata", "fullName": "KULKARNI, Suchita", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "d924379ce09a09d320031cb68c4b250a", "affiliation": "Centre National de la Recherche Scientifique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "KRAML, Sabine", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "aa2c5347986adfe88a5b804d951e75e7", "affiliation": "Austrian Academy of Sciences (AT)", "_fossil": "contributionParticipationMetadata", "fullName": "LAA, Ursula", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "cfbfe66e462d0837ac8ad479f465f5cb", "affiliation": "IFGW - UNICAMP", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. LESSA, Andre", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "0c810b98e85b3a1c574dfb9022af4e78", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "MAGERL, Wolfgang", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "63eb48d4ee4f65f848477f9171a2678f", "affiliation": "Austrian Academy of Sciences (AT)", "_fossil": "contributionParticipationMetadata", "fullName": "MAGERL, Veronika", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "9cd9bb94e2bdf07e8f9ec9afcfe1985d", "affiliation": "HEPHY Vienna", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. TRAUB, Michael", "id": "7"}], "title": "SModelS: a framework to confront theories beyond the standard model with experimental data", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "We present a general procedure to decompose Beyond the Standard Model (BSM) collider signatures into Simplified Model Spectrum (SMS) topologies. Our method provides a way to cast BSM predictions for the LHC in a model independent framework, which can be directly confronted with the relevant experimental constraints.  Our python implementation currently focusses on supersymmetry searches with missing energy, for which a large\r\nvariety of SMS results from ATLAS and CMS are available.\r\nWe have collected these results into a database, enabling us to quickly confront any BSM theory with about 200 simplified models results.\r\nOriginating in the LHC world we wish to extend beyond hadron colliders, exploiting also the information about BSM physics contained in precision measurements, or dark matter searches.\r\nAs show-case examples we will discuss application of our procedure to one specific supersymmetric model, show how the limits constrain the model, and point out regions in parameter space still unchallenged by the current SMS results.\r\nGiven experimental hints at new physics, it is our ultimate goal to build the next standard model in a bottom-up fashion from the experimental SMS results\r\nfrom several experiments.", "track": "Track2: Offline software ", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/103", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "100", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6fef2a09accf1a707bc4c992c15a8438", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KLIMENTOV, Alexei", "id": "4"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c8b8647f9e6a89a40a3aae5e517a0c30", "affiliation": "ATLAS", "_fossil": "contributionParticipationMetadata", "fullName": "VANIACHINE, Alexandre", "id": "0"}], "title": "Scaling up ATLAS production system for the LHC Run 2 and beyond: project ProdSys2", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The Big Data processing needs of the ATLAS experiment grow continuously, as more data and more use cases emerge. For Big Data processing the ATLAS experiment adopted the data transformation approach, where software applications transform the input data into outputs. In the ATLAS production system, each data transformation is represented by a task, a collection of many jobs, submitted by the ATLAS workload management system (PanDA) and executed on the Grid.\r\n\r\nOur experience shows that the rate of tasks submission grows exponentially over the years. To scale up the ATLAS production system for new challenges, we started the ProdSys2 project. PanDA has been upgraded with the Job Execution and Definition Interface (JEDI). Patterns in ATLAS data transformation workflows composed of many tasks, provided a scalable production system framework for template definitions of the many-tasks workflows. These workflows are being implemented in the Database Engine for Tasks (DEfT) that generates individual tasks for processing by JEDI.\r\n\r\nWe report on the ATLAS experience with many-task workflow patterns in preparation for the LHC Run-2.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "d69f5fff808f8ac8ab6376496b16ae5a", "affiliation": "National Research Nuclear  University MEPhI (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "BORODIN, Misha", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "e6819ee253837bc1a19f5e93c8ef001e", "affiliation": "University of Texas at Arlington (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DE, Kaushik", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "77e6974a25a29ef6da9a14157c570471", "affiliation": "Instituto de Fisica Corpuscular (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "GARCIA NAVARRO, Jose Enrique", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "6fef2a09accf1a707bc4c992c15a8438", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KLIMENTOV, Alexei", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "2e6780d520127502454830ec573033ae", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MAENO, Tadashi", "id": "5"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/100", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:15:00"}, "duration": 15, "session": "Track 8 Session", "keywords": [], "id": "101", "speakers": [{"_type": "ContributionParticipation", "emailHash": "c2b59884db53cad32a6976fc79e1bb92", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. SZOSTEK, Pawel", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c2b59884db53cad32a6976fc79e1bb92", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. SZOSTEK, Pawel", "id": "0"}], "title": "Evaluating the power efficiency and performance of multi-core platforms using HEP workloads", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T06:38:48.298284+00:00", "description": "", "title": "chep2015_haswell_szostek.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/101\/attachments\/578499\/796632\/chep2015_haswell_szostek.pdf", "filename": "chep2015_haswell_szostek.pdf", "content_type": "application\/pdf", "type": "file", "id": 796632, "size": 861326}], "title": "Slides", "default_folder": false, "id": 578499, "description": ""}], "_type": "Contribution", "description": "As Moore's Law drives the silicon industry towards higher transistor counts, processor designs are becoming more and more complex. The area of development includes core count, execution ports, vector units, uncore architecture and finally instruction sets. This increasing complexity leads us to a place where access to the shared memory is the major limiting factor, making feeding the cores with data a real challenge. On the other hand, the significant focus on power efficiency paves the way for power-aware computing and less complex architectures to data centers. In this paper we try to examine these trends and present results of our experiments with \"Haswell-EP\" processor family and highly scalable HEP workloads.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578499", "resources": [{"_type": "LocalFile", "name": "chep2015_haswell_szostek.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/101\/attachments\/578499\/796632\/chep2015_haswell_szostek.pdf", "fileName": "chep2015_haswell_szostek.pdf", "_fossil": "localFileMetadata", "id": "796632", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "5f08665ebee9ba0e427cca5060ff023a", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. INNOCENTE, Vincenzo", "id": "1"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/101", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "107", "speakers": [{"_type": "ContributionParticipation", "emailHash": "059c67f423c94e588bce78fb5a617463", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GIRONE, Maria", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "059c67f423c94e588bce78fb5a617463", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GIRONE, Maria", "id": "0"}], "title": "Investigating machine learning to classify events in CMS", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "High energy physics experiments are experiencing a growth in the number of collected and processed events that exceeds the rate of growth in computing resources sustainable by technology improvements at a flat yearly cost. This trend is expected to continue into the foreseeable future, and as the field is not expecting a big increase in support, innovative approaches are needed. In areas of science as diverse as biology and cosmology groups are deploying advanced machine learning techniques to learn to classify events based on rules and trained outcomes. High energy physics has used similar decision techniques at the analysis step, and has tried a few specific demonstrations as challenges. In this presentation we present the early exploratory work in CMS to utilize machine learning techniques for real-time event classification for triggering and analysis. The early performance and next steps will be outlined.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/107", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:45:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "104", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6049ebfaa694e419b61cfd107f553289", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FIELD, Laurence", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6049ebfaa694e419b61cfd107f553289", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FIELD, Laurence", "id": "0"}], "title": "CMS@Home: Enabling Volunteer Computing Usage for CMS", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T14:55:06.755753+00:00", "description": "", "title": "2015_CHEP_CMS.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/104\/attachments\/578500\/796633\/2015_CHEP_CMS.pdf", "filename": "2015_CHEP_CMS.pdf", "content_type": "application\/pdf", "type": "file", "id": 796633, "size": 1039489}, {"_type": "attachment", "modified_dt": "2015-04-13T14:55:06.755753+00:00", "description": "", "title": "2015_CHEP_CMS.ppt", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/104\/attachments\/578500\/796634\/2015_CHEP_CMS.ppt", "filename": "2015_CHEP_CMS.ppt", "content_type": "application\/vnd.ms-powerpoint", "type": "file", "id": 796634, "size": 2878976}], "title": "Slides", "default_folder": false, "id": 578500, "description": ""}], "_type": "Contribution", "description": "Volunteer computing remains an untapped opportunistic resource for the LHC experiments. The use of virtualization in this domain was pioneered by the Test4theory project and enabled the running of high-energy particle physics simulations on home computers. This paper describes the model for CMS to run workloads using a similar volunteer computing platform. It is shown how the original approach is explored to map onto the existing CMS workflow and identifies missing functionality along with the components and changes that are required. The final implementation of the prototype is detailed along with the identification of areas that would benefit from further development.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578500", "resources": [{"_type": "LocalFile", "name": "2015_CHEP_CMS.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/104\/attachments\/578500\/796633\/2015_CHEP_CMS.pdf", "fileName": "2015_CHEP_CMS.pdf", "_fossil": "localFileMetadata", "id": "796633", "_deprecated": true}, {"_type": "LocalFile", "name": "2015_CHEP_CMS.ppt", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/104\/attachments\/578500\/796634\/2015_CHEP_CMS.ppt", "fileName": "2015_CHEP_CMS.ppt", "_fossil": "localFileMetadata", "id": "796634", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "5bb61aa06a920de51fb738bb3f912b6a", "affiliation": "Heidelberg University", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. BORRAS, Hendrik", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "7b97af96ccc24f7684d7b1be0f2f605c", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SPIGA, Daniele", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "d4502e02b88247265f7423d8fafc6ba3", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "RIAHI, Hassen", "id": "3"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/104", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:15:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "105", "speakers": [{"_type": "ContributionParticipation", "emailHash": "194cc8606836918939ab2dc5607b5002", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "STANCU, Stefan Nicolae", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c99110761f958279ccba0d51deb0af5e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MARTELLI, Edoardo", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "194cc8606836918939ab2dc5607b5002", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "STANCU, Stefan Nicolae", "id": "1"}], "title": "LHCOPN and LHCONE: Status and Future Evolution", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T03:25:54.927010+00:00", "description": "", "title": "CHEP2015-LHCOPN-LHCONE.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/105\/attachments\/578501\/796635\/CHEP2015-LHCOPN-LHCONE.pdf", "filename": "CHEP2015-LHCOPN-LHCONE.pdf", "content_type": "application\/pdf", "type": "file", "id": 796635, "size": 4092845}], "title": "Slides", "default_folder": false, "id": 578501, "description": ""}], "_type": "Contribution", "description": "The LHC Optical Private Network, linking CERN and the Tier1s and the LHC Open Network Environment linking these to the Tier2 community successfully supported the data transfer needs of the LHC community during Run 1 and have evolved to better serve the networking requirements of the new computing models for Run 2. We present here the current status and the key changes, notably  the delivered and planned bandwidth increases, the ongoing work to better address the needs of the Asia-Pacific region, developments to improve redundancy provision and progress made for the provision of point-to-point links.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578501", "resources": [{"_type": "LocalFile", "name": "CHEP2015-LHCOPN-LHCONE.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/105\/attachments\/578501\/796635\/CHEP2015-LHCOPN-LHCONE.pdf", "fileName": "CHEP2015-LHCOPN-LHCONE.pdf", "_fossil": "localFileMetadata", "id": "796635", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/105", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:00:00"}, "duration": 15, "session": "Track 8 Session", "keywords": [], "id": "39", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2368fbf6eeafdccf7f1cf1924e2bde61", "affiliation": "Southern Methodist University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KAMA, Sami", "id": "11"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2368fbf6eeafdccf7f1cf1924e2bde61", "affiliation": "Southern Methodist University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KAMA, Sami", "id": "11"}], "title": "Triggering events with GPUs at ATLAS", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T13:47:10.628769+00:00", "description": "", "title": "ATL-DAQ-SLIDE-2015-167.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/39\/attachments\/578502\/796636\/ATL-DAQ-SLIDE-2015-167.pdf", "filename": "ATL-DAQ-SLIDE-2015-167.pdf", "content_type": "application\/pdf", "type": "file", "id": 796636, "size": 2165639}, {"_type": "attachment", "modified_dt": "2015-04-09T13:47:10.628769+00:00", "description": "", "title": "ATL-DAQ-SLIDE-2015-167.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/39\/attachments\/578502\/796637\/ATL-DAQ-SLIDE-2015-167.pptx", "filename": "ATL-DAQ-SLIDE-2015-167.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796637, "size": 2011639}], "title": "Slides", "default_folder": false, "id": 578502, "description": ""}], "_type": "Contribution", "description": "The growing size and complexity of events produced at the high luminosities expected in 2015 at the Large Hadron Collider demands much more computing power for the online event selection and for the offline data reconstruction than in the previous data taking period. In recent years, the explosive performance growth of low-cost, massively parallel processors like Graphical Processing Units (GPUs) - both in computing power and in low energy consumption - make GPUs extremely attractive for solving the challenging computing tasks of high energy physics experiment like ATLAS.\r\n\r\nAfter the optimisation of the computing intensive algorithms and their adaptation to GPUs, thus exploiting the paradigm of massively parallel computing, a small scale prototype of the full reconstruction chain of the ATLAS High Level Trigger under inclusion of the GPU-based algorithms has been implemented. We discuss the integration procedure of this prototype, the achieved performance and the prospects for the future.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578502", "resources": [{"_type": "LocalFile", "name": "ATL-DAQ-SLIDE-2015-167.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/39\/attachments\/578502\/796636\/ATL-DAQ-SLIDE-2015-167.pdf", "fileName": "ATL-DAQ-SLIDE-2015-167.pdf", "_fossil": "localFileMetadata", "id": "796636", "_deprecated": true}, {"_type": "LocalFile", "name": "ATL-DAQ-SLIDE-2015-167.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/39\/attachments\/578502\/796637\/ATL-DAQ-SLIDE-2015-167.pptx", "fileName": "ATL-DAQ-SLIDE-2015-167.pptx", "_fossil": "localFileMetadata", "id": "796637", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "d41bd1658b513d010496cace90265897", "affiliation": "LIP\/FCUL", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. SOARES AUGUSTO, Jose", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "ee92c40a897713aa60e98f857dc82844", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "BAINES, John", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "2d89d3a5af5e22cf26562136eb0a34ee", "affiliation": "Universita e INFN, Roma I (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "BAUCE, Matteo", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "c86400decf25d81fa6a9e609c133e417", "affiliation": "AGH Univ. of Science and Technology, Krakow", "_fossil": "contributionParticipationMetadata", "fullName": "BOLD, Tomasz", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "4abe74caf9843ad5eb43ef5a82cb1554", "affiliation": "LIP Laboratorio de Instrumentacao e Fisica Experimental de Part", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CONDE MUINO, Patricia", "id": "15"}, {"_type": "ContributionParticipation", "emailHash": "48fc97175b286e7f326f568f69c786db", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "EMELIYANOV, Dmitry", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "7b56b9784e587496e4648fbeeaecb1fb", "affiliation": "LIP Laboratorio de Instrumentacao e Fisica Experimental de Part", "_fossil": "contributionParticipationMetadata", "fullName": "MORAIS SILVA GONCALO, Ricardo Jose", "id": "16"}, {"_type": "ContributionParticipation", "emailHash": "1cb3a94196ae56c0f667dfb766f3b074", "affiliation": "Universita e INFN, Roma I", "_fossil": "contributionParticipationMetadata", "fullName": "MESSINA, Andrea", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "a0eb411490667251255698d1d438f5c0", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "NEGRINI, Matteo", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "638ed7cc0e24fefc30cd36c3407d22e8", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "RINALDI, Lorenzo", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "5457d4d1d55b5d8ae3f57566c84a047c", "affiliation": "INFN Bologna (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "SIDOTI, Antonio", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "4f333199a095735d3fcf2d51e71d9bf9", "affiliation": "LIP Laboratorio de Instrumentacao e Fisica Experimental de Part", "_fossil": "contributionParticipationMetadata", "fullName": "TAVARES DELGADO, Ademar", "id": "17"}, {"_type": "ContributionParticipation", "emailHash": "954de3824e10206fe930de06ac02aa50", "affiliation": "Universita e INFN, Bologna (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. TUPPUTI, Salvatore", "id": "18"}, {"_type": "ContributionParticipation", "emailHash": "16ca27e8aff7af9b1f41a5050890f81c", "affiliation": "LIP Laboratorio de Instrumentacao e Fisica Experimental de Part", "_fossil": "contributionParticipationMetadata", "fullName": "VAZ GIL LOPES, Lourenco", "id": "19"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/39", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": null, "keywords": [], "id": "38", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2368fbf6eeafdccf7f1cf1924e2bde61", "affiliation": "Southern Methodist University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KAMA, Sami", "id": "8"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2368fbf6eeafdccf7f1cf1924e2bde61", "affiliation": "Southern Methodist University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KAMA, Sami", "id": "8"}], "title": "Tracking with GPGPUs in the ATLAS experiment", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The ATLAS experiment has been designed to analyse proton-proton and heavy ion collision events produced at the LHC at a very high rate, both for the online trigger selection and during offline data reconstruction. Considering the future detector upgrades, demands on computing power will become even higher and new paradigms must be adopted for fast data processing. General Purpose Graphical Processing Units (GPGPU) can be used in a novel approach based on massive parallel computing. The immense computation power provided by GPGPUs is expected to reduce the computation time and speed-up low-latency applications for fast decision taking.\r\n\r\nIn this contribution we show how GPGPU based computing can be used for tracking applications: we discuss track finding, fitting and reconstruction algorithms, both for real time triggering and offline reconstruction purposes. The performance of parallel algorithms is compared to that of currently used tracking methods. Their interface to the ATLAS CPU based computing framework is presented.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "76e76ecd313576ea6b075126598ea412", "affiliation": "INFN CNAF", "_fossil": "contributionParticipationMetadata", "fullName": "RINALDI, Lorenzo", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "bb7ce5449c21b8078fa74b1a9455553e", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "DI SIPIO, Riccardo", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "a0eb411490667251255698d1d438f5c0", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "NEGRINI, Matteo", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "3586c15302f05ccf0ab407ffab003e6d", "affiliation": "Dipartimento di Fisica-Universita degli Studi di Bologna-Univers", "_fossil": "contributionParticipationMetadata", "fullName": "GABRIELLI, Alessandro", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "5457d4d1d55b5d8ae3f57566c84a047c", "affiliation": "Universita e INFN, Roma I (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "SIDOTI, Antonio", "id": "5"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/contribution\/38", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "33", "speakers": [{"_type": "ContributionParticipation", "emailHash": "fd0f8bf65da577a884b4bb1260da8d60", "affiliation": "New York University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "HEINRICH, Lukas Alexander", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "fd0f8bf65da577a884b4bb1260da8d60", "affiliation": "New York University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "HEINRICH, Lukas Alexander", "id": "1"}], "title": "The ATLAS Trigger Core Configuration and Execution System in Light of the ATLAS Upgrade for LHC Run 2", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T07:34:47.057590+00:00", "description": "", "title": "ATL-DAQ-SLIDE-2015-151.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/33\/attachments\/578503\/796638\/ATL-DAQ-SLIDE-2015-151.pdf", "filename": "ATL-DAQ-SLIDE-2015-151.pdf", "content_type": "application\/pdf", "type": "file", "id": 796638, "size": 1056371}], "title": "Slides", "default_folder": false, "id": 578503, "description": ""}], "_type": "Contribution", "description": "During the 2013\/14 shutdown of the Large Hadron Collider (LHC) the\r\nATLAS first level trigger (L1T) and the data acquisition system (DAQ)\r\nwere substantially upgraded to cope with the increase in luminosity\r\nand collision multiplicity, expected to be delivered by the LHC in\r\n2015.\r\n\r\nTo name a few, the L1T was extended on the calorimeter side (L1Calo)\r\nto better cope with pile-up and apply better-tuned isolation criteria\r\non electron, photon, and jet candidates. The central trigger (CT) was\r\nwidened to analyze twice as many inputs, provide more trigger lines,\r\nand serve multiple sub-detectors in parallel during calibration\r\nperiods. A new FPGA-based trigger, capable of analyzing event\r\ntopologies at 40 MHz, was added to provide further input to forming\r\nthe level 1 trigger decision (L1Topo). On the DAQ side the dataflow\r\nwas completely remodeled, merging the two previously existing stages\r\nof the software-based high level trigger into one.\r\n\r\nPartially because of these changes, partially because of the new\r\ntrigger paradigm to have more full event analysis, the high level\r\ntrigger (HLT) execution framework and the trigger configuration system\r\nhad to be upgraded, tools and data content had to be adapted to the\r\nnew ATLAS analysis model. In this paper we describe this work:\r\n\r\nThe algorithm execution framework was changed to seamlessly work\r\nwithin the merged HLT, the data access providers were adapted to the\r\nnew dataflow. The event building, at which point all data are\r\nretrieved from the readout system, can now dynamically change with\r\nprogressing event feature extraction, allowing a more flexible\r\nadjustment to dataflow constraints. The cost monitoring framework\r\nwhich analyzes data access and CPU consumption, even prior to data\r\ntaking, was extended to work within the merged system, several other\r\nimprovements followed.\r\n\r\nThe HLT execution was moved to a memory-saving multi-process\r\napplication, in which many event processors are forked after the\r\nsystem configuration. They thus share common data such as geometry and\r\nconditions information, leading to a dramatic reduction in the overall\r\nmemory consumption. Compared to Run 1 many more event processors can\r\nrun on each machine.\r\n\r\nUpon request from the ATLAS physics groups a new kind of data stream\r\nwas implement, in which only a small subset of the reconstructed\r\ntrigger objects and no detector data is stored. Since the trigger\r\nreconstruction in Run 2 almost compares to the offline in resolution,\r\nthese data make search analyses that require high statistics feasible.\r\n\r\nAs a consequence of these changes, the new ATLAS data model, and the\r\nnew dual environment analysis approach, the tools that are provided\r\nfor trigger aware analysis had to be completely restructured. In\r\nparticular the reduction and specialization of data content in derived\r\ndata sets was posing a challenge for the trigger, a new trigger data\r\nslimming was invented.\r\n\r\nThe database driven trigger configuration system, which describes the\r\nphysics implemented at L1 and HLT, needs to reflect all changes in the\r\nL1 and HLT system. It now incorporates the configuration for the new\r\nL1Topo trigger, has extended the configuration capabilities of the\r\nL1Calo and CT and the describes are merged HLT. A new system for\r\nautomatically adjust the trigger prescale factors to the dropping\r\nluminosity during a run was devised and implemented.\r\n\r\nWe also present measurements of the trigger execution on first data\r\nwith the new ATLAS trigger algorithms and selection.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578503", "resources": [{"_type": "LocalFile", "name": "ATL-DAQ-SLIDE-2015-151.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/33\/attachments\/578503\/796638\/ATL-DAQ-SLIDE-2015-151.pdf", "fileName": "ATL-DAQ-SLIDE-2015-151.pdf", "_fossil": "localFileMetadata", "id": "796638", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/33", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "32", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2ee550f7b26f5c9b2c89dd9cef7d9640", "affiliation": "Kobe University (JP)", "_fossil": "contributionParticipationMetadata", "fullName": "SHIMIZU, Shima", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2ee550f7b26f5c9b2c89dd9cef7d9640", "affiliation": "Kobe University (JP)", "_fossil": "contributionParticipationMetadata", "fullName": "SHIMIZU, Shima", "id": "1"}], "title": "ATLAS Jet Trigger Performance in LHC Run I and Initial Run II Updates", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T04:26:31.341153+00:00", "description": "", "title": "ATL-DAQ-SLIDE-2015-148.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/32\/attachments\/578504\/796639\/ATL-DAQ-SLIDE-2015-148.pdf", "filename": "ATL-DAQ-SLIDE-2015-148.pdf", "content_type": "application\/pdf", "type": "file", "id": 796639, "size": 2406390}], "title": "Poster", "default_folder": false, "id": 578504, "description": ""}], "_type": "Contribution", "description": "The immense rate of proton-proton collisions at the Large Hadron\r\nCollider (LHC) must be reduced from the nominal bunch-crossing rate of\r\n40 MHz to approximately 1 kHz before the data can be written on disk\r\noffline. The ATLAS Trigger System performs real-time selection of\r\nthese events in order to achieve this reduction. Dedicated selection\r\nof events containing jets is uniquely challenging at a hadron collider\r\nwhere nearly every event contains significant hadronic energy.\r\n\r\nFollowing the very successful first LHC run from 2010 to 2012, the\r\nATLAS Trigger was much improved, including a new hardware topological\r\nmodule and a restructured High Level Trigger system, merging two\r\nprevious software-based processing levels. This allowed the\r\noptimization of resources and a much greater re-use of the precise but\r\ncostly offline software base. After summarising the overall\r\nperformance of the jet trigger during the first LHC run, the software\r\ndesign choices and use of the topological module will be reviewed. The\r\nexpected performance of jet trigger for the second LHC run, to start\r\nin 2015, will be described together with the available commissioning\r\nmeasurements from early data taking.", "track": "Track1: Online computing ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578504", "resources": [{"_type": "LocalFile", "name": "ATL-DAQ-SLIDE-2015-148.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/32\/attachments\/578504\/796639\/ATL-DAQ-SLIDE-2015-148.pdf", "fileName": "ATL-DAQ-SLIDE-2015-148.pdf", "_fossil": "localFileMetadata", "id": "796639", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/32", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:45:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "31", "speakers": [{"_type": "ContributionParticipation", "emailHash": "c1974455161e0de67549b48656327b44", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GLATZER, Julian", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c1974455161e0de67549b48656327b44", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GLATZER, Julian", "id": "1"}], "title": "Operation of the upgraded ATLAS Level-1 Central Trigger System", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T09:57:26.424402+00:00", "description": "", "title": "CHEP_JulianGlatzer.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/31\/attachments\/578505\/796640\/CHEP_JulianGlatzer.pdf", "filename": "CHEP_JulianGlatzer.pdf", "content_type": "application\/pdf", "type": "file", "id": 796640, "size": 4952239}], "title": "Slides", "default_folder": false, "id": 578505, "description": ""}], "_type": "Contribution", "description": "The ATLAS Level-1 Central Trigger (L1CT) system is a central part of\r\nATLAS data-taking and is configured, controlled, and monitored by a\r\nsoftware framework with emphasis on reliability and flexibility. The\r\nhardware has undergone a major upgrade for Run 2 of the LHC, in order\r\nto cope with the expected increase of instantaneous luminosity of a\r\nfactor of 2 with respect to Run 1. It offers more flexibility in the\r\ntrigger decisions due to the double amount of trigger inputs and\r\nusable trigger channels. It also provides an interface to the new\r\ntopological trigger system. Operationally - particularly useful for\r\ncommissioning, calibration and test runs - it allows concurrent,\r\nindependent triggering of up to 3 different sub-detector combinations.\r\n\r\nIn this contribution, we give an overview of the fully operational\r\nsoftware framework of the L1CT system with particular emphasis on the\r\nconfiguration, controls and monitoring aspects. The software framework\r\nallows the L1CT system to be configured consistently with the ATLAS\r\nexperiment and the LHC machine, upstream and downstream trigger\r\nprocessors, and the data acquisition. Trigger and dead-time rates are\r\nmonitored coherently at all stages of processing and are logged by the\r\nonline computing system for physics analysis, data quality assurance\r\nand operation debugging. In addition, the synchronisation of trigger\r\ninputs is watched based on bunch-by-bunch trigger information. Several\r\nsoftware tools allow to efficiently display the relevant information\r\nin the control room in a way useful for shifters and experts. The\r\ndesign of the framework aims at reliability, flexiblity, and\r\nrobustness of the system and takes into account the operational\r\nexperience gained during Run 1. We present the overall performance\r\nduring cosmic-ray data taking with the full ATLAS detector and the\r\nexperience with first beams in 2015.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578505", "resources": [{"_type": "LocalFile", "name": "CHEP_JulianGlatzer.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/31\/attachments\/578505\/796640\/CHEP_JulianGlatzer.pdf", "fileName": "CHEP_JulianGlatzer.pdf", "_fossil": "localFileMetadata", "id": "796640", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/31", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:45:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:00:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "30", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a47482d71302e854c450255d360e2df8", "affiliation": "High Energy Accelerator Research Organization (JP)", "_fossil": "contributionParticipationMetadata", "fullName": "HIGUCHI, Yu", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "a47482d71302e854c450255d360e2df8", "affiliation": "High Energy Accelerator Research Organization (JP)", "_fossil": "contributionParticipationMetadata", "fullName": "HIGUCHI, Yu", "id": "1"}], "title": "The ATLAS Trigger System: Ready for Run-2", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T19:48:17.064030+00:00", "description": "", "title": "YuNakahama_ATL-DAQ-SLIDE-2015-162.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/30\/attachments\/578506\/796641\/YuNakahama_ATL-DAQ-SLIDE-2015-162.pdf", "filename": "YuNakahama_ATL-DAQ-SLIDE-2015-162.pdf", "content_type": "application\/pdf", "type": "file", "id": 796641, "size": 4256862}], "title": "Slides", "default_folder": false, "id": 578506, "description": ""}], "_type": "Contribution", "description": "The ATLAS trigger has been used very successfully for the online event\r\nselection during the first run of the LHC between 2009-2013 at a\r\ncentre-of-mass energy between 900 GeV and 8 TeV. The trigger system\r\nconsists of a hardware Level-1 (L1) and a software based high-level\r\ntrigger (HLT) that reduces the event rate from the design\r\nbunch-crossing rate of 40 MHz to an average recording rate of a few\r\nhundred Hz. During the next data-taking period starting in early 2015\r\n(Run-2) the LHC will operate at a centre-of-mass energy of about 13\r\nTeV resulting in roughly five times higher trigger rates.\r\n\r\nWe will review the upgrades to the ATLAS Trigger system that have been\r\nimplemented during the shutdown and that will allow us to cope with\r\nthese increased trigger rates while maintaining or even improving our\r\nefficiency to select relevant physics processes. This includes changes\r\nto the L1 calorimeter trigger, the introduction of a new L1\r\ntopological trigger module, improvements in the L1 muon system and the\r\nmerging of the previously two-level HLT system into a single event\r\nfilter farm. At hand of a few examples, we will show the impressive\r\nperformance improvements in the HLT trigger algorithms used to\r\nidentify leptons, hadrons and global event quantities like missing\r\ntransverse energy. And finally, we will summarize the commissioning\r\nstatus of the Trigger system in view of the imminent restart of\r\ndata-taking.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578506", "resources": [{"_type": "LocalFile", "name": "YuNakahama_ATL-DAQ-SLIDE-2015-162.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/30\/attachments\/578506\/796641\/YuNakahama_ATL-DAQ-SLIDE-2015-162.pdf", "fileName": "YuNakahama_ATL-DAQ-SLIDE-2015-162.pdf", "_fossil": "localFileMetadata", "id": "796641", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/30", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:30:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "37", "speakers": [{"_type": "ContributionParticipation", "emailHash": "d9649b94178b18e38f1f945d86a13425", "affiliation": "University of Johannesburg (ZA)", "_fossil": "contributionParticipationMetadata", "fullName": "LEE, Christopher Jon", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "d9649b94178b18e38f1f945d86a13425", "affiliation": "University of Johannesburg (ZA)", "_fossil": "contributionParticipationMetadata", "fullName": "LEE, Christopher Jon", "id": "1"}], "title": "ATLAS TDAQ System Administration: evolution and re-design", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T23:59:54.502849+00:00", "description": "", "title": "ATLAS_TDAQ_SysAdmins_-_CHEP2015v4-1.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/37\/attachments\/578507\/796642\/ATLAS_TDAQ_SysAdmins_-_CHEP2015v4-1.pptx", "filename": "ATLAS_TDAQ_SysAdmins_-_CHEP2015v4-1.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796642, "size": 5830451}], "title": "Slides", "default_folder": false, "id": 578507, "description": ""}], "_type": "Contribution", "description": "The ATLAS Trigger and Data Acquisition (TDAQ) system is responsible for the online processing of live data, streaming from the ATLAS experiment at the Large Hadron Collider (LHC) at CERN. The online farm is composed of ~3000 servers, processing the data readout from ~100 million detector channels through multiple trigger levels. \r\nDuring the two years of the first Long Shutdown (LS1) there has been a tremendous amount of work done by the ATLAS TDAQ System Administrators, implementing numerous new software applications, upgrading the OS and the hardware, changing some design philosophies and exploiting the High Level Trigger farm with different purposes.\r\nDuring the data taking only critical security updates are applied and broken hardware is replaced to ensure a stable operational environment. The LS1 provided an excellent opportunity to look into new technologies and applications that would help to improve and streamline the daily tasks of not only the System Administrators, but also of the scientists who will be working during the upcoming data taking period (Run-II).\r\nThe OS version has been upgraded to SLC6; for the largest part of the farm, which is composed by netbooted nodes, this required a completely new design of the netbooting system. In parallel, the migration to Puppet of the Configuration Management systems has been completed for both netbooted and localbooted hosts; the Post-Boot Scripts system and Quattor have been consequently dismissed.\r\nVarious new ATCA-based readout systems, with specific network requirements, have also been integrated into the overall system.\r\nVirtual Machine (VM) usage has been investigated and tested and many of our core servers are now running on VMs. This provides us with the functionality of rapidly replacing them in case of failures and increasing the number of servers when needed.\r\nVirtualization has also been used to adapt the HLT farm as a batch system, which has been used for running Monte Carlo production jobs that are mostly CPU and not I\/O bound.\r\nIn Run-II this feature could be exploited during the downtimes of the LHC.\r\nA new Satellite Control Room (SCR) has also been commissioned and in the ATLAS Control Room (ACR) the PC-over-IP network connections have been upgraded to a fully redundant network. The migration to SLC6 has also had an impact on the Control Room Desktop (CRD), the in house KDE-based desktop environment designed to enforce access policies while fulfilling the needs of the people working in the ACR and the SCR.\r\nFinally, monitoring the health and the status of ~3000 machines in the experimental area is obviously of the utmost importance, so the obsolete Nagios v2 has been replaced with Icinga, complemented by Ganglia for performance data.\r\nThis paper serves for reporting \"What\", \"Why\" and \"How\" we did in order to improve and produce a system capable of performing for the next 3 years of ATLAS data taking.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578507", "resources": [{"_type": "LocalFile", "name": "ATLAS_TDAQ_SysAdmins_-_CHEP2015v4-1.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/37\/attachments\/578507\/796642\/ATLAS_TDAQ_SysAdmins_-_CHEP2015v4-1.pptx", "fileName": "ATLAS_TDAQ_SysAdmins_-_CHEP2015v4-1.pptx", "_fossil": "localFileMetadata", "id": "796642", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "2fa117faa146ac537e42ffd3950d8cce", "affiliation": "University of Washington (US)", "_fossil": "contributionParticipationMetadata", "fullName": "TWOMEY, Matthew Shaun", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "55337c5c7f1c022b93bd6ef8401bcc5a", "affiliation": "Budker Institute of Nuclear Physics (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "VORONKOV, Artem", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "af730dbe979e72aa88ba4063a0dc7c51", "affiliation": "University of Johannesburg (ZA)", "_fossil": "contributionParticipationMetadata", "fullName": "BALLESTRERO, Sergio", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "37b1d893d911f4892e8f6a814273ccc5", "affiliation": "Budker Institute of Nuclear Physics (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "BOGDANCHIKOV, Alexander", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "12c18494664f7a1a64e606d3a72e883e", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "BRASOLIN, Franco", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "d758774bbd3d12133a6861082ec8c2d3", "affiliation": "Polytechnic University of Bucharest (RO)", "_fossil": "contributionParticipationMetadata", "fullName": "CONTESCU, Cristian", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "39e610106b8d6a0bc7c783ad2b22b9aa", "affiliation": "Budker Institute of Nuclear Physics (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "DUBROV, Sergei", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "2d6446f1d79006ce6dc10bba0cef2859", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FAZIO, Daniel", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "f52527a6ca47efb725af8d7f3252ea15", "affiliation": "Budker Institute of Nuclear Physics (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "KOROL, Aleksandr", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "ab3fe249ad2023c49cd56c4051abdaf1", "affiliation": "University of California Irvine (US)", "_fossil": "contributionParticipationMetadata", "fullName": "SCANNICCHIO, Diana", "id": "11"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/37", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:15:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "36", "speakers": [{"_type": "ContributionParticipation", "emailHash": "672615d1077e46cd9a66fcc753127d52", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. AVOLIO, Giuseppe", "id": "6"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "672615d1077e46cd9a66fcc753127d52", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. AVOLIO, Giuseppe", "id": "6"}], "title": "A Validation System for the Complex Event Processing Directives of the ATLAS Shifter Assistant Tool", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T14:51:13.270182+00:00", "description": "", "title": "CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/36\/attachments\/578508\/796643\/CHEP2015.pdf", "filename": "CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796643, "size": 1619586}, {"_type": "attachment", "modified_dt": "2015-04-09T14:51:13.270182+00:00", "description": "", "title": "CHEP2015.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/36\/attachments\/578508\/796644\/CHEP2015.pptx", "filename": "CHEP2015.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796644, "size": 2566868}], "title": "Slides", "default_folder": false, "id": 578508, "description": ""}], "_type": "Contribution", "description": "Complex Event Processing (CEP) is a methodology that combines data from different sources in order to identify events or patterns that need particular attention. It has gained a lot of momentum in the computing world in the past few years and is used in ATLAS to continuously monitor the behaviour of the data acquisition system, to trigger corrective actions and to guide the experiment\u2019s operators. This technology is very powerful, if experts regularly insert and update their knowledge about the system\u2019s behaviour into the CEP engine. Nevertheless, writing or modifying CEP directives is not trivial since the used programming paradigm is quite different with respect to what developers are normally familiar with. In order to help experts verify that the directives work as expected, we have thus developed a complete testing and validation environment.\r\nThis system consists of three main parts: the first is the persistent storage of all relevant data streams that are produced during data taking, the second is a playback tool that allows to re-inject data of specific data taking sessions from the past into the CEP engine and the third is a reporting tool that shows the output that the directives loaded into the engine would have produced in the live system.\r\nIn this paper we describe the design, implementation and performance of this validation system, highlight its strengths and short-comings and indicate how such a system could be re-used in similar projects.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578508", "resources": [{"_type": "LocalFile", "name": "CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/36\/attachments\/578508\/796643\/CHEP2015.pdf", "fileName": "CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796643", "_deprecated": true}, {"_type": "LocalFile", "name": "CHEP2015.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/36\/attachments\/578508\/796644\/CHEP2015.pptx", "fileName": "CHEP2015.pptx", "_fossil": "localFileMetadata", "id": "796644", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "cc7d8b1f866288fab6bde5f5b8ce19e8", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ANDERS, Gabriel", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "60fbebcb15ff7b13c1be15b3b59d77fc", "affiliation": "B.P. Konstantinov Petersburg Nuclear Physics Institute - PNPI (", "_fossil": "contributionParticipationMetadata", "fullName": "KAZAROV, Andrei", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "7e3f1857bfe32cbf246a8d1df6a0b8b4", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "LEHMANN MIOTTO, Giovanna", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "8bf885908a94812f483a5b4f03e889eb", "affiliation": "Universidad Nacional de La Plata (AR)", "_fossil": "contributionParticipationMetadata", "fullName": "SANTOS, Alejandro", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "bc0d3548bbf2eee74b3ef7357d28a227", "affiliation": "University of California Irvine (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. SOLOVIEV, Igor", "id": "5"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/36", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "35", "speakers": [{"_type": "ContributionParticipation", "emailHash": "672615d1077e46cd9a66fcc753127d52", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. AVOLIO, Giuseppe", "id": "17"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "672615d1077e46cd9a66fcc753127d52", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. AVOLIO, Giuseppe", "id": "17"}], "title": "Upgrade of the ATLAS Control and Configuration Software for Run 2", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T14:57:00.206283+00:00", "description": "", "title": "POSTER.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/35\/attachments\/578509\/796645\/POSTER.pdf", "filename": "POSTER.pdf", "content_type": "application\/pdf", "type": "file", "id": 796645, "size": 1892803}], "title": "Poster", "default_folder": false, "id": 578509, "description": ""}], "_type": "Contribution", "description": "The ATLAS data acquisition (DAQ) system is controlled and configured via a software infrastructure that takes care of coherently orchestrating the data taking. While the overall architecture, established at the end of the 90\u2019s, has proven to be solid and flexible, many software components have undergone a complete redesign or re-implementation in 2013\/2014 in order to fold-in the additional requirements that appeared in the course of LHC\u2019s Run 1, to profit from new technologies and to re-factorise and cleanup software.\r\nThis paper describes the approach that was taken to plan, organise and carry out this software upgrade project. It highlights the main technical choices that have guided the overall work, describes the major achievements and outlines how the control and configuration software may be further improved or re-shaped in the future.", "track": "Track1: Online computing ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578509", "resources": [{"_type": "LocalFile", "name": "POSTER.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/35\/attachments\/578509\/796645\/POSTER.pdf", "fileName": "POSTER.pdf", "_fossil": "localFileMetadata", "id": "796645", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "3e6a390d80da95a72d2966d967ede021", "affiliation": "Joint Institute for Nuclear Research (JINR)", "_fossil": "contributionParticipationMetadata", "fullName": "ALEKSANDROV, Igor", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "ecaba0b3f52f64a8886c8c3a194e998c", "affiliation": "Joint Inst. for Nuclear Research (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "KOTOV, Vladislav", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "77456d6ce9125c1cbce36f63d34f7afa", "affiliation": "University of California Irvine (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LANKFORD, Andrew James", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "90d38c8d351dd3294d817a7aaa9372dc", "affiliation": "Ecole Polytechnique Federale de Lausanne (CH)", "_fossil": "contributionParticipationMetadata", "fullName": "LAURENT, Florian", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "7e3f1857bfe32cbf246a8d1df6a0b8b4", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "LEHMANN MIOTTO, Giovanna", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "fdf72c50739c48378b9f205d84cef90d", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MAGNONI, Luca", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "cb9f345fa08436a65a141c16e4f4e4f8", "affiliation": "B.P. Konstantinov Petersburg Nuclear Physics Institute - PNPI (", "_fossil": "contributionParticipationMetadata", "fullName": "OLESHKO, Serguei", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "04ae5633e961b2e79f098bac340bce1c", "affiliation": "University of Athens (GR)", "_fossil": "contributionParticipationMetadata", "fullName": "PAPAEVGENIOU, Lykourgos", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "62cec0273790cad91c2dd4ba0eaceb8f", "affiliation": "B.P. Konstantinov Petersburg Nuclear Physics Institute - PNPI (", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. RYABOV, Yury", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "8bf885908a94812f483a5b4f03e889eb", "affiliation": "Universidad Nacional de La Plata (AR)", "_fossil": "contributionParticipationMetadata", "fullName": "SANTOS, Alejandro", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "ab3fe249ad2023c49cd56c4051abdaf1", "affiliation": "University of California Irvine (US)", "_fossil": "contributionParticipationMetadata", "fullName": "SCANNICCHIO, Diana", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "cc7d8b1f866288fab6bde5f5b8ce19e8", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ANDERS, Gabriel", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "272435df7e9fb9eca0dbe85e8149d402", "affiliation": "Univ. Federal do Rio de Janeiro (BR)", "_fossil": "contributionParticipationMetadata", "fullName": "SEIXAS, Jose", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "bc0d3548bbf2eee74b3ef7357d28a227", "affiliation": "University of California Irvine (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. SOLOVIEV, Igor", "id": "14"}, {"_type": "ContributionParticipation", "emailHash": "cff1d62758da07da6aa3714beb3fc0dc", "affiliation": "University of California Irvine (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. UNEL, Gokhan", "id": "15"}, {"_type": "ContributionParticipation", "emailHash": "0399f460e4c936ec5c702aeffb9028f1", "affiliation": "High Energy Accelerator Research Organization (JP)", "_fossil": "contributionParticipationMetadata", "fullName": "YASU, Yoshiji", "id": "16"}, {"_type": "ContributionParticipation", "emailHash": "3284703f00260862d63f79a7e6c6f7ce", "affiliation": "IFIN-HH Bucharest (RO)", "_fossil": "contributionParticipationMetadata", "fullName": "CAPRINI, Mihai", "id": "18"}, {"_type": "ContributionParticipation", "emailHash": "e072d89c995d2c19d7f9f52eeff5fd0a", "affiliation": "University of California Irvine (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CORSO RADU, Alina", "id": "19"}, {"_type": "ContributionParticipation", "emailHash": "16f1f576575e6ea3628389b63958db03", "affiliation": "Univ. Federal do Rio de Janeiro (BR)", "_fossil": "contributionParticipationMetadata", "fullName": "DE CASTRO VARGAS FERNANDES, Julio", "id": "20"}, {"_type": "ContributionParticipation", "emailHash": "7b2cda49d28631e227b0f5a7688e2718", "affiliation": "Universidad Nacional de La Plata (AR)", "_fossil": "contributionParticipationMetadata", "fullName": "DOVA, Maria Teresa", "id": "21"}, {"_type": "ContributionParticipation", "emailHash": "60fbebcb15ff7b13c1be15b3b59d77fc", "affiliation": "B.P. Konstantinov Petersburg Nuclear Physics Institute - PNPI (", "_fossil": "contributionParticipationMetadata", "fullName": "KAZAROV, Andrei", "id": "22"}, {"_type": "ContributionParticipation", "emailHash": "0c86b21d5f415d64f2e334f3f7100172", "affiliation": "Petersburg Nuclear Physics Institute (PNPI)", "_fossil": "contributionParticipationMetadata", "fullName": "KLOPOV, Nikolai", "id": "23"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/35", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:00:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "34", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e86bd467bb7abb13e5652373bf0ed998", "affiliation": "Johannes-Gutenberg-Universitaet  Mainz (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "SIMIONI, Eduard Ebron", "id": "9"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e86bd467bb7abb13e5652373bf0ed998", "affiliation": "Johannes-Gutenberg-Universitaet  Mainz (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "SIMIONI, Eduard Ebron", "id": "9"}], "title": "Upgrade of the ATLAS Level-1 Trigger with event topology information", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T18:22:34.106847+00:00", "description": "", "title": "Simioni_CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/34\/attachments\/578510\/796646\/Simioni_CHEP2015.pdf", "filename": "Simioni_CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796646, "size": 10882775}], "title": "Slides", "default_folder": false, "id": 578510, "description": ""}], "_type": "Contribution", "description": "The Large Hadron Collider (LHC) in 2015 will collide proton beams with\r\nincreased luminosity from $10^{34}$ up to $3 \\times 10^{34}$ cm$^{\u22122}$ s$^{\u22121}$. ATLAS\r\nis an LHC experiment designed to measure decay properties of highly\r\nenergetic particles produced in these proton-collisions. The high\r\nluminosity places stringent physical and operational requirements on\r\nthe ATLAS Trigger in order to reduce the 40 MHz collision rate to an\r\nevent storage rate of 1 kHz, thereby retaining events with valuable\r\nphysics content. The hardware-based first ATLAS trigger level\r\n(Level-1) has an output rate of 100 kHz and decision latency of less\r\nthan 2.5 \u00b5s. It is composed of the Calorimeter Trigger (L1Calo), the\r\nMuon Trigger (L1Muon) and the Central Trigger Processor. In 2014,\r\nthere will be a new trigger system has been added: the Topological\r\nProcessor System (L1Topo system).\r\n\r\nThe L1Topo system consists of a single AdvancedTCA shelf equipped with\r\nthree L1Topo processor blades. It processes detailed information from\r\nL1Calo and L1Muon in individual state-of-the-art FPGA processors to\r\nderive desitions based on the topology of each collision event. Such\r\ntopologies are the angles between jets and\/or leptons or global event\r\nvariables based on lists of pre-selected\/-sorted objects. The system\r\nis designed to receive and process up to 6 Tb\/s of real time data. The\r\ntalk is about the relevant upgrades of the Level-1 trigger with focus\r\non the topological processor design and commissioning.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578510", "resources": [{"_type": "LocalFile", "name": "Simioni_CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/34\/attachments\/578510\/796646\/Simioni_CHEP2015.pdf", "fileName": "Simioni_CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796646", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "299fbccefd9d1159d1d26c2456ae399d", "affiliation": "Johannes-Gutenberg-Universitaet  Mainz (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "ARTZ, Sebastian", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "53b49586a1cb9e8cb75d0d1eacc6ac44", "affiliation": "Johannes-Gutenberg-Universitaet  Mainz (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "BUESCHER, Volker", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "8effb1493d85dae2e72089ef99eab58e", "affiliation": "Johannes-Gutenberg-Universitaet  Mainz (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "JAKOBI, Katharina Bianca", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "35a9f075fe199202c1eadf0ba00a8d78", "affiliation": "Johannes-Gutenberg-Universitaet  Mainz (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "KAHRA, Christian", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "bc31f58a9ba682e6812ee1807f01e288", "affiliation": "Johannes-Gutenberg-Universitaet  Mainz (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "KALUZA, Adam", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "eb55d3c59c883d90a5036c3466973a6b", "affiliation": "Johannes-Gutenberg-Universitaet  Mainz (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "REISS, Andreas Dominik", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "ebe064336891c0451836972d613e6230", "affiliation": "Johannes-Gutenberg-Universitaet  Mainz (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "SCHAEFER, Uli", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "be82e40fde5886c44faa550f51315531", "affiliation": "Johannes-Gutenberg-Universitaet  Mainz (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "SCHAFFER, Jan", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "8d6ebb9d900b86d7c6c1596fe7dd8a4e", "affiliation": "Johannes-Gutenberg-Universitaet  Mainz (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "SIMON, Manuel Sebastian", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "b4ab3100c3edc72e470e5a8e19576047", "affiliation": "Johannes-Gutenberg-Universitaet  Mainz (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "TAPPROGGE, Stefan", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "bf0a947fae47a3f5cb3443ab3b7ce61e", "affiliation": "Johannes-Gutenberg-Universitaet  Mainz (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "ZINSER, Markus", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "7b9ed64d2ad675fdecf2ae03fc036986", "affiliation": "Johannes-Gutenberg-Universitaet  Mainz (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "VOGEL, Alexander", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "f1e9c174cba2fe76bda90bd71d36607c", "affiliation": "Johannes-Gutenberg-Universitaet  Mainz (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "BAUSS, Bruno", "id": "14"}, {"_type": "ContributionParticipation", "emailHash": "19e37e11a22935981b51c4b7e8085549", "affiliation": "Johannes-Gutenberg-Universitaet  Mainz (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "DEGELE, Reinold", "id": "15"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/34", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "438", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2463c98197fe73772aa1cd10b34131aa", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "COLLIER, Ian Peter", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2463c98197fe73772aa1cd10b34131aa", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "COLLIER, Ian Peter", "id": "0"}], "title": "Maintaining Traceability in an Evolving Distributed Computing Environment", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-07T07:27:33.334062+00:00", "description": "", "title": "CHEP_2015_-_Evolving_Security.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/438\/attachments\/578511\/796647\/CHEP_2015_-_Evolving_Security.pdf", "filename": "CHEP_2015_-_Evolving_Security.pdf", "content_type": "application\/pdf", "type": "file", "id": 796647, "size": 1174212}, {"_type": "attachment", "modified_dt": "2015-04-07T07:27:33.334062+00:00", "description": "", "title": "CHEP_2015_-_Evolving_Security.ppt", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/438\/attachments\/578511\/796648\/CHEP_2015_-_Evolving_Security.ppt", "filename": "CHEP_2015_-_Evolving_Security.ppt", "content_type": "application\/vnd.ms-powerpoint", "type": "file", "id": 796648, "size": 3646976}], "title": "Slides", "default_folder": false, "id": 578511, "description": ""}], "_type": "Contribution", "description": "The management of risk is fundamental to the operation of any distributed computing infrastructure. Identifying the cause of incidents is essential to prevent them from re-occurring. In addition, it is a goal to contain the impact of an incident while keeping services operational. For response to incidents to be acceptable this needs to be commensurate with the scale of the problem.\r\n\r\nThe minimum level of traceability for distributed computing infrastructure usage is to be able to identify the source of all actions (executables, file transfers, pilot jobs, portal jobs, etc) and the individual who initiated them. In addition, sufficiently fine-grained controls, such as blocking the originating user and monitoring to detect abnormal behaviour, are necessary for keeping services operational. It is essential to be able to understand the cause and to fix any problems before re-enabling access for the user.\r\n\r\nThe aim is to be able to answer the basic questions who, what, where, and when concerning any incident. This requires retaining all relevant information, including timestamps and the digital identity of the user, sufficient to identify, for each service instance, and for every security event including at least the following: connect, authenticate, authorize (including identity changes) and disconnect.\r\n\r\nIn traditional grid infrastructures (WLCG, EGI, OSG etc.) best practices and procedures for gathering and maintaining the information required to maintain traceability are well established. In particular, sites collect and store information required to ensure traceability of events at their sites.\r\n\r\nWith the increased use of virtualisation and private and public clouds for HEP workloads established procedures, which are unable to see 'inside' running virtual machines no longer capture all the information required. Maintaining traceability will at least involve a shift of responsibility from sites to Virtual Organisations (VOs) bringing with it new requirements for their logging infrastructures. VOs indeed need to fulfil a new operational role and become fully active participants in the incident response process.\r\n\r\nWe present an analysis of the changing requirements to maintain traceability for virtualised and cloud based workflows with particular reference to the work of the WLCG Traceability Working Group.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578511", "resources": [{"_type": "LocalFile", "name": "CHEP_2015_-_Evolving_Security.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/438\/attachments\/578511\/796647\/CHEP_2015_-_Evolving_Security.pdf", "fileName": "CHEP_2015_-_Evolving_Security.pdf", "_fossil": "localFileMetadata", "id": "796647", "_deprecated": true}, {"_type": "LocalFile", "name": "CHEP_2015_-_Evolving_Security.ppt", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/438\/attachments\/578511\/796648\/CHEP_2015_-_Evolving_Security.ppt", "fileName": "CHEP_2015_-_Evolving_Security.ppt", "_fossil": "localFileMetadata", "id": "796648", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "82d7c36b6575474fb5d7ba2ca294dfc5", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. WARTEL, Romain", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/438", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:45:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "439", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ed78764bd90b778b698711bdf715eec4", "affiliation": "University of Oslo (NO)", "_fossil": "contributionParticipationMetadata", "fullName": "RICHTER, Matthias", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ed78764bd90b778b698711bdf715eec4", "affiliation": "University of Oslo (NO)", "_fossil": "contributionParticipationMetadata", "fullName": "RICHTER, Matthias", "id": "0"}], "title": "A design study for the upgraded ALICE O2 computing facility", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T02:11:34.612737+00:00", "description": "", "title": "mrichter_CHEP2015-alice-o2-prototype.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/439\/attachments\/578512\/796649\/mrichter_CHEP2015-alice-o2-prototype.pdf", "filename": "mrichter_CHEP2015-alice-o2-prototype.pdf", "content_type": "application\/pdf", "type": "file", "id": 796649, "size": 793415}], "title": "Slides", "default_folder": false, "id": 578512, "description": ""}], "_type": "Contribution", "description": "An upgrade of the ALICE detector is currently prepared for the Run 3 period of the Large Hadron\r\nCollider (LHC) at CERN starting in 2020. The physics topics under study by ALICE during this\r\nperiod will require the inspection of all collisions at a rate of 50 kHz for minimum bias Pb-Pb and 200\r\nkHz for pp and p-Pb collisions in order to extract physics signals embedded into a large background.\r\n\r\nThe upgraded ALICE detector will produce more than 1 TByte\/s of data. Both collision and data\r\nrate impose new challenges onto the detector readout and compute system. Some detectors will not\r\nuse a triggered readout, which will require a continuous processing of the detector data.\r\n\r\nAlthough various online systems are existing for event based reconstruction, the application of a production system for time-based data processing and reconstruction is a novel case in HEP. The project\r\nwill benefit from the experience gained with the current ALICE High Level Trigger online system,\r\nwhich already implements a modular concept combining data transport, algorithms and heterogeneous hardware. Processing of individual events will however have to be replaced by the continuous\r\nprocessing of the data stream segmented according to a time-frame structure.\r\n\r\nOne challenge is the distribution of data within the compute nodes. Time-correlated data sets are\r\nreceived by the First Level Processors (FLP) and must be coherently transported to and aggregated on\r\nthe Event Processing Nodes (EPN). Several approaches for the distribution of data are being studied.\r\nAggregated time-frame data is processed on the EPN with the primary goal to reconstruct particle\r\nproperties. On-the-fly and short-latency detector calibration is necessary for the reconstruction. The\r\nimpact of the calibration strategy to the reconstruction performance is under study. Based on the\r\npartially reconstructed data, events corresponding to particular collisions can be assembled from the\r\ntime-based data. The original raw data are then replaced by these preprocessed data. This transformation together with the application of lossless data compression algorithms will provide a data\r\nvolume reduction of a factor of 20 before data is passed onto the storage system.\r\n\r\nBuilding on messaging solutions, the design and development of a flexible framework for transparent\r\ndata flow, online reconstruction, and data compression has started. The system uses parallel processing\r\non the level of processes and threads within processes in order to achieve an optimal utilization of CPU\r\ncores and memory. Furthermore, the framework provides the necessary abstraction to run common\r\ncode on heterogeneous platforms including various hardware accelerator cards.\r\n\r\nWe present in this contribution the first results of a prototype with estimates for scalability and\r\nfeasibility for a full scale system.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578512", "resources": [{"_type": "LocalFile", "name": "mrichter_CHEP2015-alice-o2-prototype.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/439\/attachments\/578512\/796649\/mrichter_CHEP2015-alice-o2-prototype.pdf", "fileName": "mrichter_CHEP2015-alice-o2-prototype.pdf", "_fossil": "localFileMetadata", "id": "796649", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/439", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "436", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6277092a9467ff13841a6595b3694018", "affiliation": "Calcutta University", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. SAU, Suman", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6277092a9467ff13841a6595b3694018", "affiliation": "Calcutta University", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. SAU, Suman", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "57db7c85c7df9cef5abd518b56f3cf0b", "affiliation": "VECC,Kolkata", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. MANDAL, Swagata", "id": "1"}], "title": "High Speed Fault Tolerant Secure Communication for Muon Chamber using FPGA based GBTx Emulator", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The Compressed Baryonic Matter (CBM) experiment is a part of the Facility for Antiproton and Ion Research (FAIR) in Darmstadt at the GSI. This experiment will examine heavy-ion collisions in fixed target geometry and will be able to measure hadrons, electrons and muons. Muon Chamber (MUCH) is used to detect low momentum muons in an environment of high particle densities. Basic read out chain of MUCH consists of MUCH-XYTER (Front End Electronics), Gigabit Transceiver (GBTx), Data Processing Board (DPB), First level event selector interface board (FLIB) [1]. MUCH- XYTER is a self-triggered Application-Specific Integrated Circuit (ASIC), which is directly connected to the detector and send the receive data to the GBTx through serial peripheral interface (SPI) like E-link. GBTx will be connected to the DPB through 4.8 Gbps optical link. In between DPB and FLIB there will be 10 Gbps optical link and at the end FLIB will be connected to the data acquisition system through Peripheral Component Interconnect Express (PCIe).\r\n  As a part of the implementation of read out chain of MUCH in India, we have  implemented FPGA emulator of GBTx [2]. GBTx is a radiation tolerant ASIC that can be used to implement multipurpose high speed bidirectional optical links for high energy physics (HEP) experiments. It is developed by CERN. It consists of packet generator, scrambler-descrambler, encoder-decoder, interleaver-deinterleaver, gearbox, and transceiver.Packet generator is used to generate test pattern (either static or dynamic) to test the optical link. Scrambler is used to scramble the data so that clock can be recovered properly in the receiver side. In the encoder block single error correcting (15, 11) Reed-Solomon (RS) encoding is used to mitigate error occurred in the communication channel due to radiation. Interleaver is used to simply interleave the encoded data to make the coded data more robust against burst error. Gearbox is used to divide the 120 bit data frame into three 40 bit wards. These 40 bit wards are transmitted through multi gigabit transceiver (MGT) and reference clock used for MGT is 120 MHz. In the receiver side apart from the predefined block one frame aligner block is also used. Frame aligner is used to detect header in the data frame properly in the receiver side. GBTx will be used in highly irradiated area and more prone to be affected by multi-bit error. To mitigate this effect instead of single bit error correcting (15, 11) RS code we have used two bit error correcting (15, 7) BCH code [3]. It will increase the redundancy, which in turn increases the reliability in the coded data. So the coded data will be less prone to be affected by noise due to radiation. Normally in the wired communication between any two fixed points there will be no such security issue. But when multiple stations will be used for long distance communication the question of security will come into play. So to make the data transmitting through optical fiber more secure, we use advanced encryption standard (AES) (a symmetric key cryptography) [4] is used after channel coding. The AES is a specification for the encryption of electronic data established by the U.S. National Institute of Standards and Technology (NIST) in 2001. AES encryption algorithm acts as a block cipher. The data will be always 128 bit block but the key size can be select from 128 bit, 192 bit or 256 bit. Here for testing purpose we used AES- 128 and AES- 256. In this paper our key contributions can be summarized as follows:\r\n\u2022\t**Implementation of multibit error correcting BCH code to make optical communication more robust against error due to radiation.\r\n\u2022\t  Implementation of AES along with GBTx to make the data communication more secure.**\r\n   We have implemented GBTx emulator on two Xilinx Kintex-7 boards (KC705). One will act as transmitter and other will act as receiver and they are connected through optical fiber using small form-factor pluggable (SFP) port. We have performed the run-time verification of the system using Xilinx Chipscope Pro Analyzer and also measured the resource utilization, throughput and power utilization of the implemented design. \r\n\r\nReferences:\r\n1.\tFLES\/DAQ summary and Outlook, 24th CBM week, Walter F.J. M\u00fcller, FAIR, Darmstadt\r\n2.\t S. Baron, M. Barros Marin, \u201cGBT-FPGA user guide,\u201d Version 1.00.\r\n3.\tError Detection and Correction using BCH code, Hank Wallance, 2001.\r\n4.\tA.M Deshpande, M.S Deshpande, D.N Kayatanavar, \u201cFPGA implementation of AES encryption and decryption,\u201d International Conference on Control, Automation, Communication and Energy Conservation (INCACEC), 2009.", "track": "Track1: Online computing ", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "5a3b9d0421aa106802da112b896a8f6f", "affiliation": "VECC,Kolkata", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. SAINI, Jogender", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "e9eaaddd16360350fde78045970f2810", "affiliation": "Cacutta University", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CHAKRABARTY, Amlan", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "b4525a6f573564dd04d0a81c7c13e39d", "affiliation": "VECC,Kolkata", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CHATTAPADHYA, Subhasis", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/436", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "437", "speakers": [{"_type": "ContributionParticipation", "emailHash": "551073afe017149bb1ae76fc2c07f817", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. MANDRICHENKO, Igor", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "551073afe017149bb1ae76fc2c07f817", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. MANDRICHENKO, Igor", "id": "0"}], "title": "Redundant Web Services Infrastructure for Data Intensive and Interactive Applications", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "RESTful web services are popular solution for distributed data access and information management. Performance, scalability and reliability of such services is critical for the success of data production and analysis in High Energy Physics as well as other areas of science. \r\n\r\nAt FNAL, we have been successfully using HTTP\/REST-based data access architecture to provide access to various types of data, in particular, access to scientific databases. We have built a simple yet versatile infrastructure which allows us to use redundant copies of web application servers to increase service performance, scalability and availability. It is designed to handle both state-less and state-full data access methods using distributed web server.\r\n\r\nThe redundant infrastructure allows us to add or remove individual application servers at any time without a visible interruption of the service. This infrastructure has been successfully used for several years now with data web services as well as with interactive web applications. \r\n\r\nWe will present components of our infrastructure and several examples of how it can be used.", "track": "Track6: Facilities, Infrastructure, Network", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "23eb1967d09a2ee18f3bef48c1fffec7", "affiliation": "FNAL", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. PODSTAVKOV, Vladimir", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/437", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "434", "speakers": [{"_type": "ContributionParticipation", "emailHash": "551073afe017149bb1ae76fc2c07f817", "affiliation": "FNAL", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. MANDRICHENKO, Igor", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "551073afe017149bb1ae76fc2c07f817", "affiliation": "FNAL", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. MANDRICHENKO, Igor", "id": "0"}], "title": "ConDB - Conditions Database for Particle Physics Experiments", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "Abstract: Conditions or calibration data are an important part of High Energy experiments.  This kind of data is typically organized in terms of intervals of validity that require a special type of database table schema and API structure.  At Fermilab we have designed and developed ConDB, a general tool to store, manage and retrieve conditions data organized into validity intervals in a database.  ConDB is highly scalable, has been proven to be capable of storing data for more than million channels, has many additional features such as data compression, a direct access API and a scalable high performance web interface. We present the conceptual design, product features, database representation and system architecture of this very powerful tool.", "track": "Track3: Data store and access ", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "177a78106eb711162ab9437b3a590d3c", "affiliation": "Argonne National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "PALEY, Jonathan", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "23eb1967d09a2ee18f3bef48c1fffec7", "affiliation": "FNAL", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. PODSTAVKOV, Vladimir", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/434", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:15:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "435", "speakers": [{"_type": "ContributionParticipation", "emailHash": "784f401565a50ecd3f5d8dfff766c9af", "affiliation": "University of Michigan (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GOLDFARB, Steven", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "784f401565a50ecd3f5d8dfff766c9af", "affiliation": "University of Michigan (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GOLDFARB, Steven", "id": "0"}], "title": "ATLAS Public Web Pages: Online Management of HEP External Communication Content", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T03:25:42.961252+00:00", "description": "", "title": "GoldfarbATLAS-20150414.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/435\/attachments\/578513\/796650\/GoldfarbATLAS-20150414.pdf", "filename": "GoldfarbATLAS-20150414.pdf", "content_type": "application\/pdf", "type": "file", "id": 796650, "size": 2660787}, {"_type": "attachment", "modified_dt": "2015-04-15T03:25:42.961252+00:00", "description": "", "title": "GoldfarbATLAS-20150414.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/435\/attachments\/578513\/796651\/GoldfarbATLAS-20150414.pptx", "filename": "GoldfarbATLAS-20150414.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796651, "size": 11396373}], "title": "Slides", "default_folder": false, "id": 578513, "description": ""}], "_type": "Contribution", "description": "The ATLAS Education and Outreach Group is in the process of migrating its public online content to a professionally designed set of web pages built on a Drupal-based content management system. Development of the front-end design passed through several key stages, including audience surveys, stakeholder interviews, usage analytics, and a series of fast design iterations, called sprints.  Implementation of the web site involved application of the html design using Drupal templates, refined development iterations, and the overall population of the site with content.\r\n\r\nWe present the design and development processes and share the lessons learnt along the way, including the results of the data-driven discovery studies. We also demonstrate the advantages of selecting a back-end supported by content management, with a focus on workflow. Finally, we discuss usage of the new public web pages to implement Outreach strategy through implementation of clearly presented themes, consistent audience targeting and messaging, and the enforcement of a well-defined visual identity.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578513", "resources": [{"_type": "LocalFile", "name": "GoldfarbATLAS-20150414.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/435\/attachments\/578513\/796650\/GoldfarbATLAS-20150414.pdf", "fileName": "GoldfarbATLAS-20150414.pdf", "_fossil": "localFileMetadata", "id": "796650", "_deprecated": true}, {"_type": "LocalFile", "name": "GoldfarbATLAS-20150414.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/435\/attachments\/578513\/796651\/GoldfarbATLAS-20150414.pptx", "fileName": "GoldfarbATLAS-20150414.pptx", "_fossil": "localFileMetadata", "id": "796651", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "1f4635245724f807a6e3ae15298a1d1c", "affiliation": "University of Birmingham (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Mrs. MARCELLONI DE OLIVEIRA, Claudia", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "728a8a568183f864b6b060aa79a77253", "affiliation": "University of Manchester (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "PHOBOO, Abha Eli", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "f70dae5475c752a3d43b176db59389b5", "affiliation": "INFN Gruppo Collegato di Udine and ICTP Trieste", "_fossil": "contributionParticipationMetadata", "fullName": "SHAW, Kate", "id": "3"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/435", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "432", "speakers": [{"_type": "ContributionParticipation", "emailHash": "f93e30ef38e68512dad5beb650c2ba19", "affiliation": "Lancaster University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "JONES, Roger", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "0bb7f6e1d5fef2f6f3309c3bba70aa6a", "affiliation": "Lancaster University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LONG, Robin Eamonn", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "f93e30ef38e68512dad5beb650c2ba19", "affiliation": "Lancaster University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "JONES, Roger", "id": "2"}], "title": "The evolving grid paradigm and code \"tuning\" for modern architectures- are the two mutually exclusive?", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T08:04:11.337903+00:00", "description": "", "title": "poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/432\/attachments\/578514\/796652\/poster.pdf", "filename": "poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796652, "size": 238709}], "title": "Slides", "default_folder": false, "id": 578514, "description": ""}], "_type": "Contribution", "description": "With the data output from the LHC increasing, many of the LHC experiments have made significant improvements to their code to take more advantage of the underlying CPU architecture and advanced features.  With the grid environment changing to heavily include virtualisation and cloud services, we look at whether these two systems\r\ncan be compatible, or whether improvements in code are lost through virtualisation. This is done by looking at the efficiency increases achieved in the latest iterations of ATLAS code and testing it within various grid paradigms.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578514", "resources": [{"_type": "LocalFile", "name": "poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/432\/attachments\/578514\/796652\/poster.pdf", "fileName": "poster.pdf", "_fossil": "localFileMetadata", "id": "796652", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/432", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "433", "speakers": [{"_type": "ContributionParticipation", "emailHash": "0bb7f6e1d5fef2f6f3309c3bba70aa6a", "affiliation": "Lancaster University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LONG, Robin Eamonn", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "0bb7f6e1d5fef2f6f3309c3bba70aa6a", "affiliation": "Lancaster University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LONG, Robin Eamonn", "id": "0"}], "title": "Using vcycle to provision compute resource on a commercial cloud provider", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "Cloud computing has the potential to provide flexible and elastic computing resource to the HEP community. This requires robust and simple orchestration tools especially when interacting with commercial cloud providers where system support is less familiar with HEP workflow. This contribution will describe the use of 'vcycle' to\r\nmanage compute resource on a commercial cloud provider. Operational details will be mentioned along with performance comparison to typical grid sites. The use of a local object store is described and compared with WAN data access.", "track": "Track7: Clouds and virtualization", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "f10a491a558480b67d78d8826c40ea5e", "affiliation": "Lancaster University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LOVE, Peter", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/433", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "430", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6f31143d6e40d8f7574d6928ac862d64", "affiliation": "Universite Claude Bernard-Lyon I (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "BOUDOUL, Gaelle", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6f31143d6e40d8f7574d6928ac862d64", "affiliation": "Universite Claude Bernard-Lyon I (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "BOUDOUL, Gaelle", "id": "0"}], "title": "Tracker software for Phase-II CMS", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T12:58:24.842397+00:00", "description": "", "title": "CHEP_POSTER_Boudoul2.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/430\/attachments\/578515\/796653\/CHEP_POSTER_Boudoul2.pdf", "filename": "CHEP_POSTER_Boudoul2.pdf", "content_type": "application\/pdf", "type": "file", "id": 796653, "size": 4223297}], "title": "Slides", "default_folder": false, "id": 578515, "description": ""}], "_type": "Contribution", "description": "The CMS experiment is has multi-faceted detector upgrade program planned over the next decade. The silicon tracker system plans an improved pixel detector for 2017 and proposes an entirely new tracker for the high-luminosity LHC run. In this presentation, we discuss the tools developed and used in the design, simulation and reconstruction of the upgraded tracker including completely new trigger functionalities.   We discuss how we adapted the standard CMS tracker software to achieve excellent physics and technical performance results in each of the upgraded tracker scenarios,  and our implementation of the track trigger simulation. We discuss our results and will demonstrate how the results of our simulation and reconstruction work  have impacted the overall program of the upgraded tracker designs.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578515", "resources": [{"_type": "LocalFile", "name": "CHEP_POSTER_Boudoul2.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/430\/attachments\/578515\/796653\/CHEP_POSTER_Boudoul2.pdf", "fileName": "CHEP_POSTER_Boudoul2.pdf", "_fossil": "localFileMetadata", "id": "796653", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/430", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "431", "speakers": [{"_type": "ContributionParticipation", "emailHash": "f93e30ef38e68512dad5beb650c2ba19", "affiliation": "Lancaster University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "JONES, Roger", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "0bb7f6e1d5fef2f6f3309c3bba70aa6a", "affiliation": "Lancaster University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LONG, Robin Eamonn", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "f93e30ef38e68512dad5beb650c2ba19", "affiliation": "Lancaster University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "JONES, Roger", "id": "2"}], "title": "Use of containerisation as an alternative to full virtualisation in grid environments.", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T08:05:07.260310+00:00", "description": "", "title": "poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/431\/attachments\/578516\/796654\/poster.pdf", "filename": "poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796654, "size": 229993}], "title": "Slides", "default_folder": false, "id": 578516, "description": ""}], "_type": "Contribution", "description": "Virtualisation is a key tool on the grid. It can be used to provide varying work environments or as pat of a cloud infrastructure. Virtualisation itself carries certain overheads that decrease the performance of the system through requiring extra resources to virtualise the software and hardware stack, and CPU-cycles wasted instantiating or destroying virtual machines for each job.  With the rise and improvements in containerisation, where only the software stack is kept separate, and no hardware or kernel virtualisation is used, their is scope for speed improvements and efficiency increases over standard virtualisation.  We compare containerisation and virtualisation, including a comparison against bare-metal machines as a benchmark.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578516", "resources": [{"_type": "LocalFile", "name": "poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/431\/attachments\/578516\/796654\/poster.pdf", "fileName": "poster.pdf", "_fossil": "localFileMetadata", "id": "796654", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/431", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "339", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a4c2629976026a424cedc51ff4d288ef", "affiliation": "STFC", "_fossil": "contributionParticipationMetadata", "fullName": "RYALL, George", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "a4c2629976026a424cedc51ff4d288ef", "affiliation": "STFC", "_fossil": "contributionParticipationMetadata", "fullName": "RYALL, George", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "88ef037500511d8fa4d89d568b57980e", "affiliation": "STFC", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PALMER, Sophy", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "2463c98197fe73772aa1cd10b34131aa", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "COLLIER, Ian Peter", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "de22c3cd14674c38d1512edbc544f940", "affiliation": "STFC RAL", "_fossil": "contributionParticipationMetadata", "fullName": "ADAMS, James", "id": "3"}], "title": "Public Outreach at RAL: Engaging the Next Generation of Scientists and Engineers", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-01T11:17:33.968554+00:00", "description": "", "title": "outreachvFINAL.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/339\/attachments\/578517\/796655\/outreachvFINAL.pdf", "filename": "outreachvFINAL.pdf", "content_type": "application\/pdf", "type": "file", "id": 796655, "size": 2158441}], "title": "Poster", "default_folder": false, "id": 578517, "description": ""}], "_type": "Contribution", "description": "Rutherford Appleton Laboratory (RAL) is part of the UK\u2019s Science and Technology Facilities Council (STFC). The Royal Charter that established the STFC requires us to generate public awareness and encourage public engagement and dialogue in relation to the science we undertake. We firmly support this activity as it is important to encourage the next generation of students to consider studying STEM (Science, Technology, Engineering, and Mathematics) subjects, providing the UK with a highly skilled work-force in the future.\r\n\r\nTo this end, we undertake a variety of outreach activities. We will describe the outreach activities undertaken by RAL, particularly focussing on those of the Scientific Computing Department. These activities include: an Arduino based activity day for 12-14 year olds to celebrate Ada Lovelace day; running a centre as part of the Young Rewired State \u2013 encouraging 11-18 year olds to created web applications with open data; sponsoring a team in the Engineering Education Scheme \u2013 supporting a small team of 16\/17 year olds to solve a real world engineering problem we set them; as well as the more traditional tours of our facilities. We believe these activities could serve as an example for other sites involved in scientific computing around the globe.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578517", "resources": [{"_type": "LocalFile", "name": "outreachvFINAL.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/339\/attachments\/578517\/796655\/outreachvFINAL.pdf", "fileName": "outreachvFINAL.pdf", "_fossil": "localFileMetadata", "id": "796655", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "6a79eb7fd763efc919c4efc3da1af4a1", "affiliation": "STFC", "_fossil": "contributionParticipationMetadata", "fullName": "APPLEYARD, Rob", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "2ee9dce25b40ac699bfacfddda517dfc", "affiliation": "STFC", "_fossil": "contributionParticipationMetadata", "fullName": "CORBETT, Greg", "id": "5"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/339", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "338", "speakers": [{"_type": "ContributionParticipation", "emailHash": "7e63c591de145c0857abbfeef3bd4d91", "affiliation": "KISTI", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. CHO, Kihyeon", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "ed9d60f245b332bb591a6567ff0ed9fe", "affiliation": "KISTI", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KIM, Junghyun", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "ee88ccba19ad0b9207fbe31361ebc9ab", "affiliation": "KISTI", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. RYU, Huiyoung", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "7e63c591de145c0857abbfeef3bd4d91", "affiliation": "KISTI", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. CHO, Kihyeon", "id": "0"}], "title": "An Research and Development for Evolving Architecture for the Beyond Standard Model", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T08:09:53.574295+00:00", "description": "", "title": "chep2015_evolving_041315_final.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/338\/attachments\/578518\/796656\/chep2015_evolving_041315_final.pdf", "filename": "chep2015_evolving_041315_final.pdf", "content_type": "application\/pdf", "type": "file", "id": 796656, "size": 1547267}], "title": "Slides", "default_folder": false, "id": 578518, "description": ""}], "_type": "Contribution", "description": "For the solution of searching for new physics beyond the standard model, we do a research and development of simulation tool kit based on the evolving computing architecture with international collaboration. Using the tools, we study particle physics beyond the standard model. We present the current status of the research and development for these.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578518", "resources": [{"_type": "LocalFile", "name": "chep2015_evolving_041315_final.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/338\/attachments\/578518\/796656\/chep2015_evolving_041315_final.pdf", "fileName": "chep2015_evolving_041315_final.pdf", "_fossil": "localFileMetadata", "id": "796656", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "ed9d60f245b332bb591a6567ff0ed9fe", "affiliation": "KISTI", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KIM, Junghyun", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "ee88ccba19ad0b9207fbe31361ebc9ab", "affiliation": "KISTI", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. RYU, Huiyoung", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/338", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:15:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "335", "speakers": [{"_type": "ContributionParticipation", "emailHash": "0d28138f3fc2216fb1d97810d9bd7e74", "affiliation": "University of Bologna", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. BONACORSI, Daniele", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "0d28138f3fc2216fb1d97810d9bd7e74", "affiliation": "University of Bologna", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. BONACORSI, Daniele", "id": "0"}], "title": "Exploiting CMS data popularity to model the evolution of data management for Run-2 and beyond", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T09:03:48.375062+00:00", "description": "", "title": "150413_CHEP15_PopAnalytics_DBonacorsi.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/335\/attachments\/578519\/796657\/150413_CHEP15_PopAnalytics_DBonacorsi.pdf", "filename": "150413_CHEP15_PopAnalytics_DBonacorsi.pdf", "content_type": "application\/pdf", "type": "file", "id": 796657, "size": 2530182}], "title": "Slides", "default_folder": false, "id": 578519, "description": ""}], "_type": "Contribution", "description": "During the LHC Run-1 data taking, all experiments collected large data volumes from proton-proton and heavy-ion collisions. The collisions data, together with massive volumes of simulated data, were replicated in multiple copies, transferred among various Tier levels, transformed\/slimmed in format\/content. These data were then accessed (both locally and remotely) by large groups of distributed analysis communities exploiting the WorldWide LHC Computing Grid infrastructure and services. While efficient data placement strategies - together with optimal data redistribution and deletions on demand - have become the core of static versus dynamic data management projects, little effort has so far been invested in understanding the detailed data-access patterns which surfaced in Run-1. These patterns, if understood, can be used as input to simulation of computing models at the LHC, to optimise existing systems by tuning their behaviour, and to explore next-generation CPU\/storage\/network co-scheduling solutions. This is of great importance, given that the scale of the computing problem will increase far faster than the resources available to the experiments, for Run-2 and beyond.\r\n\r\nStudying data-access patterns involves the validation of the quality of the monitoring data collected on the \"popularity\" of each dataset, the analysis of the frequency and pattern of accesses to different datasets by analysis end-users, the exploration of different views of the popularity data (by physics activity, by region, by data type), the study of the evolution of Run-1 data exploitation over time, the evaluation of the impact of different data placement and distribution choices on the available network and storage resources and their impact on the computing operations.\r\n\r\nThis work presents some insights from studies on the popularity data from the CMS experiment. We present the properties of a range of physics analysis activities as seen by the data popularity, and make recommendations for how to tune the initial distribution of data in anticipation of how it will be used in Run-2 and beyond.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578519", "resources": [{"_type": "LocalFile", "name": "150413_CHEP15_PopAnalytics_DBonacorsi.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/335\/attachments\/578519\/796657\/150413_CHEP15_PopAnalytics_DBonacorsi.pdf", "fileName": "150413_CHEP15_PopAnalytics_DBonacorsi.pdf", "_fossil": "localFileMetadata", "id": "796657", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "e86323288b18387d48dc1af9718c14d6", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WILDISH, Tony", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "efd528c4fecb23d4785370a2ba2989b2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MAGINI, Nicolo", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "0cb6ba9ae68e30d8bf3ef4a3e0f294c2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GIORDANO, Domenico", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "ad27265638303547200b971ea77f7b31", "affiliation": "Sezione di Pisa (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "BOCCALI, Tommaso", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "059c67f423c94e588bce78fb5a617463", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GIRONE, Maria", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "8acb0ee1fcd91662a9c755d1b6394d65", "affiliation": "University of Bologna", "_fossil": "contributionParticipationMetadata", "fullName": "MATTEO, Neri", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "bb2ea097db9db2ac2d867550aa35b5a0", "affiliation": "Cornell University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "KUZNETSOV, Valentin Y", "id": "7"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/335", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:30:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "334", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a11b77fc54a5a3fd19cf040a3c41e953", "affiliation": "Imperial College London", "_fossil": "contributionParticipationMetadata", "fullName": "MARTYNIAK, Janusz", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "7ba575b05769913d1239745e15dafee3", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "BAUER, Daniela", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "b4e7a4ea1d679a57a1467fc9ae74c443", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "FAYER, Simon", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "a11b77fc54a5a3fd19cf040a3c41e953", "affiliation": "Imperial College London", "_fossil": "contributionParticipationMetadata", "fullName": "MARTYNIAK, Janusz", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "cd17961312c904e3bb4b71723c8e87ac", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. COLLING, David", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "6cccef6d6cebb7a770526f8954e4483f", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "HUFFMAN, Adam", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "7d89f79a43b5cfacaafd400ecdaea334", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. RICHARDS, Alexander John", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "ed96c4767feeb06950d16695aa31278b", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CURRIE, Robert Andrew", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "6794b20ff1a79867a414c13ad66a52f2", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "RAND, Duncan", "id": "7"}], "title": "The GridPP DIRAC project - DIRAC for non-LHC communities", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T18:45:51.430959+00:00", "description": "", "title": "dirac_for_non-LHC_VO.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/334\/attachments\/578520\/796658\/dirac_for_non-LHC_VO.pdf", "filename": "dirac_for_non-LHC_VO.pdf", "content_type": "application\/pdf", "type": "file", "id": 796658, "size": 438725}, {"_type": "attachment", "modified_dt": "2015-04-10T18:45:51.430959+00:00", "description": "", "title": "dirac_for_non-LHC_VO.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/334\/attachments\/578520\/796659\/dirac_for_non-LHC_VO.pptx", "filename": "dirac_for_non-LHC_VO.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796659, "size": 476417}], "title": "Slides", "default_folder": false, "id": 578520, "description": ""}], "_type": "Contribution", "description": "The GridPP consortium in the UK is currently testing a multi-VO DIRAC service aimed at non-LHC VOs. These VOs are typically small (fewer than two hundred members) and generally do not have a dedicated computing support post. The majority of these represent particle physics experiments (e.g. T2K, NA62 and COMET), although the scope of the DIRAC service is not limited to this field. A few VOs have designed bespoke tools around the EMI-WMS & LFC, while others have so far eschewed distributed resources as they perceive the overhead for accessing them to be too high.  \r\nThe aim of the GridPP DIRAC project is to provide an easily adaptable toolkit for such VOs in order to lower the threshold for access to distributed resources such as grid and cloud computing. As well as hosting a centrally run DIRAC service, we will also publish our changes and additions to the base DIRAC codebase under an open-source license.   \r\nWe started by taking a survey of the existing VO specific solutions using the feedback to determine the user requirements that were driving these implementations. These details were then used to map the base requirements to available DIRAC features, implementing any additional common functionality that was needed. Once the groundwork was complete, this knowledge was shared with the existing VOs and we worked with them to adapt their grid model to use the new DIRAC service.   \r\nThe experience gained from this process was then used to recommend sensible approaches to the new VOs and assist them in getting started with distributed computing. We investigated different support models and found that a mailing list was the most accessible option for the target audience, while GGUS is used for tracking service issues.\r\n\r\nWe report on the current status of this project and show increasing adoption of DIRAC within the non-LHC communities.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578520", "resources": [{"_type": "LocalFile", "name": "dirac_for_non-LHC_VO.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/334\/attachments\/578520\/796658\/dirac_for_non-LHC_VO.pdf", "fileName": "dirac_for_non-LHC_VO.pdf", "_fossil": "localFileMetadata", "id": "796658", "_deprecated": true}, {"_type": "LocalFile", "name": "dirac_for_non-LHC_VO.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/334\/attachments\/578520\/796659\/dirac_for_non-LHC_VO.pptx", "fileName": "dirac_for_non-LHC_VO.pptx", "_fossil": "localFileMetadata", "id": "796659", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/334", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "337", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5984f6179c1328f2e8b6e50d2d0017fd", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "KATO, Yuji", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "5984f6179c1328f2e8b6e50d2d0017fd", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "KATO, Yuji", "id": "0"}], "title": "Job monitoring on DIRAC for Belle II distributed computing", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-07T05:25:08.366806+00:00", "description": "", "title": "chep2015_yujikato.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/337\/attachments\/578521\/796660\/chep2015_yujikato.pdf", "filename": "chep2015_yujikato.pdf", "content_type": "application\/pdf", "type": "file", "id": 796660, "size": 1339807}], "title": "Slides", "default_folder": false, "id": 578521, "description": ""}], "_type": "Contribution", "description": "The BelleII is an asymmetric energy $e^{+}e^{-}$ collider experiment at SuperKEKB in Japan. One of the main goals of BelleII is to search for physics beyond the Standard Model with a data set of about $5 \\times 10^{10}$ $B\\bar{B}$ pairs. In order to store such huge amount of data including MC events and analyze it in a timely manner, BelleII adopted distributed computing model with DIRAC (Distributed Infrastructure with Remote Agent Control), which is a framework developed for the LHCb experiment to manage heterogeneous computing environment.\r\n  To effectively maximize resources, we must detect and repair errors quickly.  Monitoring job processing is crucial. Currently, DIRAC provides information such as status of jobs and log files. However, this is not sufficient to detect problems. By collecting statistical information such as number of successes\/failed jobs and analyzing log files, we can understand the status of each site. Furthermore, these processes must be automated to check all sites efficiently. We implement these functions in DIRAC.\r\n In this paper, we describe the design and experience with the monitor in the MC data production campaign.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578521", "resources": [{"_type": "LocalFile", "name": "chep2015_yujikato.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/337\/attachments\/578521\/796660\/chep2015_yujikato.pdf", "fileName": "chep2015_yujikato.pdf", "_fossil": "localFileMetadata", "id": "796660", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "5479333ca5a4bde353890f2a40fc3e73", "affiliation": "Nagoya Univ.", "_fossil": "contributionParticipationMetadata", "fullName": "HAYASAKA, Kiyoshi", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "98d3e8bcf4bdf71c0b16b293c4b7c37f", "affiliation": "Osaka University", "_fossil": "contributionParticipationMetadata", "fullName": "HARA, Takanori", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "7ae6773a40c49ae5776d96621f5c7ab5", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "MIYAKE, Hideki", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "3b85a4a025a30820faf4613b16c34fee", "affiliation": "University of Tokyo (JP)", "_fossil": "contributionParticipationMetadata", "fullName": "UEDA, I", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/337", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "336", "speakers": [{"_type": "ContributionParticipation", "emailHash": "b4e7a4ea1d679a57a1467fc9ae74c443", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "FAYER, Simon", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "6cccef6d6cebb7a770526f8954e4483f", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "HUFFMAN, Adam", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "b4e7a4ea1d679a57a1467fc9ae74c443", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "FAYER, Simon", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "6cccef6d6cebb7a770526f8954e4483f", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "HUFFMAN, Adam", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "cd17961312c904e3bb4b71723c8e87ac", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. COLLING, David", "id": "2"}], "title": "The GridPP DIRAC project - Deployment of a high-availability DIRAC service", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The GridPP DIRAC project aims to provide a production quality high-availability (HA) DIRAC service, hosted in the Imperial College HEP datacentre, for use by non-LHC VOs. The standard way to provide this level of reliability is to ensure that there are two instances of all components so that any single failure will not affect the running of the system. Bearing this in mind we started by inspecting the DIRAC framework and dependencies to work out a service layout that would be most compatible with an underlying HA system. DIRAC is based around a number of agent & service modules which communicate with each other via an RPC mechanism. A series of back-end MySQL databases are also accessed by most components over a standard TCP connection. All of these design choices by the DIRAC team lend themselves directly to running the software in a high-availability configuration.   \r\nIn this paper, we detail the choices available for implementing a service like this and our rationale for selecting the solution we picked. While we primarily focus on DIRAC, our experiences from doing this should be generalisable to a number of other common HEP services. We cover the entire lifecycle of the machines from the selection and configuration of suitable hardware all the way through to the regular maintenance of the HA-DIRAC server, including how we handle service updates in a non-disruptive manner.   \r\nWith the assumption that the basic infrastructure, primarily power and networking, is already redundant we show that it is possible to deploy a highly available DIRAC server using only two servers and a separate NFS area to serve as the quorum disk. We report on the full details of our configuration, the testing required to verify that it is fully redundant and our experiences of running this in a production mode with real users.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/336", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "331", "speakers": [{"_type": "ContributionParticipation", "emailHash": "02312687d65fe06da9f992398d51d5ca", "affiliation": "Univ. Federal do Rio de Janeiro (BR)", "_fossil": "contributionParticipationMetadata", "fullName": "GOMES, Andressa", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "1d52859d0463f93b4eb39f805a6e9392", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SOLANS SANCHEZ, Carlos", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "02312687d65fe06da9f992398d51d5ca", "affiliation": "Univ. Federal do Rio de Janeiro (BR)", "_fossil": "contributionParticipationMetadata", "fullName": "GOMES, Andressa", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "1d52859d0463f93b4eb39f805a6e9392", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SOLANS SANCHEZ, Carlos", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "fcf8a90f9ded304d53a3a2222d488c6f", "affiliation": "Univ. Federal do Rio de Janeiro (BR)", "_fossil": "contributionParticipationMetadata", "fullName": "GUIMARAES FERREIRA, Fernando", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "fe96040e410a1df490d0452b81646714", "affiliation": "Univ. Federal do Rio de Janeiro (BR)", "_fossil": "contributionParticipationMetadata", "fullName": "MAIDANTCHIK, Carmen", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "048f4190e2e71edeea0d05a55eb92c1a", "affiliation": "Institute for High Energy Physics  (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "SOLODKOV, Alexander", "id": "4"}], "title": "Tile-in-ONE: A web platform which integrates Tile Calorimeter data quality and calibration assessment", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The ATLAS Tile Calorimeter assesses the quality of data in order to ensure its proper operation. A number of tasks are then performed by running several tools and systems,\r\nwhich were independently developed to meet distinct collaboration\u2019s requirements and do not necessarily builds an effective connection among them. Thus, a program is usually implemented without a global perspective of the detector, requiring basic software features. In addition, functionalities may overlap in their objectives and frequently replicate resources retrieval mechanisms.\r\n\r\nTile-in-ONE is a unique platform that assembles various web systems used by the calorimeter community through a single framework and a standard technology. It provides an infrastructure to support the code implementation, avoiding duplication of work while integrating with an overall view of the detector status. Database connectors smooth the process of information access since developers do not need to be aware of where records are placed and how to extract them. Within the environment, a dashboard stands for a particular Tile operation aspect and gets together plug-ins, i.e. software components that add specific features to an existing application.\r\n\r\nA server contains the platform core, which represents the basic environment to deal the configuration, manage user settings and load plug-ins at runtime. A web middleware assists users to develop their own plug-ins, perform tests and integrate them into the platform as a whole. Backends are employed to allow that any type of application is interpreted and displayed in a uniform way.", "track": "Track6: Facilities, Infrastructure, Network", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "4401dd600c80c0685d5ef4c33fac89b3", "affiliation": "Northern Illinois University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BURGHGRAVE, Blake Oliver", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "f7e4326c059181764a28bdf055c82c3b", "affiliation": "Northern Illinois University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "SMIRNOV, Iouri", "id": "6"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/331", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "330", "speakers": [{"_type": "ContributionParticipation", "emailHash": "f00ebbbe44eef5d1c8c2149dad2d39f7", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BRARDA, Loic", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "f00ebbbe44eef5d1c8c2149dad2d39f7", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BRARDA, Loic", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "84da75096ea6c971e92389f60aa27eba", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHEBBI, Mohamed", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "8b077ff83682eaf6c48b6ed54cb73828", "affiliation": "University of Sofia (BG)", "_fossil": "contributionParticipationMetadata", "fullName": "MOHAMED, Hristo", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "7d07f865b38767d4d2ed9895ebae0697", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "HAEN, Christophe", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "4b2dee02cc9c077ec5c48080350a3b29", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CAMPORA PEREZ, Daniel Hugo", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "603e776a16e0fe7ca160d9e475432a51", "affiliation": "Istituto Nazionale Fisica Nucleare (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "SBORZACCHI, Francesco", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "6bb6dd3105e4f5628e4868479399cf83", "affiliation": "Ministere des affaires etrangeres et europeennes (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "OTTO, Adam Jedrzej", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "9f655ee0d367b6896f4dd3a37228be6f", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "NEUFELD, Niko", "id": "7"}], "title": "Migration experiences of the LHCb Online cluster to Puppet and Icinga2", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The LHCb experiment operates a large computing infrastructure with\r\nmore than 2000 servers, 300 virtual machines and 400 embedded systems.\r\nMany of the systems are operated diskless from NFS or iSCSI\r\nroot-volumes. They are connected by more than 200 switches and\r\nrouters. We have recently completed the migration of the management of\r\nthis system from Quattor to puppet and of the original monitoring\r\nstructure based on Icinga to icinga2. We also have a tight integration\r\nof icinga2 in puppet. In this paper we present challenges and\r\nexperiences during this migration. We discuss and contrast the\r\nexpected and observed operational improvements.", "track": "Track6: Facilities, Infrastructure, Network", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/330", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:45:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "333", "speakers": [{"_type": "ContributionParticipation", "emailHash": "bd221996d4a256a8003a73d408902cf5", "affiliation": "University of Manchester (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "FORTI, Alessandra", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "bd221996d4a256a8003a73d408902cf5", "affiliation": "University of Manchester (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "FORTI, Alessandra", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "3104caab90081a7fa54ff8c20595e0c0", "affiliation": "Centro de Investigaciones Energ. Medioambientales y Tecn. - (ES", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PEREZ-CALERO YZQUIERDO, Antonio", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "30a73745afbaea1197a46a3cad7c58b9", "affiliation": "NIKHEF (NL)", "_fossil": "contributionParticipationMetadata", "fullName": "TEMPLON, Jeff", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "fd06959b93c405340e9900abf962beb7", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "LAHIFF, Andrew David", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "daa23b6af556dd88fe716b6b6826ad21", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "HARTMANN, Thomas", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "becdbc08be3962d35c9e4866e938fcc3", "affiliation": "Karlsruhe Institute of Technology (KIT)", "_fossil": "contributionParticipationMetadata", "fullName": "ALEF, Manfred", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "c54f78b4e0cce12acec6089026dae435", "affiliation": "ETH Zurich", "_fossil": "contributionParticipationMetadata", "fullName": "GILA, Miguel", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "b120fdf9f04a123ea43e83d2718cd2ef", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SKIPSEY, Samuel Cadellin", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "3c540104c238880e46ed73772d460797", "affiliation": "Universitat Aut\u00f2noma de Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "ACOSTA SILVA, Carlos", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "2bfc3dcae6061d54150b9904432bf7ee", "affiliation": "Jozef Stefan Institute (SI)", "_fossil": "contributionParticipationMetadata", "fullName": "FILIPCIC, Andrej", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "73f9d874a0a0bd47c5e380afc1d800bf", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WALKER, Rodney", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "fc2d531a1d59c0d27cf4550e45442ae6", "affiliation": "University of London (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "WALKER, Christopher John", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "092bd94d46ab62b6af2612971d14c045", "affiliation": "CC-IN2P3 - Centre de Calcul  (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "GADRAT, Sebastien", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "22a1a3e2ab267299b1754df69296edaf", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "DAL PRA, Stefano", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "d784cb31ed0d1334efa12f3200f768f0", "affiliation": "University of London (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "TRAYNOR, Daniel Peter", "id": "14"}], "title": "Multicore job scheduling in the Worldwide LHC Computing Grid", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T03:47:43.098934+00:00", "description": "", "title": "20150414-chep_mcore.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/333\/attachments\/578522\/796661\/20150414-chep_mcore.pdf", "filename": "20150414-chep_mcore.pdf", "content_type": "application\/pdf", "type": "file", "id": 796661, "size": 1875821}], "title": "Slides", "default_folder": false, "id": 578522, "description": ""}], "_type": "Contribution", "description": "After the successful first run of the LHC, data taking will restart in early 2015 with unprecedented experimental conditions leading to increased data volumes and event complexity. In order to process the data generated in such scenario and exploit the multicore architectures of current CPUs, the LHC experiments have developed parallelized software for data reconstruction and simulation. A good fraction of their computing effort is still expected to be executed as single-core tasks. Therefore, jobs with diverse resources requirements will be distributed across the Worldwide LHC Computing Grid (WLCG), making workload scheduling a complex problem in itself.\r\n\r\nIn response to this challenge, the WLCG Multicore Deployment Task Force has been created with the purpose of coordinating the joint effort from experiments and WLCG sites. The main objective is to ensure the convergence of approaches from the different LHC Virtual Organizations (VOs) to make the best use of the shared resources in order to satisfy their new computing needs and minimize any inefficiency deriving from the scheduling mechanisms. This should also be achieved without imposing unnecessary complexities in the way sites manage their resources.\r\n\r\nJob scheduling in the WLCG involves the use of grid-wide workload submission tools by the VOs linked via Computing Element (CE) middleware to the batch system technologies managing local resources at every WLCG site. Each of these elements and their interaction has been analyzed by the Task Force. The various job submission strategies proposed by the LHC VOs have been evaluated, providing feedback for the evolution of their grid-wide submission models and tools. The diverse capabilities of different CE technologies in passing the resource request from the VOs to the sites have been examined. The technical features of the most common batch systems in WLCG sites have been discussed for a better understanding of their multicore job handling capabilities. Participants in the Task Force have also been encouraged to share their system configurations with the purpose of avoiding duplicated efforts among sites operating the same technologies.\r\n\r\nThis contribution will present the activities and progress of the Task Force related to the aforementioned topics, including experiences from key sites on how to best use different batch system technologies, the evolution of workload submission tools by the experiments and the knowledge gained from scale tests of the different proposed job submission strategies.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578522", "resources": [{"_type": "LocalFile", "name": "20150414-chep_mcore.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/333\/attachments\/578522\/796661\/20150414-chep_mcore.pdf", "fileName": "20150414-chep_mcore.pdf", "_fossil": "localFileMetadata", "id": "796661", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/333", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": null, "keywords": [], "id": "332", "speakers": [{"_type": "ContributionParticipation", "emailHash": "572f6f06cb2be973d28b9d129a0a91fd", "affiliation": "University of Edinburgh (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "BHIMJI, Wahid", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "572f6f06cb2be973d28b9d129a0a91fd", "affiliation": "University of Edinburgh (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "BHIMJI, Wahid", "id": "0"}], "title": "Using HPC for data-intensive HEP workflows", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The data requirements of LHC experiments are set to grow beyond their expected resource allocation. Meanwhile \u2018Big Data\u2019 is no longer the preserve of High Energy Physics, with various communities also requiring and deploying significant resource to meet their needs. There has been substantial activity in the HEP community to make opportunistic use of compute resource including large-scale HPC resources, but this has largely avoided tackling data-intensive workloads or opportunistic storage resource. At the same time, there have been considerable recent developments on the data and storage side of HEP, which allow for more flexible use of varied storage resources, and in distributed data access mechanisms to allow seamless access and management of data regardless of actual location. HPC resources can have additional constraints in making use of this however including restricted network access and limited storage capacity on compute nodes, making use instead of a shared file-system and gateway servers for access to off-site data.\r\n \r\nWe develop a lightweight data management approach that exploits HEP data federation developments within HPC constraints, to allow for flexible, and dynamic, utilisation by different communities of diverse storage and data resources. This is demonstrated with production HEP workloads, such as those for the ATLAS experiment, run on a range of compute resources accessing data from diverse locations. This includes HPC resources at Archer, the UK national supercomputer, as well as commercial cloud and WLCG grid resources accessing data stored on the Research Data Facility (a national, shared, multi-petabyte data store) as well as commercial cloud storage and WLCG storage resources.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "d2481eb1f11b08f40d4e1ef3cdb920dc", "affiliation": "University of Edinburgh (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "WASHBROOK, Andrew John", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/contribution\/332", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "6", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5dff1cb9e65e0cb44929be5bdf05dbfd", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "FAJARDO HERNANDEZ, Edgar", "id": "8"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "d43c2381209b8e95c7be356ece5900b7", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DOST, Jeffrey Michael", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "5dff1cb9e65e0cb44929be5bdf05dbfd", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "FAJARDO HERNANDEZ, Edgar", "id": "8"}], "title": "How much higher can HTCondor fly", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T20:01:10.153394+00:00", "description": "", "title": "poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/6\/attachments\/578523\/796662\/poster.pdf", "filename": "poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796662, "size": 3904911}], "title": "Slides", "default_folder": false, "id": 578523, "description": ""}], "_type": "Contribution", "description": "The HTCondor batch system is heavily used in the HEP community as the batch system for several WLCG resources. Moreover it is the backbone of the GlideInWMS, the main pilot system used by CMS. To prepare for LHC Run 2, we are probing the scalability limits of new versions and configurations of HTCondor with the goal of reaching at least 200,000 simultaneous running jobs in a single pool.\r\n\r\nA sleeper pool of this size was achieved without a major impact in real jobs by using only 10,000 real slots distributed at several WLCG sites. We will report on how this was made and the impact it has in future scalability tests, not only of HTCondor but of any web faced service.\r\n\r\nHigh configurability is one of the main capabilities of HTCondor. In addition to the test conditions and the testbed topology, we include the suggested configuration options used to obtain the scaling results.Finally, we will list the features present in newer versions of HTCondor that allow for sustained operations at scales well beyond what was previously possible.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578523", "resources": [{"_type": "LocalFile", "name": "poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/6\/attachments\/578523\/796662\/poster.pdf", "fileName": "poster.pdf", "_fossil": "localFileMetadata", "id": "796662", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "21bc9e746b05c457c2f1ca200b016f7b", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HOLZMAN, Burt", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "55a49eed77d2c92902808de16b05f780", "affiliation": "Univ of Wisconsin-Madison, Wisconsin, USA", "_fossil": "contributionParticipationMetadata", "fullName": "TANNENBAUM, Todd", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "37a9d67c5498998436724740f824542b", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LETTS, James", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "1ab30f4066c48fc202816d553d46d1fe", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "TIRADANI, Anthony", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "e82c46a3a0d49dd555c888d113eace3a", "affiliation": "University of Nebraska (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BOCKELMAN, Brian Paul", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "96f8083a0d85892c0d51b3d05fbe0a75", "affiliation": "University of Wisconsin", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. FREY, Jaime", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "8ce8233ffe359f39d3fdd125371fe45b", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MASON, David Alexander", "id": "7"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/6", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "99", "speakers": [{"_type": "ContributionParticipation", "emailHash": "28fec050a7a0cddcbe83319b039e0c92", "affiliation": "Institut de F\u00edsica d'Altes Energies - Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "PACHECO PAGES, Andreu", "id": "8"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "28fec050a7a0cddcbe83319b039e0c92", "affiliation": "Institut de F\u00edsica d'Altes Energies - Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "PACHECO PAGES, Andreu", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "77e6974a25a29ef6da9a14157c570471", "affiliation": "Instituto de Fisica Corpuscular (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "GARCIA NAVARRO, Jose Enrique", "id": "0"}], "title": "ATLAS Monte Carlo production Run-1 experience and readiness for Run-2 challenges", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-03-30T07:38:12.354771+00:00", "description": "", "title": "150323-Poster-CHEP2015-MonteCarloProduction-v15.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/99\/attachments\/578524\/796663\/150323-Poster-CHEP2015-MonteCarloProduction-v15.pdf", "filename": "150323-Poster-CHEP2015-MonteCarloProduction-v15.pdf", "content_type": "application\/pdf", "type": "file", "id": 796663, "size": 3273774}], "title": "Poster", "default_folder": false, "id": 578524, "description": ""}], "_type": "Contribution", "description": "In this presentation we will review the ATLAS Monte Carlo production\u00a0setup including the different production steps involved in full and\u00a0fast detector simulation. A report on the Monte Carlo production\u00a0campaigns during Run-I, Long Shutdown 1 (LS1) and status of the production for Run-2 will be presented. The presentation will include the details on various performance aspects. Important\u00a0improvements in the workflow and software will be highlighted.\r\n\r\nBesides standard Monte Carlo production for data analyses at 7 and 8\u00a0TeV, the production accommodates for various specialised activities.\u00a0These range from extended Monte Carlo validation, Geant4 validation,\u00a0pileup simulation using zero bias data and production for various\u00a0upgrade studies. The challenges of these activities will be discussed.", "track": "Track2: Offline software ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578524", "resources": [{"_type": "LocalFile", "name": "150323-Poster-CHEP2015-MonteCarloProduction-v15.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/99\/attachments\/578524\/796663\/150323-Poster-CHEP2015-MonteCarloProduction-v15.pdf", "fileName": "150323-Poster-CHEP2015-MonteCarloProduction-v15.pdf", "_fossil": "localFileMetadata", "id": "796663", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "ae833af8243d4f9cefdd171d853ef91a", "affiliation": "University of Cambridge (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "CHAPMAN, John Derek", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "6435c8c2f511d9097ce12bf596716744", "affiliation": "University of Oxford", "_fossil": "contributionParticipationMetadata", "fullName": "FERRANDO, James", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "dd195c43cad3b01210e8712e17757443", "affiliation": "University of Oxford (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "GWENLAN, Claire", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "3cfc1af559364ef28008406b5f615b7a", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "MEHLHASE, Sascha", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "6aef683082e2cae7cc6fb0e086c1b9be", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "TSULAIA, Vakho", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "c8b8647f9e6a89a40a3aae5e517a0c30", "affiliation": "ATLAS", "_fossil": "contributionParticipationMetadata", "fullName": "VANIACHINE, Alexandre", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "610c85ed93bcd4366f55a5a916f0783b", "affiliation": "University of Oxford (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "ZHONG, Jiahang", "id": "7"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/99", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": null, "keywords": [], "id": "98", "speakers": [{"_type": "ContributionParticipation", "emailHash": "dccc8a1b9da6eae7c8e0ff90582f9219", "affiliation": "GSI", "_fossil": "contributionParticipationMetadata", "fullName": "KLIEMT, Ralf", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "dccc8a1b9da6eae7c8e0ff90582f9219", "affiliation": "GSI", "_fossil": "contributionParticipationMetadata", "fullName": "KLIEMT, Ralf", "id": "0"}], "title": "The PandaRoot Software and Message Queues", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "Panda is one of the main experiments at the future FAIR facility at Darmstadt, featuring antiproton induced reactions for beam momenta of 1.5-15 GeV\/c. The PandaRoot software provides detector simulations, reconstruction as well as physics analysis. It is based on the FairRoot framework, developed at GSI, employing standard software for high energy physics such as ROOT and Virtual Monte-Carlo as well as highly efficient data transport via message queues. In preparation for data taking and reconstruction at high interaction rates up to 20 MHz without a dedicated hardware trigger the software is being redesigned to run on a continuous stream of data with message queues, recently introduced in FairRoot, instead of the common linear processing chain approach. Efficient online reconstruction as well as software triggering on physics regions of interest are being developed to sufficiently reduce the background level sufficiently to store the data for in-depth offline reconstruction and analysis. This contribution will give an overview on PandaRoot, recent developments and the perspective to the future with a focus on the new message queue scheme.", "track": "Track2: Offline software ", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/contribution\/98", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:15:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:30:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "91", "speakers": [{"_type": "ContributionParticipation", "emailHash": "be94bcf4e42480309d7fd4e274b250ed", "affiliation": "DANTE", "_fossil": "contributionParticipationMetadata", "fullName": "CAPONE, Vincenzo", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "be94bcf4e42480309d7fd4e274b250ed", "affiliation": "DANTE", "_fossil": "contributionParticipationMetadata", "fullName": "CAPONE, Vincenzo", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "762889dbc7f8f6519cdd3ac971686a2e", "affiliation": "DANTE", "_fossil": "contributionParticipationMetadata", "fullName": "USMAN, Mian", "id": "1"}], "title": "The G\u00c9ANT network: addressing current and future needs for the High Energy Physics community.", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T09:26:31.961102+00:00", "description": "", "title": "2015-04-14_GEANT_CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/91\/attachments\/578525\/796664\/2015-04-14_GEANT_CHEP2015.pdf", "filename": "2015-04-14_GEANT_CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796664, "size": 2568180}, {"_type": "attachment", "modified_dt": "2015-04-14T09:26:31.961102+00:00", "description": "", "title": "2015-04-14_GEANT_CHEP2015.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/91\/attachments\/578525\/796665\/2015-04-14_GEANT_CHEP2015.pptx", "filename": "2015-04-14_GEANT_CHEP2015.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796665, "size": 4341392}], "title": "Slides", "default_folder": false, "id": 578525, "description": ""}], "_type": "Contribution", "description": "The G\u00c9ANT infrastructure is the backbone that serves the scientific communities in Europe for their data movement needs and their access to international research and education networks. Using the extensive fibre footprint and infrastructure in Europe the G\u00c9ANT network delivers a portfolio of services aimed to best fit the specific needs of the users, including Authentication and Authorization Infrastructure, end-to-end performance monitoring, advanced network services (dynamic circuits, L2-L3VPN, MD-VPN).\r\n\r\nThis talk will outline the factors that help the G\u00c9ANT network to respond to the needs of the High Energy Physics community, both in Europe and worldwide.\r\n\r\nThe Pan-European network provides the connectivity between 40 European national research and education networks. In addition, G\u00c9ANT also connects the European NRENs to the R&E networks in other world region and has reach to over 110 NREN worldwide, making G\u00c9ANT the best connected Research and Education network, with its multiple intercontinental links to different continents e.g. North and South America, Africa and Asia-Pacific.\r\nThe High Energy Physics computational needs have always had (and will keep having) a leading role among the scientific user groups of the G\u00c9ANT network: the LHCONE overlay network has been built, in collaboration with the other big world REN, specifically to address the peculiar needs of the LHC data movement. Recently, as a result of a series of coordinated efforts, the LHCONE network has been expanded to the Asia-Pacific area, and is going to include some of the main regional R&E network in the area.\r\nThe LHC community is not the only one that is actively using a distributed computing model (hence the need for a high-performance network); new communities are arising, as BELLE II. G\u00c9ANT is deeply involved also with the BELLE II Experiment, to provide full support to their distributed computing model, along with a Perfsonar-based network monitoring system. G\u00c9ANT has also coordinated the setup of the network infrastructure to perform the BELLE II Trans-Atlantic Data Challenge, and has been active on helping the BELLE II community to sort out their end-to-end performance issues.\r\n\r\nIn this talk we will provide information about the current G\u00c9ANT network architecture and of the international connectivity, along with the upcoming upgrades and the planned and foreseeable improvements. We will also describe the implementation of the solutions provided to support the LHC and BELLE II experiments.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578525", "resources": [{"_type": "LocalFile", "name": "2015-04-14_GEANT_CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/91\/attachments\/578525\/796664\/2015-04-14_GEANT_CHEP2015.pdf", "fileName": "2015-04-14_GEANT_CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796664", "_deprecated": true}, {"_type": "LocalFile", "name": "2015-04-14_GEANT_CHEP2015.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/91\/attachments\/578525\/796665\/2015-04-14_GEANT_CHEP2015.pptx", "fileName": "2015-04-14_GEANT_CHEP2015.pptx", "_fossil": "localFileMetadata", "id": "796665", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/91", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "90", "speakers": [{"_type": "ContributionParticipation", "emailHash": "d96010739664021f1bf2e72848130026", "affiliation": "Jefferson Lab", "_fossil": "contributionParticipationMetadata", "fullName": "GYURJYAN, Vardan", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "d96010739664021f1bf2e72848130026", "affiliation": "Jefferson Lab", "_fossil": "contributionParticipationMetadata", "fullName": "GYURJYAN, Vardan", "id": "0"}], "title": "Evaluation of the flow-based programming (FBP) paradigm as an alternative to standard programming practices in physics data processing applications", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [], "_type": "Contribution", "description": "The majority of developed physics data processing applications (PDP) are single, sequential processes that start at a point in time, and advance one step at a time until they are finished. In the current era of cloud computing and multi-core hardware architectures this approach has noticeable limitations. \r\nIn this paper we present a detailed evaluation of the FBP-based Clas12 event reconstruction program that was deployed and operated both in cloud and in batch processing environments. We demonstrate the programming methodology and discuss some of the issues and optimizations affecting performance. We will also discuss our choice of using the Petri-Net process modeling formalism for the representation of the Clas12 PDP application building blocks which exhibit concurrency, parallelism, and synchronization.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/90", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "93", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2f82b57b652dfb168f7db13f8204ea29", "affiliation": "University of Wisconsin (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. GHABROUS LARREA, Carlos", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2f82b57b652dfb168f7db13f8204ea29", "affiliation": "University of Wisconsin (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. GHABROUS LARREA, Carlos", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "33d54cd755cd4006f33608e816ab4b4a", "affiliation": "University of Bristol (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "BROOKE, Jim", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "ecb242299df18d1d884e57753e98c21f", "affiliation": "University of Warsaw (PL)", "_fossil": "contributionParticipationMetadata", "fullName": "BUNKOWSKI, Karol", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "7d4d250ff7947e8ad73a1d525f1e58f8", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "THEA, Alessandro", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "e1017c980475901f3ecbbfdcf81d4df9", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CALI, Ivan Amos", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "61351282388110df1a8eca5c75166283", "affiliation": "University of Wisconsin (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LAZARIDIS, Christos", "id": "5"}], "title": "SWATCH: common control SW for the uTCA-based upgraded CMS L1 Trigger", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-01T14:41:02.172593+00:00", "description": "", "title": "CHEP2015_Poster_Ghabrous.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/93\/attachments\/578526\/796666\/CHEP2015_Poster_Ghabrous.pdf", "filename": "CHEP2015_Poster_Ghabrous.pdf", "content_type": "application\/pdf", "type": "file", "id": 796666, "size": 1765461}], "title": "Poster", "default_folder": false, "id": 578526, "description": ""}], "_type": "Contribution", "description": "The CMS (Compact Muon Solenoid) L1 (Level-1) Trigger electronics are composed of a large number of different cards based on the VMEBus standard. The majority of the system is being replaced to adapt the trigger to the higher collision rates the LHC will deliver after the LS1 (Long Shutdown 1), the first phase on the CMS upgrade program. As a consequence, the software that controls, monitors and tests the hardware will need to be re-written.\r\n\r\nThe upgraded trigger will consist of a set of general purpose boards of similar technology that follow the uTCA specification, thus resulting in a more homogeneous system. A great effort has been made to identify the common firmware blocks and components shared across different cards, regardless of the role they play within the trigger data path. A similar line of work has been followed in order to identify all possible common functionalities in the control software, as well as in the database where the hardware initialisation and configuration data are stored. This will not only increase the homogeneity on the software and database sides, but it will also reduce the manpower needed to accommodate the online SW to the changes on hardware.\r\nDue to the fact that the upgrade will take place in different stages, it has been taken into consideration that these new components had to be integrated in the current SW framework. \r\n\r\nThis paper presents the design of the control SW and configuration database for the upgraded L1 Trigger.", "track": "Track1: Online computing ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578526", "resources": [{"_type": "LocalFile", "name": "CHEP2015_Poster_Ghabrous.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/93\/attachments\/578526\/796666\/CHEP2015_Poster_Ghabrous.pdf", "fileName": "CHEP2015_Poster_Ghabrous.pdf", "_fossil": "localFileMetadata", "id": "796666", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/93", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "92", "speakers": [{"_type": "ContributionParticipation", "emailHash": "02bd81dc701ab62d43cb1a80954a5396", "affiliation": "Universitaet Bern (CH)", "_fossil": "contributionParticipationMetadata", "fullName": "HOSTETTLER, Michi", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "11d0d3621ad6265a31d681a3580e1ccc", "affiliation": "Universitaet Bern (CH)", "_fossil": "contributionParticipationMetadata", "fullName": "HAUG, Sigve", "id": "0"}], "title": "ATLAS computing on the HPC Piz Daint machine", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-06T14:49:06.635400+00:00", "description": "", "title": "ATL-SOFT-SLIDE-2015-104.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/92\/attachments\/578527\/796667\/ATL-SOFT-SLIDE-2015-104.pdf", "filename": "ATL-SOFT-SLIDE-2015-104.pdf", "content_type": "application\/pdf", "type": "file", "id": 796667, "size": 2728518}], "title": "Poster", "default_folder": false, "id": 578527, "description": ""}], "_type": "Contribution", "description": "The Piz Daint Cray XC30 HPC system at CSCS, the Swiss National Supercomputing centre, is in 2014 the highest ranked European system on TOP500, also featuring GPU accelerators. Event generation and detector simulation for the\u00a0 ATLAS experiment has been enabled for this machine. We report on the technical solutions, performance, HPC policy\u00a0 challenges and possible future opportunities\u00a0for HEP on extreme HPC systems. In particular a custom made\u00a0 integration to the ATLAS job submission system has been developed via the Advanced Resource Connector (ARC) middleware. Further, some GPU acceleration of the GEANT4 detector simulations were implemented to justify the\u00a0allocation request for this machine.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578527", "resources": [{"_type": "LocalFile", "name": "ATL-SOFT-SLIDE-2015-104.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/92\/attachments\/578527\/796667\/ATL-SOFT-SLIDE-2015-104.pdf", "fileName": "ATL-SOFT-SLIDE-2015-104.pdf", "_fossil": "localFileMetadata", "id": "796667", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "2bfc3dcae6061d54150b9904432bf7ee", "affiliation": "Jozef Stefan Institute (SI)", "_fossil": "contributionParticipationMetadata", "fullName": "FILIPCIC, Andrej", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "02bd81dc701ab62d43cb1a80954a5396", "affiliation": "Universitaet Bern (CH)", "_fossil": "contributionParticipationMetadata", "fullName": "HOSTETTLER, Michi", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "73f9d874a0a0bd47c5e380afc1d800bf", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WALKER, Rodney", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/92", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:30:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:45:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "95", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5bb2c4f472b781f69b98ca2a7888137f", "affiliation": "LBNL", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GERHARDT, Lisa", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "5bb2c4f472b781f69b98ca2a7888137f", "affiliation": "LBNL", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GERHARDT, Lisa", "id": "0"}], "title": "Accelerating Scientific Analysis with SciDB", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T19:47:54.082008+00:00", "description": "", "title": "Adventures_in_SciDB_at_NERSC.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/95\/attachments\/578528\/796668\/Adventures_in_SciDB_at_NERSC.pdf", "filename": "Adventures_in_SciDB_at_NERSC.pdf", "content_type": "application\/pdf", "type": "file", "id": 796668, "size": 3243895}], "title": "Slides", "default_folder": false, "id": 578528, "description": ""}], "_type": "Contribution", "description": "SciDB is an open-source analytical database for scalable complex analytics on very large array or multi-structured data from a variety of sources, programmable from Python and R. It runs on HPC, commodity hardware grids, or in a cloud and can manage and analyze terabytes of array-structured data and do complex analytics in-database. \r\nWe present an overall description of the SciDB framework and describe its implementation at NERSC at Lawrence Berkeley National Laboratory. A case study using SciDB to analyze data from the LUX dark matter detector is described. LUX is a 370 kg liquid xenon time-projection chamber built to directly detect galactic dark matter in an underground laboratory 1 mile under the Black Hills in South Dakota, USA. In the 2013 initial data run, LUX collected 86 million events and wrote 32 TB of data of which only 160 events are retained for final analysis. The data rate for the new dark matter run starting in 2014 is expected to exceed 250 TB \/ year. We describe how SciDB is used to dramatically streamline the data collection and analysis, and discuss future plans for a large parallel SciDB array at NERSC.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578528", "resources": [{"_type": "LocalFile", "name": "Adventures_in_SciDB_at_NERSC.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/95\/attachments\/578528\/796668\/Adventures_in_SciDB_at_NERSC.pdf", "fileName": "Adventures_in_SciDB_at_NERSC.pdf", "_fossil": "localFileMetadata", "id": "796668", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "3bb7dd6fb8750be27aa88889739f40ab", "affiliation": "LBNL", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. YAO, Yushu", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "1fbe9ca34342e163edc6fdfdd323b4b5", "affiliation": "LBNL", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. FAHAM, Carlos", "id": "2"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/95", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "94", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ea3532ceb8f35de512192be0a5a2bb3c", "affiliation": "Universita' di Napoli Federico II and INFN", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. ALOISIO, Alberto", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ea3532ceb8f35de512192be0a5a2bb3c", "affiliation": "Universita' di Napoli Federico II and INFN", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. ALOISIO, Alberto", "id": "0"}], "title": "FPGAs go wireless", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "We present a feasibility study of a RF transmitters and modulators\r\n based on parametric softcores fully embedded in a general purpose FPGA fabric,\r\n  without using external components.\r\n  This architecture aims at providing wireless physical layers\r\n  to I-o-T and NFC protocols with programmable hardware.\r\nWe show preliminary results with latest generation 7-series XILINX FPGA.", "track": "Track1: Online computing ", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "f715ea72de0966c750cb995e9c0d13f4", "affiliation": "Universita' di Napoli Federico II and INFN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GIORDANO, Raffaele", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/94", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "97", "speakers": [{"_type": "ContributionParticipation", "emailHash": "4b26358accbf4dbfaa3d61a67a5392f2", "affiliation": "Japan Atomic Energy Agency", "_fossil": "contributionParticipationMetadata", "fullName": "TOMOYORI, Katsuaki", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "4b26358accbf4dbfaa3d61a67a5392f2", "affiliation": "Japan Atomic Energy Agency", "_fossil": "contributionParticipationMetadata", "fullName": "TOMOYORI, Katsuaki", "id": "0"}], "title": "Background elimination using the SNIP method for Bragg reflections from a protein crystal measured by a time-of-flight single-crystal neutron diffractometer", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T01:28:19.436488+00:00", "description": "", "title": "2015_chep_20150409.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/97\/attachments\/578529\/796669\/2015_chep_20150409.pdf", "filename": "2015_chep_20150409.pdf", "content_type": "application\/pdf", "type": "file", "id": 796669, "size": 2030497}], "title": "Slides", "default_folder": false, "id": 578529, "description": ""}], "_type": "Contribution", "description": "In neutron protein crystallography, it should be also emphasized that the weak Bragg reflections due to the large unit cells may be buried beneath the strong background caused by the incoherent scattering of hydrogen atoms. Therefore, the background estimation from the source is more reliable to improve the accuracy of Bragg integral intensity. We propose the adoption of Statistics-sensitive Nonlinear Iterative Peak-clipping (SNIP) algorithm for background estimation [1], which can eliminate the background from the source spectrum as well as statistically enhance low peaks. In addition, it was recently reported that the Landau and Vavilov distributions, which are used to describe the energy loss of charged particles traversing a thin absorber were found to be in excellent agreement with the observed TOF profile [2]. I show that it may be beneficial to establish a profile-fitting method with the combined use of the Landau\/Vavilov functions to faithfully reproduce the TOF peak shape and a SNIP background evaluation algorithm to eliminate statistical fluctuations. \r\n[1] Ryan CG et al (1988) Nucl. Instr. Meth. Phys. Res. Sect. B 34:396-402\r\n[2] Tomoyori K et al (2013) Nucl. Instr. Meth. Phys. Res. Sect. A 723:128-135", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578529", "resources": [{"_type": "LocalFile", "name": "2015_chep_20150409.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/97\/attachments\/578529\/796669\/2015_chep_20150409.pdf", "fileName": "2015_chep_20150409.pdf", "_fossil": "localFileMetadata", "id": "796669", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "2bfe9bf68cae5466c9213afeba47c5ca", "affiliation": "Japan Atomic Energy Agency", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KURIHARA, Kazuo", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "643ddeffb3612af9a73be66c00fe9b10", "affiliation": "Japan Atomic Energy Agency", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. TAMADA, Taro", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/97", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:00:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "96", "speakers": [{"_type": "ContributionParticipation", "emailHash": "f6b2cd0f375b5c0a914c4ebe77cc1015", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "RAUSCHMAYR, Nathalie", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "f6b2cd0f375b5c0a914c4ebe77cc1015", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "RAUSCHMAYR, Nathalie", "id": "0"}], "title": "A history-based estimation for LHCb job requirements", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-08T14:22:20.549603+00:00", "description": "", "title": "slides.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/96\/attachments\/578530\/796670\/slides.pdf", "filename": "slides.pdf", "content_type": "application\/pdf", "type": "file", "id": 796670, "size": 915222}], "title": "Slides", "default_folder": false, "id": 578530, "description": ""}], "_type": "Contribution", "description": "The main goal of a Workload Management System (WMS) is to find and allocate resources for the jobs it is handling. The more and more accurate information the WMS receives about the jobs, the easier it will be to accomplish its task, which will directly translate into a better utilization of resources. Traditionally, the information associated with each job, like expected runtime or memory requirement, is in the best case defined at submission time by the Production Manager or fixed by default to arbitrary conservative values. In the case of LHCb's Workload Management System, no mechanisms are provided that automatize the estimation of job requirements. As a result, in order to be conservative, much more CPU time is normally requested than actually needed. Particularly, in the context of multicore jobs this represents a major problem, since single- and multi-core jobs shall share the same resources. Therefore, in order to allow an optimization of the available resources, an accurate estimation of the necessary resources is required. As the main motivation for going to multicore jobs is the reduction of the overall memory footprint, the memory requirement of the jobs should also be correctly estimated.\r\n\r\nA detailed workload analysis of past LHCb jobs will be presented. It includes a study of which job features have a correlation with runtime and memory consumption. Based on these features, a supervised learning algorithm has been developed relying on a history-based prediction. The aim is to learn over time how jobs' runtime and memory evolve due to changes in the experimental conditions and the software versions. It will be shown that this estimation can be notably improved if the experimental conditions are taken into account.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578530", "resources": [{"_type": "LocalFile", "name": "slides.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/96\/attachments\/578530\/796670\/slides.pdf", "fileName": "slides.pdf", "_fossil": "localFileMetadata", "id": "796670", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/96", "roomFullname": null}, {"startDate": {"date": "2015-04-15", "tz": "Europe\/Zurich", "time": "02:00:00"}, "endDate": {"date": "2015-04-15", "tz": "Europe\/Zurich", "time": "02:45:00"}, "duration": 45, "session": "Plenary", "keywords": [], "id": "559", "speakers": [{"_type": "ContributionParticipation", "emailHash": "c64735b9540bc92c4d230a2ca252b7cf", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GUTSCHE, Oliver", "id": "0"}], "primaryauthors": [], "title": "Diversity in Computing Technologies: Grid, Cloud, HPC ... and Strategies for Dynamic Resource Allocation", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T12:47:16.870367+00:00", "description": "", "title": "150415_-_CHEP2015_-_Diversity_in_Computing_Technologies_and_Strategies_for_Dynamic_Resource_Allocation.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/559\/attachments\/578531\/796671\/150415_-_CHEP2015_-_Diversity_in_Computing_Technologies_and_Strategies_for_Dynamic_Resource_Allocation.pdf", "filename": "150415_-_CHEP2015_-_Diversity_in_Computing_Technologies_and_Strategies_for_Dynamic_Resource_Allocation.pdf", "content_type": "application\/pdf", "type": "file", "id": 796671, "size": 5359894}], "title": "Slides", "default_folder": false, "id": 578531, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578531", "resources": [{"_type": "LocalFile", "name": "150415_-_CHEP2015_-_Diversity_in_Computing_Technologies_and_Strategies_for_Dynamic_Resource_Allocation.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/559\/attachments\/578531\/796671\/150415_-_CHEP2015_-_Diversity_in_Computing_Technologies_and_Strategies_for_Dynamic_Resource_Allocation.pdf", "fileName": "150415_-_CHEP2015_-_Diversity_in_Computing_Technologies_and_Strategies_for_Dynamic_Resource_Allocation.pdf", "_fossil": "localFileMetadata", "id": "796671", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/559", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "05:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "05:45:00"}, "duration": 30, "session": "Plenary", "keywords": [], "id": "558", "speakers": [], "primaryauthors": [], "title": "Lenovo Corporation", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [], "_type": "Contribution", "description": "", "track": null, "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/558", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "03:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "03:30:00"}, "duration": 30, "session": "Plenary", "keywords": [], "id": "555", "speakers": [], "primaryauthors": [], "title": "KAGRA and the Global Network of Gravitational Wave Detectors -Construction Status and Prospects for GW Astronomy with Data Sharing Era-", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T23:34:35.135498+00:00", "description": "", "title": "KAGRA_CHEP20150414_kanda_r.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/555\/attachments\/578532\/796672\/KAGRA_CHEP20150414_kanda_r.pdf", "filename": "KAGRA_CHEP20150414_kanda_r.pdf", "content_type": "application\/pdf", "type": "file", "id": 796672, "size": 7713991}], "title": "Slides", "default_folder": false, "id": 578532, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578532", "resources": [{"_type": "LocalFile", "name": "KAGRA_CHEP20150414_kanda_r.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/555\/attachments\/578532\/796672\/KAGRA_CHEP20150414_kanda_r.pdf", "fileName": "KAGRA_CHEP20150414_kanda_r.pdf", "_fossil": "localFileMetadata", "id": "796672", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/555", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "02:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "03:00:00"}, "duration": 45, "session": "Plenary", "keywords": [], "id": "554", "speakers": [{"_type": "ContributionParticipation", "emailHash": "336e636659f12d5fe7aede25fd33f666", "affiliation": "University of Virginia", "_fossil": "contributionParticipationMetadata", "fullName": "GROUP, Robert", "id": "2"}], "primaryauthors": [], "title": "Computing in Intensity Frontier Accelerator Experiments", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T23:03:37.358288+00:00", "description": "", "title": "CHEP_2015_IF_Computing.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/554\/attachments\/578533\/796673\/CHEP_2015_IF_Computing.pdf", "filename": "CHEP_2015_IF_Computing.pdf", "content_type": "application\/pdf", "type": "file", "id": 796673, "size": 26343127}], "title": "Slides", "default_folder": false, "id": 578533, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578533", "resources": [{"_type": "LocalFile", "name": "CHEP_2015_IF_Computing.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/554\/attachments\/578533\/796673\/CHEP_2015_IF_Computing.pdf", "fileName": "CHEP_2015_IF_Computing.pdf", "_fossil": "localFileMetadata", "id": "796673", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/554", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "04:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "05:15:00"}, "duration": 30, "session": "Plenary", "keywords": [], "id": "557", "speakers": [], "primaryauthors": [], "title": "IBM Corporation", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T00:19:51.278806+00:00", "description": "", "title": "IBM.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/557\/attachments\/578534\/796674\/IBM.pdf", "filename": "IBM.pdf", "content_type": "application\/pdf", "type": "file", "id": 796674, "size": 2168466}], "title": "Slides", "default_folder": false, "id": 578534, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578534", "resources": [{"_type": "LocalFile", "name": "IBM.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/557\/attachments\/578534\/796674\/IBM.pdf", "fileName": "IBM.pdf", "_fossil": "localFileMetadata", "id": "796674", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/557", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "04:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "04:45:00"}, "duration": 45, "session": "Plenary", "keywords": [], "id": "556", "speakers": [{"_type": "ContributionParticipation", "emailHash": "0aa5a3b251ad4bc31debfe3f3d36103a", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "BOEHNLEIN, Amber", "id": "0"}], "primaryauthors": [], "title": "Challenges of Developing and Maintaining HEP \"Community\" Software", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T00:09:53.624954+00:00", "description": "", "title": "CHEP15-ASB.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/556\/attachments\/578535\/796675\/CHEP15-ASB.pdf", "filename": "CHEP15-ASB.pdf", "content_type": "application\/pdf", "type": "file", "id": 796675, "size": 1023451}], "title": "Slides", "default_folder": false, "id": 578535, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578535", "resources": [{"_type": "LocalFile", "name": "CHEP15-ASB.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/556\/attachments\/578535\/796675\/CHEP15-ASB.pdf", "fileName": "CHEP15-ASB.pdf", "_fossil": "localFileMetadata", "id": "796675", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/556", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "04:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "04:45:00"}, "duration": 45, "session": "Plenary", "keywords": [], "id": "551", "speakers": [{"_type": "ContributionParticipation", "emailHash": "9f60b1ecea45598b615f77b2b41150f1", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "STEWART, Graeme", "id": "0"}], "primaryauthors": [], "title": "Evolution of Computing and Software at LHC: from Run 2 to HL-LHC", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T16:47:41.520982+00:00", "description": "", "title": "comp-soft-evolution-plenary.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/551\/attachments\/578536\/796676\/comp-soft-evolution-plenary.pdf", "filename": "comp-soft-evolution-plenary.pdf", "content_type": "application\/pdf", "type": "file", "id": 796676, "size": 6560175}], "title": "Slides", "default_folder": false, "id": 578536, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578536", "resources": [{"_type": "LocalFile", "name": "comp-soft-evolution-plenary.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/551\/attachments\/578536\/796676\/comp-soft-evolution-plenary.pdf", "fileName": "comp-soft-evolution-plenary.pdf", "_fossil": "localFileMetadata", "id": "796676", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/551", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "03:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "03:30:00"}, "duration": 30, "session": "Plenary", "keywords": [], "id": "550", "speakers": [{"_type": "ContributionParticipation", "emailHash": "d41d8cd98f00b204e9800998ecf8427e", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HARA, Takanori", "id": "0"}], "primaryauthors": [], "title": "Computing at the Belle-II experiment", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T22:11:32.366397+00:00", "description": "", "title": "BelleII4CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/550\/attachments\/578537\/796677\/BelleII4CHEP2015.pdf", "filename": "BelleII4CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796677, "size": 10562043}], "title": "Slides", "default_folder": false, "id": 578537, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578537", "resources": [{"_type": "LocalFile", "name": "BelleII4CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/550\/attachments\/578537\/796677\/BelleII4CHEP2015.pdf", "fileName": "BelleII4CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796677", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/550", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "05:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "05:45:00"}, "duration": 30, "session": "Plenary", "keywords": [], "id": "553", "speakers": [], "primaryauthors": [], "title": "Dell Inc.", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [], "_type": "Contribution", "description": "", "track": null, "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/553", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "04:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "05:15:00"}, "duration": 30, "session": "Plenary", "keywords": [], "id": "552", "speakers": [], "primaryauthors": [], "title": "Intel Corporation", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T00:13:09.459544+00:00", "description": "", "title": "Intel.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/552\/attachments\/578538\/796678\/Intel.pdf", "filename": "Intel.pdf", "content_type": "application\/pdf", "type": "file", "id": 796678, "size": 4283262}], "title": "Slides", "default_folder": false, "id": 578538, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578538", "resources": [{"_type": "LocalFile", "name": "Intel.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/552\/attachments\/578538\/796678\/Intel.pdf", "fileName": "Intel.pdf", "_fossil": "localFileMetadata", "id": "796678", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/552", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "238", "speakers": [{"_type": "ContributionParticipation", "emailHash": "b9ec980cd6e266565efabbca67cd6543", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "WARDZINSKA, Aleksandra", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "b9ec980cd6e266565efabbca67cd6543", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "WARDZINSKA, Aleksandra", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "6b5f03bf6b3310bb2590358604d251ff", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. PETIT, Stephan", "id": "1"}], "title": "The evolution of CERN EDMS", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-08T12:58:58.038959+00:00", "description": "", "title": "CHEP2015_EDMS_Poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/238\/attachments\/578539\/796679\/CHEP2015_EDMS_Poster.pdf", "filename": "CHEP2015_EDMS_Poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796679, "size": 3051733}], "title": "Poster", "default_folder": false, "id": 578539, "description": ""}], "_type": "Contribution", "description": "Large-scale long-term projects such as the LHC require the ability to store, manage, organize and distribute large amounts of engineering information, covering a wide spectrum of fields. This information is a living material, evolving in time, following various lifecycles. It has to reach the next generations of engineers so they understand how their predecessors designed, crafted, operated and maintained the most complex machines ever built. \r\n\r\nThis is the role of CERN EDMS. The Engineering and Equipment Data Management Service has served the High Energy Physics Community for over 15 years. It is the CERN\u2019s official PLM (Product Lifecycle Management), supporting engineering communities in their collaborations inside and outside the laboratory. EDMS is integrated with the CAD (Computer-aided Design) and CMMS (Computerized Maintenance Management) systems used at CERN providing tools for engineers who work in different domains and who are not PLM specialists.\r\n\r\nOver the years, human collaborations and machines grew in size and complexity. So did the EDMS: it is currently home to more than 2 million files and documents, and has over 6 thousand active users. In April 2014 we released a new major version of EDMS, featuring a complete makeover of the web interface, improved responsiveness and enhanced functionality. Following the results of user surveys and building upon feedback received from key users group, we brought what we think is a system that is more attractive and makes it easy to perform complex tasks.\r\n\r\nIn this paper we will describe the main functions and the architecture of EDMS. We will discuss the available integration options, which enable further evolution and automation of engineering data management. We will also present our plans for the future development of EDMS.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578539", "resources": [{"_type": "LocalFile", "name": "CHEP2015_EDMS_Poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/238\/attachments\/578539\/796679\/CHEP2015_EDMS_Poster.pdf", "fileName": "CHEP2015_EDMS_Poster.pdf", "_fossil": "localFileMetadata", "id": "796679", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/238", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "239", "speakers": [{"_type": "ContributionParticipation", "emailHash": "865949cef918f0ff18b94a42afcaddf3", "affiliation": "Institute of High Energy Physics", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ZHANG, Xiaomei", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "865949cef918f0ff18b94a42afcaddf3", "affiliation": "Institute of High Energy Physics", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ZHANG, Xiaomei", "id": "0"}], "title": "BESIII production with distributed computing", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T15:01:26.993919+00:00", "description": "", "title": "Poster_BESIII_production_with_Distributed_Computing_v1.5.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/239\/attachments\/578540\/796680\/Poster_BESIII_production_with_Distributed_Computing_v1.5.pdf", "filename": "Poster_BESIII_production_with_Distributed_Computing_v1.5.pdf", "content_type": "application\/pdf", "type": "file", "id": 796680, "size": 776797}, {"_type": "attachment", "modified_dt": "2015-04-11T15:01:26.993919+00:00", "description": "", "title": "Poster_BESIII_production_with_Distributed_Computing_v1.5.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/239\/attachments\/578540\/796681\/Poster_BESIII_production_with_Distributed_Computing_v1.5.pptx", "filename": "Poster_BESIII_production_with_Distributed_Computing_v1.5.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796681, "size": 1583871}], "title": "Slides", "default_folder": false, "id": 578540, "description": ""}], "_type": "Contribution", "description": "Distributed computing is necessary nowadays for high energy physics experiments to organize heterogeneous computing resources all over the world to process enormous amounts of data. \r\nThe BESIII experiment in China, which has aggregated about 3 PB of data over the last 5 years, has established its own distributed computing system, based on DIRAC, as a supplement to local clusters, collecting cluster, grid, desktop and cloud resources from collaborating member institutes around the world. The system consists of workload management and data management to deal with the BESIII Monte Carlo production workflow in a distributed environment. Random trigger data are distributed with a dataset-based data transfer system at a speed of 10 TB\/day to enable processing of the whole Monte Carlo simulation and reconstruction workflow at remote sites. File and metadata management tools and a job submission frontend have been developed to provide a virtual layer for BESIII physicists to use distributed resources.\r\nThe BESIII distributed computing system, which is composed of more than 3000 CPU cores across 10 sites, with about 400TB storage, has been in production since the end of 2012. Three large-scale production tasks have been completed, with more than 150,000 jobs completed successfully. Measures have been taken to cope with lack of grid experience and low manpower at grid sites, and in particular, monitoring has been strengthened to deal with these issues. Moreover, the paper shows our experience of integrating various kinds of private cloud resources in a dynamic way to greatly ease maintenance of sites. Our efforts to extend the platform to more new high energy experiments in China are also discussed.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578540", "resources": [{"_type": "LocalFile", "name": "Poster_BESIII_production_with_Distributed_Computing_v1.5.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/239\/attachments\/578540\/796680\/Poster_BESIII_production_with_Distributed_Computing_v1.5.pdf", "fileName": "Poster_BESIII_production_with_Distributed_Computing_v1.5.pdf", "_fossil": "localFileMetadata", "id": "796680", "_deprecated": true}, {"_type": "LocalFile", "name": "Poster_BESIII_production_with_Distributed_Computing_v1.5.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/239\/attachments\/578540\/796681\/Poster_BESIII_production_with_Distributed_Computing_v1.5.pptx", "fileName": "Poster_BESIII_production_with_Distributed_Computing_v1.5.pptx", "_fossil": "localFileMetadata", "id": "796681", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "6932bbeb7bf05b8931ecb925f1435e00", "affiliation": "Institute of High Energy Physics", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. YAN, Tian", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "ef679b9a5ef0fb7eeadebb1b038f6ef6", "affiliation": "Institute of High Energy Physics", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. XIANGHU, Zhao", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/239", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:30:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:45:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "234", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a198240a6b21354217ead633867b3e31", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. FUHRMANN, Patrick", "id": "6"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6400c2cc73bf4ffb1d6d4985b52d6786", "affiliation": "FNAL", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LITVINTSEV, Dmitry", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "2f89590a83d4f4b4a573f2d8d9fb76eb", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "BERNARDT, Christian", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "ede687cc1fd66698a4d3625c52758c3d", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "SCHWANK, Karsten", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "d01d0e9eec1542b90bc8b2f80a6fb543", "affiliation": "FNAL", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ROSSI, Albert", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "d96310dde0f53841456b312025ab1877", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. MKRTCHYAN, Tigran", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "8d26ed6854ded78641202361191acea7", "affiliation": "NDGF", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BEHRMANN, Gerd", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "a198240a6b21354217ead633867b3e31", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. FUHRMANN, Patrick", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "9004fd367d357871828de260587c178b", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MILLAR, Paul", "id": "7"}], "title": "dCache, evolution by tackling new challenges.", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T13:02:04.681112+00:00", "description": "", "title": "20150416-dcache-chep2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/234\/attachments\/578541\/796682\/20150416-dcache-chep2015.pdf", "filename": "20150416-dcache-chep2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796682, "size": 1666519}, {"_type": "attachment", "modified_dt": "2015-04-15T13:02:04.681112+00:00", "description": "", "title": "20150416-dcache-chep2015.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/234\/attachments\/578541\/796683\/20150416-dcache-chep2015.pptx", "filename": "20150416-dcache-chep2015.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796683, "size": 2479136}], "title": "Slides", "default_folder": false, "id": 578541, "description": ""}], "_type": "Contribution", "description": "With the great success of the dCache Storage Technology in the framework of the World Wide LHC Computing Grid,  an increasing number of non HEP communities were attracted to use dCache for their data management infrastructure. As a natural consequence, the dCache team was presented with new use-cases that stimulated the development of interesting dCache features.\r\n\r\nPerhaps the most important group of new features is the enhanced media awareness. One aspect is the optimized migration of data between random access devices and tertiary storage, e.g. tape systems. Transparently for the user, dCache combines small files into containers before being copied to tape. Another aspect of this media awareness work is dCache's activity in making more efficient use of SSDs to boost high speed writing and chaotic reading: depending on access profile or protocol, data is placed on the most appropriate media types.\r\n\r\nA second hot topic, often requested by scientific communities, is Cloud Storage.  By marrying the OwnCloud software with dCache, a unique hybrid system becomes available that provides both the simplicity of a sync-n-share service and dCache's many unique data management features.  Beyond simple sync-n-share, users also demand control over the quality of service dCache offers.  To support this, dCache is implementing the CDMI standard.  With CDMI, dCache can present new functionality in a standard fashion; e.g. storing and querying metadata, triggering media migration or treating dCache as an object store.\r\n\r\nAs the X509 certificate infrastructure proved unpopular outside of HEP sciences, dCache will support alternative methods of authenticating, like SAML and OpenID Connect.  Such support allows sites running dCache to join identity federations as part of international collaborations.\r\n\r\nThe dCache team will describe its ongoing activities and present its future development road map.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578541", "resources": [{"_type": "LocalFile", "name": "20150416-dcache-chep2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/234\/attachments\/578541\/796682\/20150416-dcache-chep2015.pdf", "fileName": "20150416-dcache-chep2015.pdf", "_fossil": "localFileMetadata", "id": "796682", "_deprecated": true}, {"_type": "LocalFile", "name": "20150416-dcache-chep2015.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/234\/attachments\/578541\/796683\/20150416-dcache-chep2015.pptx", "fileName": "20150416-dcache-chep2015.pptx", "_fossil": "localFileMetadata", "id": "796683", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/234", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "235", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ff2f1ccd5cf4b7ebae090f4963ae3e6e", "affiliation": "TRIUMF", "_fossil": "contributionParticipationMetadata", "fullName": "LINDNER, Thomas", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ff2f1ccd5cf4b7ebae090f4963ae3e6e", "affiliation": "TRIUMF", "_fossil": "contributionParticipationMetadata", "fullName": "LINDNER, Thomas", "id": "0"}], "title": "DEAP-3600 Data Acquisition System", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T06:14:44.388133+00:00", "description": "", "title": "lindner_chep_deap_poster_v2.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/235\/attachments\/578542\/796684\/lindner_chep_deap_poster_v2.pdf", "filename": "lindner_chep_deap_poster_v2.pdf", "content_type": "application\/pdf", "type": "file", "id": 796684, "size": 9462283}], "title": "Slides", "default_folder": false, "id": 578542, "description": ""}], "_type": "Contribution", "description": "DEAP-3600 is a dark matter experiment located at SNOLAB in Ontario, Canada. The DEAP detector uses 3600kg of liquid argon to search for the interactions of Weakly Interacting Massive Particles (WIMPs), a dark matter candidate.  Light from the WIMP interactions is imaged using an array of 255 PMTs. A critical challenge for the DEAP experiment is the large background from 39Argon beta decays which occur at a rate of 3.6kHz. Beta decays are efficiently eliminated by pulse shape discrimination.\r\n\r\nThe DEAP Data AcQuisition (DAQ) system has been designed to handle the very large 39Ar event rate without compromising the detection of other interaction occurring at a much lower rate.  Key features of the DEAP DAQ include: i) factor of 5 to 10 online 39Ar event rejection using a custom trigger board by analyzing 22 waveforms sampled at 45MS\/s in an FPGA housed in a custom board, ii) digitization and pulse processing of the PMT signals using commercial CAEN 250MS\/s FADCs  iii) Data acquisition software designed using MIDAS DAQ toolkit providing online pulse feature extraction and event filtering capabilities.  The full DAQ is capable of providing a factor of 50 to 100 data rate reduction by filtering out beta decays during regular data taking and the capability to handling data rates of 200-300MB\/s during calibration runs", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578542", "resources": [{"_type": "LocalFile", "name": "lindner_chep_deap_poster_v2.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/235\/attachments\/578542\/796684\/lindner_chep_deap_poster_v2.pdf", "fileName": "lindner_chep_deap_poster_v2.pdf", "_fossil": "localFileMetadata", "id": "796684", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "e1ee69f5e35b67bb1b284205991671ee", "affiliation": "TRIUMF", "_fossil": "contributionParticipationMetadata", "fullName": "RETIERE, Fabrice", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "a53ab6115d837831dc1ad117f56d6db3", "affiliation": "TRIUMF", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. AMAUDRUZ, Pierre-Andre", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "c681edaf85ddf22c80370baed816fe60", "affiliation": "University of Alberta", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GOREL, Pierre", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "354b96c9ce0c10ff1e7be83aa0ddeba5", "affiliation": "TRIUMF", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SMITH, Ben", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/235", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "236", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ff2f1ccd5cf4b7ebae090f4963ae3e6e", "affiliation": "TRIUMF", "_fossil": "contributionParticipationMetadata", "fullName": "LINDNER, Thomas", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ff2f1ccd5cf4b7ebae090f4963ae3e6e", "affiliation": "TRIUMF", "_fossil": "contributionParticipationMetadata", "fullName": "LINDNER, Thomas", "id": "0"}], "title": "Evolution of the T2K-ND280 Computing Model", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T06:13:09.737345+00:00", "description": "", "title": "lindner_chep_t2k_poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/236\/attachments\/578543\/796685\/lindner_chep_t2k_poster.pdf", "filename": "lindner_chep_t2k_poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796685, "size": 889074}], "title": "Slides", "default_folder": false, "id": 578543, "description": ""}], "_type": "Contribution", "description": "ND280 is the off-axis near detector for the T2K neutrino experiment; it is designed to characterize the unoscillated T2K neutrino beam and measure neutrino cross-sections. We have developed a complicated system for processing and simulating the ND280 data, using computing resources from North America, Europe and Japan.\r\nRecent work has concentrated on unifying our computing framework across these regions. In particular, we have started using two HEP tools: GANGA and DIRAC. GANGA has provided a unified framework for users scripts; using a unified scripting framework makes it easier for a smaller number of people to maintain and run a multiple types of production scripts.  DIRAC is a framework providing a suite of tools for HEP computing.  In particular, we have been testing DIRAC as a Workload Manager Server (WMS), replacing the original EGI WMS.\r\nWe will describe our experiences with these developments of the ND280 computing model.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578543", "resources": [{"_type": "LocalFile", "name": "lindner_chep_t2k_poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/236\/attachments\/578543\/796685\/lindner_chep_t2k_poster.pdf", "fileName": "lindner_chep_t2k_poster.pdf", "_fossil": "localFileMetadata", "id": "796685", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/236", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:30:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "237", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6fef2a09accf1a707bc4c992c15a8438", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KLIMENTOV, Alexei", "id": "4"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e6819ee253837bc1a19f5e93c8ef001e", "affiliation": "University of Texas at Arlington (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DE, Kaushik", "id": "0"}], "title": "Integrating Network Awareness in ATLAS Distributed Computing Using the ANSE Project", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T23:19:28.799852+00:00", "description": "", "title": "CHEP2015_PanDA_Network.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/237\/attachments\/578544\/796686\/CHEP2015_PanDA_Network.pdf", "filename": "CHEP2015_PanDA_Network.pdf", "content_type": "application\/pdf", "type": "file", "id": 796686, "size": 5175316}, {"_type": "attachment", "modified_dt": "2015-04-12T23:19:28.799852+00:00", "description": "", "title": "CHEP2015_PanDA_Network.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/237\/attachments\/578544\/796687\/CHEP2015_PanDA_Network.pptx", "filename": "CHEP2015_PanDA_Network.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796687, "size": 4206610}], "title": "Slides", "default_folder": false, "id": 578544, "description": ""}], "_type": "Contribution", "description": "A crucial contributor to the success of the massively scaled global computing system that delivers the analysis needs of the LHC experiments is the networking infrastructure upon which the system is built. The experiments have been able to exploit excellent high-bandwidth networking in adapting their  computing models for the most efficient utilization of resources.\r\n\r\nNew advanced networking technologies now becoming available such as software defined networking hold the potential of further  leveraging the network to optimize workflows and dataflows, through proactive control of the network fabric on the  part of high level applications such as experiment workload management and data management systems. End to end monitoring of networks using perfSONAR combined with data flow performance metrics further allows applications to adapt based on real time conditions.\r\n\r\nWe will describe efforts underway in ATLAS on integrating network awareness at the application level, particularly in workload management, building  upon the ANSE (Advance Network  Services  for Experiments) project components.  We will show how knowledge of network  conditions, both  historical  and current, are used to optimize PanDA and other systems for  ATLAS and describe how software control of end-to-end network  paths can augment ATLAS's  ability to effectively utilize its  distributed  resources.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578544", "resources": [{"_type": "LocalFile", "name": "CHEP2015_PanDA_Network.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/237\/attachments\/578544\/796686\/CHEP2015_PanDA_Network.pdf", "fileName": "CHEP2015_PanDA_Network.pdf", "_fossil": "localFileMetadata", "id": "796686", "_deprecated": true}, {"_type": "LocalFile", "name": "CHEP2015_PanDA_Network.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/237\/attachments\/578544\/796687\/CHEP2015_PanDA_Network.pptx", "fileName": "CHEP2015_PanDA_Network.pptx", "_fossil": "localFileMetadata", "id": "796687", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "c4274381c8b05a677be4c06435386a3f", "affiliation": "U", "_fossil": "contributionParticipationMetadata", "fullName": "BATISTA, Jorge", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "a7d121dcc3185f3121537c7f6f762db8", "affiliation": "University of Michigan ATLAS Group", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MCKEE, Shawn", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "275c5b84f25777c7a0398e21b4b855bc", "affiliation": "Joint Inst. for Nuclear Research (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "PETROSYAN, Artem", "id": "3"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/237", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:00:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "230", "speakers": [{"_type": "ContributionParticipation", "emailHash": "cd17961312c904e3bb4b71723c8e87ac", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. COLLING, David", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "cd17961312c904e3bb4b71723c8e87ac", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. COLLING, David", "id": "0"}], "title": "The diverse use of clouds by CMS", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T20:59:32.970397+00:00", "description": "", "title": "Diverse_use_of_clouds_by_CMS.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/230\/attachments\/578545\/796688\/Diverse_use_of_clouds_by_CMS.pdf", "filename": "Diverse_use_of_clouds_by_CMS.pdf", "content_type": "application\/pdf", "type": "file", "id": 796688, "size": 768940}, {"_type": "attachment", "modified_dt": "2015-04-12T20:59:32.970397+00:00", "description": "", "title": "Diverse_use_of_clouds_by_CMS.ppt", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/230\/attachments\/578545\/796689\/Diverse_use_of_clouds_by_CMS.ppt", "filename": "Diverse_use_of_clouds_by_CMS.ppt", "content_type": "application\/vnd.ms-powerpoint", "type": "file", "id": 796689, "size": 1122816}], "title": "Slides", "default_folder": false, "id": 578545, "description": ""}], "_type": "Contribution", "description": "The resources CMS is using are increasingly being offered as clouds. In Run 2 of the LHC the majority of CMS CERN resources, both in Meyrin and at the Wigner Computing Centre, will be presented as cloud resources on which CMS will have to build its own infrastructure. This infrastructure will need to run all of the CMS workflows including: Tier 0, production and user analysis. In addition, the CMS High Level Trigger will provide a compute resource comparable in scale to the total offered by the CMS Tier 1 sites, when it is not running as part of the trigger system. During these periods a cloud infrastructure will be overlaid on this resource, making it accessible for general CMS use. Finally, CMS is starting to utilise cloud resources being offered by individual institutes and is gaining experience to facilitate the use of opportunistically available cloud resources.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578545", "resources": [{"_type": "LocalFile", "name": "Diverse_use_of_clouds_by_CMS.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/230\/attachments\/578545\/796688\/Diverse_use_of_clouds_by_CMS.pdf", "fileName": "Diverse_use_of_clouds_by_CMS.pdf", "_fossil": "localFileMetadata", "id": "796688", "_deprecated": true}, {"_type": "LocalFile", "name": "Diverse_use_of_clouds_by_CMS.ppt", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/230\/attachments\/578545\/796689\/Diverse_use_of_clouds_by_CMS.ppt", "fileName": "Diverse_use_of_clouds_by_CMS.ppt", "_fossil": "localFileMetadata", "id": "796689", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "068b9018b69ea40467e02d92d6a35844", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "HUFNAGEL, Dirk", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "6cccef6d6cebb7a770526f8954e4483f", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "HUFFMAN, Adam", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "7ba575b05769913d1239745e15dafee3", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "BAUER, Daniela", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "fd06959b93c405340e9900abf962beb7", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "LAHIFF, Andrew David", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "80025860f8e9f499fddcf594bae0478e", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. SGARAVATTO, Massimo", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "562ba8b8fc054e5cec93cd033f83d326", "affiliation": "INFN - Bologna", "_fossil": "contributionParticipationMetadata", "fullName": "GRANDI, Claudio", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "d33e3ae247733829a0d0c9511973c44e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHAZE, Olivier", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "1ab30f4066c48fc202816d553d46d1fe", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "TIRADANI, Anthony", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "491ee0624b76e8966fe306b8b02a93b3", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MCCRAE, Alison", "id": "9"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/230", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "231", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a4c2629976026a424cedc51ff4d288ef", "affiliation": "STFC", "_fossil": "contributionParticipationMetadata", "fullName": "RYALL, George", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "a4c2629976026a424cedc51ff4d288ef", "affiliation": "STFC", "_fossil": "contributionParticipationMetadata", "fullName": "RYALL, George", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "ea61a8b07e0d8d70c0ebf90a945af037", "affiliation": "AGH University of Science and Technology, Krakow, Poland", "_fossil": "contributionParticipationMetadata", "fullName": "FIDYK, Gabriela", "id": "1"}], "title": "The Performance of SSDs for Handling CEPH Journals", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "CEPH is becoming increasingly popular as a storage infrastructure for use within the grid.  It has the advantages of high levels of both redundancy and resilience; the promise of massive scalability and performance; and will simplify routine tasks for site administrators.  Journals are used in CEPH to provide improved performance and transaction consistency on write operations.  The CEPH community recommend the use of SSDs for journals to improve write performance over traditional spinning disk.\r\n\r\nSTFC has been developing a CEPH infrastructure for the RAL Tier1 for some time with plans to deploy a production system in 2015.  As part of this process it has been necessary to determine the optimum hardware configuration within budgetary constraints.  In order to do this we have undertaken a benchmarking campaign including comparisons between storing journals on the same disks as the OSDs and on a single SSD. Since most sites within WLCG work on a tight budget, the additional cost of SSDs must be balanced against any performance gain.  Any performance gain will increase the efficiency of the processing farm; a key metric which all sites are measured against.\r\n\r\nIn this presentation we compare the performance obtained under a variety of conditions with and without journals on SSDs.", "track": "Track3: Data store and access ", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "de22c3cd14674c38d1512edbc544f940", "affiliation": "STFC RAL", "_fossil": "contributionParticipationMetadata", "fullName": "ADAMS, James", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "da4cac912389521f0838486a2a6ebd61", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "DE WITT, Shaun", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/231", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:15:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "232", "speakers": [{"_type": "ContributionParticipation", "emailHash": "044a6c45e1089ebc257b67572ae11f8d", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "KEEBLE, Oliver", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "d4502e02b88247265f7423d8fafc6ba3", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "RIAHI, Hassen", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "2bc2abdabb8d1f21dae00de97010754f", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SALICHOS, Michail", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "044a6c45e1089ebc257b67572ae11f8d", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "KEEBLE, Oliver", "id": "2"}], "title": "Quantitative transfer monitoring for FTS3", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T03:50:59.050349+00:00", "description": "", "title": "KeebleFTS3Monitoring.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/232\/attachments\/578546\/796690\/KeebleFTS3Monitoring.pdf", "filename": "KeebleFTS3Monitoring.pdf", "content_type": "application\/pdf", "type": "file", "id": 796690, "size": 702533}], "title": "Slides", "default_folder": false, "id": 578546, "description": ""}], "_type": "Contribution", "description": "The overall success of LHC data processing depends heavily on stable, reliable and fast data distribution. The Worldwide LHC Computing Grid (WLCG) relies on the File Transfer Service (FTS) as the data movement middleware for moving sets of files from one site to another.\r\n\r\nThis paper describes the components of FTS3 monitoring infrastructure and how they are built to satisfy the common and particular requirements of the LHC experiments. We show how the system provides a complete and detailed cross-virtual organization (VO) picture of transfers for sites, operators and VOs. This information has proven critical due to the shared nature of the infrastructure, allowing a complete view of all transfers on shared network links between various workflows and VOs using the same FTS transfer manager.\r\n\r\nWe also report on the performance of the FTS service itself, using data generated by the aforementioned monitoring infrastructure both during the commissioning and the first phase of production. We also explain how this monitoring information and network metrics produced can be used both as a starting point for troubleshooting data transfer issues, but also as a mechanism to collect information such as transfer efficiency between sites, achieved throughput and its evolution over time, most common errors, etc, and take decision upon them to further optimize transfer workflows.\r\n\r\nThe service setup is subject to sites policies to control the network resource usage, as well as all the VOs making use of the Grid resources at the site to satisfy their requirements. FTS3 is the new version of FTS and has been deployed in production in August 2014.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578546", "resources": [{"_type": "LocalFile", "name": "KeebleFTS3Monitoring.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/232\/attachments\/578546\/796690\/KeebleFTS3Monitoring.pdf", "fileName": "KeebleFTS3Monitoring.pdf", "_fossil": "localFileMetadata", "id": "796690", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "9fdcc3dac42d3568bc6b9e5fb3454a3f", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ALVAREZ AYLLON, Alejandro", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "dd2d05df8869eb91f302ce4046407f24", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ANDREEVA, Julia", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "a0dd1a3a7589f1e2a04cc967f2ae6c39", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DI GIROLAMO, Alessandro", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "efd528c4fecb23d4785370a2ba2989b2", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MAGINI, Nicolo", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "e0c62f86305aa41c91d857b662068a19", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ROISER, Stefan", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "f6899d026502f8f5a43544a4ca322c95", "affiliation": "A.D.A.M. Applications of Detectors and accelerators to Medicine", "_fossil": "contributionParticipationMetadata", "fullName": "SIMON, Michal Kamil", "id": "9"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/232", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "233", "speakers": [{"_type": "ContributionParticipation", "emailHash": "50a1799bdcddc4a0accf8871044dc30f", "affiliation": "KIT", "_fossil": "contributionParticipationMetadata", "fullName": "HAUTH, Thomas", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "154e75dd73207334ed10054e93d229d0", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "KUHR, Thomas", "id": "0"}], "title": "Software Development at Belle II", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-07T06:43:19.790882+00:00", "description": "", "title": "Belle2SoftwareKuhrHauth.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/233\/attachments\/578547\/796691\/Belle2SoftwareKuhrHauth.pdf", "filename": "Belle2SoftwareKuhrHauth.pdf", "content_type": "application\/pdf", "type": "file", "id": 796691, "size": 979333}], "title": "Poster", "default_folder": false, "id": 578547, "description": ""}], "_type": "Contribution", "description": "Belle II is a next generation B factory experiment that will collect 50 times more data than its predecessor Belle. The higher luminosity at the SuperKEKB accelerator leads to higher background and requires a major upgrade of the detector. As a consequence also the simulation, reconstruction, and analysis software has to be upgraded substantially and actually most parts are newly written taking into account the experience from Belle and other experiments and advances in technology.\r\n\r\nWhile large parts of the Belle software were written by KEK staff members, the Belle II software development is distributed around the world and relies on contributions from students and staff members. The regional distribution, the different (cultural) backgrounds and skills of developers are a challenge for a project that has to make sure that all parts fit together. Several tools and organizational measures are employed to allow the developers to focus on their main work and at the same time keep them conscious of the overall goal of providing an easy-to-use, robust, and powerful software suite to the Belle II collaboration.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578547", "resources": [{"_type": "LocalFile", "name": "Belle2SoftwareKuhrHauth.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/233\/attachments\/578547\/796691\/Belle2SoftwareKuhrHauth.pdf", "fileName": "Belle2SoftwareKuhrHauth.pdf", "_fossil": "localFileMetadata", "id": "796691", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "50a1799bdcddc4a0accf8871044dc30f", "affiliation": "KIT", "_fossil": "contributionParticipationMetadata", "fullName": "HAUTH, Thomas", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/233", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:30:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "1", "speakers": [{"_type": "ContributionParticipation", "emailHash": "59e10da0fd992b903bd51059a93aea7c", "affiliation": "INFN Roma Tre", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BRANCHINI, paolo", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "59e10da0fd992b903bd51059a93aea7c", "affiliation": "INFN Roma Tre", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BRANCHINI, paolo", "id": "0"}], "title": "The Front-End Electronics and the Data Acquisition System for a Kinetic Inductance Detector", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T09:20:32.271881+00:00", "description": "", "title": "chep2015branchini.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/1\/attachments\/578548\/796692\/chep2015branchini.pptx", "filename": "chep2015branchini.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796692, "size": 50961794}], "title": "Slides", "default_folder": false, "id": 578548, "description": ""}], "_type": "Contribution", "description": "The Data Acquisition System (DAQ) and the Front-End electronics for an array of Kinetic Inductance Detectors (KIDs) are described. KIDs are superconductive detectors, in which electrons are organized in Cooper pairs. Any incident radiation could break such pairs generating quasi-particles, whose effect is increasing the inductance of the detector. Electrically, any KID is equivalent to a parallel RLC resonant circuit. An array of N KIDs is composed of N pixels, each one resonating on its own frequency. A feed line passes close to each KID and delivers a unique Stimulus signal containing all the resonant frequencies. If one of the KIDs was hit by some radiation, its inductance would change and the corresponding sine component in the readout signal would have its intensity reduced and its phase shifted. The DAQ system we developed is a hardware\/software co-design, based on state machines and a Microprocessor embedded into an FPGA. A commercial DAC\/ADC board is used to interface the FPGA to the analog environment of the array of KIDs. The DAQ system generates a Stimulus signal for an array of up to 128 KIDs, by creating and adding up 128 sinusoids parted by one MHz. The Stimulus is in the form of a Look-Up Table and it is provided to the DAC device. The analog signal generated is up-mixed with a 3 GHz carrier wave and it then travels through the KIDs array. The read-out signal from the detector is down-mixed with respect to the 3 GHz sine wave and it is read back by the ADC device. The microprocessor stores the read out data via a PCI bus into an external disk. It also elaborates the Fast Fourier Transform of the acquired read out signal: this allows to extrapolate which KID interacted and the energy of the impinging radiation. Simulations and tests have been performed successfully and experimental results are presented.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578548", "resources": [{"_type": "LocalFile", "name": "chep2015branchini.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/1\/attachments\/578548\/796692\/chep2015branchini.pptx", "fileName": "chep2015branchini.pptx", "_fossil": "localFileMetadata", "id": "796692", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "8d21d3fead8dbbf1988a9a501846378c", "affiliation": "INFN Roma Tre", "_fossil": "contributionParticipationMetadata", "fullName": "CAPASSO, Luciano", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "95ab006e5e7cef21b38042a71271fa59", "affiliation": "INFN Roma Tre", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BUDANO, Antonio", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "2a1d5d1e12d220f5bf4e2687053fe8dc", "affiliation": "INFN Roma Tre", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MARCHETTI, Dedalo", "id": "3"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/1", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:15:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "146", "speakers": [{"_type": "ContributionParticipation", "emailHash": "32e62a0d1cb79b90404b2e9b5eae5126", "affiliation": "University of Victoria (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "TAYLOR, Ryan", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "32e62a0d1cb79b90404b2e9b5eae5126", "affiliation": "University of Victoria (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "TAYLOR, Ryan", "id": "0"}], "title": "Evolution of Cloud Computing in ATLAS", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T07:25:25.297562+00:00", "description": "", "title": "CHEP2015_ATLAS_Cloud_Computing.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/146\/attachments\/578549\/796693\/CHEP2015_ATLAS_Cloud_Computing.pdf", "filename": "CHEP2015_ATLAS_Cloud_Computing.pdf", "content_type": "application\/pdf", "type": "file", "id": 796693, "size": 1671539}], "title": "Slides", "default_folder": false, "id": 578549, "description": ""}], "_type": "Contribution", "description": "The ATLAS experiment has successfully incorporated cloud computing technology and cloud resources into its primarily grid-based model of distributed computing. Cloud R&D activities continue to mature and transition into stable production systems, while ongoing evolutionary changes are still needed to adapt and refine the approaches used, in response to changes in prevailing cloud technology. In addition, completely new developments are needed to handle emerging requirements.\r\n\r\nThis work will describe the overall evolution of cloud computing in ATLAS. The current status of the VM management systems used for harnessing IAAS resources will be discussed. Monitoring and accounting systems tailored for clouds are needed to complete the integration of cloud resources within ATLAS' distributed computing framework. We are developing and deploying new solutions to address the challenge of operation in a geographically distributed multi-cloud scenario, including a system for managing VM images across multiple clouds, a system for dynamic location-based discovery of caching proxy servers, and the usage of a data federation to unify the worldwide grid of storage elements into a single namespace and access point. The usage of the experiment's HLT farm\r\nfor Monte Carlo production, in a specialized cloud environment, will be presented. Finally, we evaluate commercial clouds, and have conducted a study assessing cost vs. benchmark performance.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578549", "resources": [{"_type": "LocalFile", "name": "CHEP2015_ATLAS_Cloud_Computing.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/146\/attachments\/578549\/796693\/CHEP2015_ATLAS_Cloud_Computing.pdf", "fileName": "CHEP2015_ATLAS_Cloud_Computing.pdf", "_fossil": "localFileMetadata", "id": "796693", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "ebf920eeaeb952921229cbbafabd92d7", "affiliation": "University of Victoria (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "BERGHAUS, Frank Olaf", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "6049ebfaa694e419b61cfd107f553289", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FIELD, Laurence", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "f10a491a558480b67d78d8826c40ea5e", "affiliation": "Lancaster University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LOVE, Peter", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "82ed01ecc4d0351305568f7bd206c476", "affiliation": "University of Victoria (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "LEBLANC, Matt", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "1982509370626e760ae06e6f8f80e2a5", "affiliation": "C", "_fossil": "contributionParticipationMetadata", "fullName": "DI GIROLAMO, Alessandro", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "e490ca70cc14e2ca27f8071da6597bdc", "affiliation": "U", "_fossil": "contributionParticipationMetadata", "fullName": "PATERSON, Michael", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "f13af63ad4bbdfdce02559c629f8d6bc", "affiliation": "University of Victoria (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "GABLE, Ian", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "98d2fcdd64af6e627d3a9172bbe5253e", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CABALLERO BEJAR, Jose", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "e106556bcded279b116df4aee2de6c31", "affiliation": "University of Victoria", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. SOBIE, Randall", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "0cb8f3c84f79ded1da04cdaf2b5f719a", "affiliation": "University of Victoria", "_fossil": "contributionParticipationMetadata", "fullName": "DESMARAIS, Ron", "id": "10"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/146", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "147", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a9ecafa2c206c9e5deab6286e19f0711", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "MITREVSKI, Jovan", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "a9ecafa2c206c9e5deab6286e19f0711", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "MITREVSKI, Jovan", "id": "0"}], "title": "Preparing ATLAS Reconstruction for LHC Run 2", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T20:37:39.553167+00:00", "description": "", "title": "CHEPRecoPoster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/147\/attachments\/578550\/796694\/CHEPRecoPoster.pdf", "filename": "CHEPRecoPoster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796694, "size": 7693624}], "title": "Slides", "default_folder": false, "id": 578550, "description": ""}], "_type": "Contribution", "description": "In order to maximize the physics potential of the ATLAS detector during LHC's run 2, the Reconstruction software has been updated. Flat computing budgets required a factor of three improved run time, while the new xAOD data format forced changes in the reconstruction\r\nalgorithms. Physics performance improvements have been made in the reconstruction of various objects, using improved techniques like particle flow, multivariate discriminants, etc. This talk will present an overview of the improvements that have been made.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578550", "resources": [{"_type": "LocalFile", "name": "CHEPRecoPoster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/147\/attachments\/578550\/796694\/CHEPRecoPoster.pdf", "fileName": "CHEPRecoPoster.pdf", "_fossil": "localFileMetadata", "id": "796694", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/147", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:30:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "144", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2e6780d520127502454830ec573033ae", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MAENO, Tadashi", "id": "4"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e6819ee253837bc1a19f5e93c8ef001e", "affiliation": "University of Texas at Arlington (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DE, Kaushik", "id": "2"}], "title": "The Future of PanDA in ATLAS Distributed Computing", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T09:10:57.514256+00:00", "description": "", "title": "CHEP15_ATLASpanda.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/144\/attachments\/578551\/796695\/CHEP15_ATLASpanda.pdf", "filename": "CHEP15_ATLASpanda.pdf", "content_type": "application\/pdf", "type": "file", "id": 796695, "size": 805692}], "title": "Slides", "default_folder": false, "id": 578551, "description": ""}], "_type": "Contribution", "description": "Experiments at the Large Hadron Collider (LHC) face unprecedented computing challenges. Heterogeneous resources are distributed worldwide at hundreds of sites, thousands of physicists analyze the data remotely, the volume of processed data is beyond the exabyte scale, while data processing requires more than a few billion hours of computing usage per year. The PanDA (Production and Distributed Analysis) system was developed to meet the scale and complexity of LHC distributed computing for the ATLAS experiment. In the process, the old batch job paradigm of locally managed computing in HEP was discarded in favor of a far more automated, flexible and scalable model. The success of PanDA in ATLAS is leading to widespread adoption and testing by other experiments. PanDA is the first exascale workload management system in HEP, already operating at more than a million computing jobs per day, and processing over an exabyte of data in 2013. There are many new challenges that PanDA will face in the near future, in addition to \u00a0new challenges of scale, heterogeneity and increasing user base. PanDA will need to handle rapidly changing computing infrastructure, will require factorization of code for easier deployment, will need to incorporate additional information sources including network metrics in decision making, be able to control network circuits, handle dynamically sized workload processing, provide improved visualization, and face many other challenges. In this talk we will focus on the new features, planned or recently implemented, that are relevant to the next decade of distributed computing workload management using PanDA.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578551", "resources": [{"_type": "LocalFile", "name": "CHEP15_ATLASpanda.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/144\/attachments\/578551\/796695\/CHEP15_ATLASpanda.pdf", "fileName": "CHEP15_ATLASpanda.pdf", "_fossil": "localFileMetadata", "id": "796695", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "c8b8647f9e6a89a40a3aae5e517a0c30", "affiliation": "ATLAS", "_fossil": "contributionParticipationMetadata", "fullName": "VANIACHINE, Alexandre", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "849637192af92a0f322682b2abc1e859", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WENAUS, Torre", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "6fef2a09accf1a707bc4c992c15a8438", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KLIMENTOV, Alexei", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "2e6780d520127502454830ec573033ae", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MAENO, Tadashi", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "de99e5c8c7b2c0edb4ab024a79d9a913", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "NILSSON, Paul", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "ae65bffa7bc1bd6543d1748f5c46b7c6", "affiliation": "Joint Inst. for Nuclear Research (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "OLEYNIK, Danila", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "c961d8268c95605b7c0fe0d2cdbd4603", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "PANITKIN, Sergey", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "275c5b84f25777c7a0398e21b4b855bc", "affiliation": "Joint Inst. for Nuclear Research (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "PETROSYAN, Artem", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "a5e2efd46353da79fec29fc6172e4194", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "SCHOVANCOVA, Jaroslava", "id": "9"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/144", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:30:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "145", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2bfc3dcae6061d54150b9904432bf7ee", "affiliation": "Jozef Stefan Institute (SI)", "_fossil": "contributionParticipationMetadata", "fullName": "FILIPCIC, Andrej", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2bfc3dcae6061d54150b9904432bf7ee", "affiliation": "Jozef Stefan Institute (SI)", "_fossil": "contributionParticipationMetadata", "fullName": "FILIPCIC, Andrej", "id": "0"}], "title": "Dynamic Resource Allocation with the ARC Control Tower", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T22:58:27.590423+00:00", "description": "", "title": "chep2015act.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/145\/attachments\/578552\/796696\/chep2015act.pdf", "filename": "chep2015act.pdf", "content_type": "application\/pdf", "type": "file", "id": 796696, "size": 643629}], "title": "Slides", "default_folder": false, "id": 578552, "description": ""}], "_type": "Contribution", "description": "Distributed computing resources available for high-energy physics research are becoming less dedicated to one type of workflow and researchers\u2019 workloads are increasingly exploiting modern computing technologies such as parallelism. The current pilot job management model used by many experiments relies on static dedicated resources and cannot easily adapt to these changes. The model used for ATLAS in Nordic countries and some other places enables a flexible job management system based on dynamic resources allocation. Rather than a fixed set of resources managed centrally, the model allows resources to be requested on the fly. The ARC Computing Element (ARC-CE) and ARC Control Tower (aCT) are the key components of the model. The aCT requests jobs from the ATLAS job mangement system (Panda) and submits a fully-formed job description to ARC-CEs. ARC-CE can then dynamically request the required resources from the underlying batch system. In this paper we describe the architecture of the model and the experience of running many millions of ATLAS jobs on it.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578552", "resources": [{"_type": "LocalFile", "name": "chep2015act.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/145\/attachments\/578552\/796696\/chep2015act.pdf", "fileName": "chep2015act.pdf", "_fossil": "localFileMetadata", "id": "796696", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "269913119a8e37ae971b93b8559c6092", "affiliation": "University of Oslo (NO)", "_fossil": "contributionParticipationMetadata", "fullName": "CAMERON, David", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "3ff2d337ba260b7e730554f80f69db7c", "affiliation": "University of Oslo (NO)", "_fossil": "contributionParticipationMetadata", "fullName": "NILSEN, Jon Kerr", "id": "2"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/145", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "142", "speakers": [{"_type": "ContributionParticipation", "emailHash": "fd0f8bf65da577a884b4bb1260da8d60", "affiliation": "New York University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "HEINRICH, Lukas Alexander", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "fb286c1da0737c5907818fba6f89da2a", "affiliation": "Univ Manchester\/Cockcroft Inst.", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. JONES, Roger", "id": "0"}], "title": "Analysis Preservation in ATLAS", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T07:34:07.588182+00:00", "description": "", "title": "ATL-SOFT-SLIDE-2015-160.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/142\/attachments\/578553\/796697\/ATL-SOFT-SLIDE-2015-160.pdf", "filename": "ATL-SOFT-SLIDE-2015-160.pdf", "content_type": "application\/pdf", "type": "file", "id": 796697, "size": 806217}], "title": "Slides", "default_folder": false, "id": 578553, "description": ""}], "_type": "Contribution", "description": "Long before data taking ATLAS established a policy that all analyses need to be preserved. In the initial data-taking period, this has been achieved by various tools and techniques. ATLAS is now reviewing the analysis preservation with the aim to bring coherence and robustness to the process and with a clearer view of the level of reproducibility that is reasonably achievable. The secondary aim is to reduce the load on the analysts. Once complete, this will serve for our internal preservation needs but also provide a basis for any subsequent sharing of analysis results with external parties.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578553", "resources": [{"_type": "LocalFile", "name": "ATL-SOFT-SLIDE-2015-160.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/142\/attachments\/578553\/796697\/ATL-SOFT-SLIDE-2015-160.pdf", "fileName": "ATL-SOFT-SLIDE-2015-160.pdf", "_fossil": "localFileMetadata", "id": "796697", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "c3aa620021fa578a90d03b76601c59b5", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "SOUTH, David Michael", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "5b217a44340c73d0ddfa1b01630d0a53", "affiliation": "New York University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CRANMER, Kyle Stuart", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/142", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": null, "keywords": [], "id": "143", "speakers": [{"_type": "ContributionParticipation", "emailHash": "37d5cd476929d06a7eb93cf7d6f4cbc5", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LE COMPTE, Thomas", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "37d5cd476929d06a7eb93cf7d6f4cbc5", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LE COMPTE, Thomas", "id": "0"}], "title": "Overview of High Performance Computing in the ATLAS Experiment at the LHC", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The Large Hadron Collider (LHC), operating at the international CERN Laboratory in Geneva, Switzerland, is leading Big Data driven scientific explorations. Experiments at the LHC explore the fundamental nature of matter and the basic forces that shape our universe, and were recently credited for the discovery of a Higgs boson. ATLAS, one of the largest collaborations ever assembled in the sciences, is at the forefront of research at the LHC. To address an unprecedented multi-petabyte data processing challenge, the ATLAS experiment is relying on a heterogeneous distributed computational infrastructure. ATLAS uses of the Worldwide LHC Computing Grid (WLCG) has been remarkably successful, with a billion CPU-hours supplied per year. \u00a0However, the growth of the WLCG is constrained. \u00a0Additionally, there exist jobs \u00a0that are too computationally intense to run efficiently on the grid. High performance computing provides an attractive solution to these challenges. \u00a0\r\nIn 2013 we started an ambitious program to integrate all available computing \u00a0resources, including scheduled and opportunistic use of commercial and academic clouds, \u00a0Leadership Computing Facilities and super-computers in ATLAS Distributed Computing. We discuss an overview of ATLAS' strategy, history and global planning in this area. \u00a0Performance and scaling for various HEP-specific and generic payloads will be shown and prospects for the future will be discussed.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "31ad2958f43646ca62e343f3cab00b08", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "BENJAMIN, dechenaux", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "ae65bffa7bc1bd6543d1748f5c46b7c6", "affiliation": "Joint Inst. for Nuclear Research (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "OLEYNIK, Danila", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "c961d8268c95605b7c0fe0d2cdbd4603", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "PANITKIN, Sergey", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "e8ec1cfae41e680d1a09d74ef5e31096", "affiliation": "Jefferson Lab", "_fossil": "contributionParticipationMetadata", "fullName": "WALKER, Richard", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "0f4c669d0559c778dc39e39a479209e1", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "BENJAMIN, guiot", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "f3174c496400e0050cf9775ffa9c2796", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CHILDERS, Taylor", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "e6819ee253837bc1a19f5e93c8ef001e", "affiliation": "University of Texas at Arlington (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DE, Kaushik", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "2bfc3dcae6061d54150b9904432bf7ee", "affiliation": "Jozef Stefan Institute (SI)", "_fossil": "contributionParticipationMetadata", "fullName": "FILIPCIC, Andrej", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "6fef2a09accf1a707bc4c992c15a8438", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KLIMENTOV, Alexei", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "8fc0de19145510b5f4a8150497be283a", "affiliation": "CEA\/IRFU,Centre d'etude de Saclay Gif-sur-Yvette (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "LANCON, Eric Christian", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "a0fcddc3759379b2bdf0e359fabd7e04", "affiliation": "SLAC National Accelerator  Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MOUNT, Richard Philip", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "de99e5c8c7b2c0edb4ab024a79d9a913", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "NILSSON, Paul", "id": "12"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/contribution\/143", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:15:00"}, "duration": 15, "session": "Track 8 Session", "keywords": [], "id": "140", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6aef683082e2cae7cc6fb0e086c1b9be", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "TSULAIA, Vakho", "id": "6"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c3b302a4fc2fd1ca19138cb3a3e5479a", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CALAFIURA, Paolo", "id": "0"}], "title": "Fine grained event processing on HPCs with the ATLAS Yoda system", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T01:03:36.836809+00:00", "description": "", "title": "ATLAS-Yoda.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/140\/attachments\/578554\/796698\/ATLAS-Yoda.pdf", "filename": "ATLAS-Yoda.pdf", "content_type": "application\/pdf", "type": "file", "id": 796698, "size": 1107262}], "title": "Slides", "default_folder": false, "id": 578554, "description": ""}], "_type": "Contribution", "description": "High performance computing facilities present unique challenges and opportunities for HENP event processing. The massive scale of many HPC systems means that fractionally small utilizations can yield large returns in processing throughput. Parallel applications which can dynamically and efficiently fill any scheduling opportunities the resource presents benefit both the facility (maximal utilization) and the (compute-limited) science. The ATLAS Yoda system provides this capability to HENP-like event processing applications by implementing event-level processing in an MPI-based master-client model that integrates seamlessly with the more broadly scoped ATLAS Event Service. Fine grained, event level work assignments are intelligently dispatched to parallel workers to sustain full utilization on all cores, with outputs streamed off to destination object stores in near real time with similarly fine granularity, such that processing can proceed until termination with full utilization. The system offers the efficiency and scheduling flexibility of preemption without requiring the application actually support or employ checkpointing. We will present the new Yoda system, its motivations, architecture, implementation, and applications in ATLAS data processing at several US HPC centers.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578554", "resources": [{"_type": "LocalFile", "name": "ATLAS-Yoda.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/140\/attachments\/578554\/796698\/ATLAS-Yoda.pdf", "fileName": "ATLAS-Yoda.pdf", "_fossil": "localFileMetadata", "id": "796698", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "af353d572ba111925ed14b42afd4e2e5", "affiliation": "University of Wisconsin (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GUAN, Wen", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "2e6780d520127502454830ec573033ae", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MAENO, Tadashi", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "de99e5c8c7b2c0edb4ab024a79d9a913", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "NILSSON, Paul", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "ae65bffa7bc1bd6543d1748f5c46b7c6", "affiliation": "Joint Inst. for Nuclear Research (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "OLEYNIK, Danila", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "c961d8268c95605b7c0fe0d2cdbd4603", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "PANITKIN, Sergey", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "6aef683082e2cae7cc6fb0e086c1b9be", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "TSULAIA, Vakho", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "3883f9db68feb16e68792b1483a250cd", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. VAN GEMMEREN, Peter", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "849637192af92a0f322682b2abc1e859", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WENAUS, Torre", "id": "8"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/140", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:00:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "141", "speakers": [{"_type": "ContributionParticipation", "emailHash": "f93e30ef38e68512dad5beb650c2ba19", "affiliation": "Lancaster University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "JONES, Roger", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "f93e30ef38e68512dad5beb650c2ba19", "affiliation": "Lancaster University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "JONES, Roger", "id": "4"}], "title": "Data Preservation in ATLAS", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T04:38:00.742648+00:00", "description": "", "title": "DatPreservation.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/141\/attachments\/578555\/796699\/DatPreservation.pdf", "filename": "DatPreservation.pdf", "content_type": "application\/pdf", "type": "file", "id": 796699, "size": 999484}, {"_type": "attachment", "modified_dt": "2015-04-14T04:38:00.742648+00:00", "description": "", "title": "DatPreservation.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/141\/attachments\/578555\/796700\/DatPreservation.pptx", "filename": "DatPreservation.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796700, "size": 1970202}], "title": "Slides", "default_folder": false, "id": 578555, "description": ""}], "_type": "Contribution", "description": "Complementary to parallel open access and analysis preservation initiatives, ATLAS is taking steps to ensure that the data taken by the experiment during run-1 remain accessible and available for future analysis by the collaboration. An evaluation of what is required to achieve this is underway, examining the ATLAS data production chain to establish the effort required and potential problems. Several alternatives are explored, but the favoured solution is to bring the run 1 data and software in line with the equivalent to that which will be used for run 2. This will result in a coherent ATLAS dataset for the data already taken and that to come in the future.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578555", "resources": [{"_type": "LocalFile", "name": "DatPreservation.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/141\/attachments\/578555\/796699\/DatPreservation.pdf", "fileName": "DatPreservation.pdf", "_fossil": "localFileMetadata", "id": "796699", "_deprecated": true}, {"_type": "LocalFile", "name": "DatPreservation.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/141\/attachments\/578555\/796700\/DatPreservation.pptx", "fileName": "DatPreservation.pptx", "_fossil": "localFileMetadata", "id": "796700", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "c3aa620021fa578a90d03b76601c59b5", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "SOUTH, David Michael", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "5b217a44340c73d0ddfa1b01630d0a53", "affiliation": "New York University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CRANMER, Kyle Stuart", "id": "2"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/141", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:45:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "148", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5408e1e18dffa5d9d8bbaa52be69ba44", "affiliation": "TRIUMF (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "SEUSTER, Rolf", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6aef683082e2cae7cc6fb0e086c1b9be", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "TSULAIA, Vakho", "id": "0"}], "title": "Status and future evolution of the ATLAS offline software", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T12:00:48.437478+00:00", "description": "", "title": "seuster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/148\/attachments\/578556\/796701\/seuster.pdf", "filename": "seuster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796701, "size": 1318624}], "title": "Slides", "default_folder": false, "id": 578556, "description": ""}], "_type": "Contribution", "description": "The talk will give a summary of the broad spectrum of software upgrade projects to prepare ATLAS for the challenges of the soon coming LHC Run-2. Those projects include the reduction of the CPU required for reconstruction by a factor 3 compared to 2012, which was required to meet the challenges of the expected increase in pileup and the higher data taking rate of up to 1 kHz. As well, the new Integrated Simulation Framework (ISF) has been put into production. By far the most ambitious project is the implementation of a completely new Analysis Model, based on a new ROOT readable reconstruction format xAOD, a reduction framework based on the train model to centrally produce skimmed data samples and an analysis framework. The Data Challenge 2014 has been a first large scale test of most of the foreseen software upgrades. At the end of the talk an overview will be given of future software projects and plans that will lead up to the coming Long Shutdown 2 as the next major ATLAS software upgrade phase.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578556", "resources": [{"_type": "LocalFile", "name": "seuster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/148\/attachments\/578556\/796701\/seuster.pdf", "fileName": "seuster.pdf", "_fossil": "localFileMetadata", "id": "796701", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "9f60b1ecea45598b615f77b2b41150f1", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "STEWART, Graeme", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "5408e1e18dffa5d9d8bbaa52be69ba44", "affiliation": "TRIUMF (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "SEUSTER, Rolf", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "ce322777a6c04cb5d6b95977c554eabb", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ELSING, Markus", "id": "3"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/148", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "149", "speakers": [{"_type": "ContributionParticipation", "emailHash": "74f37452da18d70a445f84d900eff716", "affiliation": "Universita e INFN, Roma I (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "DE SALVO, Alessandro", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "7619cbf29b7dbd4004fe9d055e7b0f7b", "affiliation": "INFN Bari", "_fossil": "contributionParticipationMetadata", "fullName": "ELIA, Domenico", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "ad27265638303547200b971ea77f7b31", "affiliation": "Sezione di Pisa (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "BOCCALI, Tommaso", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "f468c773c2b29b515ce22d80a946a1ee", "affiliation": "Universit\u00e0 degli Studi  e INFN Milano (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "PERINI, Laura", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "74f37452da18d70a445f84d900eff716", "affiliation": "Universita e INFN, Roma I (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "DE SALVO, Alessandro", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "7619cbf29b7dbd4004fe9d055e7b0f7b", "affiliation": "INFN Bari", "_fossil": "contributionParticipationMetadata", "fullName": "ELIA, Domenico", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "ad27265638303547200b971ea77f7b31", "affiliation": "Sezione di Pisa (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "BOCCALI, Tommaso", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "f468c773c2b29b515ce22d80a946a1ee", "affiliation": "Universit\u00e0 degli Studi  e INFN Milano (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "PERINI, Laura", "id": "3"}], "title": "Improvements of LHC data analysis techniques at italian WLCG sites. Case-study of the transfer of this technology to other research areas", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T21:08:13.081147+00:00", "description": "", "title": "poster_PRIN_CHEP_2015-final.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/149\/attachments\/578557\/796702\/poster_PRIN_CHEP_2015-final.pdf", "filename": "poster_PRIN_CHEP_2015-final.pdf", "content_type": "application\/pdf", "type": "file", "id": 796702, "size": 5729567}, {"_type": "attachment", "modified_dt": "2015-04-10T21:08:13.081147+00:00", "description": "", "title": "poster_PRIN_CHEP_2015-final.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/149\/attachments\/578557\/796703\/poster_PRIN_CHEP_2015-final.pptx", "filename": "poster_PRIN_CHEP_2015-final.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796703, "size": 28348352}], "title": "Poster", "default_folder": false, "id": 578557, "description": ""}], "_type": "Contribution", "description": "In 2012, 14 Italian Institutions participating in all major LHC Experiments won a grant from the ITALIAN MINISTRY OF RESEARCH (MIUR), to optimise Analysis activities and in general the Tier2\/Tier3 infrastructure. We report on the activities being researched upon, on the considerable improvement in the ease of access to resources by physicists, also those with no specific computing interests. We focused on items like distributed storage federations, access to batch-like facilities, provisioning of user interfaces on demand and cloud systems. R&D on next-generation databases, distributed analysis interfaces, and new computing architectures was also carried on. The project, ending in the first months of 2016, will produce a white paper with recommendations on best practices for data-analysis support by computing centers.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578557", "resources": [{"_type": "LocalFile", "name": "poster_PRIN_CHEP_2015-final.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/149\/attachments\/578557\/796702\/poster_PRIN_CHEP_2015-final.pdf", "fileName": "poster_PRIN_CHEP_2015-final.pdf", "_fossil": "localFileMetadata", "id": "796702", "_deprecated": true}, {"_type": "LocalFile", "name": "poster_PRIN_CHEP_2015-final.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/149\/attachments\/578557\/796703\/poster_PRIN_CHEP_2015-final.pptx", "fileName": "poster_PRIN_CHEP_2015-final.pptx", "_fossil": "localFileMetadata", "id": "796703", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/149", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "133", "speakers": [{"_type": "ContributionParticipation", "emailHash": "9f60b1ecea45598b615f77b2b41150f1", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "STEWART, Graeme", "id": "10"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "dfa0bbe61af7403cd7b9b6a0556ab580", "affiliation": "Istituto Nazionale Fisica Nucleare (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "ANNOVI, Alberto", "id": "0"}], "title": "ATLAS Fast Tracker Simulation Challenges", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T15:01:10.549526+00:00", "description": "", "title": "ATL-SOFT-SLIDE-2015-095.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/133\/attachments\/578558\/796704\/ATL-SOFT-SLIDE-2015-095.pdf", "filename": "ATL-SOFT-SLIDE-2015-095.pdf", "content_type": "application\/pdf", "type": "file", "id": 796704, "size": 5433337}], "title": "Poster", "default_folder": false, "id": 578558, "description": ""}], "_type": "Contribution", "description": "To deal with Big Data flood from the ATLAS detector most events have to be rejected in the trigger system. the trigger rejection is complicated by the presence of a large number of minimum-bias events \u2013 the pileup. To limit pileup effects in the high luminosity environment of the LHC Run-2, ATLAS relies on full tracking provided by the Fast TracKer (FTK) implemented with custom electronics.\r\n\r\nThe FTK data processing pipeline has to be simulated in preparation for LHC upgrades to support electronics design and develop trigger strategies at high luminosity. The simulation of the FTK - a highly parallelized system - has inherent performance bottlenecks on general-purpose CPUs. To take advantage of the Grid Computing power, the FTK simulation is integrated with Monte Carlo simulations at the Production System level above the ATLAS workload management system PanDA.\r\n\r\nWe report on ATLAS experience with FTK simulations on the Grid and next steps for accommodating the growing requirements for resources during the LHC Run-2.", "track": "Track2: Offline software ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578558", "resources": [{"_type": "LocalFile", "name": "ATL-SOFT-SLIDE-2015-095.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/133\/attachments\/578558\/796704\/ATL-SOFT-SLIDE-2015-095.pdf", "fileName": "ATL-SOFT-SLIDE-2015-095.pdf", "_fossil": "localFileMetadata", "id": "796704", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "d69f5fff808f8ac8ab6376496b16ae5a", "affiliation": "National Research Nuclear  University MEPhI (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "BORODIN, Misha", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c8b8647f9e6a89a40a3aae5e517a0c30", "affiliation": "ATLAS", "_fossil": "contributionParticipationMetadata", "fullName": "VANIACHINE, Alexandre", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "1a1fbb9c5b3df16fe39e15d4f46c3b14", "affiliation": "Sezione di Pisa (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. VOLPI, Guido", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "9d6b2fb4af7a5d86cd6d6e7ea2b92172", "affiliation": "Northern Illinois University", "_fossil": "contributionParticipationMetadata", "fullName": "CHAKRABORTY, Dhiman", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "77e6974a25a29ef6da9a14157c570471", "affiliation": "Instituto de Fisica Corpuscular (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "GARCIA NAVARRO, Jose Enrique", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "85c603b853b0de217a396ed06b1e0ff1", "affiliation": "Institute for High Energy Physics (IHEP)-Unknown-Unknown", "_fossil": "contributionParticipationMetadata", "fullName": "GOLUBKOV, Dmitri", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "2368fbf6eeafdccf7f1cf1924e2bde61", "affiliation": "Southern Methodist University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KAMA, Sami", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "c961d8268c95605b7c0fe0d2cdbd4603", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "PANITKIN, Sergey", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "db331cedcdce208e2ba9db3713b3f1db", "affiliation": "Conseil Europeen Recherche Nucl. (CERN)-Unknown-Unknown", "_fossil": "contributionParticipationMetadata", "fullName": "SMIRNOV, Yuri", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "9f60b1ecea45598b615f77b2b41150f1", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "STEWART, Graeme", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "03d92a1ccac5d3368a412e4ba81f5214", "affiliation": "University of Chicago (US)", "_fossil": "contributionParticipationMetadata", "fullName": "TOMPKINS, Lauren Alexandra", "id": "11"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/133", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:30:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:45:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "132", "speakers": [{"_type": "ContributionParticipation", "emailHash": "959f53b1cd9e1a3573140173e1578241", "affiliation": "urn:Google", "_fossil": "contributionParticipationMetadata", "fullName": "ROCCO, Dominick", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "16bdf91b327521302d812ff7c61549ed", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "BACKHOUSE, Christopher", "id": "0"}], "title": "The Library Event Matching classifier for $\\nu_e$ events in NOvA", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T01:21:26.780270+00:00", "description": "", "title": "lem_chep_2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/132\/attachments\/578559\/796705\/lem_chep_2015.pdf", "filename": "lem_chep_2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796705, "size": 3196082}], "title": "Slides", "default_folder": false, "id": 578559, "description": ""}], "_type": "Contribution", "description": "In this paper we present the Library Event Matching (LEM) classification technique for particle identification. The LEM technique was developed for the NOvA electron neutrino appearance analysis as an alternative but complimentary approach to standard multivariate methods. Traditional multivariate PIDs are based on high-level reconstructed quantities which can obscure or discard important low-level detail in high granularity detectors. LEM, by contrast, uses the full hit by hit information of the event, comparing the hit charges and positions of each physics event to a large \"template\" library of simulated signal and background events. This is a powerful classification technique for the finely segmented NOvA detectors, but poses computational challenges due to the large Monte Carlo template libraries required for to reach the optimal physics sensitivity. We will present both the LEM classification technique as well as its technical implementation for the NOvA experiment exploiting memory mapping techniques on high memory Linux platforms.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578559", "resources": [{"_type": "LocalFile", "name": "lem_chep_2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/132\/attachments\/578559\/796705\/lem_chep_2015.pdf", "fileName": "lem_chep_2015.pdf", "_fossil": "localFileMetadata", "id": "796705", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "6643181cb4338c0a4bedd75ccdfacd86", "affiliation": "Caltech", "_fossil": "contributionParticipationMetadata", "fullName": "PATTERSON, Ryan", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "959f53b1cd9e1a3573140173e1578241", "affiliation": "urn:Google", "_fossil": "contributionParticipationMetadata", "fullName": "ROCCO, Dominick", "id": "2"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/132", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:45:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "131", "speakers": [{"_type": "ContributionParticipation", "emailHash": "f13af63ad4bbdfdce02559c629f8d6bc", "affiliation": "University of Victoria (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "GABLE, Ian", "id": "5"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e106556bcded279b116df4aee2de6c31", "affiliation": "University of Victoria (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SOBIE, Randy", "id": "0"}], "title": "HEP cloud production using the CloudScheduler\/HTCondor Architecture", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T02:52:00.743080+00:00", "description": "", "title": "Gable-Chep-2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/131\/attachments\/578560\/796706\/Gable-Chep-2015.pdf", "filename": "Gable-Chep-2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796706, "size": 1516928}], "title": "Slides", "default_folder": false, "id": 578560, "description": ""}], "_type": "Contribution", "description": "The use of distributed IaaS clouds with the CloudScheduler\/HTCondor architecture has been in production for HEP and astronomy applications for a number of years. The design has proven to be robust and reliable for batch production using HEP clouds, academic non-HEP (opportunistic) clouds and commercial clouds.   Further, the system is seamlessly integrated into the existing WLCG infrastructures for the both ATLAS and BelleII experiments.  Peak workloads continue to increase and we have utilized over 3000 cores for HEP applications.\r\n\r\nWe show that the CloudScheduler\/HTCondor architecture has adapted well to the evolving cloud ecosystem. Its design preceded the introduction of OpenStack clouds, however, the integration of these clouds has been straightforward. Key developments over the past year include the use of CVMFS for application software distribution. CVMFS together with the global array of Squid web caches and the use of the Squid-discovery service, Shoal, have significantly simplified the management and distribution of virtual machine images. The recent deployment of micro-CernVM images have reduced the size of the images to a few megabytes and make the images independent of the application software. The introduction of Glint, a distributed VM image management service, has made the distribution of images over multiple OpenStack clouds an automated operation. These new services have greatly simplified management of the application software and operating system, and enable us to respond quickly to urgent security issues such as the ShellShock (bash shell) vulnerability.\r\n\r\nHEP experiments are beginning to use multi-core applications and this has resulted in a demand for simultaneously running single-core, multi-core or high-memory jobs. We exploit HTCondor\u2019s feature that dynamically partitions the slots within a VM instance. As a result, a new VM instance can run either single or multi-core jobs depending on the demand. \r\n\r\nThe distributed cloud system has been used primarily for low I\/O production jobs, however, the goal is run higher I\/O production and analysis application jobs.   This will require the use a data federation and we are integrating the CERN UGR data federator for managing data distributed over multiple storage elements.    \r\n\r\nThe use of distributed clouds with the CloudScheduler\/HTCondor architecture continues to improve and expand in use and functionality.   We review the overall progress, and highlight the new features and future challenges.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578560", "resources": [{"_type": "LocalFile", "name": "Gable-Chep-2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/131\/attachments\/578560\/796706\/Gable-Chep-2015.pdf", "fileName": "Gable-Chep-2015.pdf", "_fossil": "localFileMetadata", "id": "796706", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "32e62a0d1cb79b90404b2e9b5eae5126", "affiliation": "University of Victoria (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "TAYLOR, Ryan", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "ebf920eeaeb952921229cbbafabd92d7", "affiliation": "University of Victoria (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "BERGHAUS, Frank Olaf", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "70bf976e0adc8bccf178516895a365dd", "affiliation": "University of Victoria (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "LEAVETT-BROWN, Colin Roy", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "e490ca70cc14e2ca27f8071da6597bdc", "affiliation": "U", "_fossil": "contributionParticipationMetadata", "fullName": "PATERSON, Michael", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "f13af63ad4bbdfdce02559c629f8d6bc", "affiliation": "University of Victoria (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "GABLE, Ian", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "0cb8f3c84f79ded1da04cdaf2b5f719a", "affiliation": "University of Victoria", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. DESMARAIS, Ron", "id": "6"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/131", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "130", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a7a7869681df44c70a20722d524703a9", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GOVI, Giacomo", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "a7a7869681df44c70a20722d524703a9", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GOVI, Giacomo", "id": "0"}], "title": "The CMS Condition Database system", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The Condition Database plays a key role in the CMS computing infrastructure. The complexity of the detector and the variety of the sub-systems involved are setting tight requirements for handling the Conditions. In the last two years the collaboration has put an effort in the re-design of the Condition Database system, with the aim to improve the scalability and the operability for the data taking starting in 2015. The re-design has focused in simplifying the architecture, using the lessons learned during the operation of the previous data-taking period. In the new system the relational features of the database schema are mainly exploited to handle the metadata ( Tag and Interval of Validity ), allowing for a limited and controlled set of queries. The bulk condition data ( Payloads ) are stored as unstructured binary data, allowing the storage in a single table with a common layout for all of the condition data types. In this presentation, we describe the full architecture of the system, including the services implemented for uploading payloads and the tools for browsing the database. Furthermore, the implementation choices for the core software will be discussed.", "track": "Track3: Data store and access ", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "c5fff1e296ec574187817b412c5fd077", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PFEIFFER, Andreas", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "2e87625d2035544993c5da10819245b8", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "OJEDA SANDONIS, Miguel", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "9c6512217061840fffdf6db3fe883a9c", "affiliation": "Universita degli Studi Guglielmo Marconi (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "DI GUIDA, Salvatore", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/130", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:00:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "137", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2886073831b4a98413ec3f6f2d71b490", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "LEGGER, Federica", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "fecbc4752eaee2cab98bdd811db0c590", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "DEWHURST, Alastair", "id": "0"}], "title": "Distributed analysis in ATLAS", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-04T15:54:05.863298+00:00", "description": "", "title": "chep2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/137\/attachments\/578561\/796707\/chep2015.pdf", "filename": "chep2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796707, "size": 903220}], "title": "Slides", "default_folder": false, "id": 578561, "description": ""}], "_type": "Contribution", "description": "The ATLAS experiment accumulated more than 140 PB of data during the first run of the Large Hadron Collider (LHC) at CERN. The analysis of such an amount of data for the distributed physics community is a challenging task. The Distributed Analysis (DA) system of the ATLAS experiment is an established and stable component of the ATLAS distributed computing operations. About half a million user jobs are daily running on DA resources, submitted by more than 1500 ATLAS physicists. The reliability of the DA system during the first run of the LHC and the following shutdown period has been high thanks to the continuous automatic validation of the distributed analysis sites and the user support provided by a dedicated team of expert shifters. During the LHC shutdown, the ATLAS computing model has undergone several changes to improve the analysis workflows, including the re-design of the production system, a new analysis data format and event model, and the development of common reduction and analysis frameworks. We report on the impact such changes have on the DA infrastructure, describe the new DA components, and include first measurements of DA performances in the first months of the second LHC run, starting early 2015.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578561", "resources": [{"_type": "LocalFile", "name": "chep2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/137\/attachments\/578561\/796707\/chep2015.pdf", "fileName": "chep2015.pdf", "_fossil": "localFileMetadata", "id": "796707", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "2886073831b4a98413ec3f6f2d71b490", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "LEGGER, Federica", "id": "1"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/137", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:15:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "499", "speakers": [{"_type": "ContributionParticipation", "emailHash": "8155146b9e62e93eaedf8f57c8045b17", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GOWDY, Stephen", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "8155146b9e62e93eaedf8f57c8045b17", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GOWDY, Stephen", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "9a8ce9aebaabdde7956acf3f73f3b7a5", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ELMER, Peter", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "5ef39cdc5ffe3205efeb348aed6b90d5", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BAUERDICK, Lothar A.T.", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "404ffd25f1af8b96d12533b8dcfc88dd", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "WUERTHWEIN, Frank", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "e82c46a3a0d49dd555c888d113eace3a", "affiliation": "University of Nebraska (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BOCKELMAN, Brian Paul", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "1905f721940f9ff8a9b0b9b1e9f7eef9", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "TADEL, Matevz", "id": "5"}], "title": "Designing Computing System Architecture and Models for the HL-LHC era", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T04:52:46.336016+00:00", "description": "", "title": "20150413-chep15-gowdy-compmodels.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/499\/attachments\/578562\/796708\/20150413-chep15-gowdy-compmodels.pdf", "filename": "20150413-chep15-gowdy-compmodels.pdf", "content_type": "application\/pdf", "type": "file", "id": 796708, "size": 3143354}], "title": "Slides", "default_folder": false, "id": 578562, "description": ""}], "_type": "Contribution", "description": "The global distributed computing system (WLCG) used by the Large Hadron\r\nCollider (LHC) is evolving. The treatment of wide-area-networking (WAN) as\r\na scarce resource that needs to be strictly managed is far less\r\nnecessary that originally foreseen. Static data placement and replication,\r\nintended to limit interdependencies among computing centers, is giving way\r\nto global data federations building on computing centers whose maturity\r\nhas increased significantly over the past decade. Different modalities for\r\nprovisioning resources, including commercial clouds, will coexist with\r\ncomputing centers in our labs and universities. Compute resources may\r\nincreasingly be shared between HEP and other fields.\r\n\r\nBy necessity today's computing system is evolving in an adiabatic\r\nfashion due to the need to support the next LHC run. In the\r\nmedium and long term, however, a number of questions arise regarding the\r\nappropriate system architecture, for example: What ratio of storage to\r\ncompute capabilities will be needed? How should storage be deployed\r\ngeographically to maximally exploit owned, opportunistic and cloud\r\ndistributed compute capabilities? What is the right mix of placement,\r\nWAN reads, and automated caching to optimize performance? How will the\r\nreliability and scalability of the system be impacted by these choices?\r\nCan different computing models or computation techniques (map reduce, etc.)\r\nbe deployed more easily with different system architectures?\r\n\r\nIn this presentation we report results from our work to simulate\r\nfuture distributed computing system architectures and computing models\r\nto answer these questions. When possible we also report our efforts\r\nto validate this simulation using today's computing system.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578562", "resources": [{"_type": "LocalFile", "name": "20150413-chep15-gowdy-compmodels.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/499\/attachments\/578562\/796708\/20150413-chep15-gowdy-compmodels.pdf", "fileName": "20150413-chep15-gowdy-compmodels.pdf", "_fossil": "localFileMetadata", "id": "796708", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/499", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": null, "keywords": [], "id": "135", "speakers": [{"_type": "ContributionParticipation", "emailHash": "269913119a8e37ae971b93b8559c6092", "affiliation": "University of Oslo (NO)", "_fossil": "contributionParticipationMetadata", "fullName": "CAMERON, David", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "b5d397d364f729abc4025f6fc8fa6aa8", "affiliation": "Laboratoire de l'Accelerateur Lineaire (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "ADAM BOURDARIOS, Claire", "id": "0"}], "title": "ATLAS@Home : \u00a0an entry point for ATLAS Software and Computing outreach", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The ATLAS collaboration has setup a volunteer computing project called ATLAS@home. Volunteers running Monte-Carlo simulation on their personal computer provide significant computing resources, but also belong to a community potentially interested in HEP. Four types of contributors have been identified, whose questions range from advanced technical details to the reason why simulation is needed, how Computing is organized and how it relates to society. The creation of relevant outreach material for simulation, event visualization and distributed production will be described, as well as lessons learned while interacting with the BOINC volunteers community.", "track": "Track7: Clouds and virtualization", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "269913119a8e37ae971b93b8559c6092", "affiliation": "University of Oslo (NO)", "_fossil": "contributionParticipationMetadata", "fullName": "CAMERON, David", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "2bfc3dcae6061d54150b9904432bf7ee", "affiliation": "Jozef Stefan Institute (SI)", "_fossil": "contributionParticipationMetadata", "fullName": "FILIPCIC, Andrej", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "8fc0de19145510b5f4a8150497be283a", "affiliation": "CEA\/IRFU,Centre d'etude de Saclay Gif-sur-Yvette (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "LANCON, Eric Christian", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "185bee9466a6e31abdadf8eb25f26dfb", "affiliation": "Institute of High Energy Physics,Chinese Academy of Sciences (CN)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WU, Wenjing", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "153a626fb8bd2f36a9f4c31a4686ddf5", "affiliation": "University of Pittsburgh (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BIANCHI, Riccardo Maria", "id": "5"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/contribution\/135", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": null, "keywords": [], "id": "134", "speakers": [{"_type": "ContributionParticipation", "emailHash": "f214dc73bc35dcabfbd391177ed6c74f", "affiliation": "Technische Universitaet Dresden (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "SOCHER, Felix", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "f214dc73bc35dcabfbd391177ed6c74f", "affiliation": "Technische Universitaet Dresden (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "SOCHER, Felix", "id": "0"}], "title": "Physics Validation: Challenges of the ATLAS Monte Carlo Production during Run-I and Beyond (LS1 and Run-2)", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "In Run 1 physics validation in ATLAS involved a lot of person power due to a historically grown code base. Each combined performance group had its own approach of producing plots, relied on different input formats\u00a0 and virtually no shared know-how existed.\r\nWith the advent of Run 2 and the introduction of ATLAS\u2019 new unified analysis data format (xAOD) the structure of the physics validation was revised and reshaped into a centrally executable framework to which the\u00a0 individual groups contribute analysis modules. The result is a faster, automated validation process utilising less person power and computing resources e.g. the validation code is now run by 2 people instead of one person per physics group.\u00a0\r\nFurthermore a common code structure was introduced easing both the development and maintenance process as well as enabling a knowledge transfer between the groups.\r\nIn the presentation we will revisit the validation code of Run 1 to point out issues that were encountered and will show how they we addressed them.", "track": "Track2: Offline software ", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "1084e50129f4b6430184b04fe9c9a12f", "affiliation": "Centre National de la Recherche Scientifique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "GENEST, Marie-Helene", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "24bab4e472df0aff0bb4206190c78f00", "affiliation": "University of Liverpool (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GWILLIAM, Carl Bryan", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/contribution\/134", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "494", "speakers": [{"_type": "ContributionParticipation", "emailHash": "c3a9d60000380ace3b532f9f854d7620", "affiliation": "INFN-Bari", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. DONVITO, Giacinto", "id": "4"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "3f09d2d07f5da6ad4292b3f8a0733054", "affiliation": "INFN-Bari", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. NICOTRI, Stefano", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "9c272f6cc4aef6d2b50716da9d576989", "affiliation": "INFN-Bari", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. VALENTINI, Roberto", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "9132dbe50f356cbd962495fe17c05814", "affiliation": "UNIBA", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. VENTOLA, Fabrizio", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "7ab42b13a80670e9a18765bfd069a576", "affiliation": "UNIBA", "_fossil": "contributionParticipationMetadata", "fullName": "Mrs. TINELLI, Eufemia", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "c3a9d60000380ace3b532f9f854d7620", "affiliation": "INFN-Bari", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. DONVITO, Giacinto", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "8e0b4b70e0c63fe9bd8bfd953277a521", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "SPINOSO, Vincenzo", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "193fcfd0e0a5b4019b7ee6ac34951820", "affiliation": "INFN-Bari", "_fossil": "contributionParticipationMetadata", "fullName": "Mrs. ANTONACCI, Marica", "id": "6"}], "title": "Storage solutions for a production-level cloud infrastructure", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "At INFN-Bari we have set-up an OpenStack-based cloud infrastructure in the framework of a publicly funded project, PRISMA, aimed at implementing a fully integrated PaaS+IaaS platform to provide services in the field of public administration and scientific data analysis. The IaaS testbed currently consists of 25 compute nodes providing in total almost 600 physical cores, 3 TB of RAM, 400 TB of storage (disks). Connectivity is ensured through 10Gbit\/s ethernet connection. Both the backend (MySQL database and RabbitMq message broker) and the core services (nova, keystone, glance, neutron, etc.) have been configured in high-availability using HA clustering techniques. The full capacity available in the early 2015 will provide 2000 cores and 8 TB of RAM.\r\nIn this work we present the storage solutions that we are currently using as backend for our production cloud services.\r\n\r\nStorage-as-service is implemented in Openstack by the Block Storage project, Cinder, and the Object Storage project, Swift. Selecting the right software to manage the underlying backend storage for these services is very important, and decisions can depend on many factors, not only merely technical, but also economic: in most cases they result from a trade-off between performance and costs.\r\n\r\nMany operators use separate compute and storage hosts. We have decided not to follow this mainstream trend, aiming at the best cost-performance scenario: for us, it makes sense to run compute and storage on the same machines, since we want to be able to dedicate as many of our hosts as possible to running instances. Therefore, each compute node is configured with a significant amount of disk space and a distributed file system (GlusterFS and\/or Ceph) ties the disks from each compute node into a single file-system. \r\nIn this case, the reliability and stability of the shared file-system is critical and defines the effort to maintain the compute hosts: tests have been performed to asses the stability of the shared file-systems changing the replica factor. For example, we have tested performance and reliability of GlusterFS in both replica 2 or 3. \r\n\r\nWe currently use CEPH distributed file system for:\r\n- storage of the running VMs enabling the live migration,\r\n- storage of the virtual images (as primary Glance image store), \r\n- implementation of several of the Cinder backends for block devices. \r\nWe have decided to enable Ceph as additional backend of Cinder in order to compare features, reliability and performances of the two solutions.\r\nOur interest in Ceph derives also from the possibility of consolidating the infrastructure overall backend storage into a unified solution. To this aim, we are currently testing Ceph to run Virtual Machines, both using RBD and Ceph-FS protocols, and to implement object storage.           \r\n\r\nIn order to test scalability and performance of the deployed system we used test cases which are derived from the typical pattern of storage utilization.\r\n\r\nThe testing tools are standard software widely used for this purpose such as: iozone and\/or dd for block storage and specific benchmarking tools like Cosbench, swift-bench and ssbench for object storage. Using different tools for testing the file-system and comparing their results with the observation of the real test case, is also a good possibility for verifying the reliability of the benchmarking tools.\r\n\r\nThroughput tests have been planned and conducted on the two system configurations in order to understand the performance of both storage solutions and their impacts on applications, aiming at achieving the better SLA and end-users experience.\r\n\r\nImplementing our cloud platform, we have also focused on providing transparent access to data using standardized protocols (both de-iure and de-facto standards). In particular, Amazon-compliant S3 and CDMI (Cloud Data Management Interface) interfaces have been installed on top of the Swift Object Storage, in order to promote interoperability at PaaS\/SaaS levels also.  \r\n\r\nData is important for businesses of all sizes. Therefore, one of the most common user requirement is the possibility to backup data, in order to minimize their loss, stay compliant, and preserve data integrity. Implementing this feature is particularly challenging when the users come from the scientific communities that produce huge quantities of heterogeneous data and\/or can have strict constraints.\r\nAn interesting feature of the Swift Object Storage is the geographic replica that can be used in order to add a disaster-recovery feature to the set of data and services exposed by our infrastructure.\r\nAlso Ceph provides a similar feature: the geo-replication through RADOS gateway.     \r\nTherefore, we have installed and configured both a Swift global cluster and a Ceph federated cluster, distributed on three different geographic sites. Results of the performance tests conducted on both clusters are presented along with a description of the parameters tuning that has been performed for optimization. The different replication methods implemented in the two middlewares, Swift and Ceph, are compared in terms of network traffic bandwidth, cpu and memory consumption. \r\n\r\nAnother important aspect we are taking care of is the QoS (Quality of Service) support, i.e. the capability of providing different levels of storage service optimized wrt the user application profile. This can be achieved defining different tiers of storage and setting parameters like how many I\/Os the storage can handle, what limit it should have on latency, what availability levels it should offer and so on. \r\nOur final goal is also to set-up a (semi-)automated system that is able of self-optimising. Therefore we are exploring the cache tiering feature of Ceph, that handles the migration of data between the cache tier and the backing storage tier automatically. Results of these testing activities are shown in this presentation.\r\nIn this work we show results achieved in terms of performance and functionalities of INFN-Bari cloud infrastructure while supporting scientific use cases as LHC experiments.", "track": "Track7: Clouds and virtualization", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/494", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "495", "speakers": [{"_type": "ContributionParticipation", "emailHash": "fe09c3af24a882d0ac6416c0dc33b0b6", "affiliation": "INFN-Bari", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. ITALIANO, Alessandro", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c3a9d60000380ace3b532f9f854d7620", "affiliation": "INFN-Bari", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. DONVITO, Giacinto", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "8e0b4b70e0c63fe9bd8bfd953277a521", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "SPINOSO, Vincenzo", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "fe09c3af24a882d0ac6416c0dc33b0b6", "affiliation": "INFN-Bari", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. ITALIANO, Alessandro", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "922c768408f383b34c5cf09203e2d25f", "affiliation": "INFN-Bari", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. DIACONO, Domenico", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "9c272f6cc4aef6d2b50716da9d576989", "affiliation": "INFN-Bari", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. VALENTINI, Roberto", "id": "4"}], "title": "A Grid and cloud computing farm exploiting HTCondor", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "This work presents the result of several tests that demonstrate the capabilities of HTCondor as batch system for a big computing farm serving both LHC use cases and others scientists. \r\nThe HTCondor testbed hosted at INFN-Bari is made of about 300 nodes and 15\u2019000 CPU slots, and meant to sustain about 50\u2019000 job in the queue. The computing farm is used both from Grid users of many VOs (HEP, Bioinformatics, Astrophysics, etc) and from local users, that come from complete different environments: meteorologists, HEP, bioinformatics, humanities, computational chemistry, remote sensing, statistics, etc. These very different use-cases express a wide range of requirements to the batch system. \r\nFor instance, the batch system must address both a huge number of very fast jobs and very large MPI based multi-CPU jobs. \r\nThe HTCondor configuration tuned in the test aims to allow several scheduling functionalities: Hierarchical FairShare, Quality of Service, priority on users, priority on group, limit on the number of jobs per user\/group\/queue, job age scheduling, job size scheduling, \u201cconsumable resources\u201d scheduling. Also it has been tested the ability to manage different job types: serial job, MPI job, multi-thread jobs, interactive jobs, whole node jobs. \r\nHTCondor has been tested as well on a HPC Cluster with GPU card and Low Latency Infiniband connection, connected to the standard HTC farm at the same time. \r\nAnother point of interest is the High Availability configuration for the master node. This possibility is indeed very useful in order to let the cluster working also in case of hardware failure of the server hosting the batch system.\r\nOther interesting features involved in our tests are: capabilities of using ACLs on resources in general; capabilities to deal with the pre-execution and post-execution script, capabilities of Condor_rooster to manage the start-up the worker node as soon as there are job waiting in the queue, and to shutdown them as soon as they are not needed anymore. \r\nA very new item is also instantiating computational resources in a cloud environment. We have developed a plug-in that is able to request the needed resources to our OpenStack local facility, choosing the right image and the flavour that fulfills the user requirements expressed with the ClassAD in the jobs. In this way there is the possibility to support the computational user requests exploiting cloud resources, and this will help the site admin to efficiently and dynamically share resources, among different use-cases (batch jobs, Cloud applications, etc), exploiting the flexibility of a Cloud Computing infrastructure.\r\nFinally, results will be shown in terms of performances achieved in job submission\/scheduled\/executed, configuration available and behaviour in terms of scheduling features, best practices on how to configure the batch master for High Availability. \r\nThe work will also provide the results in terms of stress tests for the configuration of the CREAM Computing Element when exploiting the HTCondor cluster. \r\nHence, this work summarizes the capabilities of HTCondor when supporting a big computing farm made up of diverse computational resources (Grid, Local HTC Cluster and Cloud Computing).", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/495", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:45:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "496", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5a6b0c8862953cf0796175777a912011", "affiliation": "University of Wisconsin-Madison", "_fossil": "contributionParticipationMetadata", "fullName": "SCHULTZ, David", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "5a6b0c8862953cf0796175777a912011", "affiliation": "University of Wisconsin-Madison", "_fossil": "contributionParticipationMetadata", "fullName": "SCHULTZ, David", "id": "0"}], "title": "IceProd2: A Next Generation Data Analysis Framework for the IceCube Neutrino Observatory", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T11:52:29.531476+00:00", "description": "", "title": "CHEP2015_IceProd2.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/496\/attachments\/578563\/796709\/CHEP2015_IceProd2.pdf", "filename": "CHEP2015_IceProd2.pdf", "content_type": "application\/pdf", "type": "file", "id": 796709, "size": 972219}], "title": "Slides", "default_folder": false, "id": 578563, "description": ""}], "_type": "Contribution", "description": "We describe the overall structure and new features of the second generation of IceProd, a data processing and management framework. IceProd was developed by the IceCube Neutrino Observatory for processing of Monte Carlo simulations and detector data, and has been a key component of the IceCube offline computing infrastructure since it was first deployed in 2006. It runs fully in user space as a separate layer on top of grid and batch systems. This is accomplished by a set of python daemons which process job workflow and maintain configuration and status information on every job before, during, and after processing. IceProd can also manage complex workflow DAGs across distributed computing grids in order to optimize usage of special resources such as GPUs. \r\n\r\nAs the second major version of IceProd, substantial improvements have been made to increase security, reliability, scalability, and ease of use. One new goal is to extend usage beyond centralized production to individual users and to facilitate large-scale individual analysis. Currently only a handful of IceCube users have access to more than one computing cluster; this version of IceProd should enable over 200 active IceCube data analyzers to transparently access distributed CPU and GPU resources. The scope of IceProd 2 also extends beyond IceCube-specific applications and can be used as a general grid computing tool.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578563", "resources": [{"_type": "LocalFile", "name": "CHEP2015_IceProd2.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/496\/attachments\/578563\/796709\/CHEP2015_IceProd2.pdf", "fileName": "CHEP2015_IceProd2.pdf", "_fossil": "localFileMetadata", "id": "796709", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/496", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:45:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "138", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a2dbd3d2df26e3fbc0360f6b3594fd21", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CAMPANA, Simone", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "a0dd1a3a7589f1e2a04cc967f2ae6c39", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DI GIROLAMO, Alessandro", "id": "0"}], "title": "ATLAS Distributed Computing in LHC Run2", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-07T19:22:21.362204+00:00", "description": "", "title": "CHEP2015ATLASComputing.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/138\/attachments\/578564\/796710\/CHEP2015ATLASComputing.pdf", "filename": "CHEP2015ATLASComputing.pdf", "content_type": "application\/pdf", "type": "file", "id": 796710, "size": 2419667}, {"_type": "attachment", "modified_dt": "2015-04-07T19:22:21.362204+00:00", "description": "", "title": "CHEP2015ATLASComputing.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/138\/attachments\/578564\/796711\/CHEP2015ATLASComputing.pptx", "filename": "CHEP2015ATLASComputing.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796711, "size": 9043898}], "title": "Slides", "default_folder": false, "id": 578564, "description": ""}], "_type": "Contribution", "description": "The ATLAS Distributed Computing infrastructure has evolved after the first period of LHC data taking in order to cope with the challenges of the upcoming LHC Run2. An increased data rate and computing demands of the Monte-Carlo simulation, as well as new approaches to ATLAS analysis, dictated a more dynamic workload management system (ProdSys2) and data management system (Rucio), overcoming the boundaries imposed by the design of the old computing model. In particular, the commissioning of new central computing system components was the core part of the migration toward the flexible computing model. \r\nThe flexible computing utilization exploring the opportunistic resources such as HPC, cloud, and volunteer computing is embedded in the new computing model, the data access mechanisms have been enhanced with the remote access, and the network topology and performance is deeply integrated into the core of the system. Moreover a new data management strategy, based on defined lifetime for each dataset, has been defined to better manage the lifecycle of the data. \r\nIn this note, the overview of the operational experience of the new system and its evolution is presented.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578564", "resources": [{"_type": "LocalFile", "name": "CHEP2015ATLASComputing.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/138\/attachments\/578564\/796710\/CHEP2015ATLASComputing.pdf", "fileName": "CHEP2015ATLASComputing.pdf", "_fossil": "localFileMetadata", "id": "796710", "_deprecated": true}, {"_type": "LocalFile", "name": "CHEP2015ATLASComputing.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/138\/attachments\/578564\/796711\/CHEP2015ATLASComputing.pptx", "fileName": "CHEP2015ATLASComputing.pptx", "_fossil": "localFileMetadata", "id": "796711", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/138", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:00:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "490", "speakers": [{"_type": "ContributionParticipation", "emailHash": "4535f96fcde7dd5d7c121710acfa24cd", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "GIFFELS, Manuel", "id": "5"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "d226af6c5b8949bfcd9de2e05d482260", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "KUHN, Eileen", "id": "0"}], "title": "Active job monitoring in pilots", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T08:48:33.460037+00:00", "description": "", "title": "2015_chep-pilot_monitoring.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/490\/attachments\/578565\/796712\/2015_chep-pilot_monitoring.pdf", "filename": "2015_chep-pilot_monitoring.pdf", "content_type": "application\/pdf", "type": "file", "id": 796712, "size": 1955311}], "title": "Slides", "default_folder": false, "id": 578565, "description": ""}], "_type": "Contribution", "description": "Recent developments in high energy physics (HEP) including multi-core jobs and multi-core pilots require  data centres to gain a deep understanding of the system to correctly design and upgrade computing clusters. Especially networking is a critical component as the increased usage of data federations relies on WAN connectivity and availability as a fallback to access data. The specific demands of different experiments and communities, but also the need for identification of misbehaving batch jobs requires an active monitoring.\r\n\r\nExisting monitoring tools are not capable of measuring fine-grained information at batch job level. This complicates network-specific scheduling and optimisations. In addition, pilots add another layer of abstraction. They behave like batch systems themselves by managing and executing payloads of jobs internally. As the original batch system has no access to internal information about the scheduling process inside the pilots, there is an unpredictable number of jobs being executed. Therefore, the comparability of jobs and pilots cannot be ensured to predict runtime behaviour or network performance. Hence, the identification of the actual payload is of interest.\r\n\r\nAt the GridKa Tier 1 centre a specific monitoring tool is in use, that allows the monitoring of network traffic information at batch job level. A first analysis using machine learning algorithms showed the relevance of the measured data, but indicated a possible improvement by subdividing pilots into separate jobs.\r\n\r\nThis contribution will present the current monitoring approach and will discuss recent efforts and importance to identify pilots and their substructures inside the batch system. It will also show how to determine monitoring data of specific jobs from identified pilots. Finally, the approach is evaluated and adapted to the former analysis and the results are presented.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578565", "resources": [{"_type": "LocalFile", "name": "2015_chep-pilot_monitoring.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/490\/attachments\/578565\/796712\/2015_chep-pilot_monitoring.pdf", "fileName": "2015_chep-pilot_monitoring.pdf", "_fossil": "localFileMetadata", "id": "796712", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "b38f11fda25741459bd44e135fcb3cbd", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "FISCHER, Max", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "4535f96fcde7dd5d7c121710acfa24cd", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "GIFFELS, Manuel", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "18f12d579cee5d8345191b00dae91d02", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "JUNG, Christopher", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "aaedaf0b6923da9472cbc33726464d1d", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "PETZOLD, Andreas", "id": "4"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/490", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:30:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "491", "speakers": [{"_type": "ContributionParticipation", "emailHash": "cbb0d28d39b1ca6e5d447a3e19e0e3c7", "affiliation": "Centro de Investigaciones Energ. Medioambientales y Tecn. - (ES", "_fossil": "contributionParticipationMetadata", "fullName": "FLIX MOLINA, Jose", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "cbb0d28d39b1ca6e5d447a3e19e0e3c7", "affiliation": "Centro de Investigaciones Energ. Medioambientales y Tecn. - (ES", "_fossil": "contributionParticipationMetadata", "fullName": "FLIX MOLINA, Jose", "id": "0"}], "title": "Getting prepared for the LHC Run2: the PIC Tier-1 case", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T03:25:56.535502+00:00", "description": "", "title": "20150413_PIC_Tier1.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/491\/attachments\/578566\/796713\/20150413_PIC_Tier1.pdf", "filename": "20150413_PIC_Tier1.pdf", "content_type": "application\/pdf", "type": "file", "id": 796713, "size": 3791479}], "title": "Slides", "default_folder": false, "id": 578566, "description": ""}], "_type": "Contribution", "description": "The LHC experiments will collect unprecedented data volumes in the next Physics run, with high pile-up collisions resulting in events which require a more complex processing. The collaborations have been asked to update their Computing Models to optimize the use of the available resources in order to cope with the Run2 conditions, in the midst of widespread funding restrictions. The changes in computing for Run2 represent significant efforts for the collaboration, as well as significant repercussions on how the WLCG sites are built and operated.\r\n\r\nThis contribution focuses on how these changes have been implemented and integrated in the Spanish WLCG Tier-1 center at Port d'Informaci\u00f3 Cient\u00edfica (PIC), which serves ATLAS, CMS and LHCb experiments.\u00a0The approach to adapt a multi-VO site to the new requirements while maintaining top reliability levels for all the experiments, is described. The main activities covered in this contribution include setting up\u00a0dCache disk-only pools together with access via\u00a0HTTP\/XrootD protocols to expose the\u00a0most demanded data;\u00a0enabling end user analysis at the center;\u00a0efficient integration of multi-core job handling in the site batch system and scheduler by means of the dynamic allocation of computing nodes; implementation of dynamic high memory queues; simplification, automation and virtualization of services deployment; and setting up a dCache test environment to assess the storage management readiness against experiment workflows.\u00a0In addition, innovative\u00a0free-cooling techniques along with a modulation of computing power versus electricity costs have been implemented, achieving a significant reduction of the\u00a0electricity costs of our infrastructure.\r\n\r\nThe work has been done to reduce the operational and maintenance costs of the Spanish Tier-1 center, in agreement with the expectations from WLCG.\u00a0With the state of the optimizations currently being implemented and the work foreseen during the coming years to further improve the effectiveness of the use of the provided resources, it is expected that the resources deployed in WLCG will approximately double by the end of Run2. All of the implementations done in PIC are flexible enough\u00a0to\u00a0rapidly evolve following changing technologies.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578566", "resources": [{"_type": "LocalFile", "name": "20150413_PIC_Tier1.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/491\/attachments\/578566\/796713\/20150413_PIC_Tier1.pdf", "fileName": "20150413_PIC_Tier1.pdf", "_fossil": "localFileMetadata", "id": "796713", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "3104caab90081a7fa54ff8c20595e0c0", "affiliation": "Centro de Investigaciones Energ. Medioambientales y Tecn. - (ES", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PEREZ-CALERO YZQUIERDO, Antonio", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "9464258a30d75b7a914cb05161818f35", "affiliation": "UAB\/PIC", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. CRUZ, Ricard", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "073b7fe5499f0ba3d6f1de2429069b2d", "affiliation": "Universitat Aut\u00f2noma de Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "LOPEZ MUNOZ, Fernando", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "28fec050a7a0cddcbe83319b039e0c92", "affiliation": "Institut de F\u00edsica d'Altes Energies - Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "PACHECO PAGES, Andreu", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "d80b5bee0f3cad9589417a5f2ef55906", "affiliation": "PIC", "_fossil": "contributionParticipationMetadata", "fullName": "PLANAS, Elena", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "0125081a50f3d65e03179ae90eb6a0a1", "affiliation": "UAB\/PIC", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. RODRIGUEZ, Bruno", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "da989212b229b15a6b2a1c2e9bf0bb9b", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "PORTO FERNANDEZ, Maria Del Carmen", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "b73ba1837891fe0a035552c7e6e13879", "affiliation": "Universitat Aut\u00f2noma de Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "SEDOV, Alexey", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "6f6a0a09bedfe285f5c3b56598a01274", "affiliation": "Universitat Aut\u00f2noma de Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. DELFINO REZNICEK, Manuel", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "5ba38f710155583a36a09ca9cc438c57", "affiliation": "Universitat Aut\u00f2noma de Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "ACCION GARCIA, Esther", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "b9484a9341914ec69fc4a3f599f16826", "affiliation": "Universitat Aut\u00f2noma de Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "ACIN PORTELLA, Vanessa", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "3c540104c238880e46ed73772d460797", "affiliation": "Universitat Aut\u00f2noma de Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "ACOSTA SILVA, Carlos", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "36f561ddfe694916ca25fb8bfd10bc89", "affiliation": "U", "_fossil": "contributionParticipationMetadata", "fullName": "CASALS HERNANDEZ, Jordi", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "62f4d5026d86084a3beae041f51b1728", "affiliation": "Universitat Aut\u00f2noma de Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "CAUBET SERRABOU, Marc", "id": "14"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/491", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "492", "speakers": [{"_type": "ContributionParticipation", "emailHash": "7cd5389c52622b199983b4d669acf6ac", "affiliation": "IHEP(Institute of High Energy Physics, CAS,China)", "_fossil": "contributionParticipationMetadata", "fullName": "CUI, Tao", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "6ee3ac44f5bc2a44a90d7c47d76bd92a", "affiliation": "IHEP", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CHENG, Yaodong", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "7cd5389c52622b199983b4d669acf6ac", "affiliation": "IHEP(Institute of High Energy Physics, CAS,China)", "_fossil": "contributionParticipationMetadata", "fullName": "CUI, Tao", "id": "0"}], "title": "Network technology research on IhepCloud platform", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "Traditionally, physical computer is used to run high-performance computing jobs. There are many problems such as job interference with each other, operation system crash because of abnormal operation and low computing resource utilization. IhepCloud expects to solve the job isolation, operating system fault isolation and to improve resource utilization by computing resource virtualization. IhepCloud builds virtual infrastructure platform and provides virtual machine to high-performance computing system.\r\n    In this report, we introduce our network architecture of high-performance computing system which contained virtual computing resource. We discuss dynamic synchronization between Openstack virtual environment and physic environment such as virtual machine DNS register, virtual machine network monitor, synchronization between virtual machine and some application systems. A service is deployed to collect virtual machine\u2019s information such as virtual machine name, IP, MAC and so on. The information will be stored to a network database and be synchronized to each service. These help to transparently integrate virtual system into physical computing environment.\r\n    We describe a virtual network topology based on Openstack neutron component in IhepCloud. In this topology, OVS plugin and vlan mode is deployed to achieve L2 interconnection between virtual network and physical network. Each computing node connects to physical switch through trunk links and virtual machine directly access physic network through 802.1Q supported by OVS plugin. Openstack neutron component is only responsible for distributing addresses and supporting 802.1Q. L3 is configured in physical switch. This topology avoids the performance insufficient of pure virtual network and achieves an easy interconnection with physical network. Thus, virtual system integration will not result in large changes in computing environment.\r\n    Finally we report operation and maintenance experience in our test-bed. Performance testing about glance and ceph will be given. Testing report shows image copy to each computing node is inefficient and nova with ceph is not ready. We also introduce and analyze vxlan performance testing with physical switch.", "track": "Track7: Clouds and virtualization", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "6ee3ac44f5bc2a44a90d7c47d76bd92a", "affiliation": "IHEP", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CHENG, Yaodong", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/492", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:30:00"}, "duration": 15, "session": "Track 8 Session", "keywords": [], "id": "493", "speakers": [{"_type": "ContributionParticipation", "emailHash": "7a6c8f49981d093b41b365a4b522378d", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. EULISSE, Giulio", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "5ed09727f43058232b24b8ad13f529a8", "affiliation": "Vilnius University (LT)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. ABDURACHMANOV, David", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "9a8ce9aebaabdde7956acf3f73f3b7a5", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. ELMER, Peter", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "b63b642f0d515365e8d562be36ae7a4f", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. KNIGHT, Robert", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "7a6c8f49981d093b41b365a4b522378d", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. EULISSE, Giulio", "id": "3"}], "title": "Future Computing Platforms for Science in a Power Constrained Era", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T14:50:48.236506+00:00", "description": "", "title": "slides.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/493\/attachments\/578567\/796714\/slides.pdf", "filename": "slides.pdf", "content_type": "application\/pdf", "type": "file", "id": 796714, "size": 2036284}], "title": "Slides", "default_folder": false, "id": 578567, "description": ""}], "_type": "Contribution", "description": "Power consumption will be a key constraint on the future growth of Distributed High Throughput Computing (DHTC) as used by High Energy Physics (HEP). This makes performance-per-watt a crucial metric for selecting cost-efficient computing solutions. For this paper, we have done a wide survey of current and emerging architectures becoming available on the market including x86-64 variants, ARMv7 32-bit, ARMv8 64-bit, Many-Core and GPU solutions, as well as newer System-on-Chip (SoC) solutions. We compare performance and energy efficiency using an evolving set of standardized HEP-related benchmarks and power measurement techniques we have been developing. We evaluate the potential for use of such computing solutions in the context of DHTC systems, such as the Worldwide LHC Computing Grid (WLCG).", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578567", "resources": [{"_type": "LocalFile", "name": "slides.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/493\/attachments\/578567\/796714\/slides.pdf", "fileName": "slides.pdf", "_fossil": "localFileMetadata", "id": "796714", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/493", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:30:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "24", "speakers": [{"_type": "ContributionParticipation", "emailHash": "d43c2381209b8e95c7be356ece5900b7", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DOST, Jeffrey Michael", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "d43c2381209b8e95c7be356ece5900b7", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DOST, Jeffrey Michael", "id": "0"}], "title": "Operational Experience Running Hadoop XRootD Fallback", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T08:37:58.000498+00:00", "description": "", "title": "hdfs_xrdfb_chep2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/24\/attachments\/578568\/796715\/hdfs_xrdfb_chep2015.pdf", "filename": "hdfs_xrdfb_chep2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796715, "size": 160519}], "title": "Slides", "default_folder": false, "id": 578568, "description": ""}], "_type": "Contribution", "description": "In April of 2014, the UCSD T2 Center deployed hdfs-xrootd-fallback, a UCSD-developed software system that interfaces Hadoop with XRootD to increase reliability of the Hadoop file system.  The hdfs-xrootd-fallback system allows a site to depend less on local file replication and more on global replication provided by the XRootD federation to ensure data redundancy.  Deploying the software has allowed us to reduce Hadoop replication on a significant subset of files in our cluster, freeing hundreds of terabytes in our local storage, and to recover HDFS blocks lost due to storage degradation.  An overview of the architecture of the hdfs-xrootd-fallback system will be presented, as well as details of our experience operating the service over the past year.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578568", "resources": [{"_type": "LocalFile", "name": "hdfs_xrdfb_chep2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/24\/attachments\/578568\/796715\/hdfs_xrdfb_chep2015.pdf", "fileName": "hdfs_xrdfb_chep2015.pdf", "_fossil": "localFileMetadata", "id": "796715", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "a372c029d4b10764747b443d644b6c27", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MRAK TADEL, Alja", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "1905f721940f9ff8a9b0b9b1e9f7eef9", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "TADEL, Matevz", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "404ffd25f1af8b96d12533b8dcfc88dd", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "WUERTHWEIN, Frank", "id": "3"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/24", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:15:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:30:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "25", "speakers": [{"_type": "ContributionParticipation", "emailHash": "d6dbc7abbb232d3ac6b1fe4ee5b6f1bb", "affiliation": "Radboud University Nijmegen (NL)", "_fossil": "contributionParticipationMetadata", "fullName": "BESJES, Geert Jan", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "d6dbc7abbb232d3ac6b1fe4ee5b6f1bb", "affiliation": "Radboud University Nijmegen (NL)", "_fossil": "contributionParticipationMetadata", "fullName": "BESJES, Geert Jan", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "2cc9cb7c7530eac0ffa0f72d81e41aa5", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BAAK, Max", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "b3ea03807f2116b3064b3124fddcdb7f", "affiliation": "University of Texas at Arlington (US)", "_fossil": "contributionParticipationMetadata", "fullName": "COTE, David", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "b22d27a82c7fbb5faf9ce32902f2cf0e", "affiliation": "TRIUMF (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "KOUTSMAN, Aleksej", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "1f922b40d6befa6961d9ac68e8d45bc6", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "LORENZ, Jeanette Miriam", "id": "4"}], "title": "HistFitter: a flexible framework for statistical data analysis", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T16:00:53.427154+00:00", "description": "", "title": "150416.HistFitter.CHEP.gbesjes.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/25\/attachments\/578569\/796716\/150416.HistFitter.CHEP.gbesjes.pdf", "filename": "150416.HistFitter.CHEP.gbesjes.pdf", "content_type": "application\/pdf", "type": "file", "id": 796716, "size": 1125629}], "title": "Slides", "default_folder": false, "id": 578569, "description": ""}], "_type": "Contribution", "description": "We present a software framework for statistical data analysis, called *HistFitter*, that has been used extensively in the ATLAS Collaboration to analyze data of proton-proton collisions produced by the Large Hadron Collider at CERN. Most notably, HistFitter has become a de-facto standard in searches for supersymmetric particles since 2012, with some usage for Exotic and Higgs boson physics. HistFitter coherently combines several statistics tools in a programmable and flexible framework that is capable of bookkeeping hundreds of data models under study using thousands of generated input histograms. The key innovations of HistFitter are to weave the concepts of control, validation and signal regions into its very fabric, and to treat them with rigorous methods, while providing multiple tools to visualize and interpret the results through a simple configuration interface, as will become clear throughout this presentation.\r\n\r\nHistFitter interfaces with the statistics tools \r\nHistFactory and RooStats to construct parametric models and to perform statistical \r\ntests of the data, and extends these tools in four key areas:\r\n\r\n1. Programmable framework: HistFitter puts tools from several sources together in a coherent and programmable\r\nframework, capable of performing a complete statistical analysis of pre-formatted input data samples.\r\n\r\n2. Bookkeeping: HistFitter can perform statistical tests and scan over parameter values of hundreds of \r\nsignal hypotheses in an organized way from a single user-defined configuration file.\r\n\r\n3. Analysis strategy: HistFitter uses built-in the concepts of control, signal and validation regions which are used\r\nto constrain, extrapolate and validate data-model predictions across analysis regions.\r\nHistFitter also introduces a rigorous treatment of validation regions that is new in high-energy physics.\r\n\r\n4. Presentation and interpretation: The HistFitter framework keeps track of data models before and after \r\nfits to the data, and includes a collection of methods to determine the statistical \r\nsignificance of all tested hypotheses and to produce tables and plots expressing these \r\nresults with publication-quality style.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578569", "resources": [{"_type": "LocalFile", "name": "150416.HistFitter.CHEP.gbesjes.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/25\/attachments\/578569\/796716\/150416.HistFitter.CHEP.gbesjes.pdf", "fileName": "150416.HistFitter.CHEP.gbesjes.pdf", "_fossil": "localFileMetadata", "id": "796716", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/25", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "26", "speakers": [{"_type": "ContributionParticipation", "emailHash": "f14203dee7593a809ed518bad2f3be9e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "JONES, Ben", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "f14203dee7593a809ed518bad2f3be9e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "JONES, Ben", "id": "0"}], "title": "Scaling Agile Infrastructure to people", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T12:55:15.247752+00:00", "description": "", "title": "chepagiledevelopment.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/26\/attachments\/578570\/796717\/chepagiledevelopment.pdf", "filename": "chepagiledevelopment.pdf", "content_type": "application\/pdf", "type": "file", "id": 796717, "size": 3051944}], "title": "Poster", "default_folder": false, "id": 578570, "description": ""}], "_type": "Contribution", "description": "When CERN migrated its infrastructure away from home-grown fabric management tools to emerging industry-standard open-source solutions, the immediate technical challenges and motivation were clear. The move to a multi-site Cloud Computing model meant that the toolchains that were growing around this ecosystem would be a good choice, the challenge was to leverage them. \r\n\r\nThe use of open-source tools brings challenges other than merely how to deploy them. Home-grown software, for all the deficiencies identified at the outset of the project, has the benefit of growing with the organization. This presentation will examine what challenges there were in adapting open-source tools to the needs of the organization, particularly in the areas of multi-group development and security. \r\n\r\nAdditionally, the increase in scale of the plant required changes to how Change Management was organized and managed. Continuous Integration techniques are used in order to manage the rate of change across multiple groups, and the tools and workflow for this will be examined.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578570", "resources": [{"_type": "LocalFile", "name": "chepagiledevelopment.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/26\/attachments\/578570\/796717\/chepagiledevelopment.pdf", "fileName": "chepagiledevelopment.pdf", "_fossil": "localFileMetadata", "id": "796717", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "eeab1a19a11284f01f67cebc565d64e3", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MCCANCE, Gavin", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "3b42a05d54fd89b4b0d33dc2694fd538", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "TRAYLEN, Steve", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "39b08f60f040778b8eada4af381b9824", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BARRIENTOS ARIAS, Nacho", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/26", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "27", "speakers": [{"_type": "ContributionParticipation", "emailHash": "c529a955e937a762b4f9e978e65fafee", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SANTOGIDIS, Aram", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c529a955e937a762b4f9e978e65fafee", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SANTOGIDIS, Aram", "id": "0"}], "title": "Optimizing the transport layer of the ALFA framework for the Intel Xeon Phi co-processor", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T18:38:08.345065+00:00", "description": "", "title": "ALFA_Xeon_Phi_CHEP2015_poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/27\/attachments\/578572\/796720\/ALFA_Xeon_Phi_CHEP2015_poster.pdf", "filename": "ALFA_Xeon_Phi_CHEP2015_poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796720, "size": 2039022}], "title": "Poster", "default_folder": false, "id": 578572, "description": ""}, {"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-17T02:16:48.063799+00:00", "description": "", "title": "Track8_Aram_Santogidis.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/27\/attachments\/578571\/796718\/Track8_Aram_Santogidis.pdf", "filename": "Track8_Aram_Santogidis.pdf", "content_type": "application\/pdf", "type": "file", "id": 796718, "size": 1232302}, {"_type": "attachment", "modified_dt": "2015-04-17T02:16:48.063799+00:00", "description": "", "title": "Track8_Aram_Santogidis.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/27\/attachments\/578571\/796719\/Track8_Aram_Santogidis.pptx", "filename": "Track8_Aram_Santogidis.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796719, "size": 1292913}], "title": "Slides", "default_folder": false, "id": 578571, "description": ""}], "_type": "Contribution", "description": "ALFA is the common framework of the next generation software for ALICE and FAIR high energy physics experiments. It supports both offline and online processing which includes ALICE DAQ\/HLT\/Offline and the FairRoot project. The framework is designed based on a data-flow model with message-oriented middleware (MOM) serving as a transport layer. By using multiple data-flows concurrently it facilitates parallel processing which maps naturally to emerging multi-core and many-core computing architectures.\r\n\r\nWith the introduction of the Intel Xeon Phi co-processor in the industry it is interesting to investigate whether it can be used by ALFA to increase the processing efficiency. The co-processor can be used in three main computing modes. These are offload in which portions of code are accelerated on the device, native where the full program is executed on the device and symmetric where complete tasks are executed on the device and the host processor. For acceleration via offloading there are many competitive platforms such as GPUs and FPGAs. Although Xeon Phi can be used as an accelerator, it is particularly interesting to investigate the possibility of utilizing the co-processor in the *symmetric mode* of operation. Since it is x86_64 compatible it is possible to port complete *task processes* to the device and take advantage of the manycore architecture. It is also worth noting that the next generation of the Xeon Phi codenamed Knights Landing (KNL) will be manufactured in a socket variant as well. The research of using the co-processor as an independent node as opposed to just an offload accelerator can serve as a preliminary study for the future KNL servers.\r\n\r\nThe software components ported to the device will be connected with the rest of the system via the transport layer therefore there is strong motivation to optimize it for Xeon Phi. The metrics against which it is optimized are **throughput**, **latency** and **energy** consumption, with throughput being the primary target. The two core MOM  technologies of choice for ALFA are ZeroMQ and NanoMSG. The out-of-the box versions of these libraries use primarily TCP as the transport protocol which is known to provide limited performance on Xeon Phi in terms of data transfer throughput. In this effort these libraries are extended with support for SCIF, the Xeon Phi native transport protocol over PCIe and additionally with the Co-processor Communication Link (CCL), an RDMA technology used for efficient internode communication. By introducing these extensions we will demonstrate improvements in the data transfer performance by collecting performance monitoring results both in isolation with micro-benchmarks and integrated in the ALFA framework with the respective transport layer benchmark. A successful completion of the optimizations of the transport layer will improve the performance of NanoMSG and ZeroMQ on Xeon Phi. This achievement will potentially make this architecture a viable choice for certain use cases for ALFA which will further enrich its heterogeneous computing capabilities.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578572", "resources": [{"_type": "LocalFile", "name": "ALFA_Xeon_Phi_CHEP2015_poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/27\/attachments\/578572\/796720\/ALFA_Xeon_Phi_CHEP2015_poster.pdf", "fileName": "ALFA_Xeon_Phi_CHEP2015_poster.pdf", "_fossil": "localFileMetadata", "id": "796720", "_deprecated": true}], "_deprecated": true}, {"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578571", "resources": [{"_type": "LocalFile", "name": "Track8_Aram_Santogidis.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/27\/attachments\/578571\/796718\/Track8_Aram_Santogidis.pdf", "fileName": "Track8_Aram_Santogidis.pdf", "_fossil": "localFileMetadata", "id": "796718", "_deprecated": true}, {"_type": "LocalFile", "name": "Track8_Aram_Santogidis.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/27\/attachments\/578571\/796719\/Track8_Aram_Santogidis.pptx", "fileName": "Track8_Aram_Santogidis.pptx", "_fossil": "localFileMetadata", "id": "796719", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/27", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "20", "speakers": [{"_type": "ContributionParticipation", "emailHash": "b120fdf9f04a123ea43e83d2718cd2ef", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SKIPSEY, Samuel Cadellin", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "b120fdf9f04a123ea43e83d2718cd2ef", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SKIPSEY, Samuel Cadellin", "id": "0"}], "title": "Extending DIRAC File Management with Erasure-Coding for efficient storage.", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-03-31T14:04:25.957754+00:00", "description": "", "title": "ecposter.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/20\/attachments\/578573\/796721\/ecposter.pdf", "filename": "ecposter.pdf", "content_type": "application\/pdf", "type": "file", "id": 796721, "size": 1158567}], "title": "Poster", "default_folder": false, "id": 578573, "description": ""}], "_type": "Contribution", "description": "The state of the art in Grid style data management is to achieve increased resilience of data via multiple complete replicas of data files across multiple storage endpoints. While this is effective, it is not the most space-efficient approach to resilience, especially when the reliability of individual storage endpoints is sufficiently high that only a few will be inactive at any point in time.\r\n\r\nWe extended the Dirac File Catalogue and file management interface to allow the placement of *erasure-coded* files: each file distributed as N identically-sized chunks of data striped across a vector of storage endpoints, encoded such that any M chunks can be lost and the original file can be reconstructed.\r\n\r\nThe tools developed are transparent to the user, and, as well as allowing up and downloading of data to Grid storage, also provide the possibility of parallelizing access across all of the distributed chunks at once, improving data transfer and IO performance.\r\nWe expect this approach to be of most interest to smaller VOs, who have tighter bounds on the storage available to them, but larger VOs may be interested as their total data increases during Run 2.\r\n\r\nWith this in mind, we tested the applicability of our tools to the NA62 analysis model, which already expects data to be distributed across multiple Tier-2 sites.\r\n\r\nWe provide an analysis of the costs and benefits of the approach, along with future development and implementation plans.", "track": "Track3: Data store and access ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578573", "resources": [{"_type": "LocalFile", "name": "ecposter.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/20\/attachments\/578573\/796721\/ecposter.pdf", "fileName": "ecposter.pdf", "_fossil": "localFileMetadata", "id": "796721", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "9036c001235c207315f2f1cb8f8524b7", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. BRITTON, David", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "02d2ed292c897fe19a280591097bfb08", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "PROTOPOPESCU, Dan", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "a11b77fc54a5a3fd19cf040a3c41e953", "affiliation": "Imperial College London", "_fossil": "contributionParticipationMetadata", "fullName": "MARTYNIAK, Janusz", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "7c85d37398152148d60e922d4eda76d7", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "CROOKS, David", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "da6806efb2f919f20a8d4d30e47c621c", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "ROY, Gareth Douglas", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "68bf530d7c8e2395ba20526113245e37", "affiliation": "University of Glasgow", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. TODEV, Paulin", "id": "6"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/20", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:30:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "21", "speakers": [{"_type": "ContributionParticipation", "emailHash": "30329c2270cc5b215b78897ce0cc1b8f", "affiliation": "U", "_fossil": "contributionParticipationMetadata", "fullName": "GOHN, Wesley", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "30329c2270cc5b215b78897ce0cc1b8f", "affiliation": "U", "_fossil": "contributionParticipationMetadata", "fullName": "GOHN, Wesley", "id": "0"}], "title": "Data Acquisition for the New Muon $g$-$2$ Experiment at Fermilab", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T20:37:14.707696+00:00", "description": "", "title": "gohn_chep2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/21\/attachments\/578574\/796722\/gohn_chep2015.pdf", "filename": "gohn_chep2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796722, "size": 4213652}], "title": "Slides", "default_folder": false, "id": 578574, "description": ""}], "_type": "Contribution", "description": "A new measurement of the anomalous magnetic moment of the muon, $a_{\\mu} \\equiv (g-2)\/2$, will be performed at the Fermi National Accelerator Laboratory. The most recent measurement, performed at Brookhaven National Laboratory and completed in 2001, shows a 3.3-3.6 standard deviation discrepancy with the standard model value of $g$-$2$. The new measurement will accumulate 20 times those statistics, measuring $g$-$2$ to 0.14 ppm and improving the uncertainty by a factor of 4 over that of the previous measurement. \r\n\r\nThe data acquisition system for this experiment must have the ability to create deadtime-free records from 700 $\\mu$s muon spills at a raw data rate 18 GB per second. Data will be collected using 1296 channels of $\\mu$TCA-based 800 MSPS, 12 bit waveform digitizers and processed in a layered array of networked commodity processors with 24 GPUs working in parallel to perform a fast recording of the muon decays during the spill. The system will be controlled using the MIDAS data acquisition software package. The described data acquisition system is currently being constructed, and will be fully operational before the start of the experiment in 2016.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578574", "resources": [{"_type": "LocalFile", "name": "gohn_chep2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/21\/attachments\/578574\/796722\/gohn_chep2015.pdf", "fileName": "gohn_chep2015.pdf", "_fossil": "localFileMetadata", "id": "796722", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/21", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:15:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "22", "speakers": [{"_type": "ContributionParticipation", "emailHash": "b120fdf9f04a123ea43e83d2718cd2ef", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SKIPSEY, Samuel Cadellin", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "b120fdf9f04a123ea43e83d2718cd2ef", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SKIPSEY, Samuel Cadellin", "id": "1"}], "title": "Enabling Object Storage via shims for Grid Middleware", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T00:20:35.482265+00:00", "description": "", "title": "SCS-CEPH2015-g.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/22\/attachments\/578575\/796723\/SCS-CEPH2015-g.pdf", "filename": "SCS-CEPH2015-g.pdf", "content_type": "application\/pdf", "type": "file", "id": 796723, "size": 812017}], "title": "Slides", "default_folder": false, "id": 578575, "description": ""}], "_type": "Contribution", "description": "The *Object Store* model has quickly become the de-facto basis of most commercially successful mass storage infrastructure, backing so-called \"Cloud\" storage such as Amazon S3, but also underlying the implementation of most parallel distributed storage systems. \r\nMany of the assumptions in object store design are similar, but not identical, to concepts in the design of Grid Storage Elements, although the requirement for \"POSIX-like\" filesystem structures on top of SEs makes the disjunction seem larger. \r\nAs modern object stores provide many features that most Grid SEs do not (block level striping, parallel access, automatic file repair, etc), it is of interest to see how easily we can provide interfaces to typical object stores via plugins and shims for Grid tools, and how well experiments can adapt their data models to them.\r\n\r\nWe present the experience of developing plugins for the currently-popular *ceph* parallel distributed filesystem, for GFAL2 and also other Grid file access and data management tools.\r\n\r\nAdditionally, we present evaluation of, and first-deployment experiences with, (for example) Xrootd-Ceph interfaces for direct object-store access, for ATLAS at RAL.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578575", "resources": [{"_type": "LocalFile", "name": "SCS-CEPH2015-g.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/22\/attachments\/578575\/796723\/SCS-CEPH2015-g.pdf", "fileName": "SCS-CEPH2015-g.pdf", "_fossil": "localFileMetadata", "id": "796723", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "fecbc4752eaee2cab98bdd811db0c590", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "DEWHURST, Alastair", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "da6806efb2f919f20a8d4d30e47c621c", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "ROY, Gareth Douglas", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "9036c001235c207315f2f1cb8f8524b7", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. BRITTON, David", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "572f6f06cb2be973d28b9d129a0a91fd", "affiliation": "University of Edinburgh (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "BHIMJI, Wahid", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "7c85d37398152148d60e922d4eda76d7", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "CROOKS, David", "id": "5"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/22", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:30:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:45:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "23", "speakers": [{"_type": "ContributionParticipation", "emailHash": "76af91cb3ccd21c0a9005dd94bb45b0f", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ITO, Hironori", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "76af91cb3ccd21c0a9005dd94bb45b0f", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ITO, Hironori", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "2628a966cb60c5635940b139d5df8cb9", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. ZAYTSEV, Alexandr", "id": "1"}], "title": "Current Status of the Ceph Based Storage Systems at the RACF", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T20:49:01.442848+00:00", "description": "", "title": "Ceph_CHEP_2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/23\/attachments\/578576\/796724\/Ceph_CHEP_2015.pdf", "filename": "Ceph_CHEP_2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796724, "size": 268882}], "title": "Slides", "default_folder": false, "id": 578576, "description": ""}], "_type": "Contribution", "description": "Ceph based storage solutions are becoming increasingly popular within the HEP\/NP community over the last few years. With the current status of the Ceph project, both its object storage and block storage layers are production ready on a large scale, and even the Ceph file system (CephFS) storage layer is rapidly getting to that state as well. This contribution contains a thorough review of various functionality, performance, and stability tests performed with all three (object storage, block storage and file system) levels of Ceph by using the resources of the RHIC and ATLAS Computing Facility (RACF) at Brookhaven National Laboratory (BNL) in 2012-2014 on various hardware platforms (including HP Moonshot) and with different networking solutions based on 10\/40 Gbps Ethernet and IPoIB\/4X FDR Infiniband interconnect technology. We also report the current status of a 1 PB scale (by usable capacity, taking into account the internal data replication factor 3x) Ceph based object storage system provided with Amazon S3 complaint RADOS Gateway interfaces that was built for the RACF in 2013, as well as the first results on its performance and the experience obtained while operating it in production over the last 8 months.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578576", "resources": [{"_type": "LocalFile", "name": "Ceph_CHEP_2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/23\/attachments\/578576\/796724\/Ceph_CHEP_2015.pdf", "fileName": "Ceph_CHEP_2015.pdf", "_fossil": "localFileMetadata", "id": "796724", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "81b1bf725f4540f8632673ac37508af9", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. HOLLOWELL, Christopher", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "d02731e682c9e8307b844b566d0af6aa", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WONG, Tony", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "b2974fdd58fd77f382e05194d6970677", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. RAO, Tejas", "id": "4"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/23", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:45:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "28", "speakers": [{"_type": "ContributionParticipation", "emailHash": "bc402c6bda7ee882a046421914ad957c", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. SMITH, Jason Alexander", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "bc402c6bda7ee882a046421914ad957c", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. SMITH, Jason Alexander", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "008a38822c8fa498240d4f0908209ec7", "affiliation": "Oberlin College", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. RICHMAN, Gabriel", "id": "1"}], "title": "The Careful Puppet Master: Reducing risk and fortifying acceptance testing with Jenkins CI", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-01T20:04:11.336550+00:00", "description": "", "title": "CHEP_2015_Okinawa_Jenkins.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/28\/attachments\/578577\/796725\/CHEP_2015_Okinawa_Jenkins.pdf", "filename": "CHEP_2015_Okinawa_Jenkins.pdf", "content_type": "application\/pdf", "type": "file", "id": 796725, "size": 938770}], "title": "Slides", "default_folder": false, "id": 578577, "description": ""}], "_type": "Contribution", "description": "Using centralized configuration management, including automation tools such as\r\nPuppet, can greatly increase provisioning speed and efficiency when configuring\r\nnew systems or making changes to existing systems, reduce duplication of work,\r\nand improve automated processes. However, centralized management also brings\r\nwith it a level of inherent risk: a single change in just one file can quickly\r\nbe pushed out to thousands of computers and, if that change is not properly and\r\nthoroughly tested and contains an error, could result in catastrophic damage to\r\nmany services, potentially bringing an entire computer facility offline.\r\n\r\nChange management procedures can -- and should -- be formalized in order to\r\nprevent such accidents. However, like the configuration management process\r\nitself, if such procedures are not automated, they can be difficult to enforce\r\nstrictly. Therefore, to reduce the risk of merging potentially harmful changes\r\ninto our production Puppet environment, we have created an automated testing\r\nsystem, which includes the Jenkins CI tool, to manage our Puppet testing process.\r\nThis system includes the proposed changes and runs Puppet on a pool of dozens of RHEV\r\nVMs that replicate most of our important production services. This paper\r\ndescribes our automated test system and how it hooks into our production\r\napproval process for automatic acceptance testing. All pending changes that have\r\nbeen pushed to production must pass this validation process before they can be\r\napproved and merged into production.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578577", "resources": [{"_type": "LocalFile", "name": "CHEP_2015_Okinawa_Jenkins.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/28\/attachments\/578577\/796725\/CHEP_2015_Okinawa_Jenkins.pdf", "fileName": "CHEP_2015_Okinawa_Jenkins.pdf", "_fossil": "localFileMetadata", "id": "796725", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "d02731e682c9e8307b844b566d0af6aa", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WONG, Tony", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "acc9b55a0ea43e0190e5a05b63b828f2", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. PRYOR, James", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "6a8aba43c37af88cebb6347ae8173fb5", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "DE STEFANO JR, John Steven", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "001b3e939ab534eee442ba2cbd6b82e4", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. STRECKER-KELLOGG, William", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "b2974fdd58fd77f382e05194d6970677", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. RAO, Tejas", "id": "6"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/28", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:45:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "29", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ee5c64a96e5d2b586738640dde36ae42", "affiliation": "Michigan State University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "HAUSER, Reiner", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ee5c64a96e5d2b586738640dde36ae42", "affiliation": "Michigan State University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "HAUSER, Reiner", "id": "2"}], "title": "The ATLAS Data Flow system for the second LHC run", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T01:22:55.527465+00:00", "description": "", "title": "ATL-DAQ-SLIDE-2015-174.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/29\/attachments\/578578\/796726\/ATL-DAQ-SLIDE-2015-174.pdf", "filename": "ATL-DAQ-SLIDE-2015-174.pdf", "content_type": "application\/pdf", "type": "file", "id": 796726, "size": 444169}], "title": "Slides", "default_folder": false, "id": 578578, "description": ""}], "_type": "Contribution", "description": "After its first shutdown, LHC will provide pp collisions with increased luminosity and energy. In the ATLAS experiment the Trigger and Data Acquisition (TDAQ) system has been upgraded to deal with the increased event rates. The Data Flow (DF) element of the TDAQ is a distributed hardware and software system responsible for buffering and transporting event data from the Readout system to the High Level Trigger (HLT) and to the event storage. The DF has been reshaped in order to profit from the technological progress and to maximize the flexibility and efficiency of the data selection process.\r\n\r\nThe updated DF is radically different from the previous implementation both in terms of architecture and expected performance. The pre-existing two level software filtering, known as L2 and the Event Filter, and the Event Building are now merged into a single process, performing incremental data collection and analysis. This design has many advantages, among which are: the radical simplification of the architecture, the flexible and automatically balanced distribution of the computing resources, the sharing of code and services on nodes. In addition, logical farm slicing, with each slice managed by a dedicated supervisor, has been dropped in favour of global management by a single farm master operating at 100 kHz.\r\n\r\nThe Data Collection network, that connects the HLT processing nodes to the Readout and the storage systems has evolved to provide network connectivity as required by the new Data Flow architecture. The old Data Collection and Back-End networks have been merged into a single Ethernet network and the Readout PCs have been directly connected to the network cores. The aggregate throughput and port density have been increased by an order of magnitude and the introduction of Multi Chassis Trunking significantly enhanced fault tolerance and redundancy.  \r\n\r\nWe will discuss the design choices, the strategies employed to minimize the data-collection latency, the results of scaling tests done during the commissioning phase and the operational performance after the first months of data taking.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578578", "resources": [{"_type": "LocalFile", "name": "ATL-DAQ-SLIDE-2015-174.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/29\/attachments\/578578\/796726\/ATL-DAQ-SLIDE-2015-174.pdf", "fileName": "ATL-DAQ-SLIDE-2015-174.pdf", "_fossil": "localFileMetadata", "id": "796726", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/29", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:30:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "407", "speakers": [{"_type": "ContributionParticipation", "emailHash": "c76a3d8258ea31bc67d3f93b3e68bb6f", "affiliation": "University of Michigan (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MC KEE, Shawn", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c76a3d8258ea31bc67d3f93b3e68bb6f", "affiliation": "University of Michigan (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MC KEE, Shawn", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "dc823b395ebcf8e16699e15ab5d301cd", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BABIK, Marian", "id": "1"}], "title": "Integrating network and transfer metrics  to optimize transfer efficiency and experiment workflows", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T08:17:22.889091+00:00", "description": "", "title": "perfSONAR_CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/407\/attachments\/578579\/796727\/perfSONAR_CHEP2015.pdf", "filename": "perfSONAR_CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796727, "size": 1420741}, {"_type": "attachment", "modified_dt": "2015-04-12T08:17:22.889091+00:00", "description": "", "title": "perfSONAR_CHEP2015.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/407\/attachments\/578579\/796728\/perfSONAR_CHEP2015.pptx", "filename": "perfSONAR_CHEP2015.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796728, "size": 4529621}], "title": "Slides", "default_folder": false, "id": 578579, "description": ""}], "_type": "Contribution", "description": "The Worldwide LHC Computing Grid relies on the network as a critical part of its infrastructure and therefore needs to guarantee effective network usage and prompt detection and resolution of any network issues, including connection failures, congestion, traffic routing, etc. The WLCG Network and Transfer Metrics project aims to integrate and combine all network-related monitoring data collected by the WLCG infrastructure. This includes FTS monitoring information, monitoring data from the XRootD federation, as well as results of the perfSONAR tests. The main challenge consists of further integrating and analyzing this information so that it can be turned into actionable insight for optimization of data transfers and workload management systems of the LHC experiments.\u000b\u000bThe presentation will include technical description of the WLCG network monitoring infrastructure as well as results of the analysis of the collected data. It will also highlight how results of this analysis can be used in order to improve efficiency of the WLCG computing activities.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578579", "resources": [{"_type": "LocalFile", "name": "perfSONAR_CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/407\/attachments\/578579\/796727\/perfSONAR_CHEP2015.pdf", "fileName": "perfSONAR_CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796727", "_deprecated": true}, {"_type": "LocalFile", "name": "perfSONAR_CHEP2015.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/407\/attachments\/578579\/796728\/perfSONAR_CHEP2015.pptx", "fileName": "perfSONAR_CHEP2015.pptx", "_fossil": "localFileMetadata", "id": "796728", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "a2dbd3d2df26e3fbc0360f6b3594fd21", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CAMPANA, Simone", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "e86323288b18387d48dc1af9718c14d6", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WILDISH, Tony", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "218813bf33ab66728f320424e479d695", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CLOSIER, Joel", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "a03bb0b3b47a554f9eb69c2fca89dcb7", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GRIGORAS, Costin", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "b97ba385788d95ebdb7f8a19036c8839", "affiliation": "University of Chicago (US)", "_fossil": "contributionParticipationMetadata", "fullName": "VUKOTIC, Ilija", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "2bc2abdabb8d1f21dae00de97010754f", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SALICHOS, Michail", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "e6819ee253837bc1a19f5e93c8ef001e", "affiliation": "University of Texas at Arlington (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DE, Kaushik", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "c0af0a453fb7de772f32e585bcdcd2ba", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GARONNE, Vincent", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "647320bc504c1d00830f95c71268cd75", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DIAZ CRUZ, Jorge Alberto", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "bd221996d4a256a8003a73d408902cf5", "affiliation": "University of Manchester (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "FORTI, Alessandra", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "fc2d531a1d59c0d27cf4550e45442ae6", "affiliation": "University of London (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "WALKER, Christopher John", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "6794b20ff1a79867a414c13ad66a52f2", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "RAND, Duncan", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "74f37452da18d70a445f84d900eff716", "affiliation": "Universita e INFN, Roma I (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "DE SALVO, Alessandro", "id": "14"}, {"_type": "ContributionParticipation", "emailHash": "03a8e9c6c1651d92b202f22b4e18a462", "affiliation": "Sezione di Pisa (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "MAZZONI, Enrico", "id": "15"}, {"_type": "ContributionParticipation", "emailHash": "f13af63ad4bbdfdce02559c629f8d6bc", "affiliation": "University of Victoria (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "GABLE, Ian", "id": "16"}, {"_type": "ContributionParticipation", "emailHash": "dd211791bf79f00f06715a9fd5432983", "affiliation": "Centre National de la Recherche Scientifique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "CHOLLET, Frederique", "id": "17"}, {"_type": "ContributionParticipation", "emailHash": "9eee4b4fc098df33b00b13012abf0316", "affiliation": "ASGC", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. CHEN, HSIN YEN", "id": "18"}, {"_type": "ContributionParticipation", "emailHash": "c5f35fcfdaf5745934b874b386ae52ea", "affiliation": "Helsinki Institute of Physics (FI)", "_fossil": "contributionParticipationMetadata", "fullName": "TIGERSTEDT, Ulf Bobson Severin", "id": "19"}, {"_type": "ContributionParticipation", "emailHash": "2968c9e299c4d89537c7ae1fdd636376", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "DUCKECK, Guenter", "id": "20"}, {"_type": "ContributionParticipation", "emailHash": "aaedaf0b6923da9472cbc33726464d1d", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "PETZOLD, Andreas", "id": "21"}, {"_type": "ContributionParticipation", "emailHash": "073b7fe5499f0ba3d6f1de2429069b2d", "affiliation": "PIC (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "LOPEZ MUNOZ, Fernando", "id": "22"}, {"_type": "ContributionParticipation", "emailHash": "2c86ad3a82757328b65885fcf4b99611", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SHADE, John", "id": "23"}, {"_type": "ContributionParticipation", "emailHash": "361e81ded9839e86c6f69fdee4ddca39", "affiliation": "E", "_fossil": "contributionParticipationMetadata", "fullName": "OCONNOR, Michael", "id": "24"}, {"_type": "ContributionParticipation", "emailHash": "532b8e8515976865101b2cc18865ae59", "affiliation": "Institute for Theoretical Physics, National Science Center Kharkov Institute of Physics and Technology, Nat. Acad. of Sciences of Ukraine (UA)", "_fossil": "contributionParticipationMetadata", "fullName": "KOTLYAR, Volodymyr", "id": "25"}, {"_type": "ContributionParticipation", "emailHash": "35721fed37de20dfa93d4c5ffdac389c", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "HOEFT, Bruno Heinrich", "id": "26"}, {"_type": "ContributionParticipation", "emailHash": "d4edda2d409a11d6dfd7aa2528bdb061", "affiliation": "ESnet", "_fossil": "contributionParticipationMetadata", "fullName": "ZURAWSKI, Jason", "id": "27"}, {"_type": "ContributionParticipation", "emailHash": "cbb0d28d39b1ca6e5d447a3e19e0e3c7", "affiliation": "Centro de Investigaciones Energ. Medioambientales y Tecn. - (ES", "_fossil": "contributionParticipationMetadata", "fullName": "FLIX MOLINA, Jose", "id": "28"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/407", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "406", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6c43a86ad51d6b43399ddb15379b0b06", "affiliation": "Universidade de Santiago de Compostela (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "FERNANDEZ ALBOR, Victor Manuel", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6c43a86ad51d6b43399ddb15379b0b06", "affiliation": "Universidade de Santiago de Compostela (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "FERNANDEZ ALBOR, Victor Manuel", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "95b512396ed049a237dcc42f4119a372", "affiliation": "Universidade de Santiago de Compostela (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "SECO MIGUELEZ, Marcos", "id": "1"}], "title": "Statistical analysis of virtualization performance in high energy physics software", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "Cloud Computing is emerging today as the new approach followed by computing centres, since the flexibility the Cloud provides\r\nis a powerful component to manage their resources. Through the use of virtualization, cloud promise to address with the same\r\nshared set of physical resources a large user base with different needs. However, virtualization may induce significant performance\r\npenalties for the demanding scientific computing workloads. This work presents an evaluation of the usefulness of the current\r\ncloud computing services for scientific applications. The performace of a sample of High Energy Physics (HEP) software running\r\nin a Kernel-based Virtual Machine (KVM) under different set-ups is analysed with the use of a multivariate analysis of variance\r\n(MANOVA). While clouds are still changing, our results indicate that the current cloud services have to take into account the\r\ndifferent setups of Cores and Memory in order to get reasonable performances in HEP-Software.", "track": "Track7: Clouds and virtualization", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "4948a7b47621fc6acd5d4f4c2fa943e9", "affiliation": "Santiago de Compostela University", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. FERNANDEZ, Tom\u00e1s", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "ac14c1ad83816fe960e20d5de4f748cf", "affiliation": "University of Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "GRACIANI DIAZ, Ricardo", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "a530d712db0e40016a11ff655a98f88a", "affiliation": "Universitat de Barcelona", "_fossil": "contributionParticipationMetadata", "fullName": "MENDEZ MUNOZ, Victor", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "c60ed0ef4e716a9ba45556fa850cf6ed", "affiliation": "Universidade de Santiago de Compostela (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "SABORIDO SILVA, Juan Jose", "id": "5"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/406", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:15:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "405", "speakers": [{"_type": "ContributionParticipation", "emailHash": "0283af9edc2d1122b30ebbeabe1b60fc", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SMITH, Tim", "id": "7"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "43df6fa855566c2902a33ae803a5bdb7", "affiliation": "University of Northumbria at Newcastle (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "COWTON, Jake", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "ea427e0d830525fa51b1b9add3ad8e9e", "affiliation": "Humboldt-Universitaet zu Berlin (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "DALLMEIER-TIESSEN, Sunje", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "0bc625725a7099d0e24b738432077ba2", "affiliation": "National and Kapodistrian University of Athens (GR)", "_fossil": "contributionParticipationMetadata", "fullName": "FOKIANOS, Pamfilos", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "394729183e54a474756374ccb08cf99c", "affiliation": "Humboldt-Universitaet zu Berlin (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "HERTERICH, Patricia Sigrid", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "2314f29663f77c44a3076b7dd3f00e2e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "KUNCAR, Jiri", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "45ec7f6d66b1d565c0603dacea290791", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "RUEDA GARCIA, Laura", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "4e67297166a80955b72e69e066491263", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SIMKO, Tibor", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "0283af9edc2d1122b30ebbeabe1b60fc", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SMITH, Tim", "id": "7"}], "title": "Open Data and Data Analysis Preservation Services for LHC Experiments", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T02:54:15.494514+00:00", "description": "", "title": "CODP_at_CHEP.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/405\/attachments\/578580\/796729\/CODP_at_CHEP.pdf", "filename": "CODP_at_CHEP.pdf", "content_type": "application\/pdf", "type": "file", "id": 796729, "size": 3825263}, {"_type": "attachment", "modified_dt": "2015-04-16T02:54:15.494514+00:00", "description": "", "title": "CODP_at_CHEP.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/405\/attachments\/578580\/796730\/CODP_at_CHEP.pptx", "filename": "CODP_at_CHEP.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796730, "size": 9355374}], "title": "Slides", "default_folder": false, "id": 578580, "description": ""}], "_type": "Contribution", "description": "In this paper we present newly launched services for open data and for long-term preservation and reuse of high-energy-physics data analyses. We follow the \"data continuum\" practices through several progressive data analysis phases up to the final publication. The aim is to capture all digital assets and associated knowledge inherent in the data analysis process for subsequent generations, and to make a subset available rapidly to the public.\r\n\r\nA data analysis preservation pilot study was launched in order to assess the usual workflow practices in LHC collaborations. Leveraging on synergies between ALICE, ATLAS, CMS and LHCb experiments, the analysed data was followed through various \"analysis train\" steps, from the initial capture and pre-selection of primary data, through several intermediate selection steps yielding more greatly reduced datasets, up to the final selection of N-tuples used for producing high-level plots appearing in scientific journal publications. Most of the analysis chain is kept strictly restricted within a given collaboration; only the final plots, presentations and papers are usually made public. It is therefore essential to handle access rights and embargo periods as part of the data life cycle.\r\n\r\nThe study revealed many similarities between collaborations, even though the variety of different practices existing in different groups within the collaborations make it hard to reproduce an analysis at a later time in a uniform way. One recurring problem underlined by the study was to ensure an efficient \"knowledge capture\" related to user code when the principal author of an analysis (e.g. a PhD student) leaves the collaboration later.\r\n\r\nThe pilot solution has been prototyped using the Invenio digital library platform which was extended with several data-handling capabilities. The aim was to preserve information about datasets, the underlying OS platform and the user software used to study it. The configuration parameters, the high-level physics information such as physics object selection, and any necessary documentation and discussions are optionally being recorded alongside the process as well. The metadata representation of captured assets uses the MARC bibliographic standard which had to be customised and extended in relation to specific analysis-related fields. The captured digital assets are being minted with Digital Object Identifiers, ensuring later referencability and citability of preserved data and software. Connectors were built in the platform to large-scale data storage systems (CASTOR, EOS, Ceph). In addition, to facilitate information exchange among concerned services, further connectors were built to\r\n\r\nthe internal information management systems of LHC experiments (e.g. CMS CADI), to the discussion platforms (e.g. TWiki, SharePoint), and to the final publication servers (e.g. CDS, INSPIRE) used in the process. Finally, the platform draws inspiration from the Open Archival Information System (OAIS) recommended practices in order to ensure long-term preservation of captured assets.\r\n\r\nThe ultimate goal of the analysis preservation platform is to capture enough information about the process in order to facilitate reproduction of an analysis even many years after its initial publication, permitting to extend the impact of preserved analyses through future revalidation and recasting services.\r\n\r\nA related \"open data\" service was launched for the benefit of the general public. The LHC experimental collaborations are committed to make their data open after a certain embargo period. Moreover, the collaborations also release simplified datasets for the general public within the framework of the international particle physics masterclass program. The primary and reduced datasets that the collaborations release for public use are being collected within the CERN Open Data portal service, allowing any physicist or general data scientist to access, explore, and further study the data on their own.\r\n\r\nThe CERN Open Data portal offers several high-level tools which help to visualise and work with the data, such as an interactive event display permitting to visualise CMS detector events on portal web pages, or a basic histogram plotting interface permitting to create live plots out of CMS reduced datasets. The platform guides high-school teachers and students to online masterclasses to further explore the data and improve their knowledge of particle physics. A considerable part of the CERN Open Data portal was therefore devoted to attractive presentation and ease-of-use of captured data and associated information.\r\n\r\nThe CERN Open Data platform not only offers datasets and live tools to explore them, but it also preserves the software tools used to analyse the data. It notably offers the download of Virtual Machine images permitting users to start their own working environment in order to further explore the data; for this the platform uses CernVM based images prepared by the collaborations. Moreover, the CERN Open Data platform preserves examples of user analysis code, illustrating how the general public could write their own code to perform further analyses.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578580", "resources": [{"_type": "LocalFile", "name": "CODP_at_CHEP.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/405\/attachments\/578580\/796729\/CODP_at_CHEP.pdf", "fileName": "CODP_at_CHEP.pdf", "_fossil": "localFileMetadata", "id": "796729", "_deprecated": true}, {"_type": "LocalFile", "name": "CODP_at_CHEP.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/405\/attachments\/578580\/796730\/CODP_at_CHEP.pptx", "fileName": "CODP_at_CHEP.pptx", "_fossil": "localFileMetadata", "id": "796730", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/405", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "404", "speakers": [{"_type": "ContributionParticipation", "emailHash": "c3ac7babb83d3d0692a2560930abd651", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MATO VILA, Pere", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c3ac7babb83d3d0692a2560930abd651", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MATO VILA, Pere", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "31f4ce6bb7990a6d72e7eef32dd3bd5c", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "SCHOPPE, Philipp", "id": "1"}], "title": "New Continuous Integration and Automated Testing Environment for ROOT", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "ROOT is being developed following the practice of Continuous Integration (CI) and automated testing. Since ROOT is used by the HEP experiments, many individual physicists and users outside the HEP community, a very wide range of platforms and operating systems need to be supported, as well as many combinations of optional components. We demonstrate how the Jenkins CI tool is used to handle multiple build modes on a vast amount of machines. By migrating ROOT's build system to CMake, we can handle a varying number of external packages in different versions in a portable manner. Same build procedures and scripts can be used for building on Linux, Macs and Windows, and at the same time we gain more platform interoperability and improve build time behaviour. We illustrate how CTest with custom built test driver scripts, allows us to provide a simple set of tools in order to add and manage a full range of different kind of tests such as unit, integration and regression tests. Depending on the build type, subsets of tests with varying significance may be selected and executed in parallel. Test results are nicely integrated into the Jenkins tool for the benefit of developers and system integrators.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/404", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "403", "speakers": [{"_type": "ContributionParticipation", "emailHash": "8f9be3b3afde50b61bf19e4a1e0ab965", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WENZEL, Hans-Joachim", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "8f9be3b3afde50b61bf19e4a1e0ab965", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WENZEL, Hans-Joachim", "id": "0"}], "title": "The Geant4 physics validation repository", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-08T15:00:35.843110+00:00", "description": "", "title": "CHEP2015Poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/403\/attachments\/578581\/796732\/CHEP2015Poster.pdf", "filename": "CHEP2015Poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796732, "size": 1244420}, {"_type": "attachment", "modified_dt": "2015-04-08T15:00:35.843110+00:00", "description": "", "title": "CHEP2015Poster.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/403\/attachments\/578581\/796731\/CHEP2015Poster.pptx", "filename": "CHEP2015Poster.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796731, "size": 1655233}], "title": "Slides", "default_folder": false, "id": 578581, "description": ""}], "_type": "Contribution", "description": "We describe the Geant4 physics validation repository and the technology used\r\nto implement it. The Geant4 collaboration regularly performs validation and regression tests where results\r\nobtained with a new Geant4 version are compared to data obtained by various HEP experiments\r\nor the results of previous releases. As the number of regularly performed validation tests\r\nincreases and the collection of results grows, storing them and making them available to the\r\ncollaborators and users community becomes a challenge. We decided to organize the materials\r\nin one central repository and to make this data easily available via a web application.\r\nThe Physics Validation Repository consists of three components:\r\nThe first component is a PostgresSQL relational database that stores both simulated and experimental data\r\nin form of images with meta data or raw data points. The meta data describes the test and lists the references\r\nfrom where the experimental data for comparison was obtained from, as well as other parameters that describe the test\r\n(e.g. beam particle, target material, etc.). The second component is a Java API which is based on the data\r\naccess object (DAO) design pattern and provides an abstract interface to the database. Finally a web application\r\nbased on Java Platform, Enterprise Edition (Java EE) is deployed on a GlassFish Application server. The web application\r\nallows to interactively select and overlay compatible data, e.g. to compare the results of different releases,\r\ndifferent physics models or target materials in one plot.\r\nThe web application also provides security and authentication to grant access to groups of functions and data\r\nthat are internal to the Geant4 collaboration, e.g. viewing results from development releases, upload of new tests\r\nand\/or modification of selected tests.\r\nWe use the PrimeFaces JSF (Java Server Faces) Framework to create interactive, modern looking web interfaces and\r\nthe jfreechart java library or the Highcharts JavaScript library for plotting.\r\n\r\nPrimary Authors:\r\nWenzel, Hans \r\nYarba, Julia \r\nOn behalf of the Physics Validation Working Group of the Geant4 Collaboration", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578581", "resources": [{"_type": "LocalFile", "name": "CHEP2015Poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/403\/attachments\/578581\/796732\/CHEP2015Poster.pdf", "fileName": "CHEP2015Poster.pdf", "_fossil": "localFileMetadata", "id": "796732", "_deprecated": true}, {"_type": "LocalFile", "name": "CHEP2015Poster.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/403\/attachments\/578581\/796731\/CHEP2015Poster.pptx", "fileName": "CHEP2015Poster.pptx", "_fossil": "localFileMetadata", "id": "796731", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "bfc82c0add840c41c1380d591656d55f", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "YARBA, Julia", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/403", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:15:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:30:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "402", "speakers": [{"_type": "ContributionParticipation", "emailHash": "dcfa0e82c6dcd12fb4fce0404b1fdfbd", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. POAT, Michael", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "dcfa0e82c6dcd12fb4fce0404b1fdfbd", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. POAT, Michael", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "60e98fb28af2edf9a8a6c39567ee11c7", "affiliation": "BROOKHAVEN NATIONAL LABORATORY", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LAURET, Jerome", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "49d5c63b6506d8d83f48bf87ecab7d22", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "BETTS, Wayne", "id": "2"}], "title": "POSIX and Object Distributed Storage system \u2013 performance Comparison studies and real-life usage in an experimental data taking context leveraging OpenStack\/Ceph.", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T23:28:17.325523+00:00", "description": "", "title": "CHEP2015_Swift_Ceph_v8.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/402\/attachments\/578582\/796733\/CHEP2015_Swift_Ceph_v8.pdf", "filename": "CHEP2015_Swift_Ceph_v8.pdf", "content_type": "application\/pdf", "type": "file", "id": 796733, "size": 1838810}, {"_type": "attachment", "modified_dt": "2015-04-14T23:28:17.325523+00:00", "description": "", "title": "CHEP2015_Swift_Ceph_v8.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/402\/attachments\/578582\/796734\/CHEP2015_Swift_Ceph_v8.pptx", "filename": "CHEP2015_Swift_Ceph_v8.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796734, "size": 1563735}], "title": "Slides", "default_folder": false, "id": 578582, "description": ""}], "_type": "Contribution", "description": "The STAR online computing environment is an intensive ever-growing system used for first-hand data collection and analysis. As systems become more sophisticated, they result in a more detailed dense collection of data output and inefficient limited storage systems have become an impediment to fast feedback to the online shift crews relying on data processing at near real-time speed. Motivation for a centrally accessible, scalable and redundant storage solution was led from an expansion of data growth and user data processing necessity. However, standard solutions (NAS, SAN \u2026 GPFS) are \u201cexpensive\u201d solutions and it became clear a balance of affordability and cost effectiveness was needed. Furthermore, the vast amount of sparse and distributed storage (disk attached to individual nodes) made the aggregation of the storage an attractive path. Yet, as shift crews are often composed of novice members sustaining the daily operations, providing a POSIX compliant interface within a standard namespace was, for this environment, a strong requirement for any retained solution. \r\n\r\nThe acquisition of reused hardware has offered STAR an opportunity to deploy a storage strategy at minimal to no cost. We have analyzed multiple open source object oriented cloud inspired solutions and a POSIX compliant storage system. Openstack Swift Object Storage and Ceph Object Storage were put to the test with single and parallel I\/O tests emulating real world scenario for data processing and workflows. The Ceph file system storage, offering a POSIX compliant file system mounted similar to an NFS share which maintain owner\/group permissions and historical lookup, was of particular interest as aligned with our requirements and was retained as our solution. \r\n\r\nThe Ceph storage system will allow scalability and redundancy without user or system interruption. Initial configuration for user setup is minimal while maintaining security and data integrity across the entire cluster. A distributed storage system becomes and interconnected web of machines utilizing load balancing and redundancy, however if a subset of machines or all machines go down data loss is unlikely to occur. The expansion of a cloud storage system is trivial and can allow system administrators do add capacity and load balancing in a complete transparent manner.  \r\n\r\nIn this report, we will review the necessary steps and requirements for our system, tools leveraged for performance testing and present comparative IO performance results between Swift and Ceph  Object  storage approach in similar context as well as a cross comparison of performance between the Object and POSIX compliant Ceph approaches. We will also discuss the benefit of a backbone private network for enhanced performance and fast communication between the distributed storage system components and scalability considerations for Meta-Data access and report on actual user\u2019s experience in a data taking environment. Finally, we will also present in great details the essential steps necessary to setup a Ceph storage cluster, including tweaks and hardware changes made along the way \u2013 the recipes we will present are not always easy to find in the manual and we hope our presentation will serve well the community\u2019s interest for distributed storage solutions.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578582", "resources": [{"_type": "LocalFile", "name": "CHEP2015_Swift_Ceph_v8.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/402\/attachments\/578582\/796733\/CHEP2015_Swift_Ceph_v8.pdf", "fileName": "CHEP2015_Swift_Ceph_v8.pdf", "_fossil": "localFileMetadata", "id": "796733", "_deprecated": true}, {"_type": "LocalFile", "name": "CHEP2015_Swift_Ceph_v8.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/402\/attachments\/578582\/796734\/CHEP2015_Swift_Ceph_v8.pptx", "fileName": "CHEP2015_Swift_Ceph_v8.pptx", "_fossil": "localFileMetadata", "id": "796734", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/402", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "401", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ed96c4767feeb06950d16695aa31278b", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CURRIE, Robert Andrew", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "fbe150e15f70bc833cdae7e3883657e4", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "EGEDE, Ulrik", "id": "0"}], "title": "Recent advancements in user-job management with Ganga", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T18:19:38.808283+00:00", "description": "", "title": "rcurrie_PosterCHEP3.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/401\/attachments\/578583\/796735\/rcurrie_PosterCHEP3.pdf", "filename": "rcurrie_PosterCHEP3.pdf", "content_type": "application\/pdf", "type": "file", "id": 796735, "size": 569641}], "title": "Poster", "default_folder": false, "id": 578583, "description": ""}], "_type": "Contribution", "description": "The Ganga project (http:\/\/cern.ch\/ganga) has long been used by several experimental communities within HEP, most notably Atlas and LHCb. This talk describes the most recent developments in job submission and management within Ganga with a focus on newly developed tools and features.  \r\nGanga offers a powerful unified interface for submitting complex user-jobs to many different backends,  this allows the user to exploit their available computing resources without the need to worry about implementation details.  \r\nThe release of Ganga 6.1 in 2014\/2015 will offer a new unified approach to file management within distributed analyses. One of the benefits of this is that it will allow for a user's data to be transferred directly between worker nodes and various storage solutions without the need for a centrally managed instance of Ganga.\r\nIn addition to this, Ganga now also supports a powerful new job queuing system. This system allows for users to perform job manipulation in a more automated way, providing them with more freedom over their job management.   \r\nWorking closely with recent developments in DIRAC, Ganga will soon support bulk job submission to the grid allowing users to more easily exploit the currently available distributed resources. Alongside this, and with recent GridPP developments at Imperial College, Ganga offers an out of the box solution for experiments looking to work with vanilla DIRAC.  \r\nGanga offers a modular codebase allowing for smaller experiments to easily adopt it with the ability to readily expand and develop additional modules to suit their particular requirements. Offering support for multiple HEP experiments Ganga development follows a \u201crelease early, release often\u201d mantra. This allows for the development of new features whilst still focusing on stability and support for the existing user-base. Recent Ganga developments have also added a new Jenkins based unit-testing system to provide more comprehensive testing and debugging.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578583", "resources": [{"_type": "LocalFile", "name": "rcurrie_PosterCHEP3.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/401\/attachments\/578583\/796735\/rcurrie_PosterCHEP3.pdf", "fileName": "rcurrie_PosterCHEP3.pdf", "_fossil": "localFileMetadata", "id": "796735", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "ed96c4767feeb06950d16695aa31278b", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CURRIE, Robert Andrew", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "1abe5b1516eb954743a7e0c64e7555cc", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "ELMSHEUSER, Johannes", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "2dd6016cc62d0dad8168b548c61e1020", "affiliation": "University of Liverpool", "_fossil": "contributionParticipationMetadata", "fullName": "FAY, Robert", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "da8b10992cd269417fbc04b9db5a9835", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "OWEN, Patrick Haworth", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "7d89f79a43b5cfacaafd400ecdaea334", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. RICHARDS, Alexander John", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "aa561e1e22906864f0b67626e1816494", "affiliation": "University of Birmingham (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "SLATER, Mark William", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "cc25ca9769bd6e4cda5ded486eb14d50", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "SUTCLIFFE, William Lawrence", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "b8c26bea8e7e9aeebae10bbdededc25b", "affiliation": "University of Warwick (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "WILLIAMS, Matt", "id": "8"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/401", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "400", "speakers": [{"_type": "ContributionParticipation", "emailHash": "dcfa0e82c6dcd12fb4fce0404b1fdfbd", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. POAT, Michael", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "dcfa0e82c6dcd12fb4fce0404b1fdfbd", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. POAT, Michael", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "60e98fb28af2edf9a8a6c39567ee11c7", "affiliation": "BROOKHAVEN NATIONAL LABORATORY", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LAURET, Jerome", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "49d5c63b6506d8d83f48bf87ecab7d22", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "BETTS, Wayne", "id": "2"}], "title": "Configuration Management and Infrastructure Monitoring using CFEngine and Icinga for real-time heterogeneous data taking environment", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The STAR online computing environment is an intensive ever-growing system used for real-time data collection and analysis. Composed of heterogeneous and sometimes custom-tuned machine groups (Data Acquisition or DAQ computing, Trigger group, Sow Control or user-end data quality monitoring resources do not have the same requirements) the computing infrastructure was managed by manual configurations and inconsistently monitored by combinations of known tools (Ganglia for monitoring for example) and home-made scripts sending Email reports to the (many) administrators of the diverse systems. This situation lead to configuration inconsistencies and an overload of repetitive tasks that needed to be passed from system groups to system groups with no global configuration and view of problems. Worst off, as the need to communicate between systems increased, globally securing the cyber-infrastructure was not possible to achieve and as more resources are moving closer to where the data is generated (due to event filtering and so-called \"High Level Trigger farms), an agile policy-driven system ensuring consistency was seek. \r\n\r\nSTAR has narrowed down its strategy toward deploying a versatile and sustainable solution by leveraging the configuration management tool CFEngine to automate configurations along with the deployment of the Infrastructure monitoring system Icinga providing a dashboard view of the system\u2019s health. In its first incarnation, Icinga 1 can be seen as a fork of the more commonly known tool Nagios monitoring while its version 2 is a core framework replacement and rewrite. Conjointly, CFEngine and Icinga have strengthened automation and sanctioning intricate development of the monitoring system. STAR has over 150 online systems spanning over four major sub-systems, each becoming critical during Runs. With a bird\u2019s eye view of each system, keeping track of the infrastructure becomes an ease. Similarly, STAR can now swiftly upgrade and modify the environment to our needs with ease as well as promptly react to cyber-security nodes whether it appears as a global patch for the Shellshock vulnerability or the reconfiguration of Secure shell. Though, as the DAQ resources do not need the same configurations than the user based resources (as one comparative example) but are all required to follow the same baseline, the infrastructure lay out is intricate and our strategy allows each system group (and sometimes machine) to be configured and monitored for its own particulars. Modular configuration is the key to consistency where differentiated plug-ins are distributed and configurations are updated ubiquitously. By creating a sustainable long term monitoring solution, the rate of failure detection has gone up from days to minutes, allowing rapid actions before the issue becomes a dire problem potentially causing loss of precious experimental data. \r\n\r\nAlternatives to our configuration management choice of CFEngine such as Chef and Puppet have been proposed and evaluated. We have chosen an open source minimally dependent agile tool that is powerful and simple to use. Our monitoring tool has offered STAR administrators a crisp interface reporting each system\u2019s state simultaneously and allowing historical system status lookup. \r\n\r\nIn this report, we will make a brief reminder and comparison of the diverse configuration management systems available on the market and justify our choice by requirements and functionalities. We will discuss the details and procedures for developing practical uses with configuration management and infrastructure monitoring. Our extensions to the community\u2019s plugin have been re-integrated into the main development branch and we will provide examples of extension of Icinga and demonstrate through example the versatility of the framework for adding metric.", "track": "Track6: Facilities, Infrastructure, Network", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/400", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:45:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "409", "speakers": [{"_type": "ContributionParticipation", "emailHash": "3104caab90081a7fa54ff8c20595e0c0", "affiliation": "Centro de Investigaciones Energ. Medioambientales y Tecn. - (ES", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PEREZ-CALERO YZQUIERDO, Antonio", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "3104caab90081a7fa54ff8c20595e0c0", "affiliation": "Centro de Investigaciones Energ. Medioambientales y Tecn. - (ES", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PEREZ-CALERO YZQUIERDO, Antonio", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "0e407565f4dca7350556665ec4ad8847", "affiliation": "Centro de Investigaciones Energ. Medioambientales y Tecn. - (ES", "_fossil": "contributionParticipationMetadata", "fullName": "HERNANDEZ CALAMA, Jose", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "19fbe91d19a3691e61eb5e432e34734a", "affiliation": "National Centre for Physics (PK)", "_fossil": "contributionParticipationMetadata", "fullName": "KHAN, Farrukh Aftab", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "37a9d67c5498998436724740f824542b", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LETTS, James", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "e08dc867fcec64c9a59155301b0c2dce", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MALTA RODRIGUES, Alan", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "35f4f869ebf48e550006a86e20965547", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LARSON, Krista", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "c1a42d65e29a742bc3b359fe0eb2186c", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MC CREA, Alison", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "65fd5d0f517c5ad7a467486ead92360b", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. VAANDERING, Eric", "id": "7"}], "title": "Evolution of CMS workload management towards multicore job support", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T07:38:48.387091+00:00", "description": "", "title": "CMSMulticore_APCY_CHEP15.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/409\/attachments\/578584\/796736\/CMSMulticore_APCY_CHEP15.pdf", "filename": "CMSMulticore_APCY_CHEP15.pdf", "content_type": "application\/pdf", "type": "file", "id": 796736, "size": 1591475}], "title": "Slides", "default_folder": false, "id": 578584, "description": ""}], "_type": "Contribution", "description": "The successful exploitation of the multicore processor architectures available at the computing sites is a key element of the LHC distributed computing system in the coming era of the LHC Run 2. High-pileup complex-collision events represent a challenge for the traditional sequential programming in terms of memory and processing time budget. The CMS data production and processing framework has introduced the parallel execution of the reconstruction and simulation algorithms to overcome these limitations. \r\n\r\nCMS plans to execute the data reconstruction and simulation as multicore processing yet supporting single-core processing for other tasks difficult to parallelize, such as user analysis. The CMS strategy for job management across the Grid thus aims at integrating single and multicore job scheduling. This is accomplished by scheduling multicore pilots with dynamic partitioning of the allocated resources, capable of running jobs with various core counts within a single pilot. \r\n\r\nAn extensive test programme has been conducted to enable multicore scheduling with the various local batch systems available at CMS sites. Scale tests have been run to optimize the scheduling strategy and to ensure the most efficient use of the distributed resources. This contribution will present in detail the evolution of the CMS job management and resource provisioning systems in order to support this hybrid scheduling model, as well as its optimization and deployment, which will enable CMS to transition to a multicore production model by the restart of the LHC.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578584", "resources": [{"_type": "LocalFile", "name": "CMSMulticore_APCY_CHEP15.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/409\/attachments\/578584\/796736\/CMSMulticore_APCY_CHEP15.pdf", "fileName": "CMSMulticore_APCY_CHEP15.pdf", "_fossil": "localFileMetadata", "id": "796736", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/409", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "408", "speakers": [{"_type": "ContributionParticipation", "emailHash": "c76a3d8258ea31bc67d3f93b3e68bb6f", "affiliation": "University of Michigan (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MC KEE, Shawn", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c76a3d8258ea31bc67d3f93b3e68bb6f", "affiliation": "University of Michigan (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MC KEE, Shawn", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "0be740cf97e00a2b970c6600edd2685f", "affiliation": "Georgia Institute of Technology", "_fossil": "contributionParticipationMetadata", "fullName": "DOVROLIS, Constantine", "id": "1"}], "title": "Identifying and Localizing Network Problems using the PuNDIT Project", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T19:04:50.799414+00:00", "description": "", "title": "PuNDIT_PI_mtg_2015_poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/408\/attachments\/578585\/796737\/PuNDIT_PI_mtg_2015_poster.pdf", "filename": "PuNDIT_PI_mtg_2015_poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796737, "size": 1321425}], "title": "Poster", "default_folder": false, "id": 578585, "description": ""}], "_type": "Contribution", "description": "In today's world of distributed scientific collaborations, there are many challenges to providing reliable inter-domain network infrastructure. Network operators use a combination of active monitoring and trouble tickets to detect problems. However, some of these approaches do not scale to wide area inter-domain networks due to unavailability of data. The Pythia Network Diagnostic InfrasTructure (PuNDIT) project aims to create a scalable infrastructure for automating the detection and localization of problems across these networks.\r\n\r\nThe objective is to gather and analyze metrics from monitoring infrastructure to identify the signatures of possible problems and locate affected network links. A primary goal for PuNDIT is to convert complex network metrics into easily understood diagnoses in an automated manner.\r\n\r\nPuNDIT is building upon the de-facto standard perfSONAR network measurement infrastructure deployed in Open Science Grid and the Worldwide LHC Computing Grid. The PuNDIT Team is working closely with the perfSONAR developers from ESnet and Internet2 to integrate PuNDIT as part of the perfSONAR Toolkit. \r\n\r\nWe will report on the project progress to-date in working with the OSG and WLCG communities and describe the current implementation architecture. We will also discuss some initial results, future plans and the project timeline.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578585", "resources": [{"_type": "LocalFile", "name": "PuNDIT_PI_mtg_2015_poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/408\/attachments\/578585\/796737\/PuNDIT_PI_mtg_2015_poster.pdf", "fileName": "PuNDIT_PI_mtg_2015_poster.pdf", "_fossil": "localFileMetadata", "id": "796737", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "c4274381c8b05a677be4c06435386a3f", "affiliation": "U", "_fossil": "contributionParticipationMetadata", "fullName": "BATISTA, Jorge", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "e9b45ce59d7c85fd40112d0177242964", "affiliation": "Georgia Institute of Technology", "_fossil": "contributionParticipationMetadata", "fullName": "LEE, Danny", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/408", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "379", "speakers": [{"_type": "ContributionParticipation", "emailHash": "473716ae3754302f6fe43b5513b5d399", "affiliation": "Universita' degli Studi di Padova e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "TOSI, Mia", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "473716ae3754302f6fe43b5513b5d399", "affiliation": "Universita' degli Studi di Padova e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "TOSI, Mia", "id": "1"}], "title": "Performance of Tracking, b-tagging and Jet\/MET reconstruction at the CMS High Level Trigger", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-08T13:28:18.707028+00:00", "description": "", "title": "150413_CHEP_trackingHLT.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/379\/attachments\/578586\/796738\/150413_CHEP_trackingHLT.pdf", "filename": "150413_CHEP_trackingHLT.pdf", "content_type": "application\/pdf", "type": "file", "id": 796738, "size": 4507620}], "title": "Slides", "default_folder": false, "id": 578586, "description": ""}], "_type": "Contribution", "description": "The trigger systems of the LHC detectors play a crucial role in determining the physics capabilities of experiments. In 2015, the center-of-mass energy of proton-proton collisions will reach 13 TeV up to an unprecedented luminosity of 1e34 cm-2s-1. A reduction of several orders of magnitude of the event rate is needed to reach values compatible with detector readout, offline storage and analysis capabilities. The CMS experiment has been designed with a two-level trigger system: the Level-1 Trigger (L1T), implemented on custom-designed electronics, and the High Level Trigger (HLT), a streamlined version of the offline reconstruction software running on a computer farm. A software trigger system requires a trade-off between the complexity of the algorithms, the sustainable output rate, and the selection efficiency. With the computing power available during the 2012 data taking the maximum reconstruction time at HLT was about 200 ms per event, at the nominal L1T rate of 100 kHz. Tracking algorithms are widely used in the HLT in the object reconstruction through particle-flow techniques as well as in the identification of b-jets and lepton isolation. Reconstructed tracks are also used to distinguish the primary vertex, which identifies the hard interaction process, from the pileup ones. This task is particularly important in the LHC environment given the large number of interactions per bunch crossing: on average 25 in 2012, and expected to be around 40 in Run II with a large contribution from out-of-time particles. In order to cope with tougher conditions the tracking and vertexing techniques used in 2012 have been largely improved in terms of timing and efficiency in order to keep the physics reach at the level of Run-I conditions. We will present the performance of these newly developed algorithms, discussing their impact on the b-tagging performances as well as on the jet and met reconstruction.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578586", "resources": [{"_type": "LocalFile", "name": "150413_CHEP_trackingHLT.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/379\/attachments\/578586\/796738\/150413_CHEP_trackingHLT.pdf", "fileName": "150413_CHEP_trackingHLT.pdf", "_fossil": "localFileMetadata", "id": "796738", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/379", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "378", "speakers": [{"_type": "ContributionParticipation", "emailHash": "b6dd32dd39a9956eab36bb11a86ccbc1", "affiliation": "University of Malaya (MY)", "_fossil": "contributionParticipationMetadata", "fullName": "BIN ANUAR, Afiq Aizuddin", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "b6dd32dd39a9956eab36bb11a86ccbc1", "affiliation": "University of Malaya (MY)", "_fossil": "contributionParticipationMetadata", "fullName": "BIN ANUAR, Afiq Aizuddin", "id": "1"}], "title": "Electrons and photons at High Level Trigger in CMS for Run II", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T16:57:57.329904+00:00", "description": "", "title": "eyRun2_chep15_v3.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/378\/attachments\/578587\/796739\/eyRun2_chep15_v3.pdf", "filename": "eyRun2_chep15_v3.pdf", "content_type": "application\/pdf", "type": "file", "id": 796739, "size": 740599}], "title": "Poster", "default_folder": false, "id": 578587, "description": ""}], "_type": "Contribution", "description": "The CMS experiment has been designed with a 2-level trigger system. The first level is implemented using custom-designed electronics. The second level is the so-called High Level Trigger (HLT), a streamlined version of the CMS offline reconstruction software running on a computer farm. For Run II of the Large Hadron Collider, the increase in center-of-mass energy and luminosity will raise the event rate to a level challenging for the HLT algorithms. New approaches have been studied to keep the HLT output rate manageable while maintaining thresholds low enough to cover physics analyses. The strategy mainly relies on porting online the ingredients that have been successfully applied in the offline reconstruction, thus allowing to move HLT selection closer to offline cuts. Improvements in HLT electron and photon definitions will be presented, focusing in particular on: updated clustering algorithm and the energy calibration procedure, new Particle-Flow-based isolation approach and pileup mitigation techniques, and the electron-dedicated track fitting algorithm based on Gaussian Sum Filter.", "track": "Track1: Online computing ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578587", "resources": [{"_type": "LocalFile", "name": "eyRun2_chep15_v3.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/378\/attachments\/578587\/796739\/eyRun2_chep15_v3.pdf", "fileName": "eyRun2_chep15_v3.pdf", "_fossil": "localFileMetadata", "id": "796739", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/378", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "371", "speakers": [{"_type": "ContributionParticipation", "emailHash": "37a9d67c5498998436724740f824542b", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LETTS, James", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "37a9d67c5498998436724740f824542b", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LETTS, James", "id": "0"}], "title": "Pushing HTCondor and glideinWMS to 200K+ Jobs in a Global Pool for CMS before LHC Run 2", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T04:48:10.163612+00:00", "description": "", "title": "CHEP15-POSTER-FINAL.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/371\/attachments\/578588\/796740\/CHEP15-POSTER-FINAL.pdf", "filename": "CHEP15-POSTER-FINAL.pdf", "content_type": "application\/pdf", "type": "file", "id": 796740, "size": 1402202}], "title": "Poster", "default_folder": false, "id": 578588, "description": ""}], "_type": "Contribution", "description": "The CMS experiment at the LHC relies on HTCondor and glideinWMS as its primary batch and pilot-based Grid provisioning system. So far we have been running several independent resource pools, but we are working on unifying them all to reduce the operational load and more effectively share resources between various activities in CMS. The major challenge of this unification activity is scale. The combined pool size is expected to reach 200K job slots, which is significantly bigger than any other multi-user HTCondor based system currently in production. To get there we have studied scaling limitations in our existing pools, the biggest of which tops out at about 70K slots, providing valuable feedback to the development communities, who have responded by delivering improvements which have helped us reach higher and higher scales with more stability. We have also worked on improving the organization and support model for this critical service during Run 2 of the LHC. This contribution will present the results of the scale testing and experiences from the first months of running the Global Pool.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578588", "resources": [{"_type": "LocalFile", "name": "CHEP15-POSTER-FINAL.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/371\/attachments\/578588\/796740\/CHEP15-POSTER-FINAL.pdf", "fileName": "CHEP15-POSTER-FINAL.pdf", "_fossil": "localFileMetadata", "id": "796740", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "0e16821a7ef57745e150ed35cc005ab4", "affiliation": "Vilnius University (LT)", "_fossil": "contributionParticipationMetadata", "fullName": "BALCAS, Justas", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "d854ed989b482b2f04dd5615b5a9f643", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "BELFORTE, Stefano", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "e82c46a3a0d49dd555c888d113eace3a", "affiliation": "University of Nebraska (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BOCKELMAN, Brian Paul", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "19fbe91d19a3691e61eb5e432e34734a", "affiliation": "National Centre for Physics (PK)", "_fossil": "contributionParticipationMetadata", "fullName": "KHAN, Farrukh Aftab", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "35f4f869ebf48e550006a86e20965547", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MAJEWSKI, Krista", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "91b6e0458cfc38c6e2b8701f5a7b9a24", "affiliation": "Universita & INFN, Milano-Bicocca (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "MASCHERONI, Marco", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "8ce8233ffe359f39d3fdd125371fe45b", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MASON, David Alexander", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "c1a42d65e29a742bc3b359fe0eb2186c", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MC CREA, Alison", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "a85f2ccb15edffa1d06959fbd105561b", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "SAIZ SANTOS, Maria Dolores", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "8984f616935b79e3c3c569c457c49a5f", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "SFILIGOI, Igor", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "af4d3d3bb3f04635155be9787c300691", "affiliation": "Universidad de los Andes (CO)", "_fossil": "contributionParticipationMetadata", "fullName": "LINARES GARCIA, Luis Emiro", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "c64735b9540bc92c4d230a2ca252b7cf", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GUTSCHE, Oliver", "id": "12"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/371", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:45:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "370", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6c6329341dbb5e4332e61dba24d8e80b", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "LEWENDEL, Birgit", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "0da6b2477875372ceb9ed05d603c1070", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GELLRICH, Andreas", "id": "0"}], "title": "Challenge and Future of Job Distribution at a Multi-VO Grid Site", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T00:21:47.546123+00:00", "description": "", "title": "gellrich_chep2015_batch_20150413.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/370\/attachments\/578589\/796741\/gellrich_chep2015_batch_20150413.pdf", "filename": "gellrich_chep2015_batch_20150413.pdf", "content_type": "application\/pdf", "type": "file", "id": 796741, "size": 1021027}], "title": "Slides", "default_folder": false, "id": 578589, "description": ""}], "_type": "Contribution", "description": "DESY operates a multi-VO Grid site for 20 HEP and non-HEP\r\ncollaborations and is one of the world-wide largest Tier-2 sites for\r\nATLAS, CMS, LHCb, and BELLE2. In one common Grid infrastructure\r\ncomputing resources are shared by all VOs according to MoUs and\r\nagreements, applying an opportunistic usage model allows to distribute\r\nfree resources among the VOs. Currently, the Grid site DESY-HH\r\nprovides roughly 100kHS06 in 10000 job slots, exploiting the queueing\r\nsystem PBS\/TORQUE.\r\n\r\nAs described in former CHEP conferences, resource utilization and job\r\nscheduling in a multi-VO environment is a major challenge. On one hand\r\nside all job slots should be occupied, on the other hand jobs with\r\ndiverging resource usage patterns must be cleverly distributed to\r\nthe compute nodes in order to guarantee stability and optimal resource\r\nusage.\r\n\r\nBatch systems such as PBS\/TORQUE with the scheduler MAUI only scale up\r\nto a few thousand job slots. At DESY-HH an alternative scheduler was\r\ndeveloped and brought into operation.\r\n\r\nIn the preparation for LHC Run 2 as well as the start of BELLE2,\r\nin particular the request for the support of multi-core jobs\r\nrequires appropriate job scheduling strategies which are not\r\navailable out-of-the-box. Even more, the operation of work load\r\nmanagements system and pilot factories (DIRAC, PANDA) by the big\r\ncollaborations question the way of how sites provide computing\r\nresources in the future. Is cloud computing the future? What about\r\nsmall VOs and individual users who use the standard Grid tools to\r\nsubmit jobs then?\r\n\r\nIn the contribution to CHEP2015 we will try the review what has been\r\nlearned by operating a large Grid site for many VOs. We will give a\r\nsummary of the experiences with the job scheduling at DESY-HH of the\r\nlast years and we will describe limits of the current system.\r\nA main focus will be put on the discussion of future scenarios,\r\nincluding alternatives to the approach of local resource managements\r\nsystems (LRMS) which are still widely used.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578589", "resources": [{"_type": "LocalFile", "name": "gellrich_chep2015_batch_20150413.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/370\/attachments\/578589\/796741\/gellrich_chep2015_batch_20150413.pdf", "fileName": "gellrich_chep2015_batch_20150413.pdf", "_fossil": "localFileMetadata", "id": "796741", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "6c6329341dbb5e4332e61dba24d8e80b", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "LEWENDEL, Birgit", "id": "2"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/370", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:15:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "373", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5464ee52c2740264980ed36d4553e207", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GANIS, Gerardo", "id": "4"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "771122777d10e8f8dc1c3aa7156922e8", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BLOMER, Jakob", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "69b973a64a9f8889a446059e1db57cff", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BERZANO, Dario", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "3f41c6112486480c3ebd506d39003e9e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BUNCIC, Predrag", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "8c3829aeb9782b9da4c2474b6257cbab", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHARALAMPIDIS, Ioannis", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "5464ee52c2740264980ed36d4553e207", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GANIS, Gerardo", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "b7670e16208d58f21cb7eff8f0036404", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MEUSEL, Rene", "id": "5"}], "title": "Status and Roadmap of CernVM", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T16:48:12.748078+00:00", "description": "", "title": "Ganis-CernVM.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/373\/attachments\/578590\/796742\/Ganis-CernVM.pdf", "filename": "Ganis-CernVM.pdf", "content_type": "application\/pdf", "type": "file", "id": 796742, "size": 6161054}], "title": "Slides", "default_folder": false, "id": 578590, "description": ""}], "_type": "Contribution", "description": "Cloud resources nowadays contribute an essential share of resources for computing in high-energy physics.  Such resources can be either provided by private or public IaaS clouds (e.g. OpenStack, Amazon EC2, Google Compute Engine) or by volunteers\u2019 computers (e.g. LHC@Home 2.0).  In any case, experiments need to prepare a virtual machine image that provides the execution environment for the physics application at hand.\r\nThe CernVM virtual machine since version 3 is a minimal and versatile virtual machine image capable of booting different operating systems. The virtual machine image is less than 20 megabyte in size.  The actual operating system is delivered on demand by the CernVM File System.\r\nCernVM 3 has matured from a prototype to a production environment. It is used, for instance, to run LHC applications in the cloud, to tune event generators using a network of volunteer computers, and as a container for the historic Scientific Linux 5 and Scientific Linux 4\r\nbased software environments in the course of long-term data preservation efforts of the ALICE, CMS, and ALEPH experiments. We present experience and lessons learned from the use of CernVM at scale.  We also provide an outlook on the upcoming developments. These developments include adding support for Scientific Linux 7, the use of container virtualization, such as provided by Docker, and the streamlining of virtual machine contextualization towards the cloud-init industry standard.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578590", "resources": [{"_type": "LocalFile", "name": "Ganis-CernVM.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/373\/attachments\/578590\/796742\/Ganis-CernVM.pdf", "fileName": "Ganis-CernVM.pdf", "_fossil": "localFileMetadata", "id": "796742", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/373", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "372", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2dcd8c5c21bc7166cc6d20d6c319a77b", "affiliation": "University of Notre Dame (US)", "_fossil": "contributionParticipationMetadata", "fullName": "HILDRETH, Mike", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2dcd8c5c21bc7166cc6d20d6c319a77b", "affiliation": "University of Notre Dame (US)", "_fossil": "contributionParticipationMetadata", "fullName": "HILDRETH, Mike", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "c0b3060a53bda8bb81b493ff994219dd", "affiliation": "Department of Physics-College of Science-University of Notre Da", "_fossil": "contributionParticipationMetadata", "fullName": "HILDRETH, Mike", "id": "1"}], "title": "A New Pileup Mixing Framework for CMS", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T20:37:15.069050+00:00", "description": "", "title": "CHEP2015-premixing.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/372\/attachments\/578591\/796743\/CHEP2015-premixing.pdf", "filename": "CHEP2015-premixing.pdf", "content_type": "application\/pdf", "type": "file", "id": 796743, "size": 756284}], "title": "Poster", "default_folder": false, "id": 578591, "description": ""}], "_type": "Contribution", "description": "The CMS Simulation uses minimum bias events created by a \"standard\" event generator (e.g., Pythia) to simulate the additional interactions due to peripheral proton-proton collisions in each bunch crossing at the LHC (also known as pileup). Due to the inherent time constants of the CMS front-end electronics, many bunch crossings before and after the central bunch crossing of interest must be included in the simulation, leading to hundreds of minimum bias events being used for each simulated hard-scatter event.\r\n\r\nWe report on the performance gains in I\/O load and computational speed made possible by a new framework that allows the combination of pileup events in a \"pre-mixing\" step. This has been made possible by the development of software that allows single-channel information to be combined at the digitization level in each sub-detector, rather than accumulating simulated hits from Geant. The logistics of large-scale production with pre-fabricated pileup distributions is described.", "track": "Track2: Offline software ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578591", "resources": [{"_type": "LocalFile", "name": "CHEP2015-premixing.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/372\/attachments\/578591\/796743\/CHEP2015-premixing.pdf", "fileName": "CHEP2015-premixing.pdf", "_fossil": "localFileMetadata", "id": "796743", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "3c7360aec296f9fa798132fbdab53878", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "IVANTCHENKO, Vladimir", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "a7515791a515182e25805d5455cecec2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. IVANTCHENKO, Vladimir", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/372", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:15:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "375", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a0edda8c32b7a5c82a1cf049171b0206", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BOCCI, Andrea", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "a0edda8c32b7a5c82a1cf049171b0206", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BOCCI, Andrea", "id": "1"}], "title": "The CMS High Level Trigger", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T04:13:59.032603+00:00", "description": "", "title": "Bocci_-_The_CMS_High_Level_Trigger.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/375\/attachments\/578592\/796744\/Bocci_-_The_CMS_High_Level_Trigger.pdf", "filename": "Bocci_-_The_CMS_High_Level_Trigger.pdf", "content_type": "application\/pdf", "type": "file", "id": 796744, "size": 4515738}], "title": "Slides", "default_folder": false, "id": 578592, "description": ""}], "_type": "Contribution", "description": "The CMS experiment has been designed with a 2-level trigger system: the Level 1 Trigger, implemented on custom-designed electronics, and the High Level Trigger (HLT), a streamlined version of the CMS offline reconstruction software running on a computer farm. A software trigger system requires a tradeoff between the complexity of the algorithms running on the available computing power, the sustainable output rate, and the selection efficiency. Here we will present the performance of the main triggers used during the 2012 data taking, ranging from simpler single-object selections to more complex algorithms combining different objects, and applying analysis-level reconstruction and selection. We will discuss the optimisation of the triggers and the specific techniques developed to cope with the increasing LHC pile-up, reducing its impact on the physics performance.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578592", "resources": [{"_type": "LocalFile", "name": "Bocci_-_The_CMS_High_Level_Trigger.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/375\/attachments\/578592\/796744\/Bocci_-_The_CMS_High_Level_Trigger.pdf", "fileName": "Bocci_-_The_CMS_High_Level_Trigger.pdf", "_fossil": "localFileMetadata", "id": "796744", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/375", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:15:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "374", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ad1e385a5d5654c82f66fc964f2065f1", "affiliation": "University of Cambridge (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "COLES, Jeremy", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ad1e385a5d5654c82f66fc964f2065f1", "affiliation": "University of Cambridge (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "COLES, Jeremy", "id": "0"}], "title": "GridPP \u2013 preparing for Run-2 and the wider context", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T08:07:14.310171+00:00", "description": "", "title": "Jeremy-Coles-CHEP15-1.0.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/374\/attachments\/578593\/796745\/Jeremy-Coles-CHEP15-1.0.pdf", "filename": "Jeremy-Coles-CHEP15-1.0.pdf", "content_type": "application\/pdf", "type": "file", "id": 796745, "size": 8049832}], "title": "Slides", "default_folder": false, "id": 578593, "description": ""}], "_type": "Contribution", "description": "This first section of this paper elaborates upon the operational status and directions within the UK Computing for Particle Physics (GridPP) project as we approach LHC Run-2. It details the pressures that have been gradually reshaping the deployed hardware and middleware environments at GridPP sites \u2013 from the increasing adoption of larger multicore nodes to the move towards alternative batch systems and cloud alternatives - as well as changes being driven by funding considerations. The second section focuses on work being done with non-LHC communities, communities that GridPP has supported for many years with varying degrees of engagement, and lays out findings of a survey into where such communities desire more support and their changing requirements. The section also explores some of the early outcomes of adopting a generic DIRAC based job submission and management framework within GridPP. The third and final section of the paper highlights changes being made in GridPP operations in order to meet new challenges that are arising as recent technical advancements (in areas such as cloud\/VM provision) reach production readiness.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578593", "resources": [{"_type": "LocalFile", "name": "Jeremy-Coles-CHEP15-1.0.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/374\/attachments\/578593\/796745\/Jeremy-Coles-CHEP15-1.0.pdf", "fileName": "Jeremy-Coles-CHEP15-1.0.pdf", "_fossil": "localFileMetadata", "id": "796745", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/374", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "377", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2f49f5bc4cec81c95572eee52b27ec0e", "affiliation": "Purdue University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "RADBURN-SMITH, Benjamin", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2f49f5bc4cec81c95572eee52b27ec0e", "affiliation": "Purdue University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "RADBURN-SMITH, Benjamin", "id": "1"}], "title": "Performance of muon-based triggers at the CMS High Level Trigger", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T03:54:19.248570+00:00", "description": "", "title": "CMSMuonHLTPoster_B_Radburn-Smith.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/377\/attachments\/578594\/796746\/CMSMuonHLTPoster_B_Radburn-Smith.pdf", "filename": "CMSMuonHLTPoster_B_Radburn-Smith.pdf", "content_type": "application\/pdf", "type": "file", "id": 796746, "size": 2043382}], "title": "Slides", "default_folder": false, "id": 578594, "description": ""}], "_type": "Contribution", "description": "The trigger systems of LHC detectors play a fundamental role in defining the physics capabilities of the experiments. A reduction of several orders of magnitude in the rate of collected events, with respect to the proton-proton bunch crossing rate generated by the LHC, is mandatory to cope with the limits imposed by the readout and storage systems limits. An accurate and efficient online selection mechanism is thus required to fulfill the task keeping maximal the acceptance to physics signals. The CMS experiment operates using a two-level trigger system. Firstly a Level-1 Trigger (L1T) system, implemented using custom-designed electronics, is designed to reduce the event rate to a limit compatible to the CMS Data Acquisition (DAQ) capabilities. A High Level Trigger System (HLT) follows, aimed at further reducing the rate of collected events finally stored for analysis purposes. The latter consists of a streamlined version of the CMS offline reconstruction software and operates on a computer farm. It runs algorithms optimized to make a trade-off between computational complexity, rate reduction and high selection efficiency. With the computing power available in 2012 the maximum reconstruction time at HLT was about 200 ms per event, at the nominal L1T rate of 100 kHz. An efficient selection of muons at HLT, as well as an accurate measurement of their properties, such as transverse momentum and isolation, is fundamental for the CMS physics programme. The performance of the muon HLT for single and double muon triggers achieved in Run I will be presented. Results from new developments, aimed at improving the performance of the algorithms for the harsher scenarios of collisions per event (pile-up) and luminosity expected for Run II will also be discussed.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578594", "resources": [{"_type": "LocalFile", "name": "CMSMuonHLTPoster_B_Radburn-Smith.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/377\/attachments\/578594\/796746\/CMSMuonHLTPoster_B_Radburn-Smith.pdf", "fileName": "CMSMuonHLTPoster_B_Radburn-Smith.pdf", "_fossil": "localFileMetadata", "id": "796746", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/377", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:45:00"}, "duration": 15, "session": "Track 8 Session", "keywords": [], "id": "376", "speakers": [{"_type": "ContributionParticipation", "emailHash": "771122777d10e8f8dc1c3aa7156922e8", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BLOMER, Jakob", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "771122777d10e8f8dc1c3aa7156922e8", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BLOMER, Jakob", "id": "0"}], "title": "Large-Scale Merging of Histograms using Distributed In-Memory Computing", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T14:20:37.395449+00:00", "description": "", "title": "histmerge.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/376\/attachments\/578595\/796747\/histmerge.pdf", "filename": "histmerge.pdf", "content_type": "application\/pdf", "type": "file", "id": 796747, "size": 908775}], "title": "Slides", "default_folder": false, "id": 578595, "description": ""}], "_type": "Contribution", "description": "Most high-energy physics analysis jobs are embarrassingly parallel except for the final merging of the output objects, which are typically histograms. Currently, the merging of output histograms scales badly. The running time for distributed merging depends not only on the overall number of bins but \r\nalso on the number partial histogram output files. That means, while the time to analyze data decreases linearly with the number of worker nodes, the time to merge the histograms in fact increases with the number of worker nodes.\r\nOn the grid, merging jobs that take a few hours are not unusual.  In order to improve the situation, we present a distributed and decentral merging algorithm whose running time is independent of the number of worker nodes. We exploit full bisection bandwidth of local networks and we keep all intermediate results in memory. We present benchmarks from an implementation using the parallel ROOT facility (PROOF) and RAMCloud, a distributed key-value store that keeps all data in DRAM.  Our results show that a real-world collection of ten thousand histograms with overall ten million non-zero bins can be merged in less than one minute.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578595", "resources": [{"_type": "LocalFile", "name": "histmerge.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/376\/attachments\/578595\/796747\/histmerge.pdf", "fileName": "histmerge.pdf", "_fossil": "localFileMetadata", "id": "796747", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "5464ee52c2740264980ed36d4553e207", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GANIS, Gerardo", "id": "1"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/376", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:15:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "393", "speakers": [{"_type": "ContributionParticipation", "emailHash": "380264f2badcf3ffc4b02276132e6993", "affiliation": "ITEP Institute for Theoretical and Experimental Physics (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "BARANOV, Alexander", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c10fdf0d2ddefd23d451c9de740427fe", "affiliation": "ITEP Institute for Theoretical and Experimental Physics (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "USTYUZHANIN, Andrey", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "380264f2badcf3ffc4b02276132e6993", "affiliation": "ITEP Institute for Theoretical and Experimental Physics (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "BARANOV, Alexander", "id": "1"}], "title": "SkyGrid - where cloud meets grid computing", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T05:25:42.369694+00:00", "description": "", "title": "skygrid.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/393\/attachments\/578596\/796748\/skygrid.pdf", "filename": "skygrid.pdf", "content_type": "application\/pdf", "type": "file", "id": 796748, "size": 1034976}], "title": "Slides", "default_folder": false, "id": 578596, "description": ""}], "_type": "Contribution", "description": "Computational grid (or simply 'grid') infrastructures are powerful but restricted by several aspects: grids are incapable of running user jobs compiled with a non-authentic set of libraries and it is difficult to restructure grids to adapt to peak loads. At the same time if grids are not loaded with user-tasks, owners still have to pay for electricity and hardware maintenance. So a grid is not cheap and small\/medium scientific experiments have difficulties working with such a computational model.\r\n\r\nTo address these inflexibility issues we present SkyGrid - a system that integrates cloud technologies into grid systems. By cloud technologies we mean mainly virtualization that allows both virtualization of user jobs and computational hardware. Virtualization of user jobs is performed by means of Docker - a lightweight Virtual Machine system. Virtualization of hardware is provided by YARN or similar platforms. SkyGrid unties users from the computational cluster architecture and configuration. Also the virtualization approach we propose enables some extreme cases like volunteer computing inside browsers by means of PNaCl technology and running jobs on super-computers. In this paper we present the requirements and architecture of SkyGrid, interfaces available to end-users and interfaces to other systems. Also we provide a description of a real case study of system usage for the SHiP experiment integrating private cloud resources from several universities as well as volunteer computing resources into a single computational infrastructure.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578596", "resources": [{"_type": "LocalFile", "name": "skygrid.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/393\/attachments\/578596\/796748\/skygrid.pdf", "fileName": "skygrid.pdf", "_fossil": "localFileMetadata", "id": "796748", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "94e2156da82d385a3fe5afe09c0e37a3", "affiliation": "Yandex", "_fossil": "contributionParticipationMetadata", "fullName": "NIKITIN, Konstantin", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "e5279897985c9ecca7d3c450db7d3f83", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "SARIGIANNIS, Dimitris", "id": "3"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/393", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "392", "speakers": [{"_type": "ContributionParticipation", "emailHash": "d2481eb1f11b08f40d4e1ef3cdb920dc", "affiliation": "University of Edinburgh (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "WASHBROOK, Andrew John", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "d2481eb1f11b08f40d4e1ef3cdb920dc", "affiliation": "University of Edinburgh (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "WASHBROOK, Andrew John", "id": "0"}], "title": "Acceleration of ensemble machine learning methods using many-core devices", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T10:36:13.269257+00:00", "description": "", "title": "CHEPML.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/392\/attachments\/578597\/796749\/CHEPML.pdf", "filename": "CHEPML.pdf", "content_type": "application\/pdf", "type": "file", "id": 796749, "size": 1701440}], "title": "Poster", "default_folder": false, "id": 578597, "description": ""}], "_type": "Contribution", "description": "Multivariate training and classification methods using machine learning techniques are commonly applied in data analysis at HEP experiments. Despite their success in looking for signatures of new physics beyond the standard model it is known that some of these techniques are computationally bound when input sample size and model complexity are increased. Investigating opportunities for potential performance improvements is therefore of great importance if these techniques are to be used with the much larger data volumes expected from Run 2 operations at the Large Hadron Collider. \r\n\r\nIt has been previously shown that a large degree of algorithm parallelisation was possible for MLP-based artificial neural networks by the use of many-core devices such as GPUs. Improved scaling was observed in network complexity and qualitative performance gains were attainable through the simultaneous processing of multiple neural networks. Here we investigate how many-core devices can be used to accelerate ensemble machine learning methods that are gaining traction in HEP data analysis.\r\n\r\nWe present a case study into the acceleration of decision forests using many-core devices in collaboration with Toshiba Medical Visualisation Systems Europe (TMVSE). TMVSE have developed software to process three-dimensional medical imaging data (such as CT or MRI scans), using automatic detection of anatomical landmarks defined on the skeleton, vasculature and major organs. Landmark detection underpins a semantic understanding of the medical data and thus has many diverse applications, for example, it facilitates rapid navigation to a named organ.  TMVSE have applied ensemble machine learning methods such as classification by random decision forests to efficiently compute the bounding boxes of organs in processed image data volumes. It is important that their applications using this algorithm runs efficiently and quickly. After data preparation and optimisation the execution time is on average 4.5 seconds per volume with a sub-second processing time being desirable.\r\n\r\nUsing representative medical image data as input and pre-trained decision trees we will demonstrate how the decision forest classification method maps onto the GPU data processing model. It was found that a GPU-based version of the classification method resulted in over 130 times speed-up over a single-threaded CPU implementation with further improvements possible. We will outline the main optimisation steps undertaken to maximise GPU performance and detail how this was implemented using device profiling to evaluate thread occupancy and execution efficiency. \r\n\r\nAs this solution was developed to be context independent we will demonstrate how this work can be applied to a suitably formed HEP dataset to determine potential gains in event throughput and classifier discrimination. We will also explore how the advanced analysis techniques applied to automatic landmark detection in medical data can be applied to HEP dataset to achieve further increases in performance.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578597", "resources": [{"_type": "LocalFile", "name": "CHEPML.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/392\/attachments\/578597\/796749\/CHEPML.pdf", "fileName": "CHEPML.pdf", "_fossil": "localFileMetadata", "id": "796749", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "139b8ad05646cb6f5a5778835faad03e", "affiliation": "Toshiba Medical Visualisation Systems Europe (TMVSE)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. DANIEL, Wyeth", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/392", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:00:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "88", "speakers": [{"_type": "ContributionParticipation", "emailHash": "996159099c64d7973bfbed1b3009a495", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BARRING, Olof", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "996159099c64d7973bfbed1b3009a495", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BARRING, Olof", "id": "0"}], "title": "Experience of public procurement of Open Compute servers", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T07:18:47.068984+00:00", "description": "", "title": "CHEP2015-OCP.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/88\/attachments\/578598\/796750\/CHEP2015-OCP.pdf", "filename": "CHEP2015-OCP.pdf", "content_type": "application\/pdf", "type": "file", "id": 796750, "size": 452892}, {"_type": "attachment", "modified_dt": "2015-04-12T07:18:47.068984+00:00", "description": "", "title": "CHEP2015-OCP.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/88\/attachments\/578598\/796751\/CHEP2015-OCP.pptx", "filename": "CHEP2015-OCP.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796751, "size": 3443752}], "title": "Slides", "default_folder": false, "id": 578598, "description": ""}], "_type": "Contribution", "description": "The Open Compute Project, OCP ( http:\/\/www.opencompute.org\/ (link is external)), was launched by Facebook in 2011 with the objective of building efficient computing infrastructures at lowest possible cost. The technologies are released as open hardware, with the goal to develop servers and data centers following the model traditionally associated with open source software projects. In 2013 CERN acquired a few OCP servers in order to compare performance and power consumption with standard hardware. The conclusions were that there are sufficient savings to motivate an attempt to procure a large scale installation. One objective is to evaluate if the OCP market is sufficiently mature and broad to meet the constrains of a public procurement. This talk will summarize this procurement, which started in September 2014 and involved the Request for information (RFI) to qualify bidders and Request for Tender (RFT).", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578598", "resources": [{"_type": "LocalFile", "name": "CHEP2015-OCP.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/88\/attachments\/578598\/796750\/CHEP2015-OCP.pdf", "fileName": "CHEP2015-OCP.pdf", "_fossil": "localFileMetadata", "id": "796750", "_deprecated": true}, {"_type": "LocalFile", "name": "CHEP2015-OCP.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/88\/attachments\/578598\/796751\/CHEP2015-OCP.pptx", "fileName": "CHEP2015-OCP.pptx", "_fossil": "localFileMetadata", "id": "796751", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "aa40533fe06734bf319516b35f05aa44", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GUERRI, Marco", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "b94173f79e84065afd99f7fd2b9aacaa", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BONFILLOU, Eric", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "fa3e9264ec44ee5d288d2ac8f2f83326", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "VALSAN, Liviu", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "b5c507091d31aecc4c678941b583f0f8", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GRIGORE, Alexandru", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "69df43ff999102b4eeb9d6ddb551a0cf", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DORE, Vincent", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "137b8cb8592ca8dfb85132bf6828231c", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CLEMENT, Benoit", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "f3993c9fdee59e9893002c5b47a727c2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GENTIT, Alain", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "794802c2356f242fc08bae574a419bfe", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GROSSIR, Anthony", "id": "8"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/88", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "89", "speakers": [{"_type": "ContributionParticipation", "emailHash": "244c8becc3b603c6e5722f14640d8e65", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "COUTURIER, Ben", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "244c8becc3b603c6e5722f14640d8e65", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "COUTURIER, Ben", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "2d3197cea2de7705cbaf2df989cdf1da", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CLEMENCIC, Marco", "id": "1"}], "title": "LHCb Build and Deployment Infrastructure for RUNII", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-08T11:10:07.088706+00:00", "description": "", "title": "CHEP2015-BuildRun2-f.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/89\/attachments\/578599\/796752\/CHEP2015-BuildRun2-f.pdf", "filename": "CHEP2015-BuildRun2-f.pdf", "content_type": "application\/pdf", "type": "file", "id": 796752, "size": 1930712}], "title": "Slides", "default_folder": false, "id": 578599, "description": ""}], "_type": "Contribution", "description": "After the successful RUN I of the LHC, the LHCb Core software team has taken advantage of the long shutdown to consolidate and improve its build and deployment infrastructure. Several of the related projects have already been presented like the build system using Jenkins, as well as the LHCb Performance and regression testing infrastructure. Some components are completely new, like the Software Configuration Database (using the Graph DB Neo4j), or the new packaging installation using RPM packages. Furthermore all those parts are integrated to allow easier and quicker releases of the LHCb Software stack, therefore reducing the risk of operational errors. Integration and Regression tests are also now easier to implement, allowing to improve further the software checks.\r\n\r\nIn this poster we describe the various components of the infrastructure and how they fit together to finally allow nearly automated deployment.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578599", "resources": [{"_type": "LocalFile", "name": "CHEP2015-BuildRun2-f.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/89\/attachments\/578599\/796752\/CHEP2015-BuildRun2-f.pdf", "fileName": "CHEP2015-BuildRun2-f.pdf", "_fossil": "localFileMetadata", "id": "796752", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/89", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "397", "speakers": [{"_type": "ContributionParticipation", "emailHash": "380264f2badcf3ffc4b02276132e6993", "affiliation": "ITEP Institute for Theoretical and Experimental Physics (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "BARANOV, Alexander", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "380264f2badcf3ffc4b02276132e6993", "affiliation": "ITEP Institute for Theoretical and Experimental Physics (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "BARANOV, Alexander", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "c10fdf0d2ddefd23d451c9de740427fe", "affiliation": "ITEP Institute for Theoretical and Experimental Physics (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "USTYUZHANIN, Andrey", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "94e2156da82d385a3fe5afe09c0e37a3", "affiliation": "Yandex School of Data Analysis, Moscow", "_fossil": "contributionParticipationMetadata", "fullName": "NIKITIN, Konstantin", "id": "2"}], "title": "Towards generic volunteer computing platform", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T21:36:58.238433+00:00", "description": "", "title": "presentation.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/397\/attachments\/578600\/796753\/presentation.pdf", "filename": "presentation.pdf", "content_type": "application\/pdf", "type": "file", "id": 796753, "size": 146763}], "title": "Poster", "default_folder": false, "id": 578600, "description": ""}], "_type": "Contribution", "description": "Computational power that is distributed amongst user hardware: laptops, PCs and even smartphones is enormous. It is not exceptional that volunteer computational networks provide computational power comparable to the power of the modern supercomputers. The problem is that utilization of those resources is difficult from volunteer (user) point of view as well as from computation provider (client) point of view. Users have to install special software, clients have to convert essential computational algorithms into special format and test it thoroughly.\r\nDibrocop project makes an attempt to use modern in-browser computation platforms to solve those problems. Modern browsers have special code paths for low overhead calculations, e.g. ASM.js by Firefox and PNaCl by Chromium. In contrast to SETI@Home and other public volunteer computing projects, our approach requires no software installation and starts as soon as you open provider's page in your web browser. It doesn't have any requirements for operating system or device capabilities.\r\nIn this paper we are going to share our experience in moving computational tasks for Monte Carlo event simulation and processing from regular GRID resources to ASM.js and PNaCl. Algorithms used for demonstration of Dibrocop are taken from newly developed SHiP experiment at CERN.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578600", "resources": [{"_type": "LocalFile", "name": "presentation.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/397\/attachments\/578600\/796753\/presentation.pdf", "fileName": "presentation.pdf", "_fossil": "localFileMetadata", "id": "796753", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/397", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:00:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "396", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6f31143d6e40d8f7574d6928ac862d64", "affiliation": "Universite Claude Bernard-Lyon I (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "BOUDOUL, Gaelle", "id": "5"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "f21b8b4cdfe52093bc94315ab210d323", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mrs. OSBORNE, Ianna", "id": "0"}], "title": "CMS Detector Description for Run II and Beyond", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T08:48:43.144029+00:00", "description": "", "title": "CHEP2015_TalkGB.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/396\/attachments\/578601\/796754\/CHEP2015_TalkGB.pdf", "filename": "CHEP2015_TalkGB.pdf", "content_type": "application\/pdf", "type": "file", "id": 796754, "size": 5073683}], "title": "Slides", "default_folder": false, "id": 578601, "description": ""}], "_type": "Contribution", "description": "CMS Detector Description (DD) is an integral part of the CMSSW software multithreaded framework. CMS software has evolved to be more flexible and to take advantage of new techniques, but many of the original concepts remain and are in active use. In this presentation we will discuss the limitations of the Run I DD model and changes implemented for the restart of the LHC program in 2015. Responses to Run II challenges and transition to multithreaded environment are discussed.\r\n\r\nThe DD is a common source of information for Simulation, Reconstruction, Analysis, and Visualisation, while allowing for different representations as well as specific information for each application. The DD model usage for CMS Magnetic field map description allows seamless access to variable field during the same run. Examples of the integration of DD in the GEANT4 simulation and in the reconstruction applications are provided.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578601", "resources": [{"_type": "LocalFile", "name": "CHEP2015_TalkGB.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/396\/attachments\/578601\/796754\/CHEP2015_TalkGB.pdf", "fileName": "CHEP2015_TalkGB.pdf", "_fossil": "localFileMetadata", "id": "796754", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "b3766e1bb47ff6f9cdf65d395d9c0ad8", "affiliation": "Lawrence Livermore Nat. Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LANGE, David", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "e8fb00f4f09841337efaeff677169cd2", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "SEXTON-KENNEDY, Elizabeth", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "64570537e9fce31fc2add6947b9869d1", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. JONES, Christopher", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "7a6c8f49981d093b41b365a4b522378d", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. EULISSE, Giulio", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "6f31143d6e40d8f7574d6928ac862d64", "affiliation": "Universite Claude Bernard-Lyon I (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "BOUDOUL, Gaelle", "id": "5"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/396", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "395", "speakers": [{"_type": "ContributionParticipation", "emailHash": "c10fdf0d2ddefd23d451c9de740427fe", "affiliation": "ITEP Institute for Theoretical and Experimental Physics (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "USTYUZHANIN, Andrey", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c10fdf0d2ddefd23d451c9de740427fe", "affiliation": "ITEP Institute for Theoretical and Experimental Physics (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "USTYUZHANIN, Andrey", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "380264f2badcf3ffc4b02276132e6993", "affiliation": "ITEP Institute for Theoretical and Experimental Physics (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "BARANOV, Alexander", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "e969bfc74c003a434b55f356d830dc30", "affiliation": "National Research Centre Kurchatov Institute (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "LIKHOMANENKO, Tatiana", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "2ae03a3bd281e7b4dbb4fd56ebc47fa5", "affiliation": "Yandex School of Data Analysis", "_fossil": "contributionParticipationMetadata", "fullName": "ARTEMOV, Alexey", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "1b6ee44b716a44b5e361d4273effc771", "affiliation": "Yandex School of Data Analysis, Moscow", "_fossil": "contributionParticipationMetadata", "fullName": "KHAIRULLIN, Egor", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "b2f951d3ba8f11dcf166097a97b8ec0e", "affiliation": "Yandex School of Data Analysis, Moscow", "_fossil": "contributionParticipationMetadata", "fullName": "ROGOZHNIKOV, Alexey", "id": "5"}], "title": "Towards Reproducible Experiment Platform", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T21:35:45.224142+00:00", "description": "", "title": "rep-qr.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/395\/attachments\/578602\/796755\/rep-qr.pdf", "filename": "rep-qr.pdf", "content_type": "application\/pdf", "type": "file", "id": 796755, "size": 3681241}], "title": "Poster", "default_folder": false, "id": 578602, "description": ""}], "_type": "Contribution", "description": "Data analysis in fundamental sciences nowadays is essential process that pushes frontiers of our knowledge and leads to new discoveries. At the same time we can see that complexity of those analysis increases exponentially due to a) enormous volumes of datasets being analyzed, b) variety of techniques and algorithms one have to check inside a single analysis, c) distributed nature of research teams that requires special communication media for knowledge and information exchange between individual researchers. There is a lot of resemblance between techniques and problems arising in the areas of industrial information retrieval and particle physics. To address those problems we propose a Reproducible Experiment Platform (REP) - a software infrastructure to support a collaborative ecosystem for computational science. It is a Python-based solution for research teams that allows running computational experiments on big shared datasets, obtaining reproducible and repeatable results, and consistent comparisons of the obtained results. REP supports many data formats including ROOT, allowing for easy integration with existing HEP software and analyses. We present some key features of REP based on case studies which include trigger optimization and physics analysis studies at the LHCb experiment, as well as an example case of applying the prototype of such a system in Information Retrieval research that led to a performance increase of two orders of magnitude.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578602", "resources": [{"_type": "LocalFile", "name": "rep-qr.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/395\/attachments\/578602\/796755\/rep-qr.pdf", "fileName": "rep-qr.pdf", "_fossil": "localFileMetadata", "id": "796755", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "10fc0c32be829bc87d01d57897b28fe7", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "WILLIAMS, J Michael", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "0c16591cb302e19fdb5628b9015f5313", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GLIGOROV, Vladimir", "id": "7"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/395", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:45:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:00:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "394", "speakers": [{"_type": "ContributionParticipation", "emailHash": "04e50596c8885d2652d4665e3ee5d460", "affiliation": "NIKHEF (NL)", "_fossil": "contributionParticipationMetadata", "fullName": "GADATSCH, Stefan", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2cc9cb7c7530eac0ffa0f72d81e41aa5", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BAAK, Max", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "04e50596c8885d2652d4665e3ee5d460", "affiliation": "NIKHEF (NL)", "_fossil": "contributionParticipationMetadata", "fullName": "GADATSCH, Stefan", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "a37104521a0d05c2d6aa7dbaa0052d5f", "affiliation": "University of Edinburgh", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HARRINGTON, Robert", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "367529fac993f9961a7779b8db58bfac", "affiliation": "NIKHEF (NL)", "_fossil": "contributionParticipationMetadata", "fullName": "VERKERKE, Wouter", "id": "3"}], "title": "Interpolation between multi-dimensional histograms using a new non-linear moment morphing method", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T02:10:04.277857+00:00", "description": "", "title": "2015_04_16_CHEP_morphing.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/394\/attachments\/578603\/796756\/2015_04_16_CHEP_morphing.pdf", "filename": "2015_04_16_CHEP_morphing.pdf", "content_type": "application\/pdf", "type": "file", "id": 796756, "size": 1808357}], "title": "Slides", "default_folder": false, "id": 578603, "description": ""}], "_type": "Contribution", "description": "In particle physics experiments data analyses generally use Monte Carlo (MC) simulation templates to interpret the observed data. These simulated samples may depend on one or multiple model parameters, such as a shifting mass parameter, and a set of such samples may be required to scan over the various parameter values. Since detailed detector MC simulation can be time-consuming, there is often a need to interpolate between the limited number of available MC simulation templates. Only several interpolation techniques exist for this. For example, the statistical tests widely used in particle physics, e.g. for the discovery of Higgs boson, rely critically on continuous and smooth parametric models that describe the physics processes in the data.\r\n\r\nWe present a new template morphing technique, moment morphing, for the interpolation between multi-dimensional distribution templates based on one or multiple model parameters. Moment morphing is fast, numerically stable, and is not restricted in the number of input templates, the number of model parameters or the number of input observables. For the first time, statistical tests may include the impact of a non-factorizable response between different model parameters, where varying one model parameter at a time is insufficient to capture the full response function.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578603", "resources": [{"_type": "LocalFile", "name": "2015_04_16_CHEP_morphing.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/394\/attachments\/578603\/796756\/2015_04_16_CHEP_morphing.pdf", "fileName": "2015_04_16_CHEP_morphing.pdf", "_fossil": "localFileMetadata", "id": "796756", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/394", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:15:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "82", "speakers": [{"_type": "ContributionParticipation", "emailHash": "4e9e7952b27d828eef84d9d074cf4435", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MARQUINA, Miguel", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "4e9e7952b27d828eef84d9d074cf4435", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MARQUINA, Miguel", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "8eaa52c2690c8404ba595dd969b19de2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "HOIMYR, Nils", "id": "1"}], "title": "Towards a production volunteer computing infrastructure for HEP", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T03:17:00.215891+00:00", "description": "", "title": "Towards a prod. VC infrastructure...", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/82\/attachments\/578604\/796757\/CHEP2015.pdf", "filename": "CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796757, "size": 747202}], "title": "Slides", "default_folder": false, "id": 578604, "description": ""}], "_type": "Contribution", "description": "Using virtualisation with CernVM has emerged as a de-facto standard among HEP experiments; it allows for running of HEP analysis and simulation programs in cloud environments. Following the integration of virtualisation with BOINC and CernVM(link is external), first pioneered for simulation of event generation in the Theory group at CERN, the LHC experiments ATLAS, CMS and LHCb have all adopted volunteer computing as part of their strategy to benefit from opportunistic computing resources. This presentation will describe the current technology for volunteer computing and the evolution of the BOINC service at CERN from project support for LHC@home(link sends e-mail) towards a general service. It will also provide some recommendations for teams and experiments wishing to benefit from volunteer computing resources.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578604", "resources": [{"_type": "LocalFile", "name": "Towards a prod. VC infrastructure...", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/82\/attachments\/578604\/796757\/CHEP2015.pdf", "fileName": "CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796757", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "8dd0b72e1b79063c65bb8f2c49193111", "affiliation": "University of Jyvaskyla (FI)", "_fossil": "contributionParticipationMetadata", "fullName": "ASP, Tomi Juhani", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "1a61da27119377d7a87b45f422b98b93", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "JONES, Pete", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "90a44128c8a73a712af44c2bc1647406", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GONZALEZ ALVAREZ, Alvaro", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "6049ebfaa694e419b61cfd107f553289", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FIELD, Laurence", "id": "5"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/82", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "181", "speakers": [{"_type": "ContributionParticipation", "emailHash": "1018f3caf155c881ab683fbc6288fc9b", "affiliation": "Centre National de la Recherche Scientifique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "ODIER, Jerome", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "1018f3caf155c881ab683fbc6288fc9b", "affiliation": "Centre National de la Recherche Scientifique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "ODIER, Jerome", "id": "0"}], "title": "Evolution of the Architecture of the ATLAS Metadata Interface (AMI)", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T10:15:44.265737+00:00", "description": "", "title": "poster2.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/181\/attachments\/578605\/796758\/poster2.pdf", "filename": "poster2.pdf", "content_type": "application\/pdf", "type": "file", "id": 796758, "size": 6019971}], "title": "Diapositives", "default_folder": false, "id": 578605, "description": ""}], "_type": "Contribution", "description": "The ATLAS Metadata Interface (AMI) can be considered to be a mature application because it has existed for at least 10 years. Over the years, the number of users and the number of functions provided for these users has increased. \r\n\r\nIt has been necessary to adapt the hardware infrastructure in a seamless way so that the Quality of Service remains high.\r\n \r\nWe will describe the evolution of the application from the initial one, using single server with a MySQL backend database, to the current state, where we use a cluster of Virtual Machines on the French Tier 1 Cloud at Lyon, an ORACLE database backend also at Lyon, with replication to  CERN using ORACLE streams behind a back-up server.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Diapositives", "_fossil": "materialMetadata", "id": "578605", "resources": [{"_type": "LocalFile", "name": "poster2.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/181\/attachments\/578605\/796758\/poster2.pdf", "fileName": "poster2.pdf", "_fossil": "localFileMetadata", "id": "796758", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "81248e43a9d5aed605684b827aafc8c3", "affiliation": "Centre National de la Recherche Scientifique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ALBRAND, Solveig", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "698cfd18ea89a5eb2c4029ba1c2cf381", "affiliation": "Centre National de la Recherche Scientifique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "FULACHIER, Jerome", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "851257e81d9d6fffbcac150e4477d19c", "affiliation": "Centre National de la Recherche Scientifique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "LAMBERT, Fabian", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "0217bc967e1c895085e524398d3ef421", "affiliation": "CNRS \/ CC-IN2P3", "_fossil": "contributionParticipationMetadata", "fullName": "AIDEL, Osman", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/181", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "399", "speakers": [{"_type": "ContributionParticipation", "emailHash": "00bea14561e2f2f6f019ee028eefe517", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. NIKIEL, Piotr Pawel", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "0235ada9cbf5c241ba3b6e116a33a5d7", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. FARNHAM, Benjamin", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "00bea14561e2f2f6f019ee028eefe517", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. NIKIEL, Piotr Pawel", "id": "0"}], "title": "A Generic Framework for Rapid Development of OPC UA Servers", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T16:40:34.250720+00:00", "description": "", "title": "CHEP2015_OpcUa_poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/399\/attachments\/578606\/796759\/CHEP2015_OpcUa_poster.pdf", "filename": "CHEP2015_OpcUa_poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796759, "size": 491975}], "title": "Slides", "default_folder": false, "id": 578606, "description": ""}], "_type": "Contribution", "description": "This paper describes a new approach for generic design and efficient development of OPC UA servers. Development starts with creation of a design file, in XML format, describing an object-oriented information model of the target system or device. Using this model, the framework generates an executable OPC UA server application, which exposes the per-design OPC UA address space, without the developer writing a single line of code. Furthermore, the framework generates skeleton code into which the developer adds the necessary logic for integration to the target system or device.\r\n \r\nThis approach allows both developers unfamiliar with the OPC UA standard, and advanced OPC UA developers, to create servers for the systems they are experts in while greatly reducing design and development effort as compared to developments based purely on COTS OPC UA toolkits. Higher level software may further benefit from the explicit OPC UA server model by using the XML design description as the basis for generating client connectivity configuration and server data representation. Moreover, having the XML design description at hand facilitates automatic generation of validation tools.\r\n\r\nIn this contribution, the concept and implementation of this framework\r\nis detailed along with examples of actual production-level usage in\r\nthe detector control system of the ATLAS experiment at CERN and\r\nbeyond.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578606", "resources": [{"_type": "LocalFile", "name": "CHEP2015_OpcUa_poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/399\/attachments\/578606\/796759\/CHEP2015_OpcUa_poster.pdf", "fileName": "CHEP2015_OpcUa_poster.pdf", "_fossil": "localFileMetadata", "id": "796759", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "cafcf790acb427ab9973100d30dced37", "affiliation": "B.P. Konstantinov Petersburg Nuclear Physics Institute - PNPI (", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. FILIMONOV, Viatcheslav", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "23fcbb9a9641637952b5609b385cec19", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SCHLENKER, Stefan", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "0235ada9cbf5c241ba3b6e116a33a5d7", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. FARNHAM, Benjamin", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/399", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Track 8 Session", "keywords": [], "id": "398", "speakers": [{"_type": "ContributionParticipation", "emailHash": "315bad2e5644e9920eac98b3f1e709b5", "affiliation": "NCSA", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. COARASA PEREZ, Jose Antonio", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "315bad2e5644e9920eac98b3f1e709b5", "affiliation": "NCSA", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. COARASA PEREZ, Jose Antonio", "id": "0"}], "title": "Standing up a data center for the Large Synoptic Survey Telescope at the National Center for Supercomputing applications", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [], "_type": "Contribution", "description": "The National Center for Supercomputing Applications (NCSA) at the University of Illinois at Urbana Champaign will provide the processing center for the Large Synoptic Survey Telescope (LSST) and its Data Access Center for the United States. The LSST is a planned wide-field survey reflecting telescope that will photograph the entire available sky every few nights. It will take 30 Terabytes of data nightly, and provide nearly instant alerts issued for objects that change in position or brightness.\r\n\r\nThe processing infrastructure at NCSA will receive the exposures from the LSST telescope in Chile over a dedicated, bandwidth protected network and it will process them for transient detections within a minute of their arrival producing around $10^6$  transients each night. The design and architecture of the system draws from High Energy Physics big system experiences.\r\n\r\nIn addition, NCSA will provide computing workflows for the annual release production. Initially, during the commissioning phase, NCSA will be the sole computing site. After operations commence, additional sites will most likely be added.\r\n\r\nLastly, NCSA will be the United States Data Access Center, providing data to the US Community, including the United States Dark Energy Science collaboration. \r\n\r\nIn this paper we revise the tools and solutions used to fulfill the aforementioned requirements and the requirements posed by the scale of the cluster.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "d383c91b2189022cdb1997b9ad96c06a", "affiliation": "NCSA", "_fossil": "contributionParticipationMetadata", "fullName": "DAUES, Gregory", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "39d16c2e25e02d18f68b6690d7dfd3d0", "affiliation": "NCSA", "_fossil": "contributionParticipationMetadata", "fullName": "ELLIOTT, Matt", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "53bbc14596bfcf886dddafa1ded7017a", "affiliation": "NCSA", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. FREEMON, Mike", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "0072fc921b58b45a58032043a3df6ce7", "affiliation": "NCSA", "_fossil": "contributionParticipationMetadata", "fullName": "GELMAN, Margaret", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "5c2930cc381a02d60e91b6fcf76bdef4", "affiliation": "NCSA", "_fossil": "contributionParticipationMetadata", "fullName": "GLICK, Bill", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "8c4f3d8654b4c0cd878d4f0e7af67ca2", "affiliation": "NCSA", "_fossil": "contributionParticipationMetadata", "fullName": "BRUCE, Mather", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "c5a4e6ceeef7323e031ec85429c40e04", "affiliation": "NCSA", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PETRAVICK, Donald", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "a96cd25fd47c9a9c9c62f26c1ff0909c", "affiliation": "NCSA", "_fossil": "contributionParticipationMetadata", "fullName": "PIETROWICZ, Stephen", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "35163f4b859c6b1908cfd63bad59765e", "affiliation": "NCSA", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. VOICU, Laura Cristiana", "id": "9"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/398", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:00:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "86", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a871c1e679a5c30dd80af66e12923b40", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SCHWICKERATH, Ulrich", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "a871c1e679a5c30dd80af66e12923b40", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SCHWICKERATH, Ulrich", "id": "0"}], "title": "Benchmarking and accounting for the (private) cloud", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T12:05:46.217127+00:00", "description": "", "title": "talk.odp", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/86\/attachments\/578607\/796760\/talk.odp", "filename": "talk.odp", "content_type": "application\/vnd.oasis.opendocument.presentation", "type": "file", "id": 796760, "size": 151380}, {"_type": "attachment", "modified_dt": "2015-04-12T12:05:46.217127+00:00", "description": "", "title": "talk.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/86\/attachments\/578607\/796761\/talk.pdf", "filename": "talk.pdf", "content_type": "application\/pdf", "type": "file", "id": 796761, "size": 526121}], "title": "Slides", "default_folder": false, "id": 578607, "description": ""}], "_type": "Contribution", "description": "As part of CERN's Agile Infrastructure project, large parts of the CERN batch farm have been moved to virtual machines running on CERNs private IaaS (link is external) cloud. During this process a large fraction of the resources, which had previously been used as physical batch worker nodes, were converted into hypervisors. Due to the large spread of the per-core performance (rated in HS06) in the farm, caused by its heterogenious nature, it is necessary to have a good knowledge of the performance of the virtual machines. This information is used both for scheduling and accounting. While in the previous setup worker nodes were classified and benchmarked based on the purchase order number, for virtual batch worker nodes this is no longer possible; the information is now either hidden or hard to retrieve. Therefore we developed a new scheme to classify worker nodes in terms of their performance. The new scheme is flexible enough to be usable both for virtual and physical machines in the batch farm. It should be possible to apply it as well to public clouds and more dynamic future batch farms with worker nodes coming and going at a high rate.\r\n\r\nThe used scheme, experiences and lessons learned will be presented. Possible extensions as well as application to a more general case, for example in the context of accounting within WLCG, will be covered.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578607", "resources": [{"_type": "LocalFile", "name": "talk.odp", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/86\/attachments\/578607\/796760\/talk.odp", "fileName": "talk.odp", "_fossil": "localFileMetadata", "id": "796760", "_deprecated": true}, {"_type": "LocalFile", "name": "talk.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/86\/attachments\/578607\/796761\/talk.pdf", "fileName": "talk.pdf", "_fossil": "localFileMetadata", "id": "796761", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "edfd1bebddd1922b9af428cd1ba23f6d", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BELLEMAN, Jerome", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "f08213deb97e773948296395e9644683", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "PEK, Janos Daniel", "id": "2"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/86", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:30:00"}, "duration": 15, "session": "Track 8 Session", "keywords": [], "id": "87", "speakers": [{"_type": "ContributionParticipation", "emailHash": "4baf69a0bb2b51552074e78969c8a0f5", "affiliation": "Johann-Wolfgang-Goethe Univ. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "ROHR, David Michael", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "4baf69a0bb2b51552074e78969c8a0f5", "affiliation": "Johann-Wolfgang-Goethe Univ. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "ROHR, David Michael", "id": "0"}], "title": "Fast TPC online tracking on GPUs and asynchronous data-processing in the ALICE HLT to enable online calibration", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T03:35:24.323785+00:00", "description": "", "title": "chep-2015-okinawa-david-rohr.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/87\/attachments\/578608\/796762\/chep-2015-okinawa-david-rohr.pdf", "filename": "chep-2015-okinawa-david-rohr.pdf", "content_type": "application\/pdf", "type": "file", "id": 796762, "size": 910701}], "title": "Slides", "default_folder": false, "id": 578608, "description": ""}], "_type": "Contribution", "description": "ALICE (A Large Heavy Ion Experiment) is one of the four major experiments at the Large Hadron Collider (LHC) at CERN, which is today the most powerful particle accelerator worldwide. The High Level Trigger (HLT) is an online compute farm of about 200 nodes, which reconstructs events measured by the ALICE detector in real-time. The HLT uses a custom online data-transport framework to distribute the data and the workload among the compute nodes.\r\n\r\nALICE employs several subdetectors that are sensitive to calibration, e.g. the TPC (Time Projection Chamber). For a precise reconstruction, the HLT has to perform the calibration online. Online-calibration can make certain offline calibration steps obsolete and can thus speed up offline analysis. In ALICE Run 3 starting in 2020, online calibration becomes a necessity. The main detector used for track reconstruction is the TPC. Reconstructing the trajectories in the TPC is the most compute-intense step during event reconstruction. Therefore, a fast tracking implementation is of great importance. Reconstructed TPC tracks build the basis for the calibration making a fast online-tracking mandatory.\r\n\r\nWe present several components developed for the ALICE High Level Trigger to perform fast event reconstruction and to provide features required for online calibration.\r\n\r\nAs first topic, we present our TPC tracker, which employs GPUs to speed up the processing, and which bases on a Cellular Automaton and on the Kalman filter. Our TPC tracking algorithm has been successfully used in 2011 and 2012 in the lead-lead and the proton-lead runs. We have improved it to leverage features of newer GPUs and we have ported it to support OpenCL, CUDA, and CPUs with a single common source code. This makes us vendor independent.\r\n\r\nAs second topic, we present framework extensions, which are required for online calibration. The extensions, however, are generic and can be used for other purposes as well. We have extended the framework to allow asynchronous compute chains, which are required for long-running tasks e.g. for online calibration. And we describe our method to feed in custom data sources in the data flow. This can be external parameters like environmental temperature required for calibration and this can also be used to feed back calibration results in the processing chain.\r\n\r\nOverall, the work presented in this contribution makes the ALICE HLT ready for online reconstruction and calibration for the LHC Run 2 starting in 2015.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578608", "resources": [{"_type": "LocalFile", "name": "chep-2015-okinawa-david-rohr.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/87\/attachments\/578608\/796762\/chep-2015-okinawa-david-rohr.pdf", "fileName": "chep-2015-okinawa-david-rohr.pdf", "_fossil": "localFileMetadata", "id": "796762", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "6233e92da587b0c3279f739014a7e2d9", "affiliation": "Johann-Wolfgang-Goethe Univ. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "KRZEWICKI, Mikolaj", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c41f10a395fb3c00d6b4ce69a53a3e17", "affiliation": "Johann-Wolfgang-Goethe Univ. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "LINDENSTRUTH, Volker", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "27f6f4f5b8b7305af877778459ecee39", "affiliation": "Johann-Wolfgang-Goethe Univ. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "GORBUNOV, Sergey", "id": "3"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/87", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "84", "speakers": [{"_type": "ContributionParticipation", "emailHash": "7b8b9569d76344a0c1440aec3a9e6caf", "affiliation": "SLAC National Accelerator  Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ASAI, Makoto", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "a7515791a515182e25805d5455cecec2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. IVANTCHENKO, Vladimir", "id": "0"}], "title": "Progress in Geant4 electromagnetic physics modeling and validation", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-03-26T10:36:16.719074+00:00", "description": "", "title": "PosterEMPhysHQ.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/84\/attachments\/578609\/796763\/PosterEMPhysHQ.pdf", "filename": "PosterEMPhysHQ.pdf", "content_type": "application\/pdf", "type": "file", "id": 796763, "size": 1260740}], "title": "Slides", "default_folder": false, "id": 578609, "description": ""}], "_type": "Contribution", "description": "The Geant4 electromagnetic (EM) physics sub-packages are key components of any simulation; in particular, the simulation of LHC experiments. A small variation of EM physics may affect prediction accuracy and CPU performance of large scale Monte Carlo simulations for HEP, medicine or space science.\r\n\r\nIn this work we report on recent improvements of the EM models and on new validations of EM physics of Geant4. Improvements were made in models of the photoelectric effect, Compton scattering, gamma conversion to electron and muon pairs, fluctuations of energy loss, multiple scattering, synchrotron radiation, and high energy positron annihilation. The results of these developments are included in the new Geant4 version 10.1 and in patches to previous versions 9.6 and 10.0 that are planned to be used for production for run-2 at LHC.\r\n\r\nThe Geant4 validation suite for EM physics has been extended and new validation results will be shown in this work. In particular, the effect of gamma-nuclear interactions on EM shower shape at LHC energies will be discussed.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578609", "resources": [{"_type": "LocalFile", "name": "PosterEMPhysHQ.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/84\/attachments\/578609\/796763\/PosterEMPhysHQ.pdf", "fileName": "PosterEMPhysHQ.pdf", "_fossil": "localFileMetadata", "id": "796763", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "ddb4e4ec90b85b573b5dd09e3f0e05c2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "APOSTOLAKIS, John", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "dbeaae9080e53786b4b3dcfbf78b7bab", "affiliation": "P.N. Lebedev Institute of Physics (FIAN)", "_fossil": "contributionParticipationMetadata", "fullName": "BAGULYA, Alexandre", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "7b8b9569d76344a0c1440aec3a9e6caf", "affiliation": "SLAC National Accelerator  Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ASAI, Makoto", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/84", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "85", "speakers": [{"_type": "ContributionParticipation", "emailHash": "85b48fcbbd8c3a9293a147a633af1bd3", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SILVA DE SOUSA, Bruno", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "4c578dd9ff1c6f9776e432151a644d07", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GRZYWACZEWSKI, Pawel", "id": "0"}], "title": "The CERN Lync system : voice-over-ip and much more", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-05T18:21:09.762218+00:00", "description": "", "title": "The_CERN_Lync_system.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/85\/attachments\/578610\/796764\/The_CERN_Lync_system.pdf", "filename": "The_CERN_Lync_system.pdf", "content_type": "application\/pdf", "type": "file", "id": 796764, "size": 988286}], "title": "Poster", "default_folder": false, "id": 578610, "description": ""}], "_type": "Contribution", "description": "While travelling, we expect to have access to Internet, or being able to check a mailbox. But until recently, it was difficult to maintain voice conversations while outside of your working place. For some cases we can use mobile phones but the roaming charges are high when abroad. At CERN we have deployed Lync, a Voice-over-IP system that fills this gap. Once a CERN user has requested a migration of a landline phone number to the Lync Phone system, people can place calls from range of supported clients: Lync IP phones, Lync application on Windows\/MAC, or Lync application on a smartphones (iPhone, Android, Windows Phone). This reduces costs and allows people to place phone calls using their CERN number from any place around the world.\r\n\r\nThis talk will explain the new functionalities such as instant messaging, presence detection, desktop sharing and integration with the CERN phone system, the technical architecture and the experiences with deploying the Lync solution at CERN. Lync Federation with other organisations and bridging to Skype and Google Talk will also be described.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578610", "resources": [{"_type": "LocalFile", "name": "The_CERN_Lync_system.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/85\/attachments\/578610\/796764\/The_CERN_Lync_system.pdf", "fileName": "The_CERN_Lync_system.pdf", "_fossil": "localFileMetadata", "id": "796764", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "85b48fcbbd8c3a9293a147a633af1bd3", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SILVA DE SOUSA, Bruno", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/85", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "7", "speakers": [{"_type": "ContributionParticipation", "emailHash": "b37c33835ebfdc6a83a233f08f63ecd1", "affiliation": "CEA\/IRFU,Centre d'etude de Saclay Gif-sur-Yvette (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. FERRY, Sophie Catherine", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "b37c33835ebfdc6a83a233f08f63ecd1", "affiliation": "CEA\/IRFU,Centre d'etude de Saclay Gif-sur-Yvette (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. FERRY, Sophie Catherine", "id": "0"}], "title": "Running and Testing T2 Grid Services with Puppet at GRIF-IRFU", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-01T18:54:44.185427+00:00", "description": "", "title": "posterCHEP-v4.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/7\/attachments\/578611\/796765\/posterCHEP-v4.pdf", "filename": "posterCHEP-v4.pdf", "content_type": "application\/pdf", "type": "file", "id": 796765, "size": 966636}], "title": "Poster", "default_folder": false, "id": 578611, "description": ""}], "_type": "Contribution", "description": "GRIF is a distributed Tiers2 center, made of 6 different centers in the Paris region (France), and serving many VOs. The sub-sites are connected with 10Gbits\/s private network and share tools for central management. \r\nOne of the sub-sites, GRIF-IRFU held and maintained in the CEA-Saclay center, moved a year ago, to a configuration management using Puppet. \r\nThanks to the versatility of Puppet\/Foreman automation, the GRIF-IRFU site maintains usual grid services, with among them: a Cream-CE with a Torque+Maui (running a batch with more than 4000 jobs slots), a DPM storage of more than 2PB, a Nagios monitoring essentially based on check_mk, as well as centralized services for the French NGI, like the accounting, or the argus central banning system.\r\nWe report on the actual functionalities of Puppet and present the last tests and evolutions including a monitoring with Graphite, a HT-condor multi core batch accessed with an ARC-CE and a CEPH storage file system.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578611", "resources": [{"_type": "LocalFile", "name": "posterCHEP-v4.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/7\/attachments\/578611\/796765\/posterCHEP-v4.pdf", "fileName": "posterCHEP-v4.pdf", "_fossil": "localFileMetadata", "id": "796765", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "b4b7a2ba4a8465096a3afa8236331ff3", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "MICOUT, Pierrick", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "e69fdaabfa68d59c42972706ec189d9b", "affiliation": "CEA", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. SCHAER, Frederic", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/7", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "09:30:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "09:40:00"}, "duration": 10, "session": "Plenary", "keywords": [], "id": "580", "speakers": [{"_type": "ContributionParticipation", "emailHash": "0d28138f3fc2216fb1d97810d9bd7e74", "affiliation": "University of Bologna", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. BONACORSI, Daniele", "id": "0"}], "primaryauthors": [], "title": "Introduction to CHEP parallel program summaries", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T00:49:26.922066+00:00", "description": "", "title": "150416_CHEP15_PC_DBonacorsi.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/580\/attachments\/578612\/796766\/150416_CHEP15_PC_DBonacorsi.pdf", "filename": "150416_CHEP15_PC_DBonacorsi.pdf", "content_type": "application\/pdf", "type": "file", "id": 796766, "size": 1959905}], "title": "Slides", "default_folder": false, "id": 578612, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578612", "resources": [{"_type": "LocalFile", "name": "150416_CHEP15_PC_DBonacorsi.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/580\/attachments\/578612\/796766\/150416_CHEP15_PC_DBonacorsi.pdf", "fileName": "150416_CHEP15_PC_DBonacorsi.pdf", "_fossil": "localFileMetadata", "id": "796766", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/580", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": null, "keywords": [], "id": "245", "speakers": [{"_type": "ContributionParticipation", "emailHash": "77129490a06056d8761c3ede7dea7289", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "HARA, Takanori", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "77129490a06056d8761c3ede7dea7289", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "HARA, Takanori", "id": "0"}], "title": "Overview and Highlights of the Belle II Computing", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [], "_type": "Contribution", "description": "The Belle II experiment is the next-generation flavor factory experiment at the SuperKEKB accelerator in Tsukuba, Japan. The first physics run will take place in 2017, then we plan to increase the luminosity gradually. We will reach the world's highest luminosity L=8x10^35 cm^-2s^-1 after roughly five years operation and collect a total of 50ab^-1 data by 2023. Thanks to such a huge amount of data, we can explore the new physics possibilities through a large variety of analyses in quark sectors as well as tau physics and deepen understanding of nature.\r\n\r\nThe Belle II computing system is expected to manage the process of massive raw data, production of copious simulation as well as many concurrent user analysis jobs. The required resource estimation for the Belle II computing system shows a similar evolution curve of the resource pledges in LHC experiments. Eventually, we have to handle several tens of Petayte of beam data per year.  \r\n\r\nHere, the Belle II is a worldwide collaboration of about 600 scientists\r\nworking in 23 countries and region. It is natural to adopt a distributed computing model based on existing technologies. We chose DIRAC as a workload and data management system and AMGA as a metadata service. In particular, DIRAC provides us an interoperability of heterogeneous computing systems such as grids with different middleware, academic\/commercial clouds and local computing clusters.\r\n\r\nSince the last year, we have repeated the mass MC production campaigns\r\nto test our system. Furthermore, we also have started the data transfer challenges through the transpacific and transatlantic networks.\r\n\r\nIn this report, we will present the highlights of the recent achievements of the Belle II computing system.", "track": "Track5: Computing activities and Computing models", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/contribution\/245", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:15:00"}, "duration": 15, "session": "Track 8 Session", "keywords": [], "id": "244", "speakers": [{"_type": "ContributionParticipation", "emailHash": "41c9211fa0c4d3b49e193b906880a756", "affiliation": "Institute of High Physics Chinese Academy of Sciences", "_fossil": "contributionParticipationMetadata", "fullName": "Ms. KAN, Bowen", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "41c9211fa0c4d3b49e193b906880a756", "affiliation": "Institute of High Physics Chinese Academy of Sciences", "_fossil": "contributionParticipationMetadata", "fullName": "Ms. KAN, Bowen", "id": "0"}], "title": "A new Self-Adaptive disPatching System for local cluster", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T03:15:28.926816+00:00", "description": "", "title": "SAPS-chep-2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/244\/attachments\/578613\/796767\/SAPS-chep-2015.pdf", "filename": "SAPS-chep-2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796767, "size": 931056}, {"_type": "attachment", "modified_dt": "2015-04-13T03:15:28.926816+00:00", "description": "", "title": "SAPS-chep-2015.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/244\/attachments\/578613\/796768\/SAPS-chep-2015.pptx", "filename": "SAPS-chep-2015.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796768, "size": 1431942}], "title": "Slides", "default_folder": false, "id": 578613, "description": ""}], "_type": "Contribution", "description": "Scheduler is one of the most important components of high performance cluster. This paper introduces a self-adaptive dispatching system (SAPS) based on torque\/maui which increases the resources utilization of cluster effectively and guarantees the high reliability of the computing platform. It provides great convenience for users to run   various tasks on the computing platform. First of all, the SAPS implements the GPU scheduling with multi-core. This provides the basis for effective integration and utilization of computing resources, improves the ability of the cluster computing greatly. Secondly, SAPS analysis the relationship between the number of jobs queueing and the idle resources left, tune the priority of users\u2019 job dynamically. In this way, more resources are provided for jobs running and less resources idle. Thirdly, integrated the on-line error detection with work nodes, the SAPS can excluded error nodes and include the recovered nodes automatically.\r\n   In addition, SAPS provides a monitoring management with fine granularity, a comprehensive scheduling accounting module and a scheduling real-time alarm function, and all of those ensure the cluster runs more high-efficiently, and reliably.\r\n   Currently, the SAPS has been running stable on IHEP local cluster (more than 10,000 cores and 30,000 jobs every day) and resource utilization has been improved more than 26%, and the SAPS has reduced costs for both administrator and users greatly.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578613", "resources": [{"_type": "LocalFile", "name": "SAPS-chep-2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/244\/attachments\/578613\/796767\/SAPS-chep-2015.pdf", "fileName": "SAPS-chep-2015.pdf", "_fossil": "localFileMetadata", "id": "796767", "_deprecated": true}, {"_type": "LocalFile", "name": "SAPS-chep-2015.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/244\/attachments\/578613\/796768\/SAPS-chep-2015.pptx", "fileName": "SAPS-chep-2015.pptx", "_fossil": "localFileMetadata", "id": "796768", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "baffd2fcf7094ce6229a02eab8691018", "affiliation": "IHEP", "_fossil": "contributionParticipationMetadata", "fullName": "SHI, Jingyan", "id": "1"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/244", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:45:00"}, "duration": 15, "session": "Track 8 Session", "keywords": [], "id": "247", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e6e328b9cafbe9711a68c6fae2da818f", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SCHWEMMER, Rainer", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e6e328b9cafbe9711a68c6fae2da818f", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SCHWEMMER, Rainer", "id": "0"}], "title": "Performance benchmark of LHCb code on state-of-the-art x86 architectures", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T09:14:10.474234+00:00", "description": "", "title": "LHCb_HLTBenchmark.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/247\/attachments\/578614\/796769\/LHCb_HLTBenchmark.pdf", "filename": "LHCb_HLTBenchmark.pdf", "content_type": "application\/pdf", "type": "file", "id": 796769, "size": 1080621}, {"_type": "attachment", "modified_dt": "2015-04-13T09:14:10.474234+00:00", "description": "", "title": "LHCb_HLTBenchmark.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/247\/attachments\/578614\/796770\/LHCb_HLTBenchmark.pptx", "filename": "LHCb_HLTBenchmark.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796770, "size": 1558495}], "title": "Slides", "default_folder": false, "id": 578614, "description": ""}], "_type": "Contribution", "description": "For Run 2 of the LHC, LHCb is exchanging a significant part of its event filter farm with new compute nodes. For the evaluation of the best performing solution, we have developed a method to convert our high level trigger application into a stand-alone, bootable benchmark image. With additional instrumentation we turned it into a self-optimising benchmark which explores techniques such as late forking, NUMA balancing and optimal number of threads, i.e. it automatically optimises box-level performance. We have run this procedure on a wide range of Haswell-E CPUs and numerous other architectures from both Intel and AMD, including also the latest Intel micro-blade servers. We present results in terms of performance, power consumption, overheads and relative cost.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578614", "resources": [{"_type": "LocalFile", "name": "LHCb_HLTBenchmark.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/247\/attachments\/578614\/796769\/LHCb_HLTBenchmark.pdf", "fileName": "LHCb_HLTBenchmark.pdf", "_fossil": "localFileMetadata", "id": "796769", "_deprecated": true}, {"_type": "LocalFile", "name": "LHCb_HLTBenchmark.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/247\/attachments\/578614\/796770\/LHCb_HLTBenchmark.pptx", "fileName": "LHCb_HLTBenchmark.pptx", "_fossil": "localFileMetadata", "id": "796770", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "9f655ee0d367b6896f4dd3a37228be6f", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "NEUFELD, Niko", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "4b2dee02cc9c077ec5c48080350a3b29", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CAMPORA PEREZ, Daniel Hugo", "id": "2"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/247", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:00:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "246", "speakers": [{"_type": "ContributionParticipation", "emailHash": "cbc73225e092c0fa43663ce9496fd7cc", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SCIABA, Andrea", "id": "4"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "7fbf0b5073c670f1542d356184e785ab", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ALANDES PRADILLO, Maria", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "d76c4d7ed249604b8568d68cd6a6a06c", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DIMOU, Maria", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "cbb0d28d39b1ca6e5d447a3e19e0e3c7", "affiliation": "Centro de Investigaciones Energ. Medioambientales y Tecn. - (ES", "_fossil": "contributionParticipationMetadata", "fullName": "FLIX MOLINA, Jose", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "bd221996d4a256a8003a73d408902cf5", "affiliation": "University of Manchester (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "FORTI, Alessandra", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "cbc73225e092c0fa43663ce9496fd7cc", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SCIABA, Andrea", "id": "4"}], "title": "Optimising Costs in WLCG Operations", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T15:32:00.376914+00:00", "description": "", "title": "WLCG-Ops-Optimisation.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/246\/attachments\/578615\/796771\/WLCG-Ops-Optimisation.pdf", "filename": "WLCG-Ops-Optimisation.pdf", "content_type": "application\/pdf", "type": "file", "id": 796771, "size": 1147347}, {"_type": "attachment", "modified_dt": "2015-04-12T15:32:00.376914+00:00", "description": "", "title": "WLCG-Ops-Optimisation.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/246\/attachments\/578615\/796772\/WLCG-Ops-Optimisation.pptx", "filename": "WLCG-Ops-Optimisation.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796772, "size": 3537710}], "title": "Slides", "default_folder": false, "id": 578615, "description": ""}], "_type": "Contribution", "description": "The Worldwide LHC Computing Grid project (WLCG) provides the computing and storage resources required by the LHC collaborations to store, process and analyse the ~50 Petabytes of data annually generated by the LHC. The WLCG operations are coordinated by a distributed team of managers and experts and performed by people at all participating sites and from all the experiments. Several improvements in the WLCG infrastructure have been implemented during the first long LHC shutdown to prepare for the increasing needs of the experiments during Run2 and beyond. However, constraints in funding will affect not only the computing resources but also the available effort for operations. This paper presents the results of a detailed investigation on the allocation of the effort in the different areas of WLCG operations, identifies the most important sources of inefficiency and proposes viable strategies for optimising the operational cost, taking into account the current trends in the evolution of the computing infrastructure and the computing models of the experiments.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578615", "resources": [{"_type": "LocalFile", "name": "WLCG-Ops-Optimisation.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/246\/attachments\/578615\/796771\/WLCG-Ops-Optimisation.pdf", "fileName": "WLCG-Ops-Optimisation.pdf", "_fossil": "localFileMetadata", "id": "796771", "_deprecated": true}, {"_type": "LocalFile", "name": "WLCG-Ops-Optimisation.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/246\/attachments\/578615\/796772\/WLCG-Ops-Optimisation.pptx", "fileName": "WLCG-Ops-Optimisation.pptx", "_fossil": "localFileMetadata", "id": "796772", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/246", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "241", "speakers": [{"_type": "ContributionParticipation", "emailHash": "eae116e7bba38707b0653c12fadaeb5e", "affiliation": "Georg-August-Universitaet Goettingen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "MUSHEGHYAN, Haykuhi", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "eae116e7bba38707b0653c12fadaeb5e", "affiliation": "Georg-August-Universitaet Goettingen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "MUSHEGHYAN, Haykuhi", "id": "0"}], "title": "HappyFace as a monitoring tool for the ATLAS experiment", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-07T13:55:38.824513+00:00", "description": "", "title": "HF_CHEP_5.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/241\/attachments\/578616\/796773\/HF_CHEP_5.pdf", "filename": "HF_CHEP_5.pdf", "content_type": "application\/pdf", "type": "file", "id": 796773, "size": 940748}], "title": "Slides", "default_folder": false, "id": 578616, "description": ""}], "_type": "Contribution", "description": "The importance of monitoring on HEP grid computing systems is growing due to a significant increase in their complexities. Computer scientists and administrators have been studying and building effective ways to gather information on and clarify a status of each local grid infrastructure.\r\nThe HappyFace project aims at making the above-mentioned workflow possible. It aggregates, processes and stores the information and the status of different HEP monitoring resources into the common database of HappyFace. The system displays the information and the status through a single interface. \r\nHowever, this model of HappyFace relied on the monitoring resources which are always under development in the HEP experiments. Consequently, HappyFace needed to have direct access methods to the grid application and grid service layers in the different HEP grid systems. To cope with this issue, we use a reliable HEP software repository, the CernVM File System. We propose a new implementation and an architecture of HappyFace, the so-called grid-enabled HappyFace. It allows its basic framework to connect directly to the grid user applications and the grid collective services, without involving the monitoring resources in the HEP grid systems.\r\nThis approach gives HappyFace several advantages: Portability, to provide an independent and generic monitoring system among the HEP grid systems. Functionality, to allow users to perform various diagnostic tools in the individual HEP grid systems and grid sites. Flexibility, to make HappyFace beneficial and open for the ATLAS computing environments.\r\nDifferent grid-enabled modules, to view datasets of the ATLAS Distributed Data Management system (DDM), to connect to the Ganga job monitoring system and to check the performance of grid transfers among the grid sites, have been implemented. The new HappyFace system has been successfully integrated and now it displays the information and the status of both the monitoring resources and the direct access to the grid user applications and the grid collective services in the ATLAS computing system.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578616", "resources": [{"_type": "LocalFile", "name": "HF_CHEP_5.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/241\/attachments\/578616\/796773\/HF_CHEP_5.pdf", "fileName": "HF_CHEP_5.pdf", "_fossil": "localFileMetadata", "id": "796773", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "769117bacc977b39854c27c60df0c5cc", "affiliation": "Georg-August-Universitaet Goettingen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "KAWAMURA, Gen", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "7bc813d53aa9b6d7a3c4fca8b5484de9", "affiliation": "Georg-August-Universitaet Goettingen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. MAGRADZE, Erekle", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "7115b6ceb356151a501b527f4284a8e2", "affiliation": "Georg-August-Universitaet Goettingen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "NADAL SERRANO, Jordi", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "02ca6423f165ed8da4a50ae68abe9d7e", "affiliation": "Georg-August-Universitaet Goettingen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "QUADT, Arnulf", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/241", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "240", "speakers": [{"_type": "ContributionParticipation", "emailHash": "daa23b6af556dd88fe716b6b6826ad21", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "HARTMANN, Thomas", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "daa23b6af556dd88fe716b6b6826ad21", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "HARTMANN, Thomas", "id": "0"}], "title": "Setup of a resilient FTS3 service at GridKa", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [], "title": "Slides", "default_folder": false, "id": 578617, "description": ""}], "_type": "Contribution", "description": "The FTS service provides a transfer job scheduler to distribute and replicate waste amounts of data over the heterogeneous WLCG infrastructures. The most recent version FTS3 simplifies and improves the flexibility compared to the channel model of the previous incarnations while reducing the load to the service components. The improvements allow to handle a higher number of transfers with a single FTS3 setup. Covering now continent-wide transfers compared to the previous version handling only transfers related to specific clouds, a resilient system becomes even more necessary with the increased number of depending users.\r\n\r\nHaving set up a FTS3 services at the German T1 site *GridKa* at *KIT* in Karlsruhe, we present our experiences on the preparations for a high-availability FTS3 service. Trying to avoid single points of failure, we rely on a database cluster as fault tolerant data back-end and the FTS3 service deployed on an own cluster setup to provide a resilient infrastructure for the users. With the database cluster providing a basic resilience for the data back-end, we ensure on the FTS3 service level a consistent and reliable database access through a proxy solution. On each FTS3 node a HAproxy instance is monitoring the health of each database node and distributes database queries over the whole cluster for load balancing during normal operations; in case of a broken database node, the proxy excludes it transparently to the local FTS3 service. The FTS3 service itself consists of a main and a backup instance, which takes over the identity of the main instance, i.e., IP, in case of an error using a CTDB infrastructure offering clients a consistent service.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "aaedaf0b6923da9472cbc33726464d1d", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "PETZOLD, Andreas", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "1d75705660b8422955f3c72fda785208", "affiliation": "Bulgarian Academy of Sciences (BG)", "_fossil": "contributionParticipationMetadata", "fullName": "KONSTANTINOV, Preslav Borislavov", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "a89e9576d90990cc492a740d76166e03", "affiliation": "KIT Karlsruhe", "_fossil": "contributionParticipationMetadata", "fullName": "Ms. OBHOLZ, Ludmilla", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "f684ab7fb7eab429d1aa5034d5bbfb98", "affiliation": "Karlsruhe Institut for Technology", "_fossil": "contributionParticipationMetadata", "fullName": "WISNIEWSKI, Kamil", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/240", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "243", "speakers": [{"_type": "ContributionParticipation", "emailHash": "53281eed1430ea0a1c827b38f6891b94", "affiliation": "NIKHEF", "_fossil": "contributionParticipationMetadata", "fullName": "KEIJSER, Jan Justinus", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "53281eed1430ea0a1c827b38f6891b94", "affiliation": "NIKHEF", "_fossil": "contributionParticipationMetadata", "fullName": "KEIJSER, Jan Justinus", "id": "0"}], "title": "ROOT\/RooFIT optimizations for Intel Xeon Phi's - first results", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T05:38:01.875382+00:00", "description": "", "title": "poster-chep2015-xeonphi.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/243\/attachments\/578618\/796775\/poster-chep2015-xeonphi.pdf", "filename": "poster-chep2015-xeonphi.pdf", "content_type": "application\/pdf", "type": "file", "id": 796775, "size": 426921}], "title": "Poster", "default_folder": false, "id": 578618, "description": ""}], "_type": "Contribution", "description": "In the past, grid worker nodes struck a reasonable balance between\r\nthe number of cores, the amount of available memory, available diskspace\r\nand the maximum network bandwidth. This led to an operating model where\r\nworker nodes were \"carved up\" into single core job slots, each of which\r\nwould execute a HEP workload job. Typical worker nodes would have up to\r\n16 computing cores , with roughly 2 GB RAM and 32 GB scratch space per \r\njob slot.\r\nIn the last few years the number of threads and cores per processor has\r\nrisen rapidly. With the advent of multi-core, many-core and GPU computing \r\nthe number of cores is expected to rise even more. This leads to an \r\nimbalance on modern worker nodes, where the \"job per core\" model is no\r\nlonger a valid option. \r\nIn view of these development the desire, or perhaps even the necessity,\r\nto run programs in parallel has also increased. However, the process of\r\ntransforming the HEP software to run in parallel has been slow and painful.\r\n\r\nOne of the more recent development in the GPU and many-core computing field \r\nhas been the introduction of the Intel Many Integrated Core architecture, \r\nknown now as the Intel Xeon Phi. The Xeon Phi is a PCI-Express adapter with\r\nup to 16 GB of RAM on board and with up to 61 cores, each capable of running\r\n4 threads. The Xeon Phi has a theoretical performance of more than 1 TFLOPS.\r\n\r\nInitial attempts to port HEP code to the Xeon Phi has not produced the \r\nexpected results and often a performance *decrease* has been seen, instead\r\nof an increase. This has also been reported by non-HEP researchers, and \r\nslowly the reasons for this are becoming known. Using the proper optimization\r\ntechniques it is possible to achieve 1 TFLOPS, but this requires substantial\r\neffort in rewriting and optimizing the application. The main bottleneck here\r\nis that the Xeon Phi should not be regarded \"61 independent cores\" but instead\r\nit should be seen as a vector instruction machine. By properly vectorizing\r\nan application the performance can improve significantly. This also applies\r\nwhen porting an application to GPUs. \r\n\r\nIn this talk we will focus on porting and optimizing ROOT, and in particular\r\nRooFIT to the Intel Xeon Phi. A zero-order port does indeed show a decrease\r\nin performance, but by analyzing and optimizing the 'hot spots' in the code\r\nthe performance can be improved. It is vital to validate the results of the\r\nported application, however: the Xeon Phi version of RooFIT must produce the\r\nsame results as the single-core version. \r\n\r\nMost interesting is the fact that code optimized for the Xeon Phi also \r\nperforms better on \"regular\" Xeon processors. As part of the results we will\r\nalso discuss the performance increase of the optimized code on an Xeon E5 processor.\r\nIn that way the Xeon Phi porting effort can be seen as a catalyst to transform\r\nthe HEP code to run in parallel on modern CPUs.\r\n\r\nThe main goal of this research and of this talk is to provide a direction for\r\noptimizing the HEP software for modern and future computing platforms. The\r\nmain goal of parallelizing RooFIT is not to make it scale to 60 cores or more,\r\nbut to find a way to restore the imbalance seen in modern worker nodes. If a \r\nrelatively simple code optimization leads to HEP software that can scale to \r\nroughly 10 cores then the \"core imbalance\" can be restored. Worker nodes could \r\nthen be carved up into \"10 core job slots\". This is a different approach from\r\ntrue High Performance Computing centers, who are usually interested in getting\r\nan application to scale beyond 32 cores and more.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578618", "resources": [{"_type": "LocalFile", "name": "poster-chep2015-xeonphi.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/243\/attachments\/578618\/796775\/poster-chep2015-xeonphi.pdf", "fileName": "poster-chep2015-xeonphi.pdf", "_fossil": "localFileMetadata", "id": "796775", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/243", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:00:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "242", "speakers": [{"_type": "ContributionParticipation", "emailHash": "bb52f3e15df96ec6a345a179a5552ca5", "affiliation": "IPNO, Universit\u00e9 Paris-Sud, CNRS\/IN2P3", "_fossil": "contributionParticipationMetadata", "fullName": "HRIVNACOVA, Ivana", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "bb52f3e15df96ec6a345a179a5552ca5", "affiliation": "IPNO, Universit\u00e9 Paris-Sud, CNRS\/IN2P3", "_fossil": "contributionParticipationMetadata", "fullName": "HRIVNACOVA, Ivana", "id": "0"}], "title": "Geant4 VMC 3.0", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T08:41:58.800884+00:00", "description": "", "title": "geant4_vmc_3.0.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/242\/attachments\/578619\/796776\/geant4_vmc_3.0.pdf", "filename": "geant4_vmc_3.0.pdf", "content_type": "application\/pdf", "type": "file", "id": 796776, "size": 588492}], "title": "Slides", "default_folder": false, "id": 578619, "description": ""}], "_type": "Contribution", "description": "Virtual Monte Carlo (VMC) provides an abstract interface into Monte Carlo transport codes. A user VMC based application, independent from the specific Monte Carlo codes, can be then run with any of the supported simulation programs. \u00a0Developed by the ALICE Offline Project and further included in ROOT, the interface and implementations have reached stability during the last decade and have become a foundation for other detector simulation frameworks, the FAIR facility experiments framework being among the first and largest.\r\n\r\nGeant4 VMC, which provides the implementation of the VMC interface for Geant4, is in continuous maintenance and development, driven by the evolution of Geant4 on one side and requirements from users on the other side. \u00a0Besides the implementation of the VMC interface, Geant4 VMC also provides a set of examples that demonstrate the use of VMC to new users and also serve for testing purposes. Since major release 2.0, it includes the G4Root navigator package, which implements an interface that allows one to run a Geant4 simulation using a ROOT geometry.\r\n\r\nThe release of Geant4 version 10.00 with the integration of multi-threading processing has triggered the development of the next major version of Geant4 VMC (version 3.0), whose release is planned for this year. A beta version, available for user testing since March, has helped its consolidation and improvement. We will review the new capabilities introduced in this major version, in particular the integration of multi-threading into the VMC design, its impact on the Geant4 VMC and G4Root packages, and the introduction of a new package, MTRoot, providing utility functions for ROOT parallel output in independent files with necessary additions for thread-safety. Migration of user applications to multi-threading that preserves the ease of use of VMC will be also discussed. We will also report on the introduction of a new CMake based build system, the migration to ROOT major release 6 and the improvement of the testing suites.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578619", "resources": [{"_type": "LocalFile", "name": "geant4_vmc_3.0.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/242\/attachments\/578619\/796776\/geant4_vmc_3.0.pdf", "fileName": "geant4_vmc_3.0.pdf", "_fossil": "localFileMetadata", "id": "796776", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "5e451b3b7969b96f79563ef9500e6f3e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GHEATA, Andrei", "id": "1"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/242", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "249", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ecca023b01a7f3582d9cff88a1540a20", "affiliation": "Rheinisch-Westfaelische Tech. Hoch. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "FISCHER, Robert", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ecca023b01a7f3582d9cff88a1540a20", "affiliation": "Rheinisch-Westfaelische Tech. Hoch. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "FISCHER, Robert", "id": "0"}], "title": "Analysis of Public CMS Data on the VISPA Internet Platform", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T19:17:08.557612+00:00", "description": "", "title": "chep_2015_poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/249\/attachments\/578620\/796777\/chep_2015_poster.pdf", "filename": "chep_2015_poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796777, "size": 1753905}], "title": "Poster", "default_folder": false, "id": 578620, "description": ""}], "_type": "Contribution", "description": "Within CERN's new open data portal, the CMS collaboration provides a substantial fraction of its recorded data to the public. To explore and analyse the data, computing resources, an analysis framework, and documentation are required as well. While scientists can use C++ and the experiment software CMSSW in virtual machines, a simpler approach is needed, e.g. for university students who are in the process of becoming familiar with data analyses in particle physics. The VISPA Internet platform provides interactive access to CMS public data together with analysis examples. Here, the Python programming language requires only moderate programming skills. Computing resources are offered, such that CMS analyses can be developed and executed from any device with a standard web browser. We report on the concept of the Internet platform, and present experience gained on the worldwide usage of the public CMS data.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578620", "resources": [{"_type": "LocalFile", "name": "chep_2015_poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/249\/attachments\/578620\/796777\/chep_2015_poster.pdf", "fileName": "chep_2015_poster.pdf", "_fossil": "localFileMetadata", "id": "796777", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "d2a2c425d493d843380bacd2e68a709e", "affiliation": "Rheinisch-Westfaelische Tech. Hoch. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "VAN ASSELDONK, Daniel", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "0d2004b8d6b8d0f1802034817c842050", "affiliation": "Rheinisch-Westfaelische Tech. Hoch. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "FISCHER, Benjamin", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "1f7216ef4d6f44eef1144f32e0d826ab", "affiliation": "RWTH Aachen", "_fossil": "contributionParticipationMetadata", "fullName": "GLASER, Christian", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "720b1ee57d4ee05eb433c601358127c8", "affiliation": "Rheinisch-Westfaelische Tech. Hoch. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "HEIDEMANN, Fabian", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "3018dbae0f865680f21e7350b7f6a840", "affiliation": "Rheinisch-Westfaelische Tech. Hoch. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "M\u00dcLLER, Gero", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "52be32cd037b789131f812491c0dc3e5", "affiliation": "Rheinisch-Westfaelische Tech. Hoch. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "QUAST, Thorben", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "2649d10aea72ee92f56468d4dc217b91", "affiliation": "Rheinisch-Westfaelische Tech. Hoch. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "RIEGER, Marcel", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "ec519a22b4af7a40517deac64a169e60", "affiliation": "Rheinisch-Westfaelische Tech. Hoch. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "URBAN, Martin", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "19f8bc0594410c4d7717cdb2660a9536", "affiliation": "Rheinisch-Westfaelische Tech. Hoch. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "WELLING, Christoph", "id": "9"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/249", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "248", "speakers": [{"_type": "ContributionParticipation", "emailHash": "b6314875106ecd21513161bed2195528", "affiliation": "ISS - Institute of Space Science (RO)", "_fossil": "contributionParticipationMetadata", "fullName": "GHEATA, Mihaela", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "b6314875106ecd21513161bed2195528", "affiliation": "ISS - Institute of Space Science (RO)", "_fossil": "contributionParticipationMetadata", "fullName": "GHEATA, Mihaela", "id": "0"}], "title": "Open access for ALICE analysis based on virtualisation technology", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T19:20:58.550435+00:00", "description": "", "title": "OpenAccess_CHEP15.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/248\/attachments\/578621\/796778\/OpenAccess_CHEP15.pdf", "filename": "OpenAccess_CHEP15.pdf", "content_type": "application\/pdf", "type": "file", "id": 796778, "size": 9945078}], "title": "Slides", "default_folder": false, "id": 578621, "description": ""}], "_type": "Contribution", "description": "Open access is one of the prerequisites for long term data preservation for a HEP experiment. To guarantee the usability of data analysis tools over long periods of time it is crucial that third party users from the scientific community have access to the data and associated software. The ALICE Collaboration has developed a layer of lightweight components built on top of virtualisation technology to hide the complexity and details of the experiment-specific software. Users can perform basic analysis tasks within CernVM, a lightweight generic virtual machine, paired with an ALICE specific contextualisation. Once the virtual machine is launched, a graphics user interface is automatically started without any additional configuration. This interface allows to download the base ALICE analysis software and run a set of ALICE analysis modules. Currently the available tools include fully documented tutorials for ALICE analysis, such as the measurement of strange particle production or the nuclear modification factor in Pb-Pb collisions. The interface can be easily extended to include an arbitrary number of additional analysis modules. We will present the current status of the tools used by ALICE through the CERN open access portal, our first user experience and the plans for future extensions of this system.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578621", "resources": [{"_type": "LocalFile", "name": "OpenAccess_CHEP15.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/248\/attachments\/578621\/796778\/OpenAccess_CHEP15.pdf", "fileName": "OpenAccess_CHEP15.pdf", "_fossil": "localFileMetadata", "id": "796778", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "3f41c6112486480c3ebd506d39003e9e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BUNCIC, Predrag", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "386b5ebd80511e2cb1c44050a2dc9c83", "affiliation": "IN2P3 (FR) and CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SCHUTZ, Yves", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/248", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:15:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "519", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5dff1cb9e65e0cb44929be5bdf05dbfd", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "FAJARDO HERNANDEZ, Edgar", "id": "7"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e82c46a3a0d49dd555c888d113eace3a", "affiliation": "University of Nebraska (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BOCKELMAN, Brian Paul", "id": "0"}], "title": "Commissioning HTCondor-CE for the Open Science Grid", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T08:26:32.714632+00:00", "description": "", "title": "Condor-CE-CHEP-2015-Edgar-revision.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/519\/attachments\/578622\/796779\/Condor-CE-CHEP-2015-Edgar-revision.pdf", "filename": "Condor-CE-CHEP-2015-Edgar-revision.pdf", "content_type": "application\/pdf", "type": "file", "id": 796779, "size": 1324124}], "title": "Slides", "default_folder": false, "id": 578622, "description": ""}], "_type": "Contribution", "description": "The HTCondor-CE is the next-generation gateway software for the Open Science Grid (OSG).  This is responsible for providing a network service which authorizes remote users and provides a resource provisioning service (other well-known gatekeepers include Globus GRAM, CREAM, Arc-CE, and Openstack\u2019s Nova).  Based on the venerable HTCondor software, this new CE is simply a highly-specialized configuration of HTCondor.  It was developed and adopted to provide the OSG with a more flexible, scalable, and easier-to-manage gateway software.  This software does not exist in a vacuum: to deploy this gateway across the OSG, we had to integrate it with the CE configuration, deploy a corresponding information service, coordinate with sites, and overhaul our documentation.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578622", "resources": [{"_type": "LocalFile", "name": "Condor-CE-CHEP-2015-Edgar-revision.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/519\/attachments\/578622\/796779\/Condor-CE-CHEP-2015-Edgar-revision.pdf", "fileName": "Condor-CE-CHEP-2015-Edgar-revision.pdf", "_fossil": "localFileMetadata", "id": "796779", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "ab0e4b6abdb20488fe879b32b0460192", "affiliation": "University of Nebraska (US)", "_fossil": "contributionParticipationMetadata", "fullName": "ZVADA, Marian", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "55a49eed77d2c92902808de16b05f780", "affiliation": "Univ of Wisconsin-Madison, Wisconsin, USA", "_fossil": "contributionParticipationMetadata", "fullName": "TANNENBAUM, Todd", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "96f8083a0d85892c0d51b3d05fbe0a75", "affiliation": "University of Wisconsin-Madison", "_fossil": "contributionParticipationMetadata", "fullName": "FREY, Jaime", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "090150609883aa76945f21685f21319f", "affiliation": "University of Wisconsin-Madison", "_fossil": "contributionParticipationMetadata", "fullName": "CARTWRIGHT, Tim", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "fc1838a2f2410f7ef0b9ae2531dda84e", "affiliation": "University of Wisconsin-Madison", "_fossil": "contributionParticipationMetadata", "fullName": "LIN, Brian", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "1da06010af73854e524fa392603b2004", "affiliation": "University of Wisconsin-Madison", "_fossil": "contributionParticipationMetadata", "fullName": "SELMECI, Matyas", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "5dff1cb9e65e0cb44929be5bdf05dbfd", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "FAJARDO HERNANDEZ, Edgar", "id": "8"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/519", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": null, "keywords": [], "id": "518", "speakers": [{"_type": "ContributionParticipation", "emailHash": "44774c9d30ed9c19187ac6eeafe75b1a", "affiliation": "Centre de Physique des Particules de Marseille, CNRS\/IN2P3", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HOFFMANN, Dirk", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "44774c9d30ed9c19187ac6eeafe75b1a", "affiliation": "Centre de Physique des Particules de Marseille, CNRS\/IN2P3", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HOFFMANN, Dirk", "id": "0"}], "title": "A python library for access of SharePoint without using SharePoint", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "One of the weak points of the Microsoft SharePoint systems is interoperability, especially for users living in a non-Windows world. In addition, expert-level programming of Active Server Pages (ASP.net, or \"php for IIS\") with C# or VirtualBasic is locked-away from standard users due to the high level of demands of security of the SharePoint environment. \r\n\r\nWhile experimenting with the SOAP and REST interfaces for querying existing SharePoint database contents from a Linux\/python\/libcurl environment, I discovered that this is indeed a viable alternative to the native tools provided by Microsoft. Some difficulties had to be overcome, due to the multiple possibilities to authentify to a SharePoint system through IIS (i.e. \"the httpd for Windows\"). But surprisingly table queries and joins were felt to be much faster than over HTTP, once the correct syntax had been found. Other users have started to use this pioneering work on a regular basis.\r\n\r\nDocumentation is provided nowadays by Microsoft for the interoperating interfaces SOAP and REST protocols, albeit sometims rudimentary. Ambiguous naming of variables and columns add another layer of opacity. The presentation should guide interested users to the right places for solutions and last but not least to the sample codes on Codeplex (i.e. \"sourceforge by Microsoft\"). I will present the present state of development of a python package that attempts to make abstraction of the internal difficulties of SOAP and REST for the end user, as well as possible plans to bridge access to ShPt completely behind a LAxP system, where x might become x=ShPt.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/contribution\/518", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "511", "speakers": [{"_type": "ContributionParticipation", "emailHash": "fc2d531a1d59c0d27cf4550e45442ae6", "affiliation": "University of London (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "WALKER, Christopher John", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "d784cb31ed0d1334efa12f3200f768f0", "affiliation": "University of London (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "TRAYNOR, Daniel Peter", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "fc2d531a1d59c0d27cf4550e45442ae6", "affiliation": "University of London (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "WALKER, Christopher John", "id": "0"}], "title": "Use of jumbo frames for data transfer over the WAN", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "Jumbo frames (with an MTU of 9000 bytes rather than the ethernet standard of 1500) have potential performance advantages for WAN transfers. Whilst many national and international research and education networks support their use, they are not widely supported at end sites. Furthermore, firewalls at some end sites block path MTU discovery leading to potential performance bottlenecks.\r\n\r\nQMUL has two data transfer servers, one configured to use jumbo frames, and one one. In this paper, we compare data transfer speeds to a range of sites and assess the impact of jumbo frames, and path MTU blocking on data transfer rates.", "track": "Track6: Facilities, Infrastructure, Network", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "d784cb31ed0d1334efa12f3200f768f0", "affiliation": "University of London (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "TRAYNOR, Daniel Peter", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/511", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:30:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "510", "speakers": [{"_type": "ContributionParticipation", "emailHash": "3e84f64e37bceead0577ddf1db41bb2a", "affiliation": "SLAC National Accelerator  Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GRAF, Norman Anthony", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "3e84f64e37bceead0577ddf1db41bb2a", "affiliation": "SLAC National Accelerator  Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GRAF, Norman Anthony", "id": "0"}], "title": "The Heavy Photon Search Experiment Software Environment", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T08:18:15.683177+00:00", "description": "", "title": "HPS_CHEP15_Okinawa_Graf_20150413_v2.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/510\/attachments\/578623\/796780\/HPS_CHEP15_Okinawa_Graf_20150413_v2.pdf", "filename": "HPS_CHEP15_Okinawa_Graf_20150413_v2.pdf", "content_type": "application\/pdf", "type": "file", "id": 796780, "size": 1274519}], "title": "Slides", "default_folder": false, "id": 578623, "description": ""}], "_type": "Contribution", "description": "The Heavy Photon Search (HPS) is an experiment at the Thomas Jefferson National Accelerator Facility (JLab) designed to search for a hidden sector photon (A\u2019) in fixed target electroproduction. It uses a silicon microstrip tracking and vertexing detector inside a dipole magnet to measure charged particle trajectories and a fast electromagnetic calorimeter just downstream of the magnet to provide a trigger and identify electrons. As the first stage of this project, the HPS Test Run apparatus was constructed and operated in 2012 to demonstrate the experiment\u2019s technical feasibility and to confirm that the trigger rates and occupancies were as expected. The full detector is currently being installed and will be commissioned starting in November, 2014. Data taking is expected to commence in the spring of 2015. \r\nThe HPS experiment uses both invariant mass and secondary vertex signatures to search for the A\u2019. The overall design of the detector follows from the kinematics of A\u2019 production which typically results in a final state particle within a few degrees of the incoming beam. The occupancies of sensors near the beam plane are high, so high-rate detectors, a fast trigger, and excellent time tagging are required to minimize their impact. The trigger comes from a highly-segmented lead-tungstate crystal calorimeter located just downstream of the dipole magnet. The detector was fully simulated using the flexible and performant Geant4-based program slic (abstract 445, this conference). Simulation of the readout and the event reconstruction itself were performed with the Java-based software package org.lcsim (abstract 445, this conference).. The simulation of the detector readout includes full charge deposition, drift and diffusion in the silicon wafers, followed by a detailed simulation of the readout chip and associated electronics. Full accounting of the occupancies was performed by overlaying the expected number of beam backgrounds. Track, cluster and vertex reconstruction for both simulated and real data will be described and preliminary comparisons of the expected and measured detector performance will be presented.\r\n We will begin with an overview of the physics goals of the experiment followed by a short description of the detector design. We will then describe the software tools used to design the detector layout and simulate the expected detector performance. Finally, the event reconstruction chain will be presented.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578623", "resources": [{"_type": "LocalFile", "name": "HPS_CHEP15_Okinawa_Graf_20150413_v2.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/510\/attachments\/578623\/796780\/HPS_CHEP15_Okinawa_Graf_20150413_v2.pdf", "fileName": "HPS_CHEP15_Okinawa_Graf_20150413_v2.pdf", "_fossil": "localFileMetadata", "id": "796780", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/510", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "513", "speakers": [{"_type": "ContributionParticipation", "emailHash": "d56425ca2dac38f9c0bddaabc29723fe", "affiliation": "University of Minnesota", "_fossil": "contributionParticipationMetadata", "fullName": "KASAHARA, Susan", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "d56425ca2dac38f9c0bddaabc29723fe", "affiliation": "University of Minnesota", "_fossil": "contributionParticipationMetadata", "fullName": "KASAHARA, Susan", "id": "0"}], "title": "The NOvA DAQ Monitor System", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The NO$\\nu$A (NuMI Off-Axis $\\nu_{e}$ Appearance) experiment is a long-baseline neutrino experiment using the NuMI\r\nmain injector neutrino beam at Fermilab and is designed to search for  $\\nu_{\\mu}$ ($\\bar{\\nu}_{\\mu}$) to $\\nu_{e}$\r\n($\\bar{\\nu}_{e}$) oscillations. The experiment consists of two detectors; both positioned 14 mrad off the beam axis: a\r\n220 ton  Near Detector constructed in an underground cavern at Fermilab and a 14 kton Far Detector constructed  in Ash\r\nRiver, MN, 810 km from the beam source.  The detectors have similar design, and  consist of planes of PVC extrusion\r\ncells containing liquid scintillator and wavelength shifting fibers.  The fiber ends are readout by Avalanche\r\nPhotodiodes (APDs).  The primary task for the Data Acquisition (DAQ) system is to concentrate the data from the large\r\nnumber of APD channels (340000 channels at the Far Detector, 20000 channels at the Near Detector), buffer this data\r\nlong enough to apply an online trigger, and record the selected data.\r\n\r\nThe health and performance of the DAQ system is monitored with a DAQ Monitor system which monitors 180 custom data concentrator modules, over 200 buffer farm nodes, and 40 manager nodes at two detector sites separated by 810 km.  The DAQ Monitor system is based on an open source\r\nthird-party product, the Ganglia distributed monitoring system. Ganglia provides much of the functionality needed for\r\nthe DAQ Monitor system \u201cout-of-the-box\u201d with the ability to collect standard computing performance metrics such as CPU\r\nusage, memory usage, and network transfer rates.  Ganglia also provides the basis for displaying this information in a web\r\ndisplay and storing this information in a database.  \r\n\r\nWe have augmented the Ganglia system for the specific needs of\r\nthe NO$\\nu$A DAQ Monitor system in the form of a custom metric client application interface which is used for purposes\r\nof constructing and distributing custom metrics by the components which make up the DAQ system.  Using the custom\r\nmetric client interface, monitored quantities specific to each DAQ component are sent at regular time intervals,\r\ndisplayed on the Ganglia web display and stored in the Ganglia database.  Examples of custom metrics are trigger\r\nrates, data rates and sizes, and data corruption monitored rates.  The Ganglia base has also been enhanced with a\r\nserver side\r\napplication used to read the monitored data from the Ganglia database, compare it to configurable thresholds, and\r\nissue warnings when monitored data falls out-of-range.\r\n\r\nThe design of the NO$\\nu$A DAQ Monitor system will be discussed as will experience with its deployment on the NO$\\nu$A Near and\r\nFar detectors.", "track": "Track1: Online computing ", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/513", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "512", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ffbabaa75cb6545f51e9dfd682ebb68a", "affiliation": "University of Pavia and INFN, Italy", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. BOCA, Gianluigi", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ffbabaa75cb6545f51e9dfd682ebb68a", "affiliation": "University of Pavia and INFN, Italy", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. BOCA, Gianluigi", "id": "0"}], "title": "The Pattern Recognition software for the PANDA experiment", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "PANDA is an antiproton-proton experiment that will run at center-of-mass\r\nenergies from 2.25 to 5.46 GeV at the new facility FAIR in Darmstadt, Germany.\r\nIn order to achieve the broad range of physics goals of PANDA, a triggerless\r\ndata acquisition and a high luminosity (20 MHz interaction rate) are necessary.\r\nThis talk will concentrate on the Pattern Recognition software of the experiment.\r\nThis software will use the information of all tracking devices in PANDA, namely\r\na microvertex detector (10 ns signal time resolution), straw tube subdetectors\r\n(170 ns signal collection time), a GEM detector (few tens of ns time resolution),\r\n a fast scintillator tile system (100 ps time resolution), and several planes\r\n of gas proportional drift chambers. The absence of a hardware trigger, on one\r\nhand, gives maximum flexibility in the selection of events of interest, and on\r\nthe other hand, poses severe requirements to the pattern recognition code:\r\n1- it has to be very fast to keep up with the 20 MHz interaction rate;\r\n2- it must be very efficient in finding tracks and very selective in rejecting\r\nghost tracks caused by the large signal collection time of the straw systems\r\ncombined with the 20 MHz interaction rate makes possible the (wrong) assignment\r\nof a hit to several physics events.\r\nIn this talk the ideas and the various algorithms explored in the Pattern\r\n Recognition to tackle those challenges will be shown.", "track": "Track2: Offline software ", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/512", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:15:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "515", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6ba1f95da55f3a0622e4785d12b5cc25", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. NORMAN, Andrew", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6ba1f95da55f3a0622e4785d12b5cc25", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. NORMAN, Andrew", "id": "0"}], "title": "Performance of the NOvA Data Driven Triggering System with the full 14 kT Far Detector", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T05:22:05.941720+00:00", "description": "", "title": "nova_daq_trigger_chep2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/515\/attachments\/578624\/796781\/nova_daq_trigger_chep2015.pdf", "filename": "nova_daq_trigger_chep2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796781, "size": 10542891}, {"_type": "attachment", "modified_dt": "2015-04-15T05:22:05.941720+00:00", "description": "", "title": "nova_daq_trigger_chep2015.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/515\/attachments\/578624\/796782\/nova_daq_trigger_chep2015.pptx", "filename": "nova_daq_trigger_chep2015.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796782, "size": 10886115}], "title": "Slides", "default_folder": false, "id": 578624, "description": ""}], "_type": "Contribution", "description": "The NOvA experiment uses a continuous, free-running, dead-timeless data acquisition system to collect data from the 14 kT far detector.  The DAQ system readouts the more than 344,000 detector channels and assembles the information into an raw unfiltered high bandwidth data stream.  The NOvA trigger systems operate in parallel to the readout and asynchronously to the primary DAQ readout\/event building chain, where they examine the full (unfiltered) detector data stream and perform complicated high level pattern recognition and reconstruction algorithms to identify rare and unique interaction topologies.\r\n\r\nThe data driven triggering systems for NOvA are unique in that they examine long contiguous time windows of the high resolution readout data and enable the detector to be sensitive to a wide range of physics interactions from those with fast, nanosecond scale signals up to processes with long delayed coincidences between hits at the ten\u2019s of millisecond scale.  The trigger system is able to a true 100% live time for the detector, making it sensitive to both beam spill related and off-spill physics.\r\n\r\nWe present the performance of the trigger system with the full 14 kT NOvA detector during the first year of physics operations.  We discuss the real-time and parallel computing techniques that have been used to obtain the demonstrated performance of the trigger framework.  We give details relating to the performance of key triggering algorithms and how they have been used to validate the first observations of neutrinos in the NOvA far detector as well as the challenges of implementing and simulating the performance of these algorithms in the trigger environment.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578624", "resources": [{"_type": "LocalFile", "name": "nova_daq_trigger_chep2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/515\/attachments\/578624\/796781\/nova_daq_trigger_chep2015.pdf", "fileName": "nova_daq_trigger_chep2015.pdf", "_fossil": "localFileMetadata", "id": "796781", "_deprecated": true}, {"_type": "LocalFile", "name": "nova_daq_trigger_chep2015.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/515\/attachments\/578624\/796782\/nova_daq_trigger_chep2015.pptx", "fileName": "nova_daq_trigger_chep2015.pptx", "_fossil": "localFileMetadata", "id": "796782", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "336e636659f12d5fe7aede25fd33f666", "affiliation": "University of Virginia", "_fossil": "contributionParticipationMetadata", "fullName": "GROUP, Robert", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c0c44979895c4d91d6ef23fc49f5e258", "affiliation": "U", "_fossil": "contributionParticipationMetadata", "fullName": "FRANK, Martin", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "66391efd180489cb4b61725469458858", "affiliation": "University of Sussex", "_fossil": "contributionParticipationMetadata", "fullName": "TAMSETT, Matthew", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "698c361cfbc9ed204ca9f7b8fc5bb4cd", "affiliation": "Univ. of Minnesota Duluth", "_fossil": "contributionParticipationMetadata", "fullName": "HABIG, Alec", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "a03131261466f88a56cf5516acacef47", "affiliation": "Univeristy of Minnesota", "_fossil": "contributionParticipationMetadata", "fullName": "ZIRNSTEIN, Jan", "id": "5"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/515", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "514", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5807ffd1efe5a1f54da221c0d41224b4", "affiliation": "Scuola Normale Superiore and INFN-Pisa", "_fossil": "contributionParticipationMetadata", "fullName": "MORELLO, Michael Joseph", "id": "7"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "5ceaae2c748d239ded0e921741e51733", "affiliation": "Politecnico di Milano", "_fossil": "contributionParticipationMetadata", "fullName": "ABBA, Andrea", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "e2514a2ef4ff5206afc26c2e52a1f8c5", "affiliation": "Sezione di Pisa (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BEDESCHI, Franco", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "f97601a7f4e46f254c0dd505837bfaa4", "affiliation": "Politecnico di Milano", "_fossil": "contributionParticipationMetadata", "fullName": "CAPONIO, Francesco", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "ab690b4d6e089e9c0b34fa2a1186061c", "affiliation": "Scuola Normale Superiore and INFN-Pisa", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. CENCI, Riccardo", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "00da6bc4be7af68dd73e6a507ba40abd", "affiliation": "Universit\u00e0 degli Studi  e INFN Milano (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "FU, Jinlin", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "1306a63e4c3ead0bfdadab349413e41f", "affiliation": "INFN Milano", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. GERACI, Angelo", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "19c5f401d93c8f60cf22556103bcc131", "affiliation": "Scuola Normale Superiore and INFN-Pisa", "_fossil": "contributionParticipationMetadata", "fullName": "MARINO, Pietro", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "5807ffd1efe5a1f54da221c0d41224b4", "affiliation": "Scuola Normale Superiore and INFN-Pisa", "_fossil": "contributionParticipationMetadata", "fullName": "MORELLO, Michael Joseph", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "68d3e1fabecb9e0af1f5a3b160560f1e", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "NERI, Nicola", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "087a43187d5192d7fedbc73ce724c42b", "affiliation": "Sezione di Pisa (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "NINCI, Daniele", "id": "17"}, {"_type": "ContributionParticipation", "emailHash": "c21ec228a048ba09a005a1b3a6397441", "affiliation": "INFN Milano", "_fossil": "contributionParticipationMetadata", "fullName": "PETRUZZO, Marco", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "b476c2db4ff2bd57cae314d1c1383089", "affiliation": "Sezione di Pisa (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "PIUCCI, Alessio", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "d69dba50343cb2e5ac8dc59cc0026b7c", "affiliation": "Pisa University and INFN", "_fossil": "contributionParticipationMetadata", "fullName": "PUNZI, Giovanni", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "d21516c19a496b36383e8201f9b3c968", "affiliation": "INFN and Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "RISTORI, Luciano", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "1a0990e379efe88f89607b25fedd1916", "affiliation": "Sezione di Pisa (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "SPINELLA, Franco", "id": "14"}, {"_type": "ContributionParticipation", "emailHash": "1d0cc622fe5f10a2af782831211bfe71", "affiliation": "Scuola Normale Superiore and INFN-Pisa", "_fossil": "contributionParticipationMetadata", "fullName": "STRACKA, Simone", "id": "15"}, {"_type": "ContributionParticipation", "emailHash": "12d69d97fa9bbafb09f491a826571cb0", "affiliation": "Sezione di Pisa (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "WALSH, John", "id": "16"}], "title": "An artificial retina processor for track reconstruction at the full LHC crossing rate", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [], "_type": "Contribution", "description": "The INFN-RETINA is an R&D project aimed at developing and implementing a parallel computational methodology allowing to reconstruct events with hundred of charged-particle tracks in pixel and silicon strip detectors at 40 MHz, thus suitable for processing LHC events at the full crossing frequency. For this purpose we design and test a massively parallel pattern-recognition algorithm, inspired by studies of the processing of visual images by the brain as it happens in nature. We find that high-quality tracking in large detectors is possible with sub-microseconds latencies when this algorithm is implemented in modern, high-speed, high-bandwidth FPGA devices. This opens a possibility of making track reconstruction happen transparently as part of the detector readout. The detailed geometry and charged-particle activity of a large already-built tracking detector are simulated and used to assess the performance of an artificial retina processor prototype at the current available silicon readout frequency of 1MHz.  TEL62 boards, equipped with 4 Altera Stratix III FPGAs providing the adequate computing power, are used for the processing stage of the prototype. In addition, a silicon telescope detector consisting of 8 layers of single-sided silicon strip detectors with 512 strips each, with a size of about 10 cm x 10 cm and the strip pitch of 183 microns,  has been built to perform a full realistic test of the artificial retina processor. We report on the first results of such  fast tracking device based on test with simulated LHC events, cosmic rays and on a particle beam.", "track": "Track1: Online computing ", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/514", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": null, "keywords": [], "id": "517", "speakers": [{"_type": "ContributionParticipation", "emailHash": "44774c9d30ed9c19187ac6eeafe75b1a", "affiliation": "Centre de Physique des Particules de Marseille, CNRS\/IN2P3", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HOFFMANN, Dirk", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "44774c9d30ed9c19187ac6eeafe75b1a", "affiliation": "Centre de Physique des Particules de Marseille, CNRS\/IN2P3", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HOFFMANN, Dirk", "id": "0"}], "title": "40-Gbps Data-Acquisition System for NectarCAM", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [], "_type": "Contribution", "description": "The CTA experiment will be the next generation ground-based gamma-ray instrument. It will be made up of approximately 100 telescopes of at least three different sizes, from 6 to 24 meters in diameter.\r\n\r\nThe previously presented prototype of a high speed data acquisition (DAQ) system for the Cherenkov Telescope Array has become concrete within the NectarCAM project, one of the most challenging camera projects due to his 40-Gbps average output rate on 265 Ethernet 1000baseT links, bundled to 40Gbps on SFP+ links and reduced to 10Gbps after event-building. Design constraints included procurement of a maximum of components as COTS products for an operation period of at least 30 years. Hence the results of the generic hardware characterisation are supposed to serve as a reference for similar setups. \r\n\r\nTests of single components and the whole data acquisition chain have been carried out with standard network analysing tools as well as a purpose-built SBC cluster providing 320 physical gigabit Ethernet ports. In order to mimic the total synchronicity (some nanoseconds) of the 265 camera front-end modules, we implemented a light-weight PTP synchronisation that can synchronise the outputs of all boards to an average of approximately 100ns. A multi-purpose system with FPGA boards delivering Ethernet packets on 48 ports with the same characteristics as the real front-end has completed specific aspects of our test. We will present the results of all tests that could be performed ahead of the delivery of the first complete real camera hardware foreseen for 2015.", "track": "Track1: Online computing ", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "6a27d5d2c09efddc42e98ff8c12df6f5", "affiliation": "Centre de Physique des Particules de Marseilles \/ CNRS", "_fossil": "contributionParticipationMetadata", "fullName": "HOULES, Julien", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "cabae6e485a5a9643903acdc3adcf1d6", "affiliation": "CEA", "_fossil": "contributionParticipationMetadata", "fullName": "LOUIS, frederic", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "1eaf8c393e2f471a8bb76e7f976db798", "affiliation": "CEA - Centre d'Etudes de Saclay (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "SIZUN, Patrick Yves", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "9bf829af3f7d471186ef668150a4bbc4", "affiliation": "IRFU - CEA", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MOUDDEN, Yassir", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/contribution\/517", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "516", "speakers": [{"_type": "ContributionParticipation", "emailHash": "698c361cfbc9ed204ca9f7b8fc5bb4cd", "affiliation": "Univ. of Minnesota Duluth", "_fossil": "contributionParticipationMetadata", "fullName": "HABIG, Alec", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "a03131261466f88a56cf5516acacef47", "affiliation": "University of Minnesota", "_fossil": "contributionParticipationMetadata", "fullName": "ZIRNSTEIN, Jan", "id": "0"}], "title": "Integration of the Super Nova Early Warning System with the NOvA Trigger", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-08T19:14:27.031628+00:00", "description": "", "title": "nova-snews.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/516\/attachments\/578625\/796783\/nova-snews.pdf", "filename": "nova-snews.pdf", "content_type": "application\/pdf", "type": "file", "id": 796783, "size": 6636398}], "title": "Slides", "default_folder": false, "id": 578625, "description": ""}], "_type": "Contribution", "description": "The NOvA experiment, with a baseline of 810 km, samples Fermilab's upgraded NuMI beam with a Near Detector on-site and a Far Detector (FD) at Ash River, MN, to observe oscillations of muon neutrinos. The 344,064 liquid scintillator-filled cells of the 14 kton FD provide high granularity of a large detector mass and enable us to also study non-accelerator based neutrinos with our Data Driven Trigger framework. This talk\/poster will focus on the real time integration of the SNEWS with the NOvA Trigger where we have set up an XML-RPC based messaging system to inject the SNEWS signal directly into our trigger. This presents a departure from the E-Mail based notification mechanism used by SNEWS in the past and allows NOvA more control over propagation and transmission timing. Message propagation time studies as well as DAQ upgrades to accommodate larger data buffers and improved data readout are discussed.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578625", "resources": [{"_type": "LocalFile", "name": "nova-snews.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/516\/attachments\/578625\/796783\/nova-snews.pdf", "fileName": "nova-snews.pdf", "_fossil": "localFileMetadata", "id": "796783", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "698c361cfbc9ed204ca9f7b8fc5bb4cd", "affiliation": "Univ. of Minnesota Duluth", "_fossil": "contributionParticipationMetadata", "fullName": "HABIG, Alec", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/516", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "458", "speakers": [{"_type": "ContributionParticipation", "emailHash": "30100f9005bbca9fcc88c53f4c6f5d36", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BIERY, Kurt", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "30100f9005bbca9fcc88c53f4c6f5d36", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BIERY, Kurt", "id": "0"}], "title": "Recent Developments in the Infrastructure and Use of artdaq", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T16:26:59.382933+00:00", "description": "artdaq poster for CHEP 2015.", "title": "artdaq_CHEP_2015_Poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/458\/attachments\/578626\/796784\/artdaq_CHEP_2015_Poster.pdf", "filename": "artdaq_CHEP_2015_Poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796784, "size": 3647796}], "title": "Poster", "default_folder": false, "id": 578626, "description": ""}], "_type": "Contribution", "description": "The artdaq data acquisition software toolkit has been developed within the Fermilab Scientific Computing Division, and it is being used by a growing number of high-energy and cosmology experiments.  It currently provides data transfer, event building, run control, and event analysis functionality.  The event analysis functionality is provided by the art framework, which has also been developed at Fermilab and which is used for offline reconstruction and simulation by many experiments at Fermilab.  The format that is used to store the data is ROOT.\r\n\r\nRecent enhancements to the toolkit include a custom vector class to improve the performance of buffer allocation, configurable options for dealing with back-pressure, and improved organization of the software packages that are part of the toolkit.  The enhancements that are currently in development include the addition of web-based control and monitoring interfaces, improved monitoring of DAQ system performance, and improved handling of online monitoring data.\r\n\r\nArtdaq is successfully being used for the DarkSide-50, LArIAT, LBNE 35 ton prototype, and Mu2e experiments.  In addition, a demo system has been developed so that prospective and new users can gain experience in using the toolkit to develop their own DAQ system.\r\n\r\nWe will describe the functionality that is currently available in the toolkit, the advantages that are gained by its use, our experience in deploying it to experiments, future plans, and opportunities for integrating with commercial off-the-shelf hardware modules.", "track": "Track1: Online computing ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578626", "resources": [{"_type": "LocalFile", "name": "artdaq_CHEP_2015_Poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/458\/attachments\/578626\/796784\/artdaq_CHEP_2015_Poster.pdf", "fileName": "artdaq_CHEP_2015_Poster.pdf", "_fossil": "localFileMetadata", "id": "796784", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "401695ca248365aedad9213722d76b55", "affiliation": "Department of Physics", "_fossil": "contributionParticipationMetadata", "fullName": "GREEN, Christopher", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "6fac3d6f6a7e5fdfeca402e31f4c1868", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "KOWALKOWSKI, Jim", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "965436ddc794ac4aa9fbb331c9ed5f17", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LUKHANIN, Gennadiy", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "fe605a2b3b5c9c50970007d7ad02a6dd", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PATERNO, Marc", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "bf5a1b8f1d23730b7fbe48902a05e14a", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "RECHENMACHER, Ronald", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "a948a0c6c1c44467fb55519c2d204072", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. FREEMAN, John", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "99b5f0be25ba5de19c37365d55205e60", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "FLUMERFELT, Eric", "id": "7"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/458", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "459", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e0fe28880ea21268b1b861066df28f20", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. DEMAR, Phil", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "e544eb5350aa2c56ffd3300a817b8fab", "affiliation": "Fermi National Accelerator Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WU, Wenji", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e0fe28880ea21268b1b861066df28f20", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. DEMAR, Phil", "id": "0"}], "title": "Application-Oriented Network Traffic Analysis based on GPUs", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T23:44:30.804939+00:00", "description": "", "title": "LDRD_poster_CHEP2015-v1.2.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/459\/attachments\/578627\/796785\/LDRD_poster_CHEP2015-v1.2.pdf", "filename": "LDRD_poster_CHEP2015-v1.2.pdf", "content_type": "application\/pdf", "type": "file", "id": 796785, "size": 1338220}], "title": "Slides", "default_folder": false, "id": 578627, "description": ""}], "_type": "Contribution", "description": "Software-Defined Networking (SDN) has emerged as a major development direction in network technology. Conceptually, SDN enables customization of forwarding through network infrastructure on a per-flow basis. With SDN, a high impact LHC data flow could be allocated a \u201cslice\u201d of the network infrastructure.  Functionally, the data flow would have a private path through the network infrastructure, potentially with performance guarantees. SDN offers a technology path to meet extreme data movement requirements, like LHC, over highly capacious, general use network infrastructure.  Standardized implementations of SDN, specifically OpenFlow, are now becoming available on network routers and switches.\r\n\r\nConventionally, SDN configuration is implemented by users or applications.  User-driven SDN configuration is manual, and therefore tends to be static and inherently inefficient.  It doesn\u2019t scale well to complex traffic patterns, such as the traffic patterns generated by LHC experiments.  Application-driven SDN configuration is typically dynamic, representing a more efficient on-demand type of approach for network resources. However, building SDN-awareness into applications is highly complex and extremely challenging. It places ongoing software adaptation and maintenance burdens on the application developers. Finally, there will always be major policy issues or constraints on an application\u2019s ability to modify the network.\r\n\r\nWe believe networks need an application-awareness capability. With such a capability, traffic identified as \u201cspecial\u201d by the network could be provided with a custom network \u201cslice\u201d without manual intervention or modification to applications. To facilitate this capability, we are currently developing a network traffic analysis tool to identify \u201cspecial\u201d types of network traffic. Our tool is designed to perform this analysis in real-time. We accomplish this through GPU technology, which is extremely well-suited to parallelize the processing and analysis of the network traffic. Our traffic analysis uses basic traffic characteristics such as packet size, packet spacing, flow duration, and multi-stream flow presence to develop the traffic pattern signatures and templates necessary for traffic classification and\/or identification.   Our ultimate objective is to create a tool that could facilitate dynamic customization of SDN-capable network infrastructure, based on real-time traffic analysis and detection.  \r\n\r\nA prototype of our analysis tool is currently in development.  This talk will describe the tool\u2019s architectural and design principles, how it works in implementation, the challenges faced in high speed (40GE\/100GE) network environments, and our initial trial results. In addition, future directions for the project, as well as wider applicability for the underlying technology, will be discussed.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578627", "resources": [{"_type": "LocalFile", "name": "LDRD_poster_CHEP2015-v1.2.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/459\/attachments\/578627\/796785\/LDRD_poster_CHEP2015-v1.2.pdf", "fileName": "LDRD_poster_CHEP2015-v1.2.pdf", "_fossil": "localFileMetadata", "id": "796785", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "e544eb5350aa2c56ffd3300a817b8fab", "affiliation": "Fermi National Accelerator Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WU, Wenji", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "8092b9bf81a3c59637ea46fa36c99d20", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "ZHANG, Liang", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/459", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:15:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "450", "speakers": [{"_type": "ContributionParticipation", "emailHash": "fd06959b93c405340e9900abf962beb7", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "LAHIFF, Andrew David", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "fd06959b93c405340e9900abf962beb7", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "LAHIFF, Andrew David", "id": "0"}], "title": "Implementation of the vacuum model using HTCondor", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T22:03:13.036760+00:00", "description": "", "title": "HTCondorVacuum-CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/450\/attachments\/578628\/796786\/HTCondorVacuum-CHEP2015.pdf", "filename": "HTCondorVacuum-CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796786, "size": 778160}], "title": "Slides", "default_folder": false, "id": 578628, "description": ""}], "_type": "Contribution", "description": "The recently introduced vacuum model offers an alternative to the traditional methods that virtual organisations (VOs) use to run computing tasks at sites, where they either submit jobs using grid middleware or create virtual machines (VMs) using cloud APIs. In the vacuum model VMs are created and contextualized by the site itself, and start the appropriate pilot job framework which fetches real jobs. This allows sites to avoid the effort required for running grid middleware or a cloud. Here we present an implementation of the vacuum model based entirely on HTCondor, making use of HTCondor's ability to manage VMs. Extensive use is made of job hooks, including for preparing fast local storage for use in the VMs, carrying out contextualization, and updating job ClassAds about the status of the VMs and their payloads. VMs for each supported VO are created at regular intervals. If there is no work or there are fatal errors, no additional VMs are created. On the other hand, if there is real work running, further VMs can be created. Since the HTCondor negotiator decides whether to run the VMs or not, fairshares are naturally respected. Normal grid or locally-submitted jobs can run on the same resources and share the same physical worker nodes that are also used as hypervisors for running VMs.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578628", "resources": [{"_type": "LocalFile", "name": "HTCondorVacuum-CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/450\/attachments\/578628\/796786\/HTCondorVacuum-CHEP2015.pdf", "fileName": "HTCondorVacuum-CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796786", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/450", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:00:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "451", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a11b77fc54a5a3fd19cf040a3c41e953", "affiliation": "Imperial College London", "_fossil": "contributionParticipationMetadata", "fullName": "MARTYNIAK, Janusz", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "193930d927704046a5157220451008bd", "affiliation": "IIT, Chicago", "_fossil": "contributionParticipationMetadata", "fullName": "RAJARAM, Durga", "id": "0"}], "title": "MAUS: The MICE Analysis and User Software", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T18:25:07.617851+00:00", "description": "", "title": "maus_chep2015_v3.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/451\/attachments\/578629\/796787\/maus_chep2015_v3.pdf", "filename": "maus_chep2015_v3.pdf", "content_type": "application\/pdf", "type": "file", "id": 796787, "size": 2861420}], "title": "Slides", "default_folder": false, "id": 578629, "description": ""}], "_type": "Contribution", "description": "The Muon Ionization Cooling Experiment (MICE) has developed the MICE Analysis User Software (MAUS) to simulate and analyse experimental data.  It serves as the primary codebase for the experiment, providing for offline batch simulation and reconstruction as well as online data quality checks . The software provides both traditional particle physics functionalities such as track reconstruction and particle identification, and  accelerator physics functions such as calculating transfer matrices and emittances. The code is structured in a Map-Reduce framework to allow parallelization whether on a personal computer or in the control room. MAUS allows users to develop in either Python or C++ and provides APIs for both. Various software engineering practices from industry are also used to ensure correct and maintainable physics code, which include unit, functional and integration tests, continuous integration and load testing, code reviews, and distributed version control systems. The software framework and the simulation and reconstruction capabilities are described.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578629", "resources": [{"_type": "LocalFile", "name": "maus_chep2015_v3.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/451\/attachments\/578629\/796787\/maus_chep2015_v3.pdf", "fileName": "maus_chep2015_v3.pdf", "_fossil": "localFileMetadata", "id": "796787", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/451", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "452", "speakers": [{"_type": "ContributionParticipation", "emailHash": "fd06959b93c405340e9900abf962beb7", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "LAHIFF, Andrew David", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "fd06959b93c405340e9900abf962beb7", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "LAHIFF, Andrew David", "id": "0"}], "title": "Experience with batch systems and clouds sharing the same physical resources", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-17T00:55:19.553009+00:00", "description": "", "title": "BatchCloudSameResources-CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/452\/attachments\/578631\/796789\/BatchCloudSameResources-CHEP2015.pdf", "filename": "BatchCloudSameResources-CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796789, "size": 1704015}], "title": "Poster", "default_folder": false, "id": 578631, "description": ""}, {"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-17T00:53:13.077744+00:00", "description": "", "title": "BatchCloudSameResources_Lightning.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/452\/attachments\/578630\/796788\/BatchCloudSameResources_Lightning.pdf", "filename": "BatchCloudSameResources_Lightning.pdf", "content_type": "application\/pdf", "type": "file", "id": 796788, "size": 548582}], "title": "Slides", "default_folder": false, "id": 578630, "description": ""}], "_type": "Contribution", "description": "Today it is becoming increasingly common for WLCG sites to provide both grid and cloud compute resources. In order to avoid the inefficiencies caused by static partitioning of resources it is necessary to integrate grid and cloud resources. There are two options to consider when doing this. The simplest option is to have the cloud manage all the physical hardware and use entirely virtualised worker nodes in the batch system. The downside of this is that everything is virtualised, whereas it might be useful to be able to run jobs in the batch system directly on hardware in order to achieve the best performance. In such a configuration it is essential that batch jobs can't interfere with the virtual machines running on the same node or affect the hypervisors in a negative way, hence containerisation of batch jobs is particularly important. It is also important to consider the implications of having both a batch system and cloud scheduling jobs and virtual machines on the same machines. We present an investigation into the feasibility of having a batch system and cloud sharing the same set of physical resources carried out at the RAL Tier-1.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578631", "resources": [{"_type": "LocalFile", "name": "BatchCloudSameResources-CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/452\/attachments\/578631\/796789\/BatchCloudSameResources-CHEP2015.pdf", "fileName": "BatchCloudSameResources-CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796789", "_deprecated": true}], "_deprecated": true}, {"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578630", "resources": [{"_type": "LocalFile", "name": "BatchCloudSameResources_Lightning.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/452\/attachments\/578630\/796788\/BatchCloudSameResources_Lightning.pdf", "fileName": "BatchCloudSameResources_Lightning.pdf", "_fossil": "localFileMetadata", "id": "796788", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "2463c98197fe73772aa1cd10b34131aa", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "COLLIER, Ian Peter", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "a4c2629976026a424cedc51ff4d288ef", "affiliation": "STFC", "_fossil": "contributionParticipationMetadata", "fullName": "RYALL, George", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "27e591c48f3d8d36d8501eb3b2b88453", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "BARNSLEY, Frazer", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/452", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "453", "speakers": [{"_type": "ContributionParticipation", "emailHash": "959f53b1cd9e1a3573140173e1578241", "affiliation": "urn:Google", "_fossil": "contributionParticipationMetadata", "fullName": "ROCCO, Dominick", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "959f53b1cd9e1a3573140173e1578241", "affiliation": "urn:Google", "_fossil": "contributionParticipationMetadata", "fullName": "ROCCO, Dominick", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "16bdf91b327521302d812ff7c61549ed", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "BACKHOUSE, Christopher", "id": "1"}], "title": "A Data Summary File Structure and Analysis Tools for Neutrino Oscillation Analysis at the NOvA Experiment", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The NuMI Off-axis Neutrino Experiment (NOvA) is designed to study neutrino oscillations in the NuMI beam at Fermilab. Neutrinos at the Main Injector (NuMI) is currently being upgraded to provide 700 kW for NOvA.  A 14 kt Far Detector in Ash River, MN and a functionally identical 0.3 kt Near Detector at Fermilab are positioned 810 km apart in the NuMI beam line. The fine granularity of the NOvA detectors provides a detailed representation of particle trajectories.  The data volume associated with such granularity, however, poses problems for analyzing data with ease and speed.  NOvA has developed a data summary file structure which discards the full event record in favor of higher-level reconstructed information.  A general-purpose framework for neutrino oscillation measurements has been developed for analysis of these data summary files.  We present the design methodology for this new file format as well as the analysis framework and the role it plays in producing NOvA physics results.", "track": "Track2: Offline software ", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/453", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "454", "speakers": [{"_type": "ContributionParticipation", "emailHash": "3e84f64e37bceead0577ddf1db41bb2a", "affiliation": "SLAC National Accelerator  Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GRAF, Norman Anthony", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "3e84f64e37bceead0577ddf1db41bb2a", "affiliation": "SLAC National Accelerator  Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GRAF, Norman Anthony", "id": "0"}], "title": "The Linear Collider Software Framework", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "Detectors at future electron-positron linear colliders such as ILC or CLIC will require unprecedentedly precise tracking, vertexing, and calorimetry in order to meet the ambitious physics goals of the experimental program. The physics performance of different detector geometries and technologies has to be realistically estimated. These assessments require sophisticated and flexible full detector simulation and reconstruction software. At the heart of the linear collider detectors lies the particle flow reconstruction. The goal of unambiguously associating tracks and showers to individual particles requires the combination of precise, lightweight trackers combined with fine-grained \u201cimaging\u201d calorimeters and advanced clustering software. Detailed physics and detector optimization studies are taking place in the CLICdp, ILD, and SiD groups. The similarities between the different detector concepts allow for the use of common software tools. All the concepts share an event data model and persistency format which enables the sharing of files and applications across the concepts. Particle flow clustering, vertexing and flavor tagging is already provided by standalone packages via lightweight interfaces. In the near future the geometry information for all detector layouts will be provided by a common source for the simulation and reconstruction programs, providing further re-use of software between the collaborations. In addition, a track reconstruction package is currently under development. The sharing and development of flexible software tools not only saves precious time and resources,  the use of common tools for different detectors also helps to uncover bugs or inefficiencies that would be harder to spot without multiple users. The concept of generic software tools and some of the programs themselves can be beneficial to experiments beyond the linear collider community.", "track": "Track2: Offline software ", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "f5dcc35395831a55f6722d12d4272492", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "GAEDE, Frank-Dieter", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "57f44f8c3e9fc41ad62c4fdcbf217f90", "affiliation": "High Energy Accelerator Research Organization (JP)", "_fossil": "contributionParticipationMetadata", "fullName": "MIYAMOTO, Akiya", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "84f3f721370dba12b47a611b1a36081e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SAILER, Andre", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/454", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "455", "speakers": [{"_type": "ContributionParticipation", "emailHash": "22a1a3e2ab267299b1754df69296edaf", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "DAL PRA, Stefano", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "22a1a3e2ab267299b1754df69296edaf", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "DAL PRA, Stefano", "id": "0"}], "title": "Efficient provisioning for multicore applications with LSF", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-08T12:45:58.307891+00:00", "description": "", "title": "poster_mcore_chep2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/455\/attachments\/578632\/796790\/poster_mcore_chep2015.pdf", "filename": "poster_mcore_chep2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796790, "size": 703990}], "title": "Slides", "default_folder": false, "id": 578632, "description": ""}], "_type": "Contribution", "description": "Tier-1 sites providing computing power for HEP experiments are\r\nusually tightly designed for high throughput performances. This\r\nis pursued by reducing the variety of supported usecases and\r\ntuning for performances those ones, the most important of which\r\nhave been that of single-core jobs. Moreover, the usual workload\r\nis saturation: each available core in the farm is in use and\r\nthere are queued jobs waiting for their turn to run. Enabling\r\nmulticore jobs thus requires dedicating a number of hosts where\r\nto run, and waiting for them to free the needed number of cores.\r\nThis drain-time introduces a loss of computing power driven by\r\nthe number of unusable empty cores.\r\n\r\nAs an increasing demand for multicore capable resources have emerged,\r\na Task Force have been constituted in WLCG, with the goal to define a\r\nsimple and efficient multicore resource provisioning model.\r\n\r\nThis paper details the work done at the INFN Tier1 to enable\r\nmulticore support for the LSF batch system, with the intent of\r\nreducing to the minimum the average number of unused cores.\r\n\r\nThe adopted strategy has been that of dedicating to multicore a\r\ndynamic set of nodes, whose dimension is mainly driven by the number\r\nof pending multicore requests and fairshare priority of the submitting\r\nuser. The node status transition, from single to multi core et vice\r\nversa, is driven by a finite state machine which is implemented in a\r\ncustom multicore director script, running in the cluster.\r\n\r\nAfter describing and motivating both the implementation and the\r\ndetails specific to the LSF batch system, results about performance\r\nare reported. Factors having positive and negative impact on the\r\noverall efficiency are discussed and solutions to reduce at most the\r\nnegative ones are proposed.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578632", "resources": [{"_type": "LocalFile", "name": "poster_mcore_chep2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/455\/attachments\/578632\/796790\/poster_mcore_chep2015.pdf", "fileName": "poster_mcore_chep2015.pdf", "_fossil": "localFileMetadata", "id": "796790", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/455", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:15:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "456", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e0fe28880ea21268b1b861066df28f20", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. DEMAR, Phil", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e0fe28880ea21268b1b861066df28f20", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. DEMAR, Phil", "id": "0"}], "title": "Migrating to 100GE WAN Infrastructure at Fermilab", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T11:45:24.255883+00:00", "description": "", "title": "100GE_CHEP_v1.2.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/456\/attachments\/578633\/796791\/100GE_CHEP_v1.2.pdf", "filename": "100GE_CHEP_v1.2.pdf", "content_type": "application\/pdf", "type": "file", "id": 796791, "size": 1408933}], "title": "Slides", "default_folder": false, "id": 578633, "description": ""}], "_type": "Contribution", "description": "Fermilab is in the process of upgrading its wide-area network facilities to 100GE technology.  One might assume that migration to be relatively straightforward, with forklift upgrades of our existing network perimeter devices to 100GE-capable platforms, and accompanying deployment of 100GE WAN services.  However, our migration to 100GE WAN technology has proven to be significantly more complicated than that.  Several key factors are driving this complexity:\r\n\r\n[1] The Laboratory has historically separated its high impact science (LHC) data movement from its general internet traffic.  For a decade, this has been implemented with additional physical infrastructure, included a separate border router and multiple 10GE WAN links dedicated to the science data traffic.  However, the extreme cost of 100GE routers, coupled with corresponding high cost of 100GE WAN links, has necessitated a consolidation of the current WAN infrastructure into a simpler configuration, one in which science data and general internet traffic must share the same 100GE infrastructure.\r\n\r\n[2] The Laboratory has also been able to rely on the diversity inherent in its 8x10GE WAN link configuration to provide acceptable resiliency in its WAN services.  Replacement of the multiple 10GE links with a single 100GE link has the potential to seriously compromise that diversity.  To alleviate that vulnerability, plans for deployment of a second 100GE WAN link in early 2015 are already under way. However, preserving full geographic diversity with limited 100GE equipment has presented additional challenges. \r\n\r\n[3] With the availability of 100GE WAN infrastructure, as well as 40GE-attached network R&D systems, traffic generated as part of network research activities also potentially becomes much more significant.   While this is a positive thing for the Laboratory\u2019s network R&D efforts, it adds yet another class of high-impact traffic competing for the new 100GE WAN resources. \r\n\r\nThis talk will describe how Fermilab is consolidating its WAN infrastructure in order to implement 100GE technology, while still preserving its long term philosophy about keeping high impact science data traffic, its general internet traffic, and it network R&D traffic logically isolated from each other.  Current virtualization techniques being deployed will be discussed, as will investigations into use of emerging Software Defined Networking (SDN) and OpenFlow technologies as a longer term solution.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578633", "resources": [{"_type": "LocalFile", "name": "100GE_CHEP_v1.2.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/456\/attachments\/578633\/796791\/100GE_CHEP_v1.2.pdf", "fileName": "100GE_CHEP_v1.2.pdf", "_fossil": "localFileMetadata", "id": "796791", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "05d48e3baee909cbc045a80cdfc707aa", "affiliation": "FERMILAB", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. BOBYSHEV, Andrey", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "8605ce23ffd688eaa01d625be952e072", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "GRIGALIUNAS, Vyto", "id": "2"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/456", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:00:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "457", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e544eb5350aa2c56ffd3300a817b8fab", "affiliation": "Fermi National Accelerator Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WU, Wenji", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e544eb5350aa2c56ffd3300a817b8fab", "affiliation": "Fermi National Accelerator Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WU, Wenji", "id": "0"}], "title": "Multicore-Aware Data Transfer Middleware (MDTM)", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T11:46:44.490152+00:00", "description": "", "title": "MDTM_CHEP_v1.2.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/457\/attachments\/578634\/796792\/MDTM_CHEP_v1.2.pdf", "filename": "MDTM_CHEP_v1.2.pdf", "content_type": "application\/pdf", "type": "file", "id": 796792, "size": 1962596}], "title": "Slides", "default_folder": false, "id": 578634, "description": ""}], "_type": "Contribution", "description": "Multicore and manycore have become the norm for scientific computing environments. Multicore\/manycore platform architectures provide advanced capabilities and features that can be exploited to enhance data movement performance for large-scale distributed computing environments, such as LHC.  However, existing data movement tools do not take full advantage of these capabilities and features. The result is inefficiencies in data movement operations, particularly within the wide area scope. These inefficiencies will become more pronounced as networks upgrade to 100GE infrastructure, and host systems correspondingly migrate to 10GE, Nx10GE, and 40GE technologies. To address these inefficiencies and limitations, DOE\u2019s Advanced Scientific Computing Research (ASCR) office has funded Fermilab and Brookhaven National Laboratory to collaboratively work on the Multicore-Aware Data Transfer Middleware (MDTM) project. MDTM aims to accelerate data movement toolkits on multicore systems. Essentially, the MDTM project consists of two major components: \r\n\r\n-  MDTM middleware services to harness multicore parallelism and make intelligent decisions that align CPU, memory, and I\/O device operations in a manner which optimizes performance for higher layer applications.\r\n\r\n-  MDTM-enabled data transfer applications (client or server) that can utilize MDTM middleware to reserve and manage multiple CPU cores, memory, network devices, disk storage as an integrated resource entity, thus achieving higher throughput and enhanced quality of service when compared with existing approaches.\r\n\r\nA prototype version of MDTM is currently undergoing testing and evaluation. This talk will describe MDTM\u2019s architectural and design principles, how it works in implementation, and initial test results in comparison to standard GridFTP and BBCP operations. In addition, future directions for the project will be discussed, including the notion of enabling an external resource scheduling capability for MDTM, thus making it a reservable component for application-driven end-to-end path resource reservations.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578634", "resources": [{"_type": "LocalFile", "name": "MDTM_CHEP_v1.2.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/457\/attachments\/578634\/796792\/MDTM_CHEP_v1.2.pdf", "fileName": "MDTM_CHEP_v1.2.pdf", "_fossil": "localFileMetadata", "id": "796792", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "e0fe28880ea21268b1b861066df28f20", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. DEMAR, Phil", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "8092b9bf81a3c59637ea46fa36c99d20", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "ZHANG, Liang", "id": "2"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/457", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "179", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5464ee52c2740264980ed36d4553e207", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GANIS, Gerardo", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "3265caee611e332149bc3247653d64bb", "affiliation": "Istituto Nazionale Fisica Nucleare - Laboratori Nazionali di Frascati (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "DI NARDO, Roberto", "id": "0"}], "title": "PROOF-based analysis on ATLAS Italian Tiers with Prodsys2 and Rucio", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:40:11.663857+00:00", "description": "", "title": "poster_pod_chep2015_v11_1.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/179\/attachments\/578635\/796793\/poster_pod_chep2015_v11_1.pdf", "filename": "poster_pod_chep2015_v11_1.pdf", "content_type": "application\/pdf", "type": "file", "id": 796793, "size": 1394470}], "title": "Slides", "default_folder": false, "id": 578635, "description": ""}], "_type": "Contribution", "description": "During the LHC Run-1, Grid resources in ATLAS have been managed by the PanDA and DQ2 systems. In order to meet the needs for the LHC Run-2, Prodsys2 and Rucio are used as the new ATLAS Workload and Data Management systems.\r\n\r\nThe data are stored under various formats in ROOT files and end-user physicists have the choice to use either the ATHENA framework or directly ROOT. Within the ROOT data analysis framework it is possible to perform analysis of huge sets of ROOT files in parallel with PROOF on clusters of computers (usually organised in analysis facilities) or multi-core machines. In addition, PROOF-on-Demand (PoD) can be used to enable PROOF on top of an existing resource management system.\r\n\r\nIn this work, we present the first performances obtained enabling PROOF-based analysis in the Italian ATLAS Tier-1\/Tier-2 sites within the new ATLAS workload system.\r\n\r\nBenchmark tests of data access with the httpd protocol, using also the httpd redirector, will be shown. We also present results on the startup latency tests using the new PROOF functionality of dynamic workers addition, which improves the performance of PoD using Grid resources. These new results will be compared with the expected improvements discussed in a previous work", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578635", "resources": [{"_type": "LocalFile", "name": "poster_pod_chep2015_v11_1.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/179\/attachments\/578635\/796793\/poster_pod_chep2015_v11_1.pdf", "fileName": "poster_pod_chep2015_v11_1.pdf", "_fossil": "localFileMetadata", "id": "796793", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "5464ee52c2740264980ed36d4553e207", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GANIS, Gerardo", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "309ef96502d1d60ee4dbb1232a168778", "affiliation": "Laboratori Nazionali di Frascati (LNF) - Istituto Nazionale di F", "_fossil": "contributionParticipationMetadata", "fullName": "VILUCCHI, Elisabetta", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/179", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": null, "keywords": [], "id": "178", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6e3e3a748aabb784fee511b0bb53ae7e", "affiliation": "University of California-Unknown-Unknown", "_fossil": "contributionParticipationMetadata", "fullName": "FARRELL, Steven Andrew", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6e3e3a748aabb784fee511b0bb53ae7e", "affiliation": "University of California-Unknown-Unknown", "_fossil": "contributionParticipationMetadata", "fullName": "FARRELL, Steven Andrew", "id": "0"}], "title": "ATLAS analysis framework and software tools for ROOT\/C++", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "During ATLAS Run-1, high demand for analysis software useable outside the realm of Athena (the framework used by the ATLAS offline\u00a0software) led to the development of an analysis framework that can be used in a pure ROOT\/C++ software environment. This framework is\u00a0centrally maintained by the Analysis Software Group and provides an event loop and software tools for performing analysis jobs. Users\u00a0construct algorithms and steering macros which can be run on either local machines or on the grid in a transparent way. Some of the\u00a0powerful features of the framework include easy integration with grid tools, distributed computing with batch systems and PROOF, and\u00a0management of sample datasets and meta-data. A lightweight package build and management system is provided which simplifies the work\u00a0of the tool developer and analyzer in writing and using analysis packages. Commonly used packages are organized into lightweight\u00a0analysis releases which are distributed on the grid via CVMFS and can also be downloaded and compiled easily on a local machine or\u00a0laptop. This presentation will cover the technical details and features of this framework and discuss how it fits into a typical analysis\u00a0workflow in ATLAS.", "track": "Track2: Offline software ", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "9aa8aab852d2208db54db51d005243a7", "affiliation": "Iowa State University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "KRUMNACK, Nils Erik", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "564b07d2a072f7d0151deecccf138b46", "affiliation": "Uppsala University (SE)", "_fossil": "contributionParticipationMetadata", "fullName": "MADSEN, Alexander", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/contribution\/178", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "177", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6e3e3a748aabb784fee511b0bb53ae7e", "affiliation": "Lawrence Berkeley National Lab", "_fossil": "contributionParticipationMetadata", "fullName": "FARRELL, Steven Andrew", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "d3e2277ea45849205ed9ccb255a1a566", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "FARRELL, Steven Andrew", "id": "16"}], "title": "Dual-use tools and systematics-aware analysis workflows in the ATLAS Run-II analysis model", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-08T22:38:52.403145+00:00", "description": "", "title": "ATL-SOFT-SLIDE-2015-098.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/177\/attachments\/578636\/796794\/ATL-SOFT-SLIDE-2015-098.pdf", "filename": "ATL-SOFT-SLIDE-2015-098.pdf", "content_type": "application\/pdf", "type": "file", "id": 796794, "size": 517350}], "title": "Poster", "default_folder": false, "id": 578636, "description": ""}], "_type": "Contribution", "description": "The ATLAS analysis model has been overhauled for the upcoming run of data collection in 2015 at 13 TeV. One key component of this upgrade was the Event Data Model (EDM), which now allows for greater flexibility in the choice of analysis software framework and provides powerful new features that can be exploited by analysis software tools. A second key component of the upgrade is the introduction of a dual-use tool technology, which provides abstract interfaces for analysis software tools to run in either the Athena framework or a ROOT-based framework. The tool interfaces, including a new interface for handling systematic uncertainties, have been standardized for the development of improved analysis workflows and consolidation of high-level analysis tools. This presentation will cover the details of the dual-use tool functionality, the systematics interface, and how these features fit into a centrally supported analysis environment.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578636", "resources": [{"_type": "LocalFile", "name": "ATL-SOFT-SLIDE-2015-098.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/177\/attachments\/578636\/796794\/ATL-SOFT-SLIDE-2015-098.pdf", "fileName": "ATL-SOFT-SLIDE-2015-098.pdf", "_fossil": "localFileMetadata", "id": "796794", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "9aa8aab852d2208db54db51d005243a7", "affiliation": "Iowa State University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "KRUMNACK, Nils Erik", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c3b302a4fc2fd1ca19138cb3a3e5479a", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CALAFIURA, Paolo", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "a508134eca4b46b1624639f34897d734", "affiliation": "University of Arizona (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LEI, Xiaowen", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "3034074301a29ed1439e6f48e828ad6a", "affiliation": "University of Liverpool (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "LAYCOCK, Paul James", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "8d4c0721de129e328dffeb903264d729", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "KRASZNAHORKAY, Attila", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "a269c7cf23d88a0dd9fca5c9595ea8fb", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "ADAMS, David", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "68bff6e56eff0e2eca71ed9ad96cf2ec", "affiliation": "Centre National de la Recherche Scientifique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "DELSART, Pierre-Antoine", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "ce322777a6c04cb5d6b95977c554eabb", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ELSING, Markus", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "90edf3013d33df6d6f9a07fa816970b3", "affiliation": "Albert-Ludwigs-Universitaet Freiburg (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "KOENEKE, Karsten", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "8fc0de19145510b5f4a8150497be283a", "affiliation": "CEA\/IRFU,Centre d'etude de Saclay Gif-sur-Yvette (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "LANCON, Eric Christian", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "07fcf6b59f67c4b6bea1d11ef506274d", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LAVRIJSEN, Wim", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "5763073a5417a71722f2c1da0a4e09e3", "affiliation": "Stockholm University (SE)", "_fossil": "contributionParticipationMetadata", "fullName": "STRANDBERG, Sara Kristina", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "367529fac993f9961a7779b8db58bfac", "affiliation": "NIKHEF (NL)", "_fossil": "contributionParticipationMetadata", "fullName": "VERKERKE, Wouter", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "4e3da7fb6d5874433b05fb6379048232", "affiliation": "University of Sussex (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "VIVARELLI, Iacopo", "id": "14"}, {"_type": "ContributionParticipation", "emailHash": "f3c6d263b1ecf7c1f40b5a396ddbd9f4", "affiliation": "University of Manchester (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "WOUDSTRA, Martin", "id": "15"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/177", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:45:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "176", "speakers": [{"_type": "ContributionParticipation", "emailHash": "de056a9ce188e8aab9c55d46620d4a1b", "affiliation": "University of Texas (US)", "_fossil": "contributionParticipationMetadata", "fullName": "ONYISI, Peter", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "de056a9ce188e8aab9c55d46620d4a1b", "affiliation": "University of Texas (US)", "_fossil": "contributionParticipationMetadata", "fullName": "ONYISI, Peter", "id": "0"}], "title": "Event-Driven Messaging for Offline Data Quality Monitoring at ATLAS", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T07:23:22.708601+00:00", "description": "", "title": "20150319_chep_draft.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/176\/attachments\/578637\/796795\/20150319_chep_draft.pdf", "filename": "20150319_chep_draft.pdf", "content_type": "application\/pdf", "type": "file", "id": 796795, "size": 419310}], "title": "Slides", "default_folder": false, "id": 578637, "description": ""}], "_type": "Contribution", "description": "During LHC Run 1, the information flow through the offline data quality monitoring in ATLAS relied heavily on chains of processes polling each other's outputs for handshaking purposes. \u00a0This resulted in a fragile architecture with many possible points of failure and an inability to monitor the overall state of the distributed system. \u00a0We report on the status of a project undertaken during the long shutdown to replace the ad hoc synchronization methods with a uniform message queue system. \u00a0This enables the use of standard protocols to connect processes on multiple hosts; reliable transmission of messages between possibly unreliable programs; easy monitoring of the information flow; and the removal of inefficient polling-based communication.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578637", "resources": [{"_type": "LocalFile", "name": "20150319_chep_draft.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/176\/attachments\/578637\/796795\/20150319_chep_draft.pdf", "fileName": "20150319_chep_draft.pdf", "_fossil": "localFileMetadata", "id": "796795", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "444aa524a64355eedab8a53f613f35b6", "affiliation": "University of Oxford (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "FROST, James", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "f7eacb7fe0810d71177aefac5b4e433d", "affiliation": "University of Texas (US)", "_fossil": "contributionParticipationMetadata", "fullName": "ILCHENKO, Iurii", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "64c92f199e584476c40652acdaac4344", "affiliation": "University of Oregon (US)", "_fossil": "contributionParticipationMetadata", "fullName": "TORRENCE, Eric", "id": "3"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/176", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": null, "keywords": [], "id": "175", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5b3e54efbd9206c289ae5b94c5cc4bdb", "affiliation": "University of Chicago (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GARDNER JR, Robert William", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "5b3e54efbd9206c289ae5b94c5cc4bdb", "affiliation": "University of Chicago (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GARDNER JR, Robert William", "id": "0"}], "title": "Containerizing Software and Execution Platforms for Data Preservation and Analysis Reproducibility in ATLAS", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "Preserving the ability to describe and read data, and to ensure scientific reproducibility over very long time scales are of obvious great importance. \u00a0 A number of different approaches are being pursued by high energy physics experiments to preserve usability of data over the long term, covering the full stratum of layers that comprise scientific computation: processor architectures, physical device hardware, Linux OS and kernel, system-host \u201cadd-on\u201d libraries, the experiment's software releases, external scientific libraries, network accessible databases, end-user software and custom tools, as well as the high capacity production systems required to reprocess the data at scale. \u00a0At one end of the spectrum there are continuous validations and on-going software development to achieve active backwards compatibility; requiring raw data from the experiment in prior years be readable and reconstructable by all future software releases is one example of this approach and is the baseline in ATLAS. \u00a0At the other end, there is the idea of preserving not only the data but also a full virtual machine image of the execution environment used by the processing and analysis software as it existed at the time of publication, as well as associated databases and other dependencies. \u00a0Clearly there are virtues and costs associated with each approach, and identifying the optimal \u201cpoint of preservation\u201d will differ by collaboration policies and priorities, software setup and resources available. \u00a0In the DASPOS project (Data and Software Preservation for Open Science) we are exploring the \u00a0role that tools which capture complex software dependencies and which can virtualize the essential function of a worker node environment can play in sustaining long term data usability. \u00a0Specifically in the context of the ATLAS processing and analysis environment, we investigate how software dependency gathering and environment provenance tracking using tools, such as the Parrot Virtual File System and the Provenance-to-Use (PTU) toolsets, can be used with user space isolation tools offered by Linux containers, namely cgroups and namespaces, and popular ease-of-use frameworks for containers such as Docker, to create self-contained, lightweight and portable software images. Such images could be re-instantiated by future container execution services and tracked longitudinally by an early exception detection framework (discovering for example, initially unrecognized dependencies in the environment, sensitivities to kernel changes on new container hosting platforms, etc.) potentially offering a cost-effective and manageable approach to data and software preservation, analysis reproducibility and sustained usability.\u00a0", "track": "Track5: Computing activities and Computing models", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/contribution\/175", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "174", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2cbb21e728f5ee0f6e1cb54ec854a4a7", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SALZBURGER, Andreas", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2cbb21e728f5ee0f6e1cb54ec854a4a7", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SALZBURGER, Andreas", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "07a57a39a9f3ccd64baf0d510df08dbc", "affiliation": "Universite de Geneve (CH)", "_fossil": "contributionParticipationMetadata", "fullName": "CALACE, Noemi", "id": "1"}], "title": "ATLAS Tracking Detector Upgrade studies using the fast simulation engine (FATRAS)", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The successful physics program of Run-1 of the LHC with the discovery of the higgs boson in 2012 has put a strong emphasis on design studies for future upgrades of the existing LHC detectors and for future accelerators. Ideas how to cope with instantaneous luminosities way beyond the current specifications of the LHC in future tracking detectors are emerging and need sufficiently accurate Monte Carlo simulation techniques to be evaluated. In ATLAS, testing alternative layouts through the full simulation and reconstruction chain is a work-intensive program, which is probably not worth following to full detail for all test layout configurations. We present a novel technique implemented for ATLAS that allows fast definitions of realistic detector layouts in the ATLAS TrackingGeometry library and run the fast simulation program (FATRAS), followed by a fast digitisation and reconstruction chain. The code is also implemented without dependencies on the ATLAS software in the context of the Future Collider Collaboration (FCC) framework.", "track": "Track2: Offline software ", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/174", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "173", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a22e35b5c94a8a489c7036451169e91d", "affiliation": "Instituto de Fisica Corpuscular (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "JIMENEZ PENA, Javier", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "dc53de6d0ee33ad7862db0ba4e75ed9d", "affiliation": "Polish Academy of Sciences (PL)", "_fossil": "contributionParticipationMetadata", "fullName": "BRUCKMAN DE RENSTROM, Pawel", "id": "0"}], "title": "Alignment of the ATLAS Inner Detector Upgraded for the LHC Run II", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T00:38:04.327033+00:00", "description": "", "title": "CHEP15_Poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/173\/attachments\/578638\/796796\/CHEP15_Poster.pdf", "filename": "CHEP15_Poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796796, "size": 4117096}], "title": "Slides", "default_folder": false, "id": 578638, "description": ""}], "_type": "Contribution", "description": "ATLAS is equipped with a tracking system built using different technologies, silicon planar sensors (pixel and micro-strip) and gaseous drift- tubes, all embedded in a 2T solenoidal magnetic field. For the LHC Run II, the system has been upgraded with the installation of a new pixel layer, the Insertable B-layer (IBL). Offline track alignment of the ATLAS tracking system has to deal with about 700,000 degrees of freedom (DoF) defining its geometrical parameters. The task requires using very large data sets and represents a considerable numerical challenge in terms of both CPU time and precision. The adopted strategy uses a hierarchical approach to alignment, combining local and global least squares techniques.\r\nAn outline of the track based alignment approach and its implementation within the ATLAS software will be presented. Special attention will be paid to integration to the alignment framework of the IBL, which plays the key role in precise reconstruction of the collider luminous region, interaction vertices and identification of long-lived heavy flavor states.\r\nTechniques allowing to pinpoint and eliminate tracking systematics due to alignment as well as strategies to deal with time-dependent variations will be briefly covered. Performance from Cosmic Ray commissioning run and status from proton-proton collision in LHC Run II will be discussed.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578638", "resources": [{"_type": "LocalFile", "name": "CHEP15_Poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/173\/attachments\/578638\/796796\/CHEP15_Poster.pdf", "fileName": "CHEP15_Poster.pdf", "_fossil": "localFileMetadata", "id": "796796", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/173", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:15:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "172", "speakers": [{"_type": "ContributionParticipation", "emailHash": "3883f9db68feb16e68792b1483a250cd", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. VAN GEMMEREN, Peter", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "87a100f703ae951e1cd6f8f0c55b8582", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CRANSHAW, Jack", "id": "0"}], "title": "ATLAS Metadata Infrastructure Evolution for Run 2 and Beyond", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T03:06:09.755378+00:00", "description": "", "title": "CHEP_2015_MetaData.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/172\/attachments\/578639\/796797\/CHEP_2015_MetaData.pdf", "filename": "CHEP_2015_MetaData.pdf", "content_type": "application\/pdf", "type": "file", "id": 796797, "size": 561637}, {"_type": "attachment", "modified_dt": "2015-04-16T03:06:09.755378+00:00", "description": "", "title": "CHEP_2015_MetaData.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/172\/attachments\/578639\/796798\/CHEP_2015_MetaData.pptx", "filename": "CHEP_2015_MetaData.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796798, "size": 223910}], "title": "Slides", "default_folder": false, "id": 578639, "description": ""}], "_type": "Contribution", "description": "ATLAS developed and employed for Run 1 of the Large Hadron Collider a sophisticated infrastructure for metadata handling in event processing jobs. \u00a0This infrastructure profits from a rich feature set provided by the ATLAS execution control framework, including standardized interfaces and invocation mechanisms for tools and services, segregation of transient data stores with concomitant object lifetime management, and mechanisms for handling occurrences asynchronous to the control framework\u2019s state machine transitions. \u00a0\r\n\r\nThis metadata infrastructure is evolving and being extended for Run 2 to allow its use and reuse in downstream physics analyses, analyses that may or may not utilize the ATLAS control framework. \u00a0At the same time, multiprocessing versions of the control framework and the requirements of future multithreaded frameworks are leading to redesign of components that use an incident-handling approach to asynchrony. \u00a0The increased use of scatter-gather architectures, both local and distributed, requires further enhancement of metadata infrastructure in order to ensure semantic coherence and robust bookkeeping. \u00a0\r\n\r\nThis paper describes the evolution of ATLAS metadata infrastructure for Run 2 and beyond, including the transition to dual-use tools\u2014tools that can operate inside or outside  the ATLAS control framework\u2014and the implications thereof. \u00a0\u00a0It further examines how the design of this infrastructure is changing to accommodate the requirements of future frameworks and emerging event processing architectures.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578639", "resources": [{"_type": "LocalFile", "name": "CHEP_2015_MetaData.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/172\/attachments\/578639\/796797\/CHEP_2015_MetaData.pdf", "fileName": "CHEP_2015_MetaData.pdf", "_fossil": "localFileMetadata", "id": "796797", "_deprecated": true}, {"_type": "LocalFile", "name": "CHEP_2015_MetaData.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/172\/attachments\/578639\/796798\/CHEP_2015_MetaData.pptx", "fileName": "CHEP_2015_MetaData.pptx", "_fossil": "localFileMetadata", "id": "796798", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "9308ae80f0f8aaf14aef2fa1243f3fec", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MALON, David", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "3883f9db68feb16e68792b1483a250cd", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. VAN GEMMEREN, Peter", "id": "2"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/172", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:15:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "171", "speakers": [{"_type": "ContributionParticipation", "emailHash": "d3634db3da58ca9ff86683900b19f4f6", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "MAIER, Thomas", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "9308ae80f0f8aaf14aef2fa1243f3fec", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MALON, David", "id": "0"}], "title": "ATLAS I\/O Performance Optimization in As-Deployed Environments", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T14:20:22.318325+00:00", "description": "", "title": "ThomasM_ATLASIO.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/171\/attachments\/578640\/796799\/ThomasM_ATLASIO.pdf", "filename": "ThomasM_ATLASIO.pdf", "content_type": "application\/pdf", "type": "file", "id": 796799, "size": 588441}], "title": "Slides", "default_folder": false, "id": 578640, "description": ""}], "_type": "Contribution", "description": "I\/O is a fundamental determinant in the overall performance of physics analysis and other data-intensive scientific computing. It is, further, crucial to effective resource delivery by the facilities and infrastructure that support data-intensive science. To understand I\/O performance, clean measurements in controlled environments are essential, but effective optimization requires as well an understanding of the complicated realities of as-deployed environments. These include a spectrum of local and wide-area data delivery and resilience models, heterogeneous storage systems, matches and mismatches between data organization and access patterns, multi-user considerations that may help or hinder individual job performance, and more. \u00a0\r\n\r\nThe ATLAS experiment has organized an interdisciplinary working group of I\/O, persistence, analysis framework, distributed infrastructure,  site deployment, and external experts to understand and improve I\/O performance in preparation for Run 2 of the Large Hadron Collider. The adoption of a new analysis data model for Run 2 has afforded the collaboration a unique opportunity to incorporate instrumentation and monitoring from the outset. \u00a0 \r\nThis paper describes a program of instrumentation, monitoring, measurement, and data collection both in cleanroom and grid environments, and discusses how such information is propagated and employed. \u00a0\r\nThe paper further explores how these findings inform decision-making on many fronts, including persistent data organization, caching, best practices, framework interactions with underlying service layers, and settings at many levels, from sites to application code. Related developments that increase robustness and resilience in the presence of faults by improving communication between frameworks and underlying infrastructure layers are also discussed. \u00a0", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578640", "resources": [{"_type": "LocalFile", "name": "ThomasM_ATLASIO.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/171\/attachments\/578640\/796799\/ThomasM_ATLASIO.pdf", "fileName": "ThomasM_ATLASIO.pdf", "_fossil": "localFileMetadata", "id": "796799", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/171", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:30:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "170", "speakers": [{"_type": "ContributionParticipation", "emailHash": "269913119a8e37ae971b93b8559c6092", "affiliation": "University of Oslo (NO)", "_fossil": "contributionParticipationMetadata", "fullName": "CAMERON, David", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "269913119a8e37ae971b93b8559c6092", "affiliation": "University of Oslo (NO)", "_fossil": "contributionParticipationMetadata", "fullName": "CAMERON, David", "id": "0"}], "title": "ATLAS@Home: Harnessing Volunteer Computing for HEP", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T06:05:34.551643+00:00", "description": "", "title": "atlasathome_chep_15.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/170\/attachments\/578641\/796800\/atlasathome_chep_15.pdf", "filename": "atlasathome_chep_15.pdf", "content_type": "application\/pdf", "type": "file", "id": 796800, "size": 1664988}, {"_type": "attachment", "modified_dt": "2015-04-13T06:05:34.551643+00:00", "description": "", "title": "atlasathome_chep_15.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/170\/attachments\/578641\/796801\/atlasathome_chep_15.pptx", "filename": "atlasathome_chep_15.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796801, "size": 4856319}], "title": "Slides", "default_folder": false, "id": 578641, "description": ""}], "_type": "Contribution", "description": "A recent common theme among HEP computing is exploitation of opportunistic resources in order to provide the maximum statistics possible for Monte-Carlo simulation. Volunteer computing has been used over the last few years in many other scientific fields and by CERN itself to run simulations of the LHC beams. The ATLAS@Home project was started to allow volunteers to run simulations of collisions in the ATLAS detector. So far many thousands of members of the public have signed up to contribute their spare CPU cycles for ATLAS, and there is potential for volunteer computing to provide a significant fraction of ATLAS computing resources. Here we describe the design of the project, the lessons learned so far and the future plans.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578641", "resources": [{"_type": "LocalFile", "name": "atlasathome_chep_15.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/170\/attachments\/578641\/796800\/atlasathome_chep_15.pdf", "fileName": "atlasathome_chep_15.pdf", "_fossil": "localFileMetadata", "id": "796800", "_deprecated": true}, {"_type": "LocalFile", "name": "atlasathome_chep_15.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/170\/attachments\/578641\/796801\/atlasathome_chep_15.pptx", "fileName": "atlasathome_chep_15.pptx", "_fossil": "localFileMetadata", "id": "796801", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "b5d397d364f729abc4025f6fc8fa6aa8", "affiliation": "Laboratoire de l'Accelerateur Lineaire (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "ADAM BOURDARIOS, Claire", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "2bfc3dcae6061d54150b9904432bf7ee", "affiliation": "Jozef Stefan Institute (SI)", "_fossil": "contributionParticipationMetadata", "fullName": "FILIPCIC, Andrej", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "8fc0de19145510b5f4a8150497be283a", "affiliation": "CEA\/IRFU,Centre d'etude de Saclay Gif-sur-Yvette (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "LANCON, Eric Christian", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "185bee9466a6e31abdadf8eb25f26dfb", "affiliation": "Institute of High Energy Physics,Chinese Academy of Sciences (CN)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WU, Wenjing", "id": "4"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/170", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:00:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "182", "speakers": [{"_type": "ContributionParticipation", "emailHash": "63c234ddb57fdeaaa0b900eb1d7ffd71", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "SNYDER, Scott", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "63c234ddb57fdeaaa0b900eb1d7ffd71", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "SNYDER, Scott", "id": "0"}], "title": "Implementation of the ATLAS Run 2 event data model", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T01:39:02.395567+00:00", "description": "", "title": "2015-04-chep.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/182\/attachments\/578642\/796802\/2015-04-chep.pdf", "filename": "2015-04-chep.pdf", "content_type": "application\/pdf", "type": "file", "id": 796802, "size": 331063}], "title": "Diapositives", "default_folder": false, "id": 578642, "description": ""}], "_type": "Contribution", "description": "During the 2013-2014 shutdown of the Large Hadron Collider, ATLAS switched to a new event data model for analysis, called the xAOD.  A key feature of this model is the separation of the object data from the objects themselves (the `auxiliary store').  Rather being stored as member variables of the analysis classes, all object data are stored separately, as vectors of simple values.  Thus, the data are stored in a `structure of arrays' format, while the user still can access it as an `array of structures'.  This organization allows for on-demand partial reading of objects, the selective removal of object properties, and the addition of arbitrary user-defined properties in a uniform manner.  It also improves performance by increasing the locality of memory references in typical analysis code.  The resulting data structures can be written to ROOT files with data properties represented as simple ROOT tree branches.  This talk will focus on the design and implementation of the auxiliary store and its interaction with ROOT.  Results on reconstruction and analysis performance will also be discussed.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Diapositives", "_fossil": "materialMetadata", "id": "578642", "resources": [{"_type": "LocalFile", "name": "2015-04-chep.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/182\/attachments\/578642\/796802\/2015-04-chep.pdf", "fileName": "2015-04-chep.pdf", "_fossil": "localFileMetadata", "id": "796802", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "e0697301aff1c96dafe63b2f8c29cf2e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "EIFERT, Till", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "ce322777a6c04cb5d6b95977c554eabb", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ELSING, Markus", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "7f3ec3bf3655b9095aa1d75a5cd24537", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GILLBERG, Dag", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "90edf3013d33df6d6f9a07fa816970b3", "affiliation": "Albert-Ludwigs-Universitaet Freiburg (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "KOENEKE, Karsten", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "8d4c0721de129e328dffeb903264d729", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "KRASZNAHORKAY, Attila", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "8800113afe171e3f6df776ab19b07d57", "affiliation": "University of Massachusetts (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MOYSE, Edward", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "ce3b0e795e6bc2b718dd4a540b57e14c", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "NOWAK, Marcin", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "3883f9db68feb16e68792b1483a250cd", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. VAN GEMMEREN, Peter", "id": "8"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/182", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:30:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "183", "speakers": [{"_type": "ContributionParticipation", "emailHash": "849637192af92a0f322682b2abc1e859", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WENAUS, Torre", "id": "7"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "849637192af92a0f322682b2abc1e859", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "WENAUS, Torre", "id": "9"}], "title": "The ATLAS Event Service: A new approach to event processing", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T15:15:15.835089+00:00", "description": "", "title": "Wenaus-EventService-CHEP2015-V2.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/183\/attachments\/578643\/796803\/Wenaus-EventService-CHEP2015-V2.pdf", "filename": "Wenaus-EventService-CHEP2015-V2.pdf", "content_type": "application\/pdf", "type": "file", "id": 796803, "size": 9072538}], "title": "Slides", "default_folder": false, "id": 578643, "description": ""}], "_type": "Contribution", "description": "The ATLAS Event Service (ES) implements a new fine grained approach to HEP event processing, designed to be agile and efficient in exploiting transient, short-lived resources such as HPC hole-filling, spot market commercial clouds, and volunteer computing. Input and output control and data flows, bookkeeping, monitoring, and data storage are all managed at the event level in an implementation capable of supporting ATLAS-scale distributed processing throughputs (about 4M CPU-hours\/day). Input data flows utilize remote data repositories with no data locality or pre\u00adstaging requirements, minimizing the use of costly storage in favor of strongly leveraging powerful networks. Object stores provide a highly scalable means of remotely storing the quasi-continuous, fine grained outputs that give ES based applications a very light data footprint on a processing resource, and ensure negligible losses should the resource suddenly vanish. We will describe the motivations for the ES system, its unique features and capabilities, its architecture and the highly scalable tools and technologies employed in its implementation, and its applications in ATLAS processing on HPCs, commercial cloud resources, volunteer computing, and grid resources.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578643", "resources": [{"_type": "LocalFile", "name": "Wenaus-EventService-CHEP2015-V2.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/183\/attachments\/578643\/796803\/Wenaus-EventService-CHEP2015-V2.pdf", "fileName": "Wenaus-EventService-CHEP2015-V2.pdf", "_fossil": "localFileMetadata", "id": "796803", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "e6819ee253837bc1a19f5e93c8ef001e", "affiliation": "University of Texas at Arlington (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DE, Kaushik", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "af353d572ba111925ed14b42afd4e2e5", "affiliation": "University of Wisconsin (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GUAN, Wen", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "2e6780d520127502454830ec573033ae", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MAENO, Tadashi", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "de99e5c8c7b2c0edb4ab024a79d9a913", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "NILSSON, Paul", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "6aef683082e2cae7cc6fb0e086c1b9be", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "TSULAIA, Vakho", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "3883f9db68feb16e68792b1483a250cd", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. VAN GEMMEREN, Peter", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "c3b302a4fc2fd1ca19138cb3a3e5479a", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CALAFIURA, Paolo", "id": "8"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/183", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "180", "speakers": [{"_type": "ContributionParticipation", "emailHash": "1018f3caf155c881ab683fbc6288fc9b", "affiliation": "Centre National de la Recherche Scientifique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "ODIER, Jerome", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "1018f3caf155c881ab683fbc6288fc9b", "affiliation": "Centre National de la Recherche Scientifique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "ODIER, Jerome", "id": "0"}], "title": "The migration of the ATLAS Metadata Interface (AMI) to Web 2.0", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T10:14:08.527445+00:00", "description": "", "title": "poster1.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/180\/attachments\/578644\/796804\/poster1.pdf", "filename": "poster1.pdf", "content_type": "application\/pdf", "type": "file", "id": 796804, "size": 2251002}], "title": "Diapositives", "default_folder": false, "id": 578644, "description": ""}], "_type": "Contribution", "description": "The ATLAS Metadata Interface (AMI) can be considered to be a mature application because it has existed for at least 10 years. Over the last year, we have been adapting the application to some recently available technologies. The web interface, which previously manipulated XML documents using XSL transformations, has been migrated to Asynchronous Java Script (AJAX). Web development has been considerably simplified by the development of a framework for AMI based on JQuery and Twitter Bootstrap. Finally there has been a major upgrade of the python web service client.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Diapositives", "_fossil": "materialMetadata", "id": "578644", "resources": [{"_type": "LocalFile", "name": "poster1.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/180\/attachments\/578644\/796804\/poster1.pdf", "fileName": "poster1.pdf", "_fossil": "localFileMetadata", "id": "796804", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "81248e43a9d5aed605684b827aafc8c3", "affiliation": "Centre National de la Recherche Scientifique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ALBRAND, Solveig", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "698cfd18ea89a5eb2c4029ba1c2cf381", "affiliation": "Centre National de la Recherche Scientifique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "FULACHIER, Jerome", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "851257e81d9d6fffbcac150e4477d19c", "affiliation": "Centre National de la Recherche Scientifique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "LAMBERT, Fabian", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/180", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:45:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "2", "speakers": [{"_type": "ContributionParticipation", "emailHash": "81b1bf725f4540f8632673ac37508af9", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "HOLLOWELL, Christopher", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "d02731e682c9e8307b844b566d0af6aa", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WONG, Tony", "id": "0"}], "title": "Mean PB to Failure -- Initial results from a long-term study of disk storage patterns at the RACF", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T17:54:38.126183+00:00", "description": "", "title": "Mean_PB_to_Failure.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/2\/attachments\/578645\/796805\/Mean_PB_to_Failure.pdf", "filename": "Mean_PB_to_Failure.pdf", "content_type": "application\/pdf", "type": "file", "id": 796805, "size": 360723}, {"_type": "attachment", "modified_dt": "2015-04-10T17:54:38.126183+00:00", "description": "", "title": "Mean_PB_to_Failure.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/2\/attachments\/578645\/796806\/Mean_PB_to_Failure.pptx", "filename": "Mean_PB_to_Failure.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796806, "size": 194968}], "title": "Slides", "default_folder": false, "id": 578645, "description": ""}], "_type": "Contribution", "description": "The RACF (RHIC-ATLAS Computing Facility) has operated a large, multi-purpose dedicated computing facility since the mid-1990's, serving a worldwide, geographically diverse scientific community that is a major contributor to various HEPN projects. A central component of the RACF is the Linux-based worker node cluster that is used for both computing and data storage purposes. It currently has nearly 50,000 computing cores and over 23 PB of storage capacity distributed over 12,000+ (non-SSD) disk drives.\r\n\r\nThe majority of the 12,000+ disk drives provides a cost-effective solution for dCache\/xRootd-managed storage, and a key concern is the reliability of this solution over the lifetime of the hardware, particularly as the number of disk drives and the storage capacity of individual drives grow. We report initial results of a long-term study to measure lifetime PB read\/written to disk drives in the worker node cluster. We discuss the historical disk drive mortality rate, disk drive manufacturers' published MPBTF (Mean PB to Failure) data and how they are correlated to our results. \r\n\r\nThe results helps the RACF understand the productivity and reliability of its storage solutions and has implications for other highly-available storage systems (NFS, GPFS, CVMFS, etc) with large I\/O requirements.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578645", "resources": [{"_type": "LocalFile", "name": "Mean_PB_to_Failure.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/2\/attachments\/578645\/796805\/Mean_PB_to_Failure.pdf", "fileName": "Mean_PB_to_Failure.pdf", "_fossil": "localFileMetadata", "id": "796805", "_deprecated": true}, {"_type": "LocalFile", "name": "Mean_PB_to_Failure.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/2\/attachments\/578645\/796806\/Mean_PB_to_Failure.pptx", "fileName": "Mean_PB_to_Failure.pptx", "_fossil": "localFileMetadata", "id": "796806", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "81b1bf725f4540f8632673ac37508af9", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "HOLLOWELL, Christopher", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "851ef3cb50eeb935408ee9ae6e35ff00", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CARAMARCU, Costin", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "2628a966cb60c5635940b139d5df8cb9", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. ZAYTSEV, Alexandr", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "001b3e939ab534eee442ba2cbd6b82e4", "affiliation": "Brookhaven National Lab", "_fossil": "contributionParticipationMetadata", "fullName": "STRECKER-KELLOGG, William", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "b2974fdd58fd77f382e05194d6970677", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. RAO, Tejas", "id": "5"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/2", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:45:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "186", "speakers": [{"_type": "ContributionParticipation", "emailHash": "428c01fbcb7520555ed00f2b06246d16", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "RADEMAKERS, Fons", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "428c01fbcb7520555ed00f2b06246d16", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "RADEMAKERS, Fons", "id": "0"}], "title": "Overview of Different Exciting Technologies Being Researched in CERN openlab V", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T03:49:22.130252+00:00", "description": "", "title": "CERN_openlab_V_CHEP15.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/186\/attachments\/578646\/796807\/CERN_openlab_V_CHEP15.pdf", "filename": "CERN_openlab_V_CHEP15.pdf", "content_type": "application\/pdf", "type": "file", "id": 796807, "size": 2086094}, {"_type": "attachment", "modified_dt": "2015-04-14T03:49:22.130252+00:00", "description": "", "title": "CERN_openlab_V_CHEP15.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/186\/attachments\/578646\/796808\/CERN_openlab_V_CHEP15.pptx", "filename": "CERN_openlab_V_CHEP15.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796808, "size": 7099238}], "title": "Slides", "default_folder": false, "id": 578646, "description": ""}], "_type": "Contribution", "description": "CERN openlab is a unique public-private partnership between CERN and leading ICT companies. Its mission is to accelerate the development of cutting-edge solutions to be used by the worldwide HEP community. Since January 2015 openlab phase V has started. To bring the openlab conducted research closer to the experiments, phase V has been changed to a project based structure which allows research being done not only in CERN IT but also directly in the different experiments or even in different research institutes. In this talk I will give an overview of the different exciting technologies being researched by CERN openlab V.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578646", "resources": [{"_type": "LocalFile", "name": "CERN_openlab_V_CHEP15.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/186\/attachments\/578646\/796807\/CERN_openlab_V_CHEP15.pdf", "fileName": "CERN_openlab_V_CHEP15.pdf", "_fossil": "localFileMetadata", "id": "796807", "_deprecated": true}, {"_type": "LocalFile", "name": "CERN_openlab_V_CHEP15.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/186\/attachments\/578646\/796808\/CERN_openlab_V_CHEP15.pptx", "fileName": "CERN_openlab_V_CHEP15.pptx", "_fossil": "localFileMetadata", "id": "796808", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/186", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:15:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "187", "speakers": [{"_type": "ContributionParticipation", "emailHash": "22e6241ca5a4735ff50e206923a03d38", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ARSUAGA RIOS, Maria", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "22e6241ca5a4735ff50e206923a03d38", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ARSUAGA RIOS, Maria", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "a69b00ae38867156b46a4f18fbe1bfe4", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HEIKKILA, Seppo", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "cee8f9bdf304d0dba45ae82a7cb9ae5e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. DUELLMANN, Dirk", "id": "4"}], "title": "Using S3 cloud storage with ROOT and CvmFS", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-02T09:21:50.854416+00:00", "description": "", "title": "2015_CHEP_ArsuagaRios.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/187\/attachments\/578647\/796809\/2015_CHEP_ArsuagaRios.pdf", "filename": "2015_CHEP_ArsuagaRios.pdf", "content_type": "application\/pdf", "type": "file", "id": 796809, "size": 4179839}], "title": "Slides", "default_folder": false, "id": 578647, "description": ""}], "_type": "Contribution", "description": "Amazon S3 is a widely adopted protocol for scalable cloud storage that could also fulfill storage requirements of the high-energy physics community. CERN has been evaluating this option using some key HEP applications such as ROOT and the CernVM filesystem (CvmFS) with S3 back-ends. In this contribution we present our evaluation based on two versions of the Huawei UDS storage system used from a large number of clients executing HEP software applications.\r\nThe performance of concurrently storing individual objects is presented alongside with more complex data access patterns as produced by the ROOT data analysis framework. We further report on the S3 integration with recent CvmFS versions and summarize the performance and pre-production experience with CvmFS\/S3 for publishing daily releases of the full LHCb experiment software stack.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578647", "resources": [{"_type": "LocalFile", "name": "2015_CHEP_ArsuagaRios.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/187\/attachments\/578647\/796809\/2015_CHEP_ArsuagaRios.pdf", "fileName": "2015_CHEP_ArsuagaRios.pdf", "_fossil": "localFileMetadata", "id": "796809", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "771122777d10e8f8dc1c3aa7156922e8", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BLOMER, Jakob", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "b7670e16208d58f21cb7eff8f0036404", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MEUSEL, Rene", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "244c8becc3b603c6e5722f14640d8e65", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "COUTURIER, Ben", "id": "5"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/187", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "184", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5e8331956a21872dbd832e781b3d163a", "affiliation": "NATIONAL RESEARCH CENTRE \"KURCHATOV INSTITUTE\"", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. POYDA, Alexey", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "5152afe0ee224171ff18a8a6a0abb342", "affiliation": "National Research Centre Kurchatov Institute (RU); Moscow Institute for Physics and Technology, Applied computational geophysics lab", "_fossil": "contributionParticipationMetadata", "fullName": "RYABINKIN, Eygene", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "518244ea32e309912b9ab07fed0974fe", "affiliation": "NATIONAL RESEARCH CENTRE \"KURCHATOV INSTITUTE\"; P.N. Lebedev Institute of Physics (Russian Academy of Sciences)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MASHINISTOV, Ruslan", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "5e8331956a21872dbd832e781b3d163a", "affiliation": "NATIONAL RESEARCH CENTRE \"KURCHATOV INSTITUTE\"", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. POYDA, Alexey", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "5152afe0ee224171ff18a8a6a0abb342", "affiliation": "National Research Centre Kurchatov Institute (RU); Moscow Institute for Physics and Technology, Applied computational geophysics lab", "_fossil": "contributionParticipationMetadata", "fullName": "RYABINKIN, Eygene", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "518244ea32e309912b9ab07fed0974fe", "affiliation": "NATIONAL RESEARCH CENTRE \"KURCHATOV INSTITUTE\"; P.N. Lebedev Institute of Physics (Russian Academy of Sciences)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MASHINISTOV, Ruslan", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "6fef2a09accf1a707bc4c992c15a8438", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KLIMENTOV, Alexei", "id": "3"}], "title": "Integration of Russian Tier-1 Grid Center with High Performance Computers at NRC-KI for LHC experiments and beyond HENP", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "During LHC Run1 ATLAS and ALICE produced more than 30 Petabytes of data, That rate outstripped any other scientific effort going on, even in data-rich fields such as genomics and climate science. To address an unprecedented multi-petabyte data processing challenge, the experiments are relying on the computational grid infrastructure deployed by the Worldwide LHC Computing Grid (WLCG).  \r\nLHC experiments preparing for the precision measurements and further discoveries that will be made possible by much higher LHC collision rates from early 2015 (Run2). The need for simulation, data processing and analysis and would overwhelm the expected capacity of WLCG computing facilities unless the range and precision of physics studies were to be curtailed. To meet this challenge the integration of the opportunistic resources into LHC computing model is highly important.\r\n\r\nTier-1 facility at Kurchatov Institute (NRC-KI) in Moscow is a part of WLCG and it will process and store up to 10% of total data obtained from ALICE, ATLAS and LHCb experiments. In addition Kurchatov Institute has supercomputers with peak performance 0.12 PFLOPS. Delegation of even a fraction of super-computing resources to the LHC Computing will notably increase total capacity.\r\n\r\nIn 2014, we have started a pioneer work to develop a portal combining a Tier-1 and a supercomputer in Kurchatov Institute. This portal is aimed to provide interfaces to run Monte-Carlo simulation at the Tier-1 Grid and supercomputer, using common portal and storage. PanDA (Production and Distributed Analysis) workload management system having great results at the ATLAS was chosen as underlying technology. In the nearest future we are planning to evolve the portal to fullscale data- and task- management system for federative heterogeneous resources.\r\n\r\nThe portal will be used not only for HEP, but also for other data-intensive sciences like biology with genome sequencing analysis; astrophysics with cosmic rays analysis, antimatter and dark matter search, etc. At this moment we have developed portal architecture, deployed prototype and run the first HENP and biology applications. Also we have deployed the local independent PanDA WMS instance in Kurchatov Institute adapted for the local supercomputer. \r\n\r\nWe want to present our current accomplishments with running PanDA WMS at NRC-KI super-computers and use PanDA as a portal independent to the computing facilities infrastructure for High-Energy  and Nuclear Physics as well as other data-intensive science applications.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "5a6b72abaf2d59578213bddab34923a9", "affiliation": "National Research Centre  \"Kurchatov Institute\"", "_fossil": "contributionParticipationMetadata", "fullName": "Ms. BEREZHNAYA, Alexandra", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "2b4e5274e6b37fc772c4189e542d2d12", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BETEV, Latchezar", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "3f41c6112486480c3ebd506d39003e9e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BUNCIC, Predrag", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "e6819ee253837bc1a19f5e93c8ef001e", "affiliation": "University of Texas at Arlington (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. DE, Kaushik", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "ce4f324ee1e6774ac9a61efe066b0381", "affiliation": "NATIONAL RESEARCH CENTRE \"KURCHATOV INSTITUTE\"", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. DRIZHUK, Daniel", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "8d61a5b46796c56f3b255d7fd4032f9a", "affiliation": "NATIONAL RESEARCH CENTRE \"KURCHATOV INSTITUTE\"", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. LAZIN, Yury", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "b60e367ce78de7825212e6443e668c5c", "affiliation": "NATIONAL RESEARCH CENTRE \"KURCHATOV INSTITUTE\"", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. LYALIN, Ilya", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "61ed0ceffb6775e573801aac0ce88ccf", "affiliation": "NATIONAL RESEARCH CENTRE \"KURCHATOV INSTITUTE\"", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. NOVIKOV, Alexander", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "ae65bffa7bc1bd6543d1748f5c46b7c6", "affiliation": "University of Texas at Arlington (Texas, US); Joint Institute of Nuclear Research (Dubna, Russia)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. OLEYNIK, Danila", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "9b21eff7bcdf1f315c28aee3c6de1f70", "affiliation": "NATIONAL RESEARCH CENTRE \"KURCHATOV INSTITUTE\"", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. POLYAKOV, Andrey", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "71215f87970c115e30dab0a983b46039", "affiliation": "RRC Kurchatov Institute", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. TESLYUK, Anton", "id": "14"}, {"_type": "ContributionParticipation", "emailHash": "d0b2967bf7a189ae700a3a4e8388de54", "affiliation": "NATIONAL RESEARCH CENTRE \"KURCHATOV INSTITUTE\"", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. TKACHENKO, Igor", "id": "15"}, {"_type": "ContributionParticipation", "emailHash": "86b4128b1621a2b9b377ad55c6752eb0", "affiliation": "NATIONAL RESEARCH CENTRE \"KURCHATOV INSTITUTE\"", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. YASNOPOLSKIY, Leonid", "id": "16"}, {"_type": "ContributionParticipation", "emailHash": "c09d16ceef0fbef64eeaecd9489d16d2", "affiliation": "NATIONAL RESEARCH CENTRE \"KURCHATOV INSTITUTE\"", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. BELYAEV, Alexander", "id": "17"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/184", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "185", "speakers": [{"_type": "ContributionParticipation", "emailHash": "306c339b4893664dfa1e83cc7a1db780", "affiliation": "LAL-IN2P3-CNRS", "_fossil": "contributionParticipationMetadata", "fullName": "GARNIER, Laurent", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "306c339b4893664dfa1e83cc7a1db780", "affiliation": "LAL-IN2P3-CNRS", "_fossil": "contributionParticipationMetadata", "fullName": "GARNIER, Laurent", "id": "0"}], "title": "Recent developments and upgrade to the Geant4 visualization Qt driver", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "Geant4 is a toolkit for the simulation of the passage of particles through matter. This object-oriented toolkit supports a variety of visualisation drivers including OpenGL, OpenInventor, HepRep, DAWN, VRML, RayTracer, gMocren and ASCIITree, with diverse and complementary functionalities.\r\n\r\nIn 2013, Gean4-MT[1] has brought multi-threading to Geant4. The OpenGL suite of visualization drivers has been significantly improved by adding many functionalities, some specially for MT, in particular in the OpenGL Qt driver. Users may browse the scene tree and render the scene in multiple frames. The Qt driver is also able to treat multi-thread output separately and change view parameters in a graphic window. Migrating from old OpenGL (based on Display Lists) to modern OpenGL (based on Vertex Buffer Object) compliant code was not only a major implementation, but it also allowed us to include WebGL [2]. Furthermore, there have been efforts to make the user interface more user friendly, but still allow users to customize it by adding menus, buttons, toolbars, viewpoints on scene and many others.\r\n\r\n[1] : http:\/\/http:\/\/geant4.web.cern.ch\r\n[2] : https:\/\/www.khronos.org\/webgl\/", "track": "Track2: Offline software ", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/185", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:30:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "188", "speakers": [{"_type": "ContributionParticipation", "emailHash": "044a6c45e1089ebc257b67572ae11f8d", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "KEEBLE, Oliver", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ee9c64f603d34dcc43bbbc466ea840d1", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FURANO, Fabrizio", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "044a6c45e1089ebc257b67572ae11f8d", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "KEEBLE, Oliver", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c2766b6fe0b68b9d4cf3c7d2d2572ef3", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DEVRESSE, Adrien", "id": "2"}], "title": "Protocol benchmarking for HEP data access using HTTP and Xrootd", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T06:36:22.555806+00:00", "description": "", "title": "KeebleProtocolComparison.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/188\/attachments\/578648\/796810\/KeebleProtocolComparison.pdf", "filename": "KeebleProtocolComparison.pdf", "content_type": "application\/pdf", "type": "file", "id": 796810, "size": 343125}], "title": "Slides", "default_folder": false, "id": 578648, "description": ""}], "_type": "Contribution", "description": "The DPM project offers an excellent opportunity for comparative testing of the HTTP and xroot protocols for data analysis.\r\n\r\n- The DPM storage itself is multi-protocol, allowing comparisons to be performed on the same hardware\r\n- The DPM has been instrumented to produce an i\/o monitoring stream, familiar from the xrootd project, regardless of the protocol being used for access\r\n- The continuous builds of DPM have been instrumented in 2013 with automated testing procedures that regularly produce, and collect metadata stress test information\r\n- The DPM Collaboration involves a number of active grid sites who have made testing resources available. These sites are continuously exercised by a ROOT analysis benchmark several times per day, and comparisons can be conducted in a variety of environments, also involving access through Wide Area Network.\r\n\r\nWe present the results of our performance analyses of realistic use cases, and discuss their implications for the use of HTTP as a data and metadata access protocol for HEP.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578648", "resources": [{"_type": "LocalFile", "name": "KeebleProtocolComparison.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/188\/attachments\/578648\/796810\/KeebleProtocolComparison.pdf", "fileName": "KeebleProtocolComparison.pdf", "_fossil": "localFileMetadata", "id": "796810", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/188", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "189", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e0c62f86305aa41c91d857b662068a19", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ROISER, Stefan", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ee9c64f603d34dcc43bbbc466ea840d1", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FURANO, Fabrizio", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "e0c62f86305aa41c91d857b662068a19", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ROISER, Stefan", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c2766b6fe0b68b9d4cf3c7d2d2572ef3", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DEVRESSE, Adrien", "id": "2"}], "title": "Seamless access to HTTP\/WebDAV distributed storage: the LHCb storage federation case study and prototype", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "In this contribution we describe the activities and the technical aspects that led to the construction of a public prototype for LHCb file access that is built on HTTP and WebDAV, supporting file access for distributed computing data management and data processing activities as well as seamless interactive access via web browsers.\r\n\r\nThe LHCb replica naming scheme provides characteristics that makes it an excellent candidate for evaluating the kind of interaction that an HTTP deployment can give on top of an existing, already running computing model.\r\nDeploying HTTP access in the context of LHCb gives the possibility of accessing all 19 individual experiment storage areas, with 5 different disk-only and tape storage technologies deployed, using web tools like browsers, curl, wget and the Davix client.\r\n\r\nThese options allow a wide range of applications frameworks such as ROOT to access data, using techniques that may range from simple, single file transfers to vectored reads. Coordinating the deployment of WebDAV access in the sites has been one of the main efforts, and is still ongoing.\r\n\r\nGiven the very high level of coherency of the LHCb site namespaces, the quick pace at which sites joined the effort, and the desire to quickly setup an ambitious prototype, we decided to use the already existing storage federation testbed that is run at DESY, by configuring it with the HTTP\/WebDAV endpoints of the LHCb storage elements as they joined the exercise.\r\nWe obtained a Dynamic Federations instance that is already covering a good part of the LHCb data namespace. The front-end shows an unified repository that is composed by the content of all the sites that it aggregates. The aggregation is performed on the fly, by distributing quick WebDAV queries towards the endpoints and scheduling, aggregating and caching the responses. We will further show how the HTTP protocol access will be incorporated into the LHCb\/DIRAC distributed computing tool and can be beneficial for individual end users.", "track": "Track3: Data store and access ", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/189", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:45:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "11", "speakers": [{"_type": "ContributionParticipation", "emailHash": "1baeffa4c1ef9f748709ae060bf62bc1", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. JAYATILAKA, Bodhitha", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "1baeffa4c1ef9f748709ae060bf62bc1", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. JAYATILAKA, Bodhitha", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "b7a3d367488441ffe67eff25c8fff69d", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HERNER, Ken", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "cd219d6dc765b420d07d8264b598e523", "affiliation": "University of Rochester (US)", "_fossil": "contributionParticipationMetadata", "fullName": "SAKUMOTO, Willis", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "f00d7f043f9f6ef6c49645dc6e036b67", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "BOYD, Joe", "id": "3"}], "title": "Data Preservation at the Fermilab Tevatron", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T05:40:09.137827+00:00", "description": "", "title": "chep-dp-jayatilaka.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/11\/attachments\/578649\/796811\/chep-dp-jayatilaka.pdf", "filename": "chep-dp-jayatilaka.pdf", "content_type": "application\/pdf", "type": "file", "id": 796811, "size": 4066032}], "title": "Slides", "default_folder": false, "id": 578649, "description": ""}], "_type": "Contribution", "description": "The Fermilab Tevatron collider's data-taking run ended in September 2011, yielding a dataset with rich scientific potential. The CDF and D0 experiments each have nearly 9 PB of collider and simulated data stored on tape. A large computing infrastructure consisting of tape storage, disk cache, and distributed grid computing for physics analysis with the Tevatron data is present at Fermilab.\r\n\r\nThe Fermilab Run II data preservation project intends to keep this analysis capability sustained through the year 2020 or beyond. To achieve this, we are implementing a system that utilizes virtualization, automated validation, and migration to new standards in both software and data storage technology as well as leveraging resources available from currently-running experiments at Fermilab. These efforts will provide useful lessons in ensuring long-term data access for numerous experiments throughout high-energy physics, and provide a roadmap for high-quality scientific output for years to come. We will present the status, benefits, and challenges of data preservation efforts within the CDF and D0 collaborations at Fermilab.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578649", "resources": [{"_type": "LocalFile", "name": "chep-dp-jayatilaka.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/11\/attachments\/578649\/796811\/chep-dp-jayatilaka.pdf", "fileName": "chep-dp-jayatilaka.pdf", "_fossil": "localFileMetadata", "id": "796811", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "9b3f2d801e3deb3f88a4e74c79fe5571", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "ROSER, Robert", "id": "4"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/11", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:15:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "10", "speakers": [{"_type": "ContributionParticipation", "emailHash": "b404210b382755c9d24196d0120b1cd1", "affiliation": "Ecole Polytechnique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "MAGNIETTE, Frederic Bruno", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "b404210b382755c9d24196d0120b1cd1", "affiliation": "Ecole Polytechnique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "MAGNIETTE, Frederic Bruno", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "e0713e49fb9fff94602b2d63d036be56", "affiliation": "CNRS", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. RUBIO-ROY, Miguel", "id": "1"}], "title": "Pyrame, a rapid-prototyping framework for online systems", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T06:01:51.396656+00:00", "description": "", "title": "talk_chep2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/10\/attachments\/578650\/796812\/talk_chep2015.pdf", "filename": "talk_chep2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796812, "size": 990581}], "title": "Diapositives", "default_folder": false, "id": 578650, "description": ""}], "_type": "Contribution", "description": "High-energy physics experiments produce huge amounts of data that need to be processed and stored for further analysis and eventually treated in real time for triggering and monitoring purposes. In addition, more and more often these requirements are also being found on other fields such as on-line video processing, proteomics and astronomical facilities.\r\n\r\nThe complexity of such experiments usually involves long development times on which simple test-bench prototypes evolve over time and require increasingly performant software solutions for both data acquisition and control systems. Flexibility and wide application range are, therefore, important conditions in order to keep a single software solution that can evolve over the duration of the development period.\r\n\r\nExisting solutions such as LabView provide proprietary solutions for control applications and small scale data acquisition, but fail on scaling to high-throughput experiments, add an important performance penalty and become difficult to maintain when the complexity of the software flow increases. Other lower-level solutions such as LabWindows\/C, Matlab or TANGO allow to build more complex systems but still fail on providing an integrated and close to turn-key solution for the specific needs of prototypes evolving from bench-based tests to distributed environments with high data-throughput needs.\r\n\r\nThe present work reports on the software Pyrame, an open-source framework designed with high-energy physics applications in mind and providing a light-weight, distributed, stable, performant and easy-to-deploy solution. Pyrame is a data-acquisition chain, a data-exchange network protocol and a wide set of drivers allowing the control of hardware components. Data-acquisition throughput is on the order of 1.7\u00a0Gb\/s for UDP\/IP and Pyrame\u2019s protocol overhead is about 1.6\u00a0ms per command\/response using the stock tools.\r\n\r\nPyrame provides basic blocks (modules) for hardware control or data acquisition. It contains an important number of modules to manage commonly used hardware: high and low voltage power supplies (Agilent, CAEN, Hameg, Keithley...), pattern generators (Agilent), digital oscilloscopes (LeCroy), motion controllers (Newport) and bus adapters (GPIB, USB, RS-232, Ethernet, UDP, TCP\u2026). Generic modules give a unified access to machines from different generations and native or emulated functions depending on the possibilities of the machines.\r\n\r\nPyrame also provides service modules, including: a multimedia and multistream high-performance acquisition chain; a variable module allowing to share data between all the hardware modules; and a configuration management module allowing to save or load an image of the global configuration of the system at any time.\r\n\r\nThese blocks can be assembled together through Pyrame\u2019s software bus and protocol to quickly obtain complete systems for test benches. The framework is very flexible and provides a wide range of options, allowing the system to evolve as fast as the prototype.\r\n\r\nIn order to ease the development of extensions, Pyrame uses open standards: TCP and XML. They are implemented on most platforms and in most languages making the development of new modules fast and easy. The use of TCP\/IP eases the distribution of code on multi-computer setups, but also its use on embedded platforms.\r\n\r\nMultiple test-beams at DESY have validated the stability of the system. In particular, our tests have involved data acquisition sessions of hundreds of hours from up to 10 simultaneous Si-based particle detectors distributed over multiple computers. Very long calibration procedures involving hundreds of thousands of configuration operations have also been performed. On another optical bench, simultaneous control and data acquisition of oscilloscopes, motorized stages and power supplies confirmed the same level of stability for long acquisition sessions.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Diapositives", "_fossil": "materialMetadata", "id": "578650", "resources": [{"_type": "LocalFile", "name": "talk_chep2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/10\/attachments\/578650\/796812\/talk_chep2015.pdf", "fileName": "talk_chep2015.pdf", "_fossil": "localFileMetadata", "id": "796812", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/10", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "13", "speakers": [{"_type": "ContributionParticipation", "emailHash": "18f12d579cee5d8345191b00dae91d02", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "JUNG, Christopher", "id": "8"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "68faeeddc02be7fe67e69f448fe01d64", "affiliation": "FZ J\u00fclich", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. GIESLER, Andr\u00e9", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "77049ca5413ebfd83e7a56900cb680d5", "affiliation": "GSI - Helmholtzzentrum fur Schwerionenforschung GmbH (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SCHWARZ, Kilian", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "e8f5f6811898914eda13848d629d48b0", "affiliation": "Kalrsruhe Institute of Technology", "_fossil": "contributionParticipationMetadata", "fullName": "HARDT, Marcus", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "ff70f52fed9ff339a19d5460ac5f61c5", "affiliation": "KIT", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. STOTZKA, Rainer", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "cbcc450bc203d110216c4c98878594df", "affiliation": "KIT", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MEYER, J\u00f6rg", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "88bb212ea61adbd06bb830016b40cb6e", "affiliation": "KIT", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. RIGOLL, Fabian", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "8752fa4ff53ff75ecd6c6eb3cd406427", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. GASTHUBER, Martin", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "18f12d579cee5d8345191b00dae91d02", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "JUNG, Christopher", "id": "8"}], "title": "Progress in Multi-Disciplinary Data Life Cycle Management", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-08T07:02:41.674469+00:00", "description": "", "title": "LSDMA2015_final.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/13\/attachments\/578651\/796813\/LSDMA2015_final.pdf", "filename": "LSDMA2015_final.pdf", "content_type": "application\/pdf", "type": "file", "id": 796813, "size": 1092252}], "title": "Slides", "default_folder": false, "id": 578651, "description": ""}], "_type": "Contribution", "description": "Modern science is most often driven by data. Improvements in state-of-the-art technologies and methods in many scientific disciplines lead not only to increasing data rates, but also to the need to improve or even completely overhaul their data life cycle management.\r\n\r\nCommunities usually face two kinds of challenges: generic ones like federated authorization and authentication infrastructures and data preservation, and ones that are specific to their community and their respective data life cycle. In practice, the specific requirements often hinder the use of generic tools and methods.\r\n\r\nThe German Helmholtz Association project \"Large-Scale Data Management and Analysis\" (LSDMA) addresses both challenges: its five Data Life Cycle Labs (DLCLs) closely collaborate with communities in joint research and development to optimize the communities\u2019 data life cycle management, while its Data Services Integration Team (DSIT) provides generic data tools and services.\r\n\r\nWe present most recent developments and results from the DLCLs covering communities ranging from heavy ion physics and photon science to high-throughput microscopy, and from DSIT.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578651", "resources": [{"_type": "LocalFile", "name": "LSDMA2015_final.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/13\/attachments\/578651\/796813\/LSDMA2015_final.pdf", "fileName": "LSDMA2015_final.pdf", "_fossil": "localFileMetadata", "id": "796813", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "1c98cf2619b004ec69f1d68d0e33e952", "affiliation": "KIT", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. STREIT, Achim", "id": "7"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/13", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:45:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:00:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "12", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e969bfc74c003a434b55f356d830dc30", "affiliation": "National Research Centre Kurchatov Institute (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "LIKHOMANENKO, Tatiana", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e969bfc74c003a434b55f356d830dc30", "affiliation": "National Research Centre Kurchatov Institute (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "LIKHOMANENKO, Tatiana", "id": "0"}], "title": "LHCb topological trigger reoptimization", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T17:02:27.071737+00:00", "description": "", "title": "hlt.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/12\/attachments\/578652\/796814\/hlt.pdf", "filename": "hlt.pdf", "content_type": "application\/pdf", "type": "file", "id": 796814, "size": 4277546}], "title": "Slides", "default_folder": false, "id": 578652, "description": ""}], "_type": "Contribution", "description": "The main b-physics trigger algorithm used by the LHCb experiment is the so-called topological trigger. The topological trigger selects vertices which are a) detached from the primary proton-proton collision and b) compatible with coming from the decay of a b-hadron. In the LHC Run 1, this trigger utilized a custom boosted decision tree algorithm, selected an almost 100% pure sample of b-hadrons with a typical efficiency of 60-70%, and its output was used in about 60% of LHCb papers. This talk presents studies carried out to optimize the topological trigger for LHC Run 2. In particular, we have carried out a detailed comparison of various machine learning classifier algorithms, e.g., AdaBoost, MatrixNet and neural networks. The topological trigger algorithm is designed to select all \"interesting\" decays of b-hadrons, but cannot be trained on every such decay. Studies have therefore been performed to determine how to optimize the performance of the classification algorithm on decays not used in the training.  These include cascading, ensembling and blending techniques.   Furthermore, novel boosting techniques have been implemented that will help reduce systematic uncertainties in Run 2 measurements. We demonstrate that the reoptimized topological trigger is expected to significantly improve on the Run 1 performance for a wide range of b-hadron decays.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578652", "resources": [{"_type": "LocalFile", "name": "hlt.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/12\/attachments\/578652\/796814\/hlt.pdf", "fileName": "hlt.pdf", "_fossil": "localFileMetadata", "id": "796814", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "c10fdf0d2ddefd23d451c9de740427fe", "affiliation": "ITEP Institute for Theoretical and Experimental Physics (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "USTYUZHANIN, Andrey", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "10fc0c32be829bc87d01d57897b28fe7", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "WILLIAMS, J Michael", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "27faa83afcba580f4892f27d8628703c", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "ILTEN, Philip", "id": "3"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/12", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:00:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "14", "speakers": [{"_type": "ContributionParticipation", "emailHash": "c6ad22bc69a816a6d38cfe4094011237", "affiliation": "Johann-Wolfgang-Goethe Univ. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "GOMEZ RAMIREZ, Andres", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c6ad22bc69a816a6d38cfe4094011237", "affiliation": "Johann-Wolfgang-Goethe Univ. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "GOMEZ RAMIREZ, Andres", "id": "0"}], "title": "Intrusion Detection in Grid computing by Intelligent Analysis of Jobs Behavior \u2013 The LHC ALICE Case", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T02:07:33.803757+00:00", "description": "", "title": "CHEP2015-slides.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/14\/attachments\/578653\/796815\/CHEP2015-slides.pdf", "filename": "CHEP2015-slides.pdf", "content_type": "application\/pdf", "type": "file", "id": 796815, "size": 958202}], "title": "Slides", "default_folder": false, "id": 578653, "description": ""}], "_type": "Contribution", "description": "Grid infrastructures allow users flexible on-demand usage of computing resources using an Internet connection. A remarkable example of a Grid in High Energy Physics (HEP) research is used by the ALICE experiment at European Organization for Nuclear Research CERN. Physicists can submit jobs used to process the huge amount of particle collision data produced by the Large Hadron Collider (LHC) at CERN. Grids allow the submission of user developed jobs (code and data). They also have interfaces to Internet, storage systems, experiment infrastructure and other networks. This creates an important security challenge. Even when Grid system administrators perform a careful security assessment of sites, worker nodes, storage elements and central services, an attacker could still take advantage of unknown vulnerabilities (zero day). This attacker could enter and escalate her access privileges to misuse the computational resources for unauthorized or even criminal purposes. She could even manipulate the data of the experiments.\r\n\r\nAccordingly, Grid systems require an automatic tool to monitor and analyze the behavior of user jobs. This tool should analyze data generated by jobs such as log entries, traces, system calls, to determine if they run in a desired behavior or are performing any kind of attack on the system. The tool should react to the attack by sending alerts, logging information about relevant events and performing automatic defensive actions (for example stopping a suspicious process). This piece of software could\r\nbe classified as Grid Intrusion Detection Systems (Grid-IDS). Traditional IDS allow detection of attacks by fixed if-then rules based on signatures. It compares the input data with known predefined conditions from previous events. This strategy fails when a new type of intrusion is used, even with a slightly difference. Artificial intelligence algorithms have been suggested as a method to improve Intrusion Detection Systems. By the usage of a Machine Learning approach it is possible to train the IDS on generalizing among attacks even when they are completely new. Intelligent IDS can also analyze the huge amount of data generated in Grid logs and process traces to determine a misbehaving scenario (data mining). This Grid IDS has to be adapted to highly distributed scenarios, when collaboration among geographically separate sites is necessary and reliability on central services is not always an option.\r\n\r\nCurrently there is no framework that allows us to fulfill all the above requirements. We will design and build such framework. This framework should allow the monitoring and analysis of grid job behavior to detect attack attempts, even if new techniques or zero day vulnerabilities are utilized. This framework should also perform required countermeasures for its protection. In a first step, we plan to analyze the\r\nbehavior of the usual job execution in the ALICE experiment Grid. We will determine the most important metrics to characterize a \u201cbad\u201d behavior (an attack). Later we will collect data from the Grid logs using these metrics and will use this data to train a machine learning algorithm. The algorithm will allow us classification of jobs as in desired or undesired state depending on the data produced in their execution. We plan to implement the proposed framework as a software prototype that will be tested as a component of the ALICE Grid middelware.\r\n\r\n**Keywords \u2013** grid computing, distributed computing, distributed System security, artificial intelligence,\r\ndata mining, Intrusion Detection Systems.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578653", "resources": [{"_type": "LocalFile", "name": "CHEP2015-slides.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/14\/attachments\/578653\/796815\/CHEP2015-slides.pdf", "fileName": "CHEP2015-slides.pdf", "_fossil": "localFileMetadata", "id": "796815", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "995be3d78e3dd65b0cfa90aaf4e39018", "affiliation": "Johann-Wolfgang-Goethe Univ. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "LARA MARTINEZ, Camilo Ernesto", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "ee51c700ab6218a943d6a99626c05c74", "affiliation": "Johann-Wolfgang-Goethe Univ. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "KEBSCHULL, Udo Wolfgang", "id": "2"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/14", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "17", "speakers": [{"_type": "ContributionParticipation", "emailHash": "92e4772504fd9714378dc526fff44800", "affiliation": "Ruprecht-Karls-Universitaet Heidelberg (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "KRETZ, Moritz", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "92e4772504fd9714378dc526fff44800", "affiliation": "Ruprecht-Karls-Universitaet Heidelberg (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "KRETZ, Moritz", "id": "0"}], "title": "Performance evaluation of the ATLAS IBL Calibration", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T11:30:33.451594+00:00", "description": "", "title": "poster_revised2.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/17\/attachments\/578654\/796816\/poster_revised2.pdf", "filename": "poster_revised2.pdf", "content_type": "application\/pdf", "type": "file", "id": 796816, "size": 1510236}], "title": "Poster", "default_folder": false, "id": 578654, "description": ""}], "_type": "Contribution", "description": "With the installation of the Insertable B-Layer in 2014 the Pixel Detector of the ATLAS experiment has been extended by about 12 million pixels. Scanning and tuning procedures have been implemented by employing newly designed read-out hardware which is now able to support the full detector bandwidth even for calibration. The hardware is supported by an embedded software stack running on the read-out boards.  Compute-intensive operations that are necessary during the calibration process are performed on a small cluster of commodity servers, which are included in the ATLAS TDAQ framework. In order to exploit the potential of this heterogenious data processing architecture to significantly speed-up detector calibration, timing and performance of the various software components is critical.  \r\n\r\nWe present a timing model including the hardware and software components and procedures that are involved in the calibration process. By instrumenting these components at critical locations we are then able to accurately evaluate the timing of a calibration scan, analyze the performance, and eventually locate and eliminate bottlenecks in the chain.", "track": "Track1: Online computing ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578654", "resources": [{"_type": "LocalFile", "name": "poster_revised2.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/17\/attachments\/578654\/796816\/poster_revised2.pdf", "fileName": "poster_revised2.pdf", "_fossil": "localFileMetadata", "id": "796816", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/17", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "16", "speakers": [{"_type": "ContributionParticipation", "emailHash": "bf44411ab6d7b3f15c0c94b0be897d83", "affiliation": "U", "_fossil": "contributionParticipationMetadata", "fullName": "MENG, Haiyan", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "bab71c85fc036e0aa68dc4a9b0d86078", "affiliation": "University of Notre Dame", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. THAIN, Douglas", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "668f672c420463425227ed7d99d9dbd7", "affiliation": "University of Notre Dame", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. HILDRETH, Michael", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "bf44411ab6d7b3f15c0c94b0be897d83", "affiliation": "U", "_fossil": "contributionParticipationMetadata", "fullName": "MENG, Haiyan", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "bab71c85fc036e0aa68dc4a9b0d86078", "affiliation": "University of Notre Dame", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. THAIN, Douglas", "id": "1"}], "title": "A Case Study in Preserving a High Energy Physics Application", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-03-17T18:51:48.843803+00:00", "description": "", "title": "hmeng_W30in_H26in.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/16\/attachments\/578655\/796817\/hmeng_W30in_H26in.pdf", "filename": "hmeng_W30in_H26in.pdf", "content_type": "application\/pdf", "type": "file", "id": 796817, "size": 566396}], "title": "Poster", "default_folder": false, "id": 578655, "description": ""}], "_type": "Contribution", "description": "The reproducibility of scientific results increasingly depends upon the preservation of computational artifacts. Although preserving a computation to be used later sounds trivial, it is surprisingly difficult due to the complexity of existing software and systems. Implicit dependencies, networked resources, and shifting compatibility all conspire to break applications that appear to work well.  Tools are needed which can automatically identify both local and remote dependencies, so that they can be captured and preserved.\r\n\r\nTo investigate these issues, we present a case study of preserving a CMS application using Parrot. We analyze the application and attempt several methods at extracting its dependencies for the purposes of preservation. We demonstrate a fine-grained dependency management toolkit which can observe both the local filesystem and remote network dependencies, using the system call tracing capabilities of Parrot. We observe that even a simple application depends upon 22,068 files and directories totalling 21 GB of data and software drawn from 8 different sources including cvmfs, hdfs, afs, git, http, cvs, PanFS and local root filesystem.\r\n\r\nOnce the dependencies are observed, a portable execution package can be generated.  This package is not tied to any particular technology and can be re-run using Parrot, Docker, a chroot Jail, or as a virtual machine image, depending on the technology available at the execution site.  We will report on the performance and completeness of re-execution using both public and private clouds and offer some guidance for future work in application preservation.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578655", "resources": [{"_type": "LocalFile", "name": "hmeng_W30in_H26in.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/16\/attachments\/578655\/796817\/hmeng_W30in_H26in.pdf", "fileName": "hmeng_W30in_H26in.pdf", "_fossil": "localFileMetadata", "id": "796817", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "3ef11477becf35256032b913ffa168f1", "affiliation": "University of Notre Dame (US)", "_fossil": "contributionParticipationMetadata", "fullName": "WOLF, Matthias", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "668f672c420463425227ed7d99d9dbd7", "affiliation": "University of Notre Dame", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. HILDRETH, Michael", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "194593c9e9bec8140e2b0a995fd2ebbd", "affiliation": "University of Notre Dame", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. IVIE, Peter", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "b1fcb7fb925578aad9c6cc70862e4f2a", "affiliation": "University of Notre Dame", "_fossil": "contributionParticipationMetadata", "fullName": "Ms. WOODARD, Anna", "id": "5"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/16", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": null, "keywords": [], "id": "19", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5385b4ff0a4c33c34a546e18ee4af04f", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "ZANGRANDO, Lisa", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "5385b4ff0a4c33c34a546e18ee4af04f", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "ZANGRANDO, Lisa", "id": "0"}], "title": "Optimizing resource allocation in Cloud based environments", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [], "_type": "Contribution", "description": "Computing activities performed by user groups in the Public Research and in the Public Administrations are usually not constant over long periods of time (e.g. one year). The amount of computing resources effectively used by such user teams may therefore vary in a quite significant way.\r\nOn these environments it is usual for the teams to stipulate with the Data Centers contracts for the provision of an average computing capacity to be guaranteed during a long period (e.g. one year) rather than of an agreed amount of computing resources that should be available at any given time. In these Data Centers new hardware resources are acquired according to the user best estimates of the annual computing resources needed for their activities and partitioned among them. The partitioning policy is defined in terms of fractions of average usage of the total available capacity (i.e. the percentage of the Data Center's computing resources each team has the right to use averaging over a fixed time window). In order to respect the contracts, the administrators have to enforce that each stakeholder team, at the end of any sufficiently long period of time, and hence of the entire year, has got its agreed average number of resources. Moreover, since in general the request for resources is much greater than the amount of the available resources, it becomes necessary to seek to maximize their utilization by adopting a proper resource sharing model.\r\nIn the current OpenStack model, the resource allocation to the user teams (i.e. the projects) can be done only by granting fixed quotas. Such amount of resources cannot be exceeded by one group even if there are unused resources allocated to other groups. So, in a scenario of full resource usage for a specific project, new requests are simply rejected. \r\nPast experience has shown that, when resources are statically partitioned (e.g. via quota) among user teams, the global efficiency in the Data Center's resource usage is usually quite low (often less than 50%).\r\nINFN has started to address this issue by developing a pluggable scheduler, named FairShareScheduler, as extension of the current OpenStack scheduler. In particular the FairShareScheduler provides a persistent queuing mechanism for handling user requests and adopts a resources provisioning model based on the advanced SLURM's fair-share algorithm which allows to maximize the resources usage as well as to guarantee the resources are equally distributed among users and groups by considering the portion of the resources allocated to them and the resources already consumed.\r\nWe describe how this FairShare scheduler has been implemented, and we report about some first experiences on its usage.", "track": "Track7: Clouds and virtualization", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "e18ae93b896942d02f484f2d29af2c69", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "FRIZZIERO, Eric", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/contribution\/19", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "18", "speakers": [{"_type": "ContributionParticipation", "emailHash": "7bc813d53aa9b6d7a3c4fca8b5484de9", "affiliation": "Georg-August-Universitaet Goettingen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. MAGRADZE, Erekle", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "7bc813d53aa9b6d7a3c4fca8b5484de9", "affiliation": "Georg-August-Universitaet Goettingen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. MAGRADZE, Erekle", "id": "0"}], "title": "Automation of Large-scale Computer Cluster Monitoring Information Analysis", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-06T21:55:11.169699+00:00", "description": "", "title": "ErekleMagradze310320153.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/18\/attachments\/578656\/796818\/ErekleMagradze310320153.pdf", "filename": "ErekleMagradze310320153.pdf", "content_type": "application\/pdf", "type": "file", "id": 796818, "size": 1026795}], "title": "Slides", "default_folder": false, "id": 578656, "description": ""}], "_type": "Contribution", "description": "High-throughput computing platforms consist of complex infrastructure and provide a number of services apt to failures. To mitigate the impact of failures on the quality of the provided services, a constant monitoring and in time reaction is required, which is impossible without automation of the system administration\r\nprocesses. This paper introduces a way of automation of the process of monitoring information analysis to provide long and short term predictions of the service response time (SRT) of the mass storage and the batch systems and to identify the status of a service at a given time. The approach for the SRT predictions is based on Adaptive Neuro Fuzzy Inference System (ANFIS) while for a proper service status\r\nidentification the K-means clustering algorithm was employed. An evaluation of the approaches is performed on real monitoring data from the WLCG Tier 2 center GoeGrid. Ten fold cross validation results demonstrate high efficiency of both approaches in comparison to known methods.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578656", "resources": [{"_type": "LocalFile", "name": "ErekleMagradze310320153.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/18\/attachments\/578656\/796818\/ErekleMagradze310320153.pdf", "fileName": "ErekleMagradze310320153.pdf", "_fossil": "localFileMetadata", "id": "796818", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "769117bacc977b39854c27c60df0c5cc", "affiliation": "Georg-August-Universitaet Goettingen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "KAWAMURA, Gen", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "eae116e7bba38707b0653c12fadaeb5e", "affiliation": "Georg-August-Universitaet Goettingen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "MUSHEGHYAN, Haykuhi", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "7115b6ceb356151a501b527f4284a8e2", "affiliation": "Georg-August-Universitaet Goettingen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "NADAL SERRANO, Jordi", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "02ca6423f165ed8da4a50ae68abe9d7e", "affiliation": "Georg-August-Universitaet Goettingen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "QUADT, Arnulf", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/18", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "322", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5ae4855aa9d3c6685bd9f450814f9af6", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "STAGNI, Federico", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "34fa6009bd34aced06fff1dd6d7fed34", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MATHE, Zoltan", "id": "0"}], "title": "The DIRAC Web Portal 2.0", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T08:36:39.236210+00:00", "description": "", "title": "webapp.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/322\/attachments\/578657\/796819\/webapp.pdf", "filename": "webapp.pdf", "content_type": "application\/pdf", "type": "file", "id": 796819, "size": 7561582}], "title": "Slides", "default_folder": false, "id": 578657, "description": ""}], "_type": "Contribution", "description": "For many years the DIRAC interware (Distributed Infrastructure with Remote Agent Control) has had a web interface, allowing the users to monitor DIRAC activities and also interact with the system. Since then many new web technologies have emerged, therefore a redesign and a new implementation of the DIRAC Web portal were necessary, taking into account the lessons learnt using the old portal.\r\nThese new technologies allowed to build a more compact and more responsive web interface that is robust and that enables users to have more control over the whole system while keeping a simple interface. The framework provides a large set of \"applications\", each of which can be used for interacting with various parts of the system. Communities can also create their own set of personalised web applications, and can easily extend already existing web applications with a minimal effort. Each user can configure and personalise the view for each application and save it using the DIRAC User Profile service as RESTful state provider, instead of using cookies.\r\nThe owner of a view can share it with other users or within a user community. Compatibility between different browsers is assured, as well as with mobile versions.\r\nIn this paper, we present the new DIRAC Web framework as well as the LHCb extension of the DIRAC Web portal.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578657", "resources": [{"_type": "LocalFile", "name": "webapp.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/322\/attachments\/578657\/796819\/webapp.pdf", "fileName": "webapp.pdf", "_fossil": "localFileMetadata", "id": "796819", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "ddd3265ff49e33b97b9ba200f3a332d8", "affiliation": "University of Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "CASAJUS RAMO, Adrian", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "a8f7836ccff8e1ea0913025c07f96f28", "affiliation": "University of Barcelona", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. LAZOVSKI, Nikola", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "5ae4855aa9d3c6685bd9f450814f9af6", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "STAGNI, Federico", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/322", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:00:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "323", "speakers": [{"_type": "ContributionParticipation", "emailHash": "bc9744b244becb77d8e097418d081341", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MASCETTI, Luca", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "bc9744b244becb77d8e097418d081341", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MASCETTI, Luca", "id": "0"}], "title": "Disk storage at CERN", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T23:03:30.900370+00:00", "description": "", "title": "DiskStorageCERN-CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/323\/attachments\/578658\/796820\/DiskStorageCERN-CHEP2015.pdf", "filename": "DiskStorageCERN-CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796820, "size": 10121319}], "title": "Slides", "default_folder": false, "id": 578658, "description": ""}], "_type": "Contribution", "description": "CERN IT DSS operates the main storage resources for data taking and physics analysis mainly via three system: AFS, CASTOR and EOS. The total usable space available for users is about 100 PB (with relative ratios 1:20:120). EOS deploys disk resources across the two CERN computer centres (Meyrin and Wigner) with a current ratio 60% to 40%. IT DSS is also providing sizable on-demand resources for general IT services most notably OpenStack and NFS clients. This is provided by our Ceph infrastructure and a few of proprietary servers (NetApp) for a total capacity of ~1 PB.\r\n\r\nWe will describe our operational experience and recent changes to these systems with special emphasis to the following items:\r\n\r\n\r\n - Present usages for LHC data taking (new roles of CASTOR and EOS)\r\n - Convergence to commodity hardware (nodes with 200-TB each with optional SSD) shared across all services\r\n - Detailed study of the failure modes in the different services and approaches (RAID, RAIN, ZFS vs XFS, etc...)\r\n - Disaster recovery strategies (across the two CERN computer centres)\r\n - Experience in coupling commodity and home-grown solution (e.g. Ceph disk pools for AFS, CASTOR and NFS)\r\n - Future evolution of these systems in the WLCG realm and beyond", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578658", "resources": [{"_type": "LocalFile", "name": "DiskStorageCERN-CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/323\/attachments\/578658\/796820\/DiskStorageCERN-CHEP2015.pdf", "fileName": "DiskStorageCERN-CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796820", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "bce5c14595f76db054a5c96d090c8d11", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ESPINAL CURULL, Xavier", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "925eddbe32f7924a4ac5bbfb3d696f74", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ROUSSEAU, Herve", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "526ebd66173cf4a22d191ac12c5edab8", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "VAN DER STER, Dan", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "6de52bf63fb628b76ec546b7b6d0df21", "affiliation": "University of Vigo (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "GONZALEZ LABRADOR, Hugo", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "27516240aee589612d903385c8d1fe91", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "IVEN, Jan", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "5931284fe6587485302a0da1f01ecbab", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "LAMANNA, Massimo", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "3b4cb47e53861b9951637052e3e58c58", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHAN KWOK CHEONG, Belinda", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "268ab3ff04e2cab2969f2c17e8b74c02", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FIOROT, Alessandro", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "8485c9c3b6371ebf21028c60c5135a7d", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "LO PRESTI, Giuseppe", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "8466cb6b4a3d0ee5c8e5b8aa9c4a7d36", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "PONCE, Sebastien", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "c884679b9d2b90f239195635833dd4ea", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "IERI, Andrea", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "6fb12183fa231f1c1d56de0659e0ccc9", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MOSCICKI, Jakub", "id": "12"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/323", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "320", "speakers": [{"_type": "ContributionParticipation", "emailHash": "8f70f1659c6be8e7735d4f8afb74d756", "affiliation": "Virginia Tech", "_fossil": "contributionParticipationMetadata", "fullName": "PIILONEN, Leo", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "8f70f1659c6be8e7735d4f8afb74d756", "affiliation": "Virginia Tech", "_fossil": "contributionParticipationMetadata", "fullName": "PIILONEN, Leo", "id": "2"}], "title": "The Simulation Library of the Belle II Software System", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T03:15:08.059501+00:00", "description": "", "title": "BelleII-Simulation-CHEP2015-A0.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/320\/attachments\/578659\/796821\/BelleII-Simulation-CHEP2015-A0.pdf", "filename": "BelleII-Simulation-CHEP2015-A0.pdf", "content_type": "application\/pdf", "type": "file", "id": 796821, "size": 5528905}], "title": "Slides", "default_folder": false, "id": 578659, "description": ""}], "_type": "Contribution", "description": "SuperKEKB and Belle II, the next generation B factory and its detector counterpart, are being constructed in Japan, as an upgrade of KEKB and Belle, respectively. The commissioning of the new SuperKEKB collider will be started in 2015. The luminosity of this e+ e\u2212 collider will be increased by a factor of 40, which will create a data sample 50 times larger than the previous Belle sample. Both the triggered and the background event rates will be increased by at least a factor of 10 compared to the previous rates, creating a very challenging data taking environment for the Belle II detector. The software system of the Belle II experiment is designed to execute this ambitious plan. A full detector simulation library, which is a part of the Belle II software system, is created based on Geant4. The construction of the library is progressing steadily and it is utilized actively in producing Monte Carlo data sets for pre-commission studies. In this talk, we will explain the detailed structure of the simulation library and the various interfaces to other packages including generators, geometry, and background simulation.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578659", "resources": [{"_type": "LocalFile", "name": "BelleII-Simulation-CHEP2015-A0.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/320\/attachments\/578659\/796821\/BelleII-Simulation-CHEP2015-A0.pdf", "fileName": "BelleII-Simulation-CHEP2015-A0.pdf", "_fossil": "localFileMetadata", "id": "796821", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/320", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:45:00"}, "duration": 15, "session": "Track 8 Session", "keywords": [], "id": "321", "speakers": [{"_type": "ContributionParticipation", "emailHash": "c6cb7804680ef5f9fa5f056bb684411b", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KANZAKI, Junichi", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c6cb7804680ef5f9fa5f056bb684411b", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KANZAKI, Junichi", "id": "0"}], "title": "Fast event generation on graphics processing unit (GPU) and its integration into the MadGraph system.", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T07:46:01.038246+00:00", "description": "", "title": "20150413_kanzaki_Fast_event_generation_on_GPU.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/321\/attachments\/578660\/796822\/20150413_kanzaki_Fast_event_generation_on_GPU.pdf", "filename": "20150413_kanzaki_Fast_event_generation_on_GPU.pdf", "content_type": "application\/pdf", "type": "file", "id": 796822, "size": 1510638}], "title": "Slides", "default_folder": false, "id": 578660, "description": ""}], "_type": "Contribution", "description": "Fast event generation system of physics processes is developed using graphics processing unit (GPU).\r\nThe system is based on the Monte Carlo integration and event generation programs, BASES\/SPRING, which were originally developed in FORTRAN.\r\nThey were rewritten on the CUDA platform provided by NVIDIA in order for the implementation of these programs to GPUs.\r\nSince the Monte Carlo integration algorithm is composed of a lot of independent function calls at multi-dimensional space, highly parallel architecture of GPU is very suitable for the improvement of their performance.\r\nThe performance of event generations based on the integrated results can be also easily improved by the event parallelization on GPU.\r\nParallelized programs show very good performance in process time compared to the existing event generation programs.\r\n\r\nFor the computation of cross sections of physics processes on GPU the helicity amplitude calculation package in FORTRAN is implanted in the CUDA framework as \"HEGET\" library, and new phase space generation library and random number generator are developed in order for the better generation efficiency on GPU. \r\nThe event generation system on GPU is tested using general Standard Model processes and computed cross sections are consistent with those obtained with the MadGraph system which is widely used in the field of elementary particle physics.\r\nThe total process time of the new system on GPU is compared with the equivalent programs on CPU and its improvement factors for various physics processes range from 10 to 100 depending on the complexity of their final states.\r\n\r\nIn order to achieve better interface for the generation of various physics processes and also to realize wider application of the fast Monte Carlo integration program the system has been integrated into the MadGraph5.\r\nAlso the HEGET library for the helicity amplitude computations is integrated into the ALOHA system of the MadGraph5.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578660", "resources": [{"_type": "LocalFile", "name": "20150413_kanzaki_Fast_event_generation_on_GPU.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/321\/attachments\/578660\/796822\/20150413_kanzaki_Fast_event_generation_on_GPU.pdf", "fileName": "20150413_kanzaki_Fast_event_generation_on_GPU.pdf", "_fossil": "localFileMetadata", "id": "796822", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/321", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "326", "speakers": [{"_type": "ContributionParticipation", "emailHash": "0b046a9b1029fc321d6586f4603d21c9", "affiliation": "Czech Technical University in Prague (CZ)", "_fossil": "contributionParticipationMetadata", "fullName": "TOMSA, Jan", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "55154d5947f3225c410325be9cb08ece", "affiliation": "Czech Technical University in Prague (CZ)", "_fossil": "contributionParticipationMetadata", "fullName": "NOVY, Josef", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "0b046a9b1029fc321d6586f4603d21c9", "affiliation": "Czech Technical University in Prague (CZ)", "_fossil": "contributionParticipationMetadata", "fullName": "TOMSA, Jan", "id": "0"}], "title": "Monitoring tools of COMPASS experiment at CERN", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-05T21:16:55.882058+00:00", "description": "POSTER - Monitoring tools of COMPASS experiment at CERN", "title": "Monitoring tools of COMPASS experiment at CERN", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/326\/attachments\/578661\/796823\/posterCHEP.pdf", "filename": "posterCHEP.pdf", "content_type": "application\/pdf", "type": "file", "id": 796823, "size": 904736}], "title": "Poster", "default_folder": false, "id": 578661, "description": ""}], "_type": "Contribution", "description": "Nowadays, all modern high energy physics experiments are substantially dependent on fast and reliable data acquisition systems that are able to collect large quantities of data supplied by various detectors. To ensure smooth and errorless operation, it is necessary to control and monitor the behavior and state of processes running in the system.\r\n\r\nCOMPASS is a high energy particle experiment with fixed target located at SPS at CERN laboratory in Geneva, Switzerland. This poster briefly introduces the data acquisition system of COMPASS and is mainly focused on the part that is responsible for the monitoring of the nodes in the whole newly developed data acquisition system of the experiment. \r\n\r\nThe technical shutdown of CERN during years 2013 and 2014 has been used to upgrade the data acquisition system of the COMPASS experiment. Both new harware, which uses PFGA cards for event building, and new software has been developed and deployed. The monitoring of the system is managed by tools called Message Logger and Message Browser. \r\n\r\nThe Message Logger is a simple console application that collects informative and error messages. The messages are received via standard switched network and the communication is handled by the DIM library. The DIM library is a communication tool originally designed and developed for the DELPHI experiment at CERN. It provides C++, JAVA, and Python interfaces. The relevant collected messages are then saved into the central COMPASS DAQ MySQL database. \r\n\r\nThe Message Browser is a tool with a graphical user interface created in Qt framework based upon a MVC (model - view - controller) design pattern. It is used to fetch and display messages previously stored by the Message Logger. It also features so called \"online mode\" which serves for acquisition of new messages via network at the time of their origin (this works similarly to the Message Logger). This functionality saves time and system resources - the new messages are displayed immediately and the program does not need to poll the database for new messages. Even though the Message Browser is connected to other nodes via the DIM service, it is designed to run independently from the other nodes (it requires only the database to be connected). Therefore it allows quicker response from the operators of the experiment in case of unexpected behavior or crash of the data acquisition system of the experiment. The Message Browser is also equipped with ordering and rich and intuitive filtering options (by all parameters of the messages). \r\n\r\nThe central data acquisition database stores system configuration and the messages. The Message_log table is expected to be the largest table in the database. Because the Message Browser requires high amount of data from the database, the Message_log table uses the MyISAM storage engine that is optimized for environments with heavy read operations. For further increase in speed, several indices are created on some of the columns, most importantly on the Stamp column (ordering by the time of the creation of the message is most important).\r\n\r\nThe poster also contains performance tests of the created monitoring tools.", "track": "Track1: Online computing ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578661", "resources": [{"_type": "LocalFile", "name": "Monitoring tools of COMPASS experiment at CERN", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/326\/attachments\/578661\/796823\/posterCHEP.pdf", "fileName": "posterCHEP.pdf", "_fossil": "localFileMetadata", "id": "796823", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "c42f2e84acb4f795be4950c7281641eb", "affiliation": "CERN (CH)", "_fossil": "contributionParticipationMetadata", "fullName": "BODLAK, Martin", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "55154d5947f3225c410325be9cb08ece", "affiliation": "Czech Technical University in Prague (CZ)", "_fossil": "contributionParticipationMetadata", "fullName": "NOVY, Josef", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "a144cbe9f02dbc8dfb576d0a00dc6581", "affiliation": "Czech Technical University in Prague (CZ)", "_fossil": "contributionParticipationMetadata", "fullName": "JARY, Vladimir", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "1c85d93abed3063c485a4549b8241601", "affiliation": "Czech Technical University in Prague (CZ)", "_fossil": "contributionParticipationMetadata", "fullName": "VIRIUS, Miroslav", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/326", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:15:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:30:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "327", "speakers": [{"_type": "ContributionParticipation", "emailHash": "bc9744b244becb77d8e097418d081341", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MASCETTI, Luca", "id": "4"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6fb12183fa231f1c1d56de0659e0ccc9", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MOSCICKI, Jakub", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "999374fec7057ef5475cac0d50d640f0", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. PETERS, Andreas Joachim", "id": "1"}], "title": "Cernbox + EOS: End-user Storage for Science", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T06:17:28.360641+00:00", "description": "", "title": "CERNBox-CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/327\/attachments\/578662\/796824\/CERNBox-CHEP2015.pdf", "filename": "CERNBox-CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796824, "size": 6950330}], "title": "Slides", "default_folder": false, "id": 578662, "description": ""}], "_type": "Contribution", "description": "Cernbox is a cloud synchronisation service for end-users: it allows to sync and share files on all major mobile and desktop platforms (Linux, Windows, MacOSX, Android, iOS) aiming to provide offline availability to any data stored in the CERN EOS infrastructure.  \r\n\r\nThe successful beta phase of the service confirmed the high demand in the community for such easily accessible cloud storage solution. Integration of the Cernbox service with the EOS storage back-end is the next step towards providing sync and share capabilities for scientific and engineering use-cases.\r\n\r\nIn this report we will present lessons learnt offering the Cernbox service, key technical aspects of Cernbox\/EOS integration and new, emerging usage possibilities. The latter include the ongoing integration of sync and share capabilities with the LHC data analysis tools and transfer services.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578662", "resources": [{"_type": "LocalFile", "name": "CERNBox-CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/327\/attachments\/578662\/796824\/CERNBox-CHEP2015.pdf", "fileName": "CERNBox-CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796824", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "5931284fe6587485302a0da1f01ecbab", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "LAMANNA, Massimo", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "6de52bf63fb628b76ec546b7b6d0df21", "affiliation": "University of Vigo (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "GONZALEZ LABRADOR, Hugo", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "bc9744b244becb77d8e097418d081341", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MASCETTI, Luca", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "0210065dafb782e4d922a706dbed482e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SINDRILARU, Elvin Alin", "id": "5"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/327", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:00:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "324", "speakers": [{"_type": "ContributionParticipation", "emailHash": "7d07f865b38767d4d2ed9895ebae0697", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "HAEN, Christophe", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "7d07f865b38767d4d2ed9895ebae0697", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "HAEN, Christophe", "id": "0"}], "title": "Federating LHCb datasets using the Dirac File Catalog", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T11:57:37.942557+00:00", "description": "", "title": "CHEP2015-ChristopheHaen_DFC.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/324\/attachments\/578663\/796825\/CHEP2015-ChristopheHaen_DFC.pdf", "filename": "CHEP2015-ChristopheHaen_DFC.pdf", "content_type": "application\/pdf", "type": "file", "id": 796825, "size": 2538723}], "title": "Slides", "default_folder": false, "id": 578663, "description": ""}], "_type": "Contribution", "description": "In the distributed computing model of LHCb the File Catalog (FC) is a central component that keeps track of each file and replica stored on the Grid. It is federating the LHCb data files in a logical namespace used by all LHCb applications. As a replica catalog, it is used for brokering jobs to sites where their input data is meant to be present, but also by jobs for finding alternative replicas if necessary. \r\nThe LCG File Catalog (LFC) used originally by LHCb and other experiments is now being retired and needs to be replaced. The DIRAC File Catalog (DFC) was developed within the framework of the DIRAC Project and presented during CHEP 2012. From the technical point of view, the code powering the DFC follows an Aspect oriented programming (AOP): each type of entity that is manipulated by the DFC (Users, Files, Replicas, etc) is treated as a separate 'concern' in the AOP terminology. Hence, the database schema can also be adapted to the needs of a Virtual Organization. LHCb opted for a highly tuned MySQL database, with optimized requests and stored procedures.  \r\nThis paper will present the improvements brought to the DFC presented at CHEP 2012, its performance with respect to the LFC, as well as the migration procedure used to migrate the LHCb data from the LFC to the DFC. Finally it will show how a combination of the DFC and the LHCb framework Gaudi allow LHCb to build a data federation at low cost.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578663", "resources": [{"_type": "LocalFile", "name": "CHEP2015-ChristopheHaen_DFC.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/324\/attachments\/578663\/796825\/CHEP2015-ChristopheHaen_DFC.pdf", "fileName": "CHEP2015-ChristopheHaen_DFC.pdf", "_fossil": "localFileMetadata", "id": "796825", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "2fe9b1dadba8d42a544cfd525c69fd57", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHARPENTIER, Philippe", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "abfcc508622af21078d4ac47c92ceff6", "affiliation": "CPPM, Aix-Marseille Universit\u00e9, CNRS\/IN2P3, Marseille, France", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. TSAREGORODTSEV, Andrei", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "84e347808f71c25608b52dc4bd77c2ad", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FRANK, Markus", "id": "3"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/324", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "325", "speakers": [{"_type": "ContributionParticipation", "emailHash": "7d07f865b38767d4d2ed9895ebae0697", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "HAEN, Christophe", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "7d07f865b38767d4d2ed9895ebae0697", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "HAEN, Christophe", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "abfcc508622af21078d4ac47c92ceff6", "affiliation": "CPPM, Aix-Marseille Universit\u00e9, CNRS\/IN2P3, Marseille, France", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. TSAREGORODTSEV, Andrei", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "2fe9b1dadba8d42a544cfd525c69fd57", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHARPENTIER, Philippe", "id": "2"}], "title": "Data Management System of the DIRAC Project", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T12:00:10.033726+00:00", "description": "", "title": "CHEP2015-DM-poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/325\/attachments\/578665\/796828\/CHEP2015-DM-poster.pdf", "filename": "CHEP2015-DM-poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796828, "size": 1913161}], "title": "Poster", "default_folder": false, "id": 578665, "description": ""}, {"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-17T01:00:25.766365+00:00", "description": "", "title": "CHEP2015-LHCbDMS-Poster-Slides.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/325\/attachments\/578664\/796826\/CHEP2015-LHCbDMS-Poster-Slides.pdf", "filename": "CHEP2015-LHCbDMS-Poster-Slides.pdf", "content_type": "application\/pdf", "type": "file", "id": 796826, "size": 558205}, {"_type": "attachment", "modified_dt": "2015-04-17T01:00:25.766365+00:00", "description": "", "title": "CHEP2015-LHCbDMS-Poster-Slides.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/325\/attachments\/578664\/796827\/CHEP2015-LHCbDMS-Poster-Slides.pptx", "filename": "CHEP2015-LHCbDMS-Poster-Slides.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796827, "size": 860879}], "title": "Slides", "default_folder": false, "id": 578664, "description": ""}], "_type": "Contribution", "description": "The DIRAC Interware provides a development framework and a complete set of components for building distributed computing systems. The DIRAC Data Management System (DMS) offers all the necessary tools to ensure data handling operations for small and large user communities. It supports transparent access to storage resources based on multiple technologies, and is easily expandable. The information on data files and \r\nreplicas is kept in a File Catalog of which DIRAC offers a powerful and versatile implementation (DFC). Data movement can be performed using third party services including FTS3. Bulk data operations are resilient with respect to failures due to the use of the Request Management System (RMS) that keeps track of ongoing tasks. \r\nIn this contribution we will present an overview of the DIRAC DMS capabilities and its connection with other DIRAC subsystems such as the Transformation System.\r\nThe DIRAC DMS is in use by several user communities now. The contribution will present the experience of the LHCb experiment. The experience of other experiements as well as multi-VO DIRAC services with the use of the DIRAC DMS will be described.", "track": "Track3: Data store and access ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578665", "resources": [{"_type": "LocalFile", "name": "CHEP2015-DM-poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/325\/attachments\/578665\/796828\/CHEP2015-DM-poster.pdf", "fileName": "CHEP2015-DM-poster.pdf", "_fossil": "localFileMetadata", "id": "796828", "_deprecated": true}], "_deprecated": true}, {"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578664", "resources": [{"_type": "LocalFile", "name": "CHEP2015-LHCbDMS-Poster-Slides.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/325\/attachments\/578664\/796826\/CHEP2015-LHCbDMS-Poster-Slides.pdf", "fileName": "CHEP2015-LHCbDMS-Poster-Slides.pdf", "_fossil": "localFileMetadata", "id": "796826", "_deprecated": true}, {"_type": "LocalFile", "name": "CHEP2015-LHCbDMS-Poster-Slides.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/325\/attachments\/578664\/796827\/CHEP2015-LHCbDMS-Poster-Slides.pptx", "fileName": "CHEP2015-LHCbDMS-Poster-Slides.pptx", "_fossil": "localFileMetadata", "id": "796827", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/325", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:00:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "328", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5ae4855aa9d3c6685bd9f450814f9af6", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "STAGNI, Federico", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "34fa6009bd34aced06fff1dd6d7fed34", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MATHE, Zoltan", "id": "0"}], "title": "Evaluation of NoSQL databases for DIRAC monitoring and beyond", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T01:54:35.826194+00:00", "description": "", "title": "nosqldiraclogo.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/328\/attachments\/578666\/796829\/nosqldiraclogo.pdf", "filename": "nosqldiraclogo.pdf", "content_type": "application\/pdf", "type": "file", "id": 796829, "size": 2220668}, {"_type": "attachment", "modified_dt": "2015-04-12T01:54:35.826194+00:00", "description": "", "title": "nosqldiraclogo.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/328\/attachments\/578666\/796830\/nosqldiraclogo.pptx", "filename": "nosqldiraclogo.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796830, "size": 3044928}], "title": "Slides", "default_folder": false, "id": 578666, "description": ""}], "_type": "Contribution", "description": "Nowadays, many database systems are available but they may not be optimized for storing time series data. The DIRAC job monitoring is a typical use case of such time series. So far it was done using a MySQL database, which is not well suited for such an application. Therefore alternatives have been investigated.\r\nChoosing an appropriate database for storing huge amounts of time series is not trivial as one must take into account different aspects such manageability, scalability, extensibility etc.\r\nWe compared the performance of Elasticsearch, OpenTSDB that is based on HBase and InfluxDB time series NoSQL databases using the same set of machines and the same data. We also evaluated the effort required for maintaining them.\r\nUsing the LHCb Workload Management System, based on DIRAC, as a use case we have setup a new monitoring system in parallel with the current MySQL system and we publish the same data into the databases under test. We have evaluated Grafana (for OpenTSDB) and Kibana (for ElasticSearch) metrics and graph editors for creating dashboards in order to have clear picture on the usability of each candidate. In this paper we present the result of this study and the performance of the selected technology. We also give an outlook of other potential applications of NoSQL databases with DIRAC project.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578666", "resources": [{"_type": "LocalFile", "name": "nosqldiraclogo.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/328\/attachments\/578666\/796829\/nosqldiraclogo.pdf", "fileName": "nosqldiraclogo.pdf", "_fossil": "localFileMetadata", "id": "796829", "_deprecated": true}, {"_type": "LocalFile", "name": "nosqldiraclogo.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/328\/attachments\/578666\/796830\/nosqldiraclogo.pptx", "fileName": "nosqldiraclogo.pptx", "_fossil": "localFileMetadata", "id": "796830", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "ddd3265ff49e33b97b9ba200f3a332d8", "affiliation": "University of Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "CASAJUS RAMO, Adrian", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "629dcce46f6fc937c9d767339166c446", "affiliation": "University of Ferrara and INFN", "_fossil": "contributionParticipationMetadata", "fullName": "TOMASSETTI, Luca", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "5ae4855aa9d3c6685bd9f450814f9af6", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "STAGNI, Federico", "id": "3"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/328", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:00:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "329", "speakers": [{"_type": "ContributionParticipation", "emailHash": "7ae6773a40c49ae5776d96621f5c7ab5", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "MIYAKE, Hideki", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "7ae6773a40c49ae5776d96621f5c7ab5", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "MIYAKE, Hideki", "id": "0"}], "title": "Belle II production system", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T07:35:12.547322+00:00", "description": "", "title": "CHEP2015_miyake_Belle2ProductionSystem.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/329\/attachments\/578667\/796831\/CHEP2015_miyake_Belle2ProductionSystem.pdf", "filename": "CHEP2015_miyake_Belle2ProductionSystem.pdf", "content_type": "application\/pdf", "type": "file", "id": 796831, "size": 5230195}], "title": "Slides", "default_folder": false, "id": 578667, "description": ""}], "_type": "Contribution", "description": "In Belle II experiment a large amount of physics data will be continuously taken and the production rate is equivalent to LHC experiments. \r\nConsiderable resources of computing, storage, and network, are necessary to handle not only the taken data but also substantial simulated data.\r\nTherefore Belle II exploits distributed computing system based on DIRAC interware.\r\nDIRAC is a general software framework to provide unified interface among heterogeneous computing resources.\r\nAs well as proven DIRAC software stack, Belle II is developing its own extension called BelleDIRAC.\r\nBelleDIRAC gives a transparent user experience of Belle II analysis framework (basf2) on various environments and access to file information managed by LFC and AMGA metadata catalog.\r\nWith unifying DIRAC and BelleDIRAC functionalities, Belle II plans to operate automated mass data processing framework named a production system.\r\nBelle II production system covers sizable raw data transfer from experimental site to raw data centers, followed by massive data processing, and smart output deployment to each remote site.\r\nThe production system is also utilized for simulated data production and data analysis.\r\nAlthough development of the production system is still on-going, recently Belle II has prepared prototype version and evaluated it with large scale of simulated data production test.\r\nIn this presentation we will report the evaluation of the prototype system and future development plan.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578667", "resources": [{"_type": "LocalFile", "name": "CHEP2015_miyake_Belle2ProductionSystem.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/329\/attachments\/578667\/796831\/CHEP2015_miyake_Belle2ProductionSystem.pdf", "fileName": "CHEP2015_miyake_Belle2ProductionSystem.pdf", "_fossil": "localFileMetadata", "id": "796831", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "ddcb03df7d14847b03901e32543b7ae1", "affiliation": "Charles Univ. in Prague", "_fossil": "contributionParticipationMetadata", "fullName": "LUDACKA, Radek", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "ad59ece4e48448bc02511d8867861e3d", "affiliation": "Pacific Northwest National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "SCHRAM, Malachi", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "017f6964531992962b690dc19924f69c", "affiliation": "Institute of Nuclear Physics PAN", "_fossil": "contributionParticipationMetadata", "fullName": "GRZYMKOWSKI, Rafal", "id": "3"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/329", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "201", "speakers": [{"_type": "ContributionParticipation", "emailHash": "336e636659f12d5fe7aede25fd33f666", "affiliation": "University of Virginia", "_fossil": "contributionParticipationMetadata", "fullName": "GROUP, Robert", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "66391efd180489cb4b61725469458858", "affiliation": "University of Sussex", "_fossil": "contributionParticipationMetadata", "fullName": "TAMSETT, Matthew", "id": "0"}], "title": "Software framework testing at the Intensity Frontier", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-05T10:49:42.920759+00:00", "description": "", "title": "1503_NOvA_CHEP_production_testing.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/201\/attachments\/578668\/796832\/1503_NOvA_CHEP_production_testing.pdf", "filename": "1503_NOvA_CHEP_production_testing.pdf", "content_type": "application\/pdf", "type": "file", "id": 796832, "size": 2950967}], "title": "Slides", "default_folder": false, "id": 578668, "description": ""}], "_type": "Contribution", "description": "The NOvA experiment at Fermilab is a long-baseline neutrino experiment designed to study nu-e appearance in a nu-mu beam. NOvA has already produced more than 1 million Monte Carlo and detector generated files amounting to more than 1 PB in size. This data is divided between a number of parallel streams such as far and near detector beam spills, cosmic ray backgrounds, a number of data-driven triggers and over 20 different Monte Carlo configurations.\r\n\r\nEach of these data streams must be processed through the appropriate steps of the rapidly evolving, multi-tiered, interdependent NOvA software framework. In total there are greater than 12 individual software tiers, each of which performs a different function and can be configured differently depending on the input stream.\r\n\r\nIn order to regularly test and validate that all of these software stages are working correctly NOvA has designed a powerful, modular testing framework that enables detailed validation and benchmarking to be performed in a fast, efficient and accessible way with minimal expert knowledge.  \r\n\r\nThe core of this system is a novel series of python modules which wrap, monitor and handle the underlying C++ software framework and then report the results to a slick front-end web-based interface. This interface utilises modern, cross-platform, visualisation libraries to render the test results in a meaningful way. They are fast and flexible, allowing us to cater, easily, for new tests and datasets. \r\n\r\nIn total upwards of 14 individual streams are regularly tested amounting to over 70 individual software processes, producing over 25 GB of output files. The rigour enforced through this flexible testing framework enables NOvA to rapidly verify configurations, results and software and thus ensure that data is available for physics analysis in a timely and robust manner.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578668", "resources": [{"_type": "LocalFile", "name": "1503_NOvA_CHEP_production_testing.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/201\/attachments\/578668\/796832\/1503_NOvA_CHEP_production_testing.pdf", "fileName": "1503_NOvA_CHEP_production_testing.pdf", "_fossil": "localFileMetadata", "id": "796832", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "336e636659f12d5fe7aede25fd33f666", "affiliation": "University of Virginia", "_fossil": "contributionParticipationMetadata", "fullName": "GROUP, Robert", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/201", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:30:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "200", "speakers": [{"_type": "ContributionParticipation", "emailHash": "698c361cfbc9ed204ca9f7b8fc5bb4cd", "affiliation": "Univ. of Minnesota Duluth", "_fossil": "contributionParticipationMetadata", "fullName": "HABIG, Alec", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "336e636659f12d5fe7aede25fd33f666", "affiliation": "University of Virginia", "_fossil": "contributionParticipationMetadata", "fullName": "GROUP, Robert", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "6ba1f95da55f3a0622e4785d12b5cc25", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "NORMAN, Andrew", "id": "1"}], "title": "Recent Evolution of the Offline Computing Model of the NOvA Experiment", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-08T19:12:21.705392+00:00", "description": "", "title": "NOvA_CHEP_Computing_2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/200\/attachments\/578669\/796833\/NOvA_CHEP_Computing_2015.pdf", "filename": "NOvA_CHEP_Computing_2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796833, "size": 8605003}], "title": "Slides", "default_folder": false, "id": 578669, "description": ""}], "_type": "Contribution", "description": "The NOvA experiment at Fermilab is a long-baseline neutrino experiment designed to study nu-e appearance in a nu-mu beam.   Over the last few years there has been intense work to streamline the computing infrastructure in preparation for data, which started to flow in from the far detector in Fall 2013.  Major accomplishments for this effort include migration to the use of offsite resources through the use of the Open Science Grid and upgrading the file handling framework from simple disk storage to a tiered system using a comprehensive data management and delivery system to find and access files on either disk, dCache, or tape storage.  NOvA has already produced more than 6.5 million files and more than 1 PB of raw data and Monte Carlo simulation generated files which are managed under this model.  The current system has demonstrated sustained rates of up to 1 TB per hour of file transfer to permanent storage.   NOvA pioneered the use of new tools and this paved the way for their use by other Intensity Frontier experiments at Fermilab.  Most importantly, the new framework places the experiment\u2019s computing infrastructure on a firm foundation, which is ready to produce the files required for NOvA's first physics results.  In this talk we discuss the offline computing model and infrastructure that has been put in place for NOvA and how we have used it to produce the experiment\u2019s first neutrino oscillation results.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578669", "resources": [{"_type": "LocalFile", "name": "NOvA_CHEP_Computing_2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/200\/attachments\/578669\/796833\/NOvA_CHEP_Computing_2015.pdf", "fileName": "NOvA_CHEP_Computing_2015.pdf", "_fossil": "localFileMetadata", "id": "796833", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "698c361cfbc9ed204ca9f7b8fc5bb4cd", "affiliation": "Univ. of Minnesota Duluth", "_fossil": "contributionParticipationMetadata", "fullName": "HABIG, Alec", "id": "2"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/200", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:30:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "203", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ce168f7b391d7e4113d58dc9aae233f7", "affiliation": "Albert-Ludwigs-Universitaet Freiburg (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "BOEHLER, Michael", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ce168f7b391d7e4113d58dc9aae233f7", "affiliation": "Albert-Ludwigs-Universitaet Freiburg (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "BOEHLER, Michael", "id": "0"}], "title": "Evolution of ATLAS conditions data and its management for LHC run-2", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T10:39:28.520957+00:00", "description": "", "title": "ID_203_Evolution_of_ATLAS_Conditions.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/203\/attachments\/578670\/796834\/ID_203_Evolution_of_ATLAS_Conditions.pdf", "filename": "ID_203_Evolution_of_ATLAS_Conditions.pdf", "content_type": "application\/pdf", "type": "file", "id": 796834, "size": 5235677}], "title": "Slides", "default_folder": false, "id": 578670, "description": ""}], "_type": "Contribution", "description": "The ATLAS detector consists of several sub-detector systems. Both data taking and Monte Carlo (MC) simulation rely on an accurate description of the detector conditions from every sub system, such as calibration constants, different scenarios of pile-up and noise conditions, size and position of the beam spot, etc. In order to guarantee database availability for critical online applications during data-taking, two database systems, one for online access and another one for all other database access have been implemented.\r\n\r\nThe long shutdown period has provided the opportunity to review and improve the run-1 system: revise workflows, include new and innovative monitoring and maintenance tools and implement a new database instance for run-2 conditions data.\r\nThe detector conditions are organized by tag identification strings and managed independently from the different sub-detector experts. The individual tags are then collected and associated into a global conditions tag, assuring synchronization of various sub-detector improvements. Furthermore, a new concept was introduced to maintain conditions over all the different data run periods into a single tag, by using Interval of Validity (IOV) dependent detector conditions for the MC database as well. This allows on-flight preservation of past conditions for data and MC and assure their sustainability with software evolution.\r\n\r\nThis contribution presents an overview of the commissioning of the new database instance, improved tools and workflows, and summarizes the actions taken during the run-2 commissioning phase beginning of 2015.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578670", "resources": [{"_type": "LocalFile", "name": "ID_203_Evolution_of_ATLAS_Conditions.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/203\/attachments\/578670\/796834\/ID_203_Evolution_of_ATLAS_Conditions.pdf", "fileName": "ID_203_Evolution_of_ATLAS_Conditions.pdf", "_fossil": "localFileMetadata", "id": "796834", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "6901c47affe7b6a62afbc11f26f7b0a6", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "RADESCU, Voica Ana Maria", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "d69f5fff808f8ac8ab6376496b16ae5a", "affiliation": "National Research Nuclear  University MEPhI (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "BORODIN, Misha", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "0924dfb6cfaf429459832827db5a0041", "affiliation": "CEA\/IRFU,Centre d'etude de Saclay Gif-sur-Yvette (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "FORMICA, Andrea", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "c195ef3b576ede75b0ccfd47c33fba3a", "affiliation": "University of Oxford (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "GALLAS, Elizabeth", "id": "4"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/203", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:30:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:45:00"}, "duration": 15, "session": "Track 8 Session", "keywords": [], "id": "202", "speakers": [{"_type": "ContributionParticipation", "emailHash": "003ad3f5b216f9c6fb11ed30cfff59c1", "affiliation": "LLR - \u00c9cole polytechnique", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CHAMONT, David", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "003ad3f5b216f9c6fb11ed30cfff59c1", "affiliation": "LLR - \u00c9cole polytechnique", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CHAMONT, David", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "0454eceb42bd4ff2ec2a37a01425f8d2", "affiliation": "Ecole Polytechnique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "GRASSEAU, Gilles", "id": "1"}], "title": "Matrix Element Method for High Performance Computing platforms", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T04:42:28.872006+00:00", "description": "", "title": "ChepCmsMemHpc-final.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/202\/attachments\/578671\/796835\/ChepCmsMemHpc-final.pdf", "filename": "ChepCmsMemHpc-final.pdf", "content_type": "application\/pdf", "type": "file", "id": 796835, "size": 1538085}], "title": "Slides", "default_folder": false, "id": 578671, "description": ""}], "_type": "Contribution", "description": "The Matrix Element Method (MEM) is a well known powerful approach in particle physics to extract maximal information of the events arising from the LHC pp collisions. Compared to other methods requiring trainings, the MEM allows direct comparisons between a theory and the observation. Since the phase space has a higher dimensionality to explore, MEM is much more CPU time consuming at the analysis level than classic methods. As a consequence, this method is hardly exploitable with sequential implementation, in particular, when one has to deal with channels with large backgrounds. For the upcoming LHC data-taking, this issue will become even more crucial.\r\n\r\nThe major challenge of this project is to provide a MEM implementation based on widely used standards such as MPI and OpenCL, which, taking advantage of multi-acccelerators and multi-node architectures, offers a drastic speed-up with low cost technologies.\r\n\r\nIn this talk, we will present how, in the context of the CMS experiment, we parallelized the MEM with the OpenCL abstract model and the memory distributed model (MPI layer). We will describe the necessary work on the different ingredients: the Parton Distribution Functions library (LHAPDF), the computation of the matrix element (MadGraph) and a small subset of ROOT tools. In addition, we will focus on the main tricky points to have access to an implementation of quality, e.g. uncorrelated random number sequences, minimizing synchronizations, minimizing the number of kernel calls. Finally, we will present the performance obtained on various platforms : CPUs, GPGPU, MIC, multi-GPGPUs or MICs, and cluster of GPU-nodes.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578671", "resources": [{"_type": "LocalFile", "name": "ChepCmsMemHpc-final.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/202\/attachments\/578671\/796835\/ChepCmsMemHpc-final.pdf", "fileName": "ChepCmsMemHpc-final.pdf", "_fossil": "localFileMetadata", "id": "796835", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "f557c94c1d5a92ae078b7f895524661c", "affiliation": "Ecole Polytechnique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "BEAUDETTE, Florian", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "50a3d05a3d87839c6b8128ff2edc8de5", "affiliation": "Eidgenoessische Tech. Hochschule Zuerich (CH)", "_fossil": "contributionParticipationMetadata", "fullName": "BIANCHINI, Lorenzo", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "9e3d68a7a0e161fb635a245535f65052", "affiliation": "Ecole Polytechnique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "DAVIGNON, Olivier", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "dd3aeda3bfd57aaef6ac3118e468910e", "affiliation": "Ecole Polytechnique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "MASTROLORENZO, Luca", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "7979630e6458eed59421a378aa2d0716", "affiliation": "Ecole Polytechnique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "STREBLER, Thomas", "id": "8"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/202", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:30:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "205", "speakers": [{"_type": "ContributionParticipation", "emailHash": "c0af0a453fb7de772f32e585bcdcd2ba", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GARONNE, Vincent", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c0af0a453fb7de772f32e585bcdcd2ba", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GARONNE, Vincent", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "024219837a74f1e2a8825afaccd81fbf", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SERFON, Cedric", "id": "1"}], "title": "The ATLAS Data Management system - Rucio: commissioning, migration and operational experiences", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T12:44:58.227232+00:00", "description": "", "title": "ATLAS_DDM_RUCIO_2015_GARONNE.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/205\/attachments\/578672\/796836\/ATLAS_DDM_RUCIO_2015_GARONNE.pdf", "filename": "ATLAS_DDM_RUCIO_2015_GARONNE.pdf", "content_type": "application\/pdf", "type": "file", "id": 796836, "size": 1298947}], "title": "Slides", "default_folder": false, "id": 578672, "description": ""}], "_type": "Contribution", "description": "For more than 8 years, the Distributed Data Management (DDM) system of ATLAS called DQ2 has been able to demonstrate very large scale data management capabilities with more than 600M files, 160 petabytes spread worldwide across 130 sites, and accesses from 1,000 active users. However, the system does not  scale for LHC run2 and a new DDM system called Rucio has been developed to be DQ2's successor. Rucio is based on different concepts and has new functionalities not provided by DQ2 which make the migration from the old to the new system a big challenge. The main issues are  the large amount of data to move between the two systems, the number of users affected by the change, and the fact that the ATLAS Distributing Computing system, on the contrary to  the sub-detectors, must stay continuously up and running during the LHC long shutdown to ensure the continuity of analysis and Monte-Carlo production. We will detail here the difficulties of this transition and will present the steps that were realized to ensure a smooth and transparent transition from DQ2 to Rucio. We will also discuss the new features and gains from the Rucio system.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578672", "resources": [{"_type": "LocalFile", "name": "ATLAS_DDM_RUCIO_2015_GARONNE.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/205\/attachments\/578672\/796836\/ATLAS_DDM_RUCIO_2015_GARONNE.pdf", "fileName": "ATLAS_DDM_RUCIO_2015_GARONNE.pdf", "_fossil": "localFileMetadata", "id": "796836", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "46207806844c107a8fefe566f669b206", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BARISITS, Martin", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "6db250f3509377bad379e91097c39ae5", "affiliation": "Bergische Universitaet Wuppertal (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "BEERMANN, Thomas", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "269913119a8e37ae971b93b8559c6092", "affiliation": "University of Oslo (NO)", "_fossil": "contributionParticipationMetadata", "fullName": "CAMERON, David", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "a0dd1a3a7589f1e2a04cc967f2ae6c39", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DI GIROLAMO, Alessandro", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "af353d572ba111925ed14b42afd4e2e5", "affiliation": "University of Wisconsin (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GUAN, Wen", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "4a98ef3c3b5961fee22e9582b78fe5ae", "affiliation": "Acad. of Sciences of the Czech Rep. (CZ)", "_fossil": "contributionParticipationMetadata", "fullName": "KOUBA, Tomas", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "ef2db26b320ecf7a9b75abbaef349287", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LASSNIG, Mario", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "cf29a05330d5f43955651b32543f7283", "affiliation": "University of Vienna (AT)", "_fossil": "contributionParticipationMetadata", "fullName": "VIGNE, Ralph", "id": "9"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/205", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:00:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "204", "speakers": [{"_type": "ContributionParticipation", "emailHash": "74f37452da18d70a445f84d900eff716", "affiliation": "Universita e INFN, Roma I (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "DE SALVO, Alessandro", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "74f37452da18d70a445f84d900eff716", "affiliation": "Universita e INFN, Roma I (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "DE SALVO, Alessandro", "id": "0"}], "title": "The ATLAS Software Installation System v2: a highly available system to install and validate Grid and Cloud sites via Panda", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T21:11:59.653331+00:00", "description": "", "title": "ATLAS_Installation_System_CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/204\/attachments\/578673\/796837\/ATLAS_Installation_System_CHEP2015.pdf", "filename": "ATLAS_Installation_System_CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796837, "size": 608477}, {"_type": "attachment", "modified_dt": "2015-04-10T21:11:59.653331+00:00", "description": "", "title": "ATLAS_Installation_System_CHEP2015.ppt", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/204\/attachments\/578673\/796838\/ATLAS_Installation_System_CHEP2015.ppt", "filename": "ATLAS_Installation_System_CHEP2015.ppt", "content_type": "application\/vnd.ms-powerpoint", "type": "file", "id": 796838, "size": 682496}], "title": "Slides", "default_folder": false, "id": 578673, "description": ""}], "_type": "Contribution", "description": "The ATLAS Installation System v2 is the evolution of the original system, used since 2003. The original tool has been completely re-designed in terms of database backend and components, adding support for submission to multiple backends, including the original WMS and the new Panda modules. The database engine has been changed from plain MySQL to Galera\/Percona and the table structure has been optimized to allow a full High-Availability (HA) solution over WAN. The\r\nservlets, running on each frontend, have been also decoupled from local settings, to allow an easy scalability of the system, including the possibility of an HA system with multiple sites. The clients can also be run in multiple copies and in different geographical locations, and take care of sending the installation and validation jobs to the target Grid or Cloud sites.\r\nMoreover, the Installation DB is used as source of parameters by the automatic agents running in CVMFS, in order to install the software and distribute it to the sites.\r\nThe system is in production for ATLAS since 2013, having as main sites in HA the INFN Roma T2 and CERN AI. The LJSFi2 engine is directly interfacing with Panda for the Job Management, AGIS for the site parameter configurations, and CVMFS for both core components and the installation of the software itself.\r\nLJSFi2 is also able to use other plugins, and is essentially VO-agnostic, so can be directly used and extended to cope with the requirements of any Grid or Cloud enabled VO.\r\nIn this work we'll present the architecture, performance, status and possible evolutions to the system for the LHC Run2 and beyond.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578673", "resources": [{"_type": "LocalFile", "name": "ATLAS_Installation_System_CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/204\/attachments\/578673\/796837\/ATLAS_Installation_System_CHEP2015.pdf", "fileName": "ATLAS_Installation_System_CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796837", "_deprecated": true}, {"_type": "LocalFile", "name": "ATLAS_Installation_System_CHEP2015.ppt", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/204\/attachments\/578673\/796838\/ATLAS_Installation_System_CHEP2015.ppt", "fileName": "ATLAS_Installation_System_CHEP2015.ppt", "_fossil": "localFileMetadata", "id": "796838", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "5540c6e6861aff9d2e78a4e170552985", "affiliation": "University of Texas at Arlington (US)", "_fossil": "contributionParticipationMetadata", "fullName": "KATAOKA, Mayuko", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "6ccc07fd015ee291538d306b2a1f82dd", "affiliation": "Universita di Napoli Federico II-Universita e INFN", "_fossil": "contributionParticipationMetadata", "fullName": "SANCHEZ PINEDA, Arturo", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "db331cedcdce208e2ba9db3713b3f1db", "affiliation": "Conseil Europeen Recherche Nucl. (CERN)-Unknown-Unknown", "_fossil": "contributionParticipationMetadata", "fullName": "SMIRNOV, Yuri", "id": "3"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/204", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:45:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "207", "speakers": [{"_type": "ContributionParticipation", "emailHash": "46207806844c107a8fefe566f669b206", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BARISITS, Martin", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "46207806844c107a8fefe566f669b206", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BARISITS, Martin", "id": "0"}], "title": "Resource control in ATLAS distributed data management: Rucio Accounting and Quotas", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T06:17:45.217079+00:00", "description": "", "title": "presentation.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/207\/attachments\/578674\/796839\/presentation.pdf", "filename": "presentation.pdf", "content_type": "application\/pdf", "type": "file", "id": 796839, "size": 394772}], "title": "Slides", "default_folder": false, "id": 578674, "description": ""}], "_type": "Contribution", "description": "The ATLAS Distributed Data Management system stores more than 160PB of physics data across more than 130 sites globally. Rucio, the next-generation data management system of ATLAS has been introduced to cope with the anticipated workload of the coming decade. The previous data management system DQ2 pursued a rather simplistic approach for resource management, but with the increased data volume and more dynamic handling of data workflows required by the experiment, a more elaborate approach to this issue is needed. This document describes how resources, like storage, accounts and replication requests, are accounted in Rucio. Especially the measurement of used logical storage space is fundamentally different in Rucio than it\u2019s predecessor DQ2. We introduce a new concept of declaring quota policies (limits) for accounts in Rucio. This new quota concept is based on accounts and RSE (Rucio storage element) expressions, which allows the definition of account limits in a dynamic way. This concept enables the operators of the data management system to establish very specific limits in which users, physics groups and production systems use the distributed data management system while, at the same time, lowering the operational burden. This contribution describes the architecture behind those components, the interfaces to other internal and external components and will show the benefits made by this system.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578674", "resources": [{"_type": "LocalFile", "name": "presentation.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/207\/attachments\/578674\/796839\/presentation.pdf", "fileName": "presentation.pdf", "_fossil": "localFileMetadata", "id": "796839", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "6db250f3509377bad379e91097c39ae5", "affiliation": "Bergische Universitaet Wuppertal (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "BEERMANN, Thomas", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c0af0a453fb7de772f32e585bcdcd2ba", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GARONNE, Vincent", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "ef2db26b320ecf7a9b75abbaef349287", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LASSNIG, Mario", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "024219837a74f1e2a8825afaccd81fbf", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SERFON, Cedric", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "cf29a05330d5f43955651b32543f7283", "affiliation": "University of Vienna (AT)", "_fossil": "contributionParticipationMetadata", "fullName": "VIGNE, Ralph", "id": "5"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/207", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "206", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ef2db26b320ecf7a9b75abbaef349287", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LASSNIG, Mario", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ef2db26b320ecf7a9b75abbaef349287", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LASSNIG, Mario", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "6db250f3509377bad379e91097c39ae5", "affiliation": "Bergische Universitaet Wuppertal (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "BEERMANN, Thomas", "id": "7"}], "title": "Monitoring and controlling ATLAS data management: The Rucio web user interface", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-03-27T14:24:59.172490+00:00", "description": "", "title": "Monitoring_and_controlling_ATLAS_data_management.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/206\/attachments\/578675\/796840\/Monitoring_and_controlling_ATLAS_data_management.pdf", "filename": "Monitoring_and_controlling_ATLAS_data_management.pdf", "content_type": "application\/pdf", "type": "file", "id": 796840, "size": 1092888}], "title": "Poster", "default_folder": false, "id": 578675, "description": ""}], "_type": "Contribution", "description": "The monitoring and controlling interfaces of the previous data management system DQ2 followed the evolutionary requirements and needs of the ATLAS collaboration. The new system, Rucio, has put in place a redesigned web-based interface based upon the lessons learnt from DQ2, and the increased volume of managed information. This interface encompasses both a monitoring and controlling component, and allows easy integration for user-generated views. The interface follows three design principles. First, the collection and storage of data from internal and external systems is asynchronous to reduce latency. This includes the use of technologies like ActiveMQ or Nagios. Second, analysis of the data into information is done massively parallel due to its volume, using a combined approach with an Oracle database and Hadoop MapReduce. Third, sharing of the information does not distinguish between human or programmatic access, making it easy to access selective parts of the information both in constrained frontends like web-browsers as well as remote services. This contribution will detail the reasons for these principles and the design choices taken. Additionally, the implementation, the interactions with external systems, and an evaluation of the system in production, both from a technological and user perspective, conclude this contribution.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578675", "resources": [{"_type": "LocalFile", "name": "Monitoring_and_controlling_ATLAS_data_management.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/206\/attachments\/578675\/796840\/Monitoring_and_controlling_ATLAS_data_management.pdf", "fileName": "Monitoring_and_controlling_ATLAS_data_management.pdf", "_fossil": "localFileMetadata", "id": "796840", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "46207806844c107a8fefe566f669b206", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BARISITS, Martin", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c0af0a453fb7de772f32e585bcdcd2ba", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GARONNE, Vincent", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "024219837a74f1e2a8825afaccd81fbf", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SERFON, Cedric", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "cf29a05330d5f43955651b32543f7283", "affiliation": "University of Vienna (AT)", "_fossil": "contributionParticipationMetadata", "fullName": "VIGNE, Ralph", "id": "5"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/206", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:15:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "209", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2cbb21e728f5ee0f6e1cb54ec854a4a7", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SALZBURGER, Andreas", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2cbb21e728f5ee0f6e1cb54ec854a4a7", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SALZBURGER, Andreas", "id": "0"}], "title": "Optimisation of the ATLAS Track Reconstruction Software for Run-2", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T12:50:10.577744+00:00", "description": "", "title": "CHEP-Salzburger.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/209\/attachments\/578676\/796841\/CHEP-Salzburger.pdf", "filename": "CHEP-Salzburger.pdf", "content_type": "application\/pdf", "type": "file", "id": 796841, "size": 4480309}], "title": "Slides", "default_folder": false, "id": 578676, "description": ""}], "_type": "Contribution", "description": "Track reconstruction is one of the most complex elements of the reconstruction of events recorded by ATLAS from collisions delivered by the LHC. It is the most time consuming reconstruction component in high luminosity environments. After a hugely successful Run-1, the flat budget projections for computing resources for Run-2 of the LHC together with the demands of reconstructing higher pile-up collision data at rates more than double those in Run-1 (an increase from 400 Hz to 1 kHz in trigger output) have put stringent requirements on the track reconstruction software. The ATLAS experiment has performed a two year long software campaign which aimed to reduce the reconstruction rate by a factor of three to meet the resource limitations for Run-2: a major part of the changes to achieve this were improvements to the track reconstruction software and will be presented in this contribution . The CPU processing time of ATLAS track reconstruction was reduced by more than a factor of three during this campaign without any loss of output information of the track reconstruction.\r\nWe present the methods used for analysing the tracking software and the code changes and new methods implemented to optimise both algorithmic performance and event data. Although most improvements were obtained without dedicated targetting concurrency strategies, major parts of the ATLAS tracking software were updated to allow for future improvements based on parallelism which will also be discussed.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578676", "resources": [{"_type": "LocalFile", "name": "CHEP-Salzburger.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/209\/attachments\/578676\/796841\/CHEP-Salzburger.pdf", "fileName": "CHEP-Salzburger.pdf", "_fossil": "localFileMetadata", "id": "796841", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/209", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:45:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "208", "speakers": [{"_type": "ContributionParticipation", "emailHash": "35f0509459ee62f9ddcf584936cca96a", "affiliation": "Universit\u00e0 e INFN Genova (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BARBERIS, Dario", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "35f0509459ee62f9ddcf584936cca96a", "affiliation": "Universit\u00e0 e INFN Genova (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BARBERIS, Dario", "id": "0"}], "title": "The ATLAS EventIndex: architecture, design choices, deployment and first operation experience.", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-01T16:51:40.777814+00:00", "description": "", "title": "ATLAS_EventIndex_CHEP2015v5.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/208\/attachments\/578677\/796842\/ATLAS_EventIndex_CHEP2015v5.pdf", "filename": "ATLAS_EventIndex_CHEP2015v5.pdf", "content_type": "application\/pdf", "type": "file", "id": 796842, "size": 1785106}], "title": "Slides", "default_folder": false, "id": 578677, "description": ""}], "_type": "Contribution", "description": "The EventIndex is the complete catalogue of all ATLAS events, keeping the references to all files that contain a given event in any processing stage. It replaces the TAG database, which had been in use during LHC Run 1. For each event it contains its identifiers, the trigger pattern and the GUIDs of the files containing it. Major use cases are event picking, feeding the Event Service used on some production sites, and technical checks of the completion and consistency of processing campaigns. The system design is highly modular so that its components (data collection system, storage system based on Hadoop, query web service and interfaces to other ATLAS systems) could be developed separately and in parallel during LS1. The EventIndex is in operation for the start of LHC Run 2. This talk describes the high level system architecture, the technical design choices and the deployment process and issues. The performance of the data collection and storage systems, as well as the query services, will be reported.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578677", "resources": [{"_type": "LocalFile", "name": "ATLAS_EventIndex_CHEP2015v5.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/208\/attachments\/578677\/796842\/ATLAS_EventIndex_CHEP2015v5.pdf", "fileName": "ATLAS_EventIndex_CHEP2015v5.pdf", "_fossil": "localFileMetadata", "id": "796842", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "87a100f703ae951e1cd6f8f0c55b8582", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CRANSHAW, Jack", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "92cc19bc26fd5920c677f699da8c4264", "affiliation": "Universit\u00e0 degli Studi e INFN Genova", "_fossil": "contributionParticipationMetadata", "fullName": "FAVARETO, Andrea", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "19701f2de82fdbcb183a5117cb576c42", "affiliation": "Instituto de Fisica Corpuscular (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "FERNANDEZ CASANI, Alvaro", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "c195ef3b576ede75b0ccfd47c33fba3a", "affiliation": "University of Oxford (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "GALLAS, Elizabeth", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "7ca77722c99b3cb5ba43a17522e24d50", "affiliation": "Universidad Autonoma de Madrid", "_fossil": "contributionParticipationMetadata", "fullName": "GLASMAN, Claudia", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "35833110c8c23bc2b197a21fb79fe2c3", "affiliation": "IFIC-Valencia", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GONZ\u00c1LEZ DE LA HOZ, Santiago", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "a5efba4a70913e2df05be5f3fdfb3ee5", "affiliation": "Laboratoire de l'Accelerateur Lineaire (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HRIVNAC, Julius", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "9308ae80f0f8aaf14aef2fa1243f3fec", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MALON, David", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "5b3e126078a6dedb8418173ea224b6c8", "affiliation": "Federico Santa Maria Technical University (CL)", "_fossil": "contributionParticipationMetadata", "fullName": "PROKOSHIN, Fedor", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "d8619a9dff304fef97736b1d40cb7569", "affiliation": "Laboratoire de l'Accelerateur Lineaire (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "YUAN, Ruijun", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "c11231d8066a85165b23821c2e018d66", "affiliation": "Universidad de Valencia (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "SANCHEZ, Javier", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "6275a7f17f35e57d40b8d08267f7d79a", "affiliation": "IFIC-VALENCIA", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SALT, JOSE", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "1ac84e9dd5fc3113c1e4ee308aac1693", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "TOEBBICKE, Rainer", "id": "13"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/208", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:15:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "77", "speakers": [{"_type": "ContributionParticipation", "emailHash": "bce47fa0aea8f065a048be8ff811db8f", "affiliation": "High Energy Accelerator Research Organization (KEK)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. HAMADA, Eitaro", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "bce47fa0aea8f065a048be8ff811db8f", "affiliation": "High Energy Accelerator Research Organization (KEK)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. HAMADA, Eitaro", "id": "0"}], "title": "The Application of DAQ-Middleware to the J-PARC E16 Experiment", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T08:08:49.488988+00:00", "description": "", "title": "CHEP2015_hamada.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/77\/attachments\/578678\/796844\/CHEP2015_hamada.pdf", "filename": "CHEP2015_hamada.pdf", "content_type": "application\/pdf", "type": "file", "id": 796844, "size": 1015083}, {"_type": "attachment", "modified_dt": "2015-04-13T08:08:49.488988+00:00", "description": "", "title": "CHEP2015_hamada.ppt", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/77\/attachments\/578678\/796843\/CHEP2015_hamada.ppt", "filename": "CHEP2015_hamada.ppt", "content_type": "application\/vnd.ms-powerpoint", "type": "file", "id": 796843, "size": 2547200}], "title": "Slides", "default_folder": false, "id": 578678, "description": ""}], "_type": "Contribution", "description": "**1. Introduction**\r\n\r\nWe developed a DAQ system of the J-PARC E16 Experiment by using the DAQ-Middleware. We evaluated the DAQ system and confirmed that the DAQ system can be applied to the experiment.\r\nThe DAQ system receives an average 660MB\/spill of data (2-seconds spill per 6 seconds cycle). In order to receive such a large quantity of data, we need a network-distributed system. DAQ-Middleware is a software framework of a network-distributed DAQ system. Therefore, the framework is a useful tool for the DAQ system development. In our talk, we are going to talk about useful features of DAQ-Middleware, an architecture and DAQ performance of the J-PARC E16 Experiment DAQ system.\r\n\r\n\r\n**2. The J-PARC E16 Experiment**\r\n\r\nThe aim of the J-PARC E16 Experiment is to measure mass spectra of vector mesons in nucleus using electron pair decays with a huge statistics. For such purpose, a high intensity proton beam is used and the interaction rate of the experiment becomes $1\\times10^{10}$. To cope with such high intensity and the rate, we plan to use the Gas Electron Multiplier (GEM) Tracker and strip read outs. Figure1 shows the estimation of data transfer to DAQ PCs.\r\n\r\n![Estimation of data transfer to DAQ PCs][1]\r\n \r\n\r\n**3. DAQ-Middleware**\r\n\r\nDAQ-Middleware is a software framework of a network-distributed DAQ system. The framework consists of some software components called DAQ-Components. The framework provides the basic functionalities, such as communication between DAQ-Components, transferring data, starting and stopping the DAQ system. DAQ-Components can be set on separate computers. \r\n\r\n\r\n**4. DAQ system for the E16 Experiment**\r\n\r\nThe DAQ system performs following functions.\r\n \r\n\r\n - store all data on storage devices\r\n - build and monitor event data from a part of all events\r\n\r\n\r\nFigure2 shows  architecture of an entire DAQ system.\r\n\r\n![Architecture of an entire DAQ system][2]\r\n\r\nThe DAQ system consists of two stages. 1st stage PCs read and store all data. The 2nd stage PC builds and monitors event data from a part of all events.\r\n\r\nFigure3 shows DAQ-Component architecture on one 1st stage PC. \r\n \r\n![1st stage for the DAQ system][3]\r\n\r\nBlue boxes of the figure represent DAQ-Component. Gatherer reads data from one read-out module. Merger receives data from multiple Gatherers and sends the data to Dispatcher. Dispatcher sends data to Logger and Filter. Logger stores data on storage devices. Filter sends data, which meet specific conditions, to a next DAQ-Component on the 2nd stage PC. 1st stage consists of multiple PCs which have these DAQ-Components.\r\n\r\nFigure4 shows DAQ-Component architecture on one 2nd stage PC. \r\n\r\n\r\n![2nd Stage for the DAQ system][4]\r\n\r\nBlue boxes of the figure represent DAQ-Component. Merger receives data from multiple Filters on the 1st stage PC and sends the data to Eventbuilder. This Merger is the same as one of the 1st stage PC. Eventbuilder builds event data from received data. Monitor analysis and monitor the event data.\r\n\r\n\r\n**5. Evaluation for the DAQ System**\r\n\r\nBecause we did not have enough PCs, we evaluated 1st stage and 2nd stage separately.\r\n\r\n![Specification of evaluation PC][5]\r\n\r\n \r\n**5.1 1st stage evaluation**\r\n \r\n![The environment of 1st stage evaluation][6]\r\n\r\nWe measured maximum throughput of one 1st stage PC. Figure6 shows the environment of the evaluation. Because we did not have read-out modules, we used the emulators instead of those. Data format of the emulators was the same as that of the read-out modules. We installed DAQ-Components of the 1st stage to an evaluation PC. Figure5 shows the specification of the evaluation PC. We prepared the 2nd stage PC which is installed Skeltonsink. Skeltonsink received data and is used only for 1st stage evaluation.\r\nEach emulator sent test data to the evaluation PC at a maximum rate. As shown in Table 1, one event data size is 45KB per one read-out module and event rate is 2000Hz. \r\n\r\n \r\n![Evaluation result of 1st Stage PC][7]\r\n\r\nFigure7 shows the evaluation result. The points mean measured value, line means ideal value. When the number of emulators was up to 7, measured value matched ideal value. The result shows the evaluation PC can process up to 7 emulators, when read-out modules send data at a maximum rate. One evaluation PC can process around 600MB\/s of data at a maximum. During this evaluation, we observed data loss size. There was no data loss when the number of emulators was up to 7.\r\n\r\n\r\n**5.2 2nd stage evaluation**\r\n \r\n\r\n![The environment of 2nd stage evaluation][8]\r\n\r\nWe evaluated 2nd stage. Figure8 shows the environment of the evaluation. We installed DAQ-Components of the 2nd stage on the evaluation PC. Figure5 shows specification of the evaluation PC. \r\nWe assumed that 50 read-out modules transferred 45kB\/event of data and event rate was 10 Hz after passing Filter. In this case, the 2nd PC received 22MB\/s of data. We confirmed that the evaluation PC was able to process 22MB\/s of data losing no data. This result shows the evaluation PC has an enough capability.\r\n\r\n\r\n**5. Conclusion**\r\n\r\nThe DAQ system of the J-PARC E16 Experiment consists of two stages. One evaluation PC on 1st stage can process 600MB\/s of data at a maximum. Average total data transfer Rate to DAQ PCs is 330MB\/s. Therefore, 1st stage can be developed by one PC or a few PCs. 2nd stage can be developed by one PC. Therefore, we were able to confirm the DAQ system can be applied to the experiment by a few PCs.\r\n\r\n\r\n  [1]: http:\/\/research.kek.jp\/people\/ehamada\/test3.png\r\n  [2]: http:\/\/research.kek.jp\/people\/ehamada\/figure1.png\r\n  [3]: http:\/\/research.kek.jp\/people\/ehamada\/figure2.png\r\n  [4]: http:\/\/research.kek.jp\/people\/ehamada\/figure3.png\r\n  [5]: http:\/\/research.kek.jp\/people\/ehamada\/table2.png\r\n  [6]: http:\/\/research.kek.jp\/people\/ehamada\/figure5.png\r\n  [7]: http:\/\/research.kek.jp\/people\/ehamada\/figure4.png\r\n  [8]: http:\/\/research.kek.jp\/people\/ehamada\/figure6.png", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578678", "resources": [{"_type": "LocalFile", "name": "CHEP2015_hamada.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/77\/attachments\/578678\/796844\/CHEP2015_hamada.pdf", "fileName": "CHEP2015_hamada.pdf", "_fossil": "localFileMetadata", "id": "796844", "_deprecated": true}, {"_type": "LocalFile", "name": "CHEP2015_hamada.ppt", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/77\/attachments\/578678\/796843\/CHEP2015_hamada.ppt", "fileName": "CHEP2015_hamada.ppt", "_fossil": "localFileMetadata", "id": "796843", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "80b639329ca995258f43de70a074a64a", "affiliation": "High Energy Accelerator Research Organization (KEK)", "_fossil": "contributionParticipationMetadata", "fullName": "IKENO, Masahiro", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "1606e50bea6b72d12b7ee332cb84d0e0", "affiliation": "RIKEN Nishina Center", "_fossil": "contributionParticipationMetadata", "fullName": "KAWAMA, Daisuke", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "aa374f30cfce68e84306c871caa2058a", "affiliation": "High Energy Accelerator Research Organization (KEK)", "_fossil": "contributionParticipationMetadata", "fullName": "MORINO, Yuhei", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "17488d1ac2de0b46336dbfeca29abe85", "affiliation": "University of Tokyo", "_fossil": "contributionParticipationMetadata", "fullName": "NAKAI, Wataru", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "cc7c3f3a894576c3678e52e4632de850", "affiliation": "University of Tokyo", "_fossil": "contributionParticipationMetadata", "fullName": "OBARA, Yuki", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "9fef0eb8eef4e2453f1c3f7ab2638a6d", "affiliation": "High Energy Accelerator Research Organization (KEK)", "_fossil": "contributionParticipationMetadata", "fullName": "OZAWA, Kyoichiro", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "10c6782b9f4aa418aff83d40e88c2d30", "affiliation": "High Energy Accelerator Research Organization (KEK)", "_fossil": "contributionParticipationMetadata", "fullName": "SENDAI, Hiroshi", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "ad7cd8169726e930e5ac6d6ec11409ce", "affiliation": "Research Center for Nuclear Physics, Osaka University", "_fossil": "contributionParticipationMetadata", "fullName": "TAKAHASHI, Tomonori", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "460c2e7145b6be90b93a59055bd272d4", "affiliation": "High Energy Accelerator Research Organization (KEK)", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. TANAKA, Manobu", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "14fe8cb28e5f268fe0430955a2bb7980", "affiliation": "RIKEN Nishina Center", "_fossil": "contributionParticipationMetadata", "fullName": "YOKKAICHI, Satoshi", "id": "10"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/77", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:45:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "76", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2d2eeb1c20793644592372fb6876fb69", "affiliation": "Eotvos Lorand University (HU)", "_fossil": "contributionParticipationMetadata", "fullName": "SIPOS, Roland", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2d2eeb1c20793644592372fb6876fb69", "affiliation": "Eotvos Lorand University (HU)", "_fossil": "contributionParticipationMetadata", "fullName": "SIPOS, Roland", "id": "0"}], "title": "NoSQL technologies for the CMS Conditions Database", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T13:19:05.724829+00:00", "description": "", "title": "CHEP2015_-_NoSQL_CondDB.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/76\/attachments\/578679\/796845\/CHEP2015_-_NoSQL_CondDB.pdf", "filename": "CHEP2015_-_NoSQL_CondDB.pdf", "content_type": "application\/pdf", "type": "file", "id": 796845, "size": 1234346}], "title": "Slides", "default_folder": false, "id": 578679, "description": ""}], "_type": "Contribution", "description": "With the restart of the LHC in 2015, the growth of the CMS Conditions dataset will continue, therefore the need of consistent and highly available access to the Conditions makes a great cause to revisit different aspects of the current data storage solutions.\r\n\r\nWe present a study of alternative data storage backends for the Conditions Databases, by evaluating some of the most popular NoSQL databases to support a key-value representation of the CMS Conditions. An important detail about the Conditions that the payloads are stored as BLOBs, and they can reach sizes that may require special treatment (splitting) in these NoSQL databases. As big binary objects may be a bottleneck in several database systems, and also to give an accurate baseline, a testing framework extension was implemented to measure the characteristics of the handling of arbitrary binary data in these databases. Based on the evaluation, prototypes of a document store, using a column-oriented and plain key-value store, are deployed. An adaption layer to access the backends in the CMS Offline software was developed to provide transparent support for these NoSQL databases in the CMS context. Additional data modelling approaches and considerations in the software layer, deployment and automatization of the databases are also covered in the research. In this paper we present the results of the evaluation as well as a performance comparison of the prototypes studied.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578679", "resources": [{"_type": "LocalFile", "name": "CHEP2015_-_NoSQL_CondDB.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/76\/attachments\/578679\/796845\/CHEP2015_-_NoSQL_CondDB.pdf", "fileName": "CHEP2015_-_NoSQL_CondDB.pdf", "_fossil": "localFileMetadata", "id": "796845", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/76", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:15:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "75", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6f6a0a09bedfe285f5c3b56598a01274", "affiliation": "Universitat Aut\u00f2noma de Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "DELFINO REZNICEK, Manuel", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6f6a0a09bedfe285f5c3b56598a01274", "affiliation": "Universitat Aut\u00f2noma de Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "DELFINO REZNICEK, Manuel", "id": "0"}], "title": "Architectures and methodologies for future deployment of multi-site Zettabyte-Exascale data handling platforms", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T03:39:37.174384+00:00", "description": "", "title": "ZEPHYR_CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/75\/attachments\/578680\/796846\/ZEPHYR_CHEP2015.pdf", "filename": "ZEPHYR_CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796846, "size": 1662741}], "title": "Slides", "default_folder": false, "id": 578680, "description": ""}], "_type": "Contribution", "description": "Several scientific fields, including Astrophysics, Astroparticle Physics, Cosmology, Nuclear and Particle Physics, and Research with Photons, are estimating that by the 2020 decade they will require data handling systems with data volumes approaching the Zettabyte distributed amongst as many as 10<sup>18<\/sup> individually addressable data objects (Zettabyte-Exascale systems). It may be convenient or necessary to deploy such systems using multiple physical sites. This paper describes the findings of a working group composed of experts from several large European scientific data centres on architectures and methodologies that should be studied by building proof-of-concept systems, in order to prepare the way for building reliable and economic Zettabyte-Exascale systems. Key ideas emerging from the study are: the introduction of a global Storage Virtualization Layer which is logically separated from the individual storage sites; the need for maximal simplification and automation in the deployment of the physical sites; the need to present the user with an integrated view of their custom metadata and technical metadata (such as the last time an object was accessed, etc.); the need to apply modern efficient techniques to handle the large metadata volumes (e.g. Petabytes) that will be involved; and the challenges generated by the very large rate of technical metadata updates. It also addresses the challenges associated with the need to preserve scientific data for many decades. The paper is presented in the spirit of sharing the findings with both the user communities and data centre experts, in order to receive feedback and generate interest in starting prototyping work on the Zettabyte-Exascale challenges.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578680", "resources": [{"_type": "LocalFile", "name": "ZEPHYR_CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/75\/attachments\/578680\/796846\/ZEPHYR_CHEP2015.pdf", "fileName": "ZEPHYR_CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796846", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "b9484a9341914ec69fc4a3f599f16826", "affiliation": "IFAE", "_fossil": "contributionParticipationMetadata", "fullName": "ACIN PORTELLA, Vanessa", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "24417253a5e1b6f80eec44a3859fae2c", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BIRD, Ian", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "ad27265638303547200b971ea77f7b31", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "BOCCALI, Tommaso", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "a1d998cfc22eb1dad41eeef15ddd0452", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CANCIO, German", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "2463c98197fe73772aa1cd10b34131aa", "affiliation": "RAL", "_fossil": "contributionParticipationMetadata", "fullName": "COLLIER, Ian Peter", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "e6edda42169d9a93c40a588e0ca2246f", "affiliation": "RAL", "_fossil": "contributionParticipationMetadata", "fullName": "CORNEY, David", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "7656907625650bb5ce4deae7abf9d089", "affiliation": "IN2P3", "_fossil": "contributionParticipationMetadata", "fullName": "DELAUNAY, Benoit", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "d04a5fe2a2ee8472d129c31df45ec1e5", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "DELL'AGNELLO, Luca", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "cbb0d28d39b1ca6e5d447a3e19e0e3c7", "affiliation": "CIEMAT", "_fossil": "contributionParticipationMetadata", "fullName": "FLIX MOLINA, Jose", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "a198240a6b21354217ead633867b3e31", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "FUHRMANN, Patrick", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "8752fa4ff53ff75ecd6c6eb3cd406427", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "GASTHUBER, Martin", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "bf63b5305594c47600ad190114b08d07", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "GUELZOW, Volker", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "d66ca54e1e6d174854fed218fb359277", "affiliation": "KIT", "_fossil": "contributionParticipationMetadata", "fullName": "HEISS, Andreas", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "a0da53b2cbe856aeefd53a2109cd873d", "affiliation": "CNRS", "_fossil": "contributionParticipationMetadata", "fullName": "LAMANNA, Giovanni", "id": "14"}, {"_type": "ContributionParticipation", "emailHash": "4000cb16fb33e214140c55e79bbb6910", "affiliation": "IN2P3", "_fossil": "contributionParticipationMetadata", "fullName": "MACCHI, Pierre-Etienne", "id": "15"}, {"_type": "ContributionParticipation", "emailHash": "2562ffc777c0d91f02cb4acbb03b8687", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "MAGGI, Marcello", "id": "16"}, {"_type": "ContributionParticipation", "emailHash": "44b70565c91121fb6b89f22240983887", "affiliation": "RAL", "_fossil": "contributionParticipationMetadata", "fullName": "MATTHEWS, Brian", "id": "17"}, {"_type": "ContributionParticipation", "emailHash": "692ada39031f6c6d9ac4ea8878b46942", "affiliation": "IFAE", "_fossil": "contributionParticipationMetadata", "fullName": "NEISSNER, Christian", "id": "18"}, {"_type": "ContributionParticipation", "emailHash": "4e7e5f7684fa000fd77b9b541a4e6b00", "affiliation": "IN2P3", "_fossil": "contributionParticipationMetadata", "fullName": "NIEF, Jean-Yves", "id": "19"}, {"_type": "ContributionParticipation", "emailHash": "3371a544f47229acf4576b1ec09a69de", "affiliation": "CIEMAT", "_fossil": "contributionParticipationMetadata", "fullName": "PORTO FERNANDEZ, Maria Del Carmen", "id": "20"}, {"_type": "ContributionParticipation", "emailHash": "2c6c9f08aa8c020884f8c2942039677c", "affiliation": "RAL", "_fossil": "contributionParticipationMetadata", "fullName": "SANSUM, Andrew", "id": "21"}, {"_type": "ContributionParticipation", "emailHash": "68c97bd2a016fa65faad2c6e3e91bb2a", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SCHULZ, Markus", "id": "22"}, {"_type": "ContributionParticipation", "emailHash": "1491c724a51245e6b9808c5dcfba13a2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SHIERS, Jamie", "id": "23"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/75", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "74", "speakers": [{"_type": "ContributionParticipation", "emailHash": "33a2f7e17267454ac7f2b96ef380dcae", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "TELESCA, Adriana", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ffa894de573ce232de02dba98cd759ba", "affiliation": "Univ. Federal do Rio de Janeiro (BR)", "_fossil": "contributionParticipationMetadata", "fullName": "HENRIQUE MARTINS SILVA, Heron", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "800959988379fcb539785797520d9fd7", "affiliation": "Johann-Wolfgang-Goethe Univ. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "BOETTGER, Stefan", "id": "1"}], "title": "The ALICE Glance Membership Management System", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T12:57:17.255842+00:00", "description": "", "title": "ALICE_Glance_membership_poster_final.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/74\/attachments\/578681\/796847\/ALICE_Glance_membership_poster_final.pdf", "filename": "ALICE_Glance_membership_poster_final.pdf", "content_type": "application\/pdf", "type": "file", "id": 796847, "size": 4248856}], "title": "Poster", "default_folder": false, "id": 578681, "description": ""}], "_type": "Contribution", "description": "ALICE (A Large Ion Collider Experiment) is an experiment at the CERN LHC (Large Hadron Collider) studying the physics of strongly interacting matter and the quark-gluon plasma. The experiment collaboration counts more than 1500 members from 148 institutes in 39 countries. \r\n\r\nDuring the experiment start up in 2008 and the following years of data taking the information about members was manually maintaned in a static database called the ALICE Collaboration Database (ACDB). The increased size and complexity of the collaboration in terms of contracts diversity and institutions responsibility in physics and experiment operation made clear the need of using a dynamic and flexible system. \r\n\r\nIn this paper, we introduce the ALICE Glance Membership system, which is the result of a fruitful joint effort between UFRJ (Federal University of Rio de Janeiro) and the ALICE Collaboration at CERN. The Glance tecnology, developed by the UFRJ and the ATLAS experiment, sits at the basis of the system as an intermediate layer isolating the particularities of the databases. \r\n\r\nThe developed Web system manages members data and their employments contracts, activities and appointments in the collaboration. It handles institutes and funding agencies information, agreements and representatives, which is essential to give the right access to ALICE members, create mailing lists, electronic groups and automatic notifications as well as generating the list of publication authors. The ALICE Glance Membership interacts with external software such as the CERN Foundation database and the Greybook.  The system supports the decentralization of functions, allowing Institute leaders to directly manage their team and members to access their own information, enabling a dynamic collaboration network through the Web.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578681", "resources": [{"_type": "LocalFile", "name": "ALICE_Glance_membership_poster_final.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/74\/attachments\/578681\/796847\/ALICE_Glance_membership_poster_final.pdf", "fileName": "ALICE_Glance_membership_poster_final.pdf", "_fossil": "localFileMetadata", "id": "796847", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "fe96040e410a1df490d0452b81646714", "affiliation": "Univ. Federal do Rio de Janeiro (BR)", "_fossil": "contributionParticipationMetadata", "fullName": "MAIDANTCHIK, Carmen", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "33a2f7e17267454ac7f2b96ef380dcae", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "TELESCA, Adriana", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "c9ab5514fd7f1e0cbe7b5f2f330bf626", "affiliation": "Univ. Federal do Rio de Janeiro (BR)", "_fossil": "contributionParticipationMetadata", "fullName": "ABREU DA SILVA, Igor", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/74", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "73", "speakers": [{"_type": "ContributionParticipation", "emailHash": "33a2f7e17267454ac7f2b96ef380dcae", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "TELESCA, Adriana", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ffa894de573ce232de02dba98cd759ba", "affiliation": "Univ. Federal do Rio de Janeiro (BR)", "_fossil": "contributionParticipationMetadata", "fullName": "HENRIQUE MARTINS SILVA, Heron", "id": "0"}], "title": "The ALICE Glance Shift Accounting Management System", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T12:57:56.104456+00:00", "description": "", "title": "ALICE_SAMS_poster_final.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/73\/attachments\/578682\/796848\/ALICE_SAMS_poster_final.pdf", "filename": "ALICE_SAMS_poster_final.pdf", "content_type": "application\/pdf", "type": "file", "id": 796848, "size": 2831178}], "title": "Slides", "default_folder": false, "id": 578682, "description": ""}], "_type": "Contribution", "description": "ALICE (A Large Ion Collider Experiment) is an experiment at the CERN LHC (Large Hadron Collider) studying the physics of strongly interacting matter and the quark-gluon plasma. \r\n\r\nThe experiment operation requires a 24 hours per day and 7 days a week \u201cshift\u201d crew at the experimental site, composed by the ALICE collaboration members. Shift duties are calculated for each institute according to their correlated members. In order to ensure the full coverage of the experiment operation as well as its good quality, the ALICE Shift Accounting Management System (SAMS) is used to manage the shift bookings as well as the needed training. \r\n\r\nALICE SAMS is the result of a joint effort between the Federal University of Rio de Janeiro (UFRJ) and the ALICE Collaboration. The Glance technology, developed by the UFRJ and the ATLAS experiment, sits at the basis of the system as an intermediate layer isolating the particularities of the databases. \r\n\r\nIn this paper, we describe the ALICE SAMS development process and functionalities.  The database has been modelled according to the collaboration needs and is fully integrated with the ALICE Collaboration repository in order to access members information and respectively roles and activities. Run, period and training coordinators can manage their subsystem operation and ensure an efficient personnel management. Members of the ALICE collaboration can book shifts and on call according to pre-defined rights. \r\n\r\nALICE SAMS features a user\u2019s profile containing all the statistics and user contact information as well as the Institutes profile. Both the user and institute profiles are public (within the scope of the collaboration) and show the done over due credit balance in real time. A shift calendar allows the run Coordinator to plan data taking periods in terms of which subsystems shifts are enabled or disabled and on call responsibles and slots. An overview display presents the shift crew attending the control room and allows the run coordination team to confirm the presence of both regular and trainees shift personnel, necessary for credit accounting.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578682", "resources": [{"_type": "LocalFile", "name": "ALICE_SAMS_poster_final.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/73\/attachments\/578682\/796848\/ALICE_SAMS_poster_final.pdf", "fileName": "ALICE_SAMS_poster_final.pdf", "_fossil": "localFileMetadata", "id": "796848", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "fe96040e410a1df490d0452b81646714", "affiliation": "Univ. Federal do Rio de Janeiro (BR)", "_fossil": "contributionParticipationMetadata", "fullName": "MAIDANTCHIK, Carmen", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "5e036af30785d173c4ba3862e1ee4705", "affiliation": "Istituto Nazionale Fisica Nucleare (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "RONCHETTI, Federico", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "33a2f7e17267454ac7f2b96ef380dcae", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "TELESCA, Adriana", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/73", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "72", "speakers": [{"_type": "ContributionParticipation", "emailHash": "3f178268d16cd36e5e407b61dec4d320", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CASS, Tony", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "4cc204eb901b51935e8ecb47dda933cf", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "VALENTIN VINAGRERO, Francisco", "id": "0"}], "title": "Towards a 21st Century Telephone Exchange at CERN", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-01T13:35:47.273200+00:00", "description": "", "title": "chep_2015_FixedTelephonyEvolution.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/72\/attachments\/578683\/796849\/chep_2015_FixedTelephonyEvolution.pdf", "filename": "chep_2015_FixedTelephonyEvolution.pdf", "content_type": "application\/pdf", "type": "file", "id": 796849, "size": 1771064}], "title": "Poster", "default_folder": false, "id": 578683, "description": ""}], "_type": "Contribution", "description": "The advent of mobile telephony and VoIP has significantly impacted the traditional telephone exchange industry---to such an extent that private branch exchanges are likely to disappear completely in the near future. For large organisations, such as CERN, it is important to be able to smooth this transition by implementing new voice platforms that can protect past investments and the flexibility needed to securely interconnect emerging VoIP solutions and forthcoming developments such as VoLTE. We present the results of ongoing studies and tests at CERN of the latest technologies in this area.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578683", "resources": [{"_type": "LocalFile", "name": "chep_2015_FixedTelephonyEvolution.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/72\/attachments\/578683\/796849\/chep_2015_FixedTelephonyEvolution.pdf", "fileName": "chep_2015_FixedTelephonyEvolution.pdf", "_fossil": "localFileMetadata", "id": "796849", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "599fbc750b1cac5405e7672a019a8125", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHAPRON, Frederic", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "38f26177ac1b8c35d88e07c9cab7711e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SIERRA, Rodrigo", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "3f178268d16cd36e5e407b61dec4d320", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CASS, Tony", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/72", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:00:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "71", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6292827fc3b6083110854c2516de52ae", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "AGOSTA, Stefano", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6292827fc3b6083110854c2516de52ae", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "AGOSTA, Stefano", "id": "0"}], "title": "High-Speed Mobile Communications in Hostile Environments", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T05:19:05.100128+00:00", "description": "", "title": "HighSpeed_Comm_-Agosta.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/71\/attachments\/578684\/796851\/HighSpeed_Comm_-Agosta.pdf", "filename": "HighSpeed_Comm_-Agosta.pdf", "content_type": "application\/pdf", "type": "file", "id": 796851, "size": 470848}, {"_type": "attachment", "modified_dt": "2015-04-13T05:19:05.100128+00:00", "description": "", "title": "HighSpeed_Comm_-Agosta.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/71\/attachments\/578684\/796850\/HighSpeed_Comm_-Agosta.pptx", "filename": "HighSpeed_Comm_-Agosta.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796850, "size": 1857657}], "title": "Slides", "default_folder": false, "id": 578684, "description": ""}], "_type": "Contribution", "description": "With the inexorable increase in the use of mobile devices, for both general communications and mission-critical applications, wireless connectivity is required anytime and anywhere. This requirement is addressed in office buildings through the use of Wi-Fi technology but Wi-Fi is ill adapted for use in large experiment halls and complex underground environments such as the LHC tunnel and experimental caverns. CERN is instead investigating the use of 4G\/LTE technology to address issues such as radiation tolerance and distance constraints. This presentation will describe these studies, presenting results on the level of data throughput that can be achieved and discussing issues such as the provision of a consistent user experience as devices migrate between Wi-Fi and 4G\/LTE.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578684", "resources": [{"_type": "LocalFile", "name": "HighSpeed_Comm_-Agosta.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/71\/attachments\/578684\/796851\/HighSpeed_Comm_-Agosta.pdf", "fileName": "HighSpeed_Comm_-Agosta.pdf", "_fossil": "localFileMetadata", "id": "796851", "_deprecated": true}, {"_type": "LocalFile", "name": "HighSpeed_Comm_-Agosta.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/71\/attachments\/578684\/796850\/HighSpeed_Comm_-Agosta.pptx", "fileName": "HighSpeed_Comm_-Agosta.pptx", "_fossil": "localFileMetadata", "id": "796850", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "38f26177ac1b8c35d88e07c9cab7711e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SIERRA, Rodrigo", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "599fbc750b1cac5405e7672a019a8125", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHAPRON, Frederic", "id": "2"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/71", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:15:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "70", "speakers": [{"_type": "ContributionParticipation", "emailHash": "060d7d9521c75cc21082d88edbe36aae", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MOMMSEN, Remi", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "060d7d9521c75cc21082d88edbe36aae", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MOMMSEN, Remi", "id": "0"}], "title": "A New Event Builder for CMS Run II", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T23:58:37.063365+00:00", "description": "", "title": "EvB.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/70\/attachments\/578685\/796853\/EvB.pdf", "filename": "EvB.pdf", "content_type": "application\/pdf", "type": "file", "id": 796853, "size": 7749002}, {"_type": "attachment", "modified_dt": "2015-04-13T23:58:37.063365+00:00", "description": "", "title": "EvB.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/70\/attachments\/578685\/796852\/EvB.pptx", "filename": "EvB.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796852, "size": 5498749}], "title": "Slides", "default_folder": false, "id": 578685, "description": ""}], "_type": "Contribution", "description": "The data acquisition system (DAQ) of the CMS experiment at the CERN Large Hadron Collider (LHC) assembles events at a rate of 100 kHz, transporting event data at an aggregate throughput of 100 GB\/s to the high-level trigger (HLT) farm. The DAQ system has been redesigned during the LHC shutdown in 2013\/14. The new DAQ architecture is based on state-of-the-art network technologies for the event building. For the data concentration, 10\/40 Gb\/s Ethernet technologies are used together with a reduced TCP\/IP protocol implemented in FPGA for a reliable transport between custom electronics and commercial computing hardware. A 56 Gb\/s Infiniband FDR CLOS network has been chosen for the event builder with a throughput of 4 Tb\/s. This paper will discuss the software design, protocols and optimizations for exploiting the hardware capabilities. We will present performance measurements from small-scale prototypes and from the full-scale production system.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578685", "resources": [{"_type": "LocalFile", "name": "EvB.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/70\/attachments\/578685\/796853\/EvB.pdf", "fileName": "EvB.pdf", "_fossil": "localFileMetadata", "id": "796853", "_deprecated": true}, {"_type": "LocalFile", "name": "EvB.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/70\/attachments\/578685\/796852\/EvB.pptx", "fileName": "EvB.pptx", "_fossil": "localFileMetadata", "id": "796852", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "148e6617a9a734113e8ef6b1f86cb767", "affiliation": "University of Wisconsin (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BAWEJ, Tomasz Adrian", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "34b7170966da6ae240c3a7c096c08295", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "BEHRENS, Ulf", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "3a7e6bfcd4dbb638710fce76e52d2342", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BRANSON, James Gordon", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "d33e3ae247733829a0d0c9511973c44e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHAZE, Olivier", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "779d4951aab39403b13ffbc89e9f0c90", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CITTOLIN, Sergio", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "14823d25b1099daf48e1923cddbe3811", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DARLEA, Georgiana Lavinia", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "c5db4520580bcc09fe8df060c043fc7b", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DELDICQUE, Christian", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "ddaeb584ae450aae5064a948e08ead3c", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. DOBSON, Marc", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "8b7c922625aaf91ee706a1518921a755", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DUPONT, Aymeric Arnaud", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "b03db255046e47e29d52f585a312fecf", "affiliation": "Univ. of California Los Angeles (US)", "_fossil": "contributionParticipationMetadata", "fullName": "ERHAN, Samim", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "b4aa2021387f14d802bddeb2eb454884", "affiliation": "University of Kent (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "FORREST, Andrew Kevin", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "4694ee2e44416e0163f30d20408ccad0", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GIGI, Dominique", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "6fb6d2d7f55ab188239ef3702f47965a", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GLEGE, Frank", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "37e8bb7f1971a51948ad4fce29334bf7", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GOMEZ CEBALLOS RETUERTO, Guillelmo", "id": "14"}, {"_type": "ContributionParticipation", "emailHash": "0686dcacfe13161c58c9e8a9fd9d65d2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HEGEMAN, Jeroen", "id": "15"}, {"_type": "ContributionParticipation", "emailHash": "98493644fe4b7f32b27b9e10cf03a340", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "HOLZNER, Andre Georg", "id": "16"}, {"_type": "ContributionParticipation", "emailHash": "932403f3327b0e3b2903995e513382a2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MASETTI, Lorenzo", "id": "17"}, {"_type": "ContributionParticipation", "emailHash": "82e7a21a5011f52cb8606f0f0601f8d0", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MEIJERS, Frans", "id": "18"}, {"_type": "ContributionParticipation", "emailHash": "2d10c567422b53eb7fccd0134c728788", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MESCHI, Emilio", "id": "19"}, {"_type": "ContributionParticipation", "emailHash": "7e2c5e676f7ba0f9af6e617d71de2a44", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "NUNEZ BARRANCO FERNANDEZ, Carlos", "id": "20"}, {"_type": "ContributionParticipation", "emailHash": "b22d1baa246adbc92c9a6050219aeea8", "affiliation": "Fermi National Accelerator Laboratory (FNAL)", "_fossil": "contributionParticipationMetadata", "fullName": "O'DELL, Vivian", "id": "21"}, {"_type": "ContributionParticipation", "emailHash": "7cdd48f784a087c6d9b6f6003cfc03f7", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ORSINI, Luciano", "id": "22"}, {"_type": "ContributionParticipation", "emailHash": "c20242a86672d5fc162e300100556857", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "PAUS, Christoph", "id": "23"}, {"_type": "ContributionParticipation", "emailHash": "1951a3ffe6d5cf1393f669f572be4751", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "PETRUCCI, Andrea", "id": "24"}, {"_type": "ContributionParticipation", "emailHash": "88f4d725c22d118782d55d91895473a9", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "PIERI, Marco", "id": "25"}, {"_type": "ContributionParticipation", "emailHash": "dea4bd4e0305108f26ad10d80d8a923c", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. RACZ, Attila", "id": "26"}, {"_type": "ContributionParticipation", "emailHash": "e61604d9f35ae72b3531378bca0e6216", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SAKULIN, Hannes", "id": "27"}, {"_type": "ContributionParticipation", "emailHash": "01ca3cdd821be71f01a7fa78f1cd3e2c", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SCHWICK, Christoph", "id": "28"}, {"_type": "ContributionParticipation", "emailHash": "f458e4333696558cf9533ea62f29fc06", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "STIEGER, Benjamin", "id": "29"}, {"_type": "ContributionParticipation", "emailHash": "7b150004ac09fb1883e427cdc460cedc", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "SUMOROK, Konstanty", "id": "30"}, {"_type": "ContributionParticipation", "emailHash": "0b3372b1a406c07547c0658d54373e3a", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. VEVERKA, Jan", "id": "31"}, {"_type": "ContributionParticipation", "emailHash": "ab329ca1ce2b90cbb96f4c581619f0d7", "affiliation": "Staffordshire University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "WAKEFIELD, Christopher Colin", "id": "32"}, {"_type": "ContributionParticipation", "emailHash": "842f03deb43d34961e29b5c1ee95392e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ZEJDL, Petr", "id": "33"}, {"_type": "ContributionParticipation", "emailHash": "48d6acce00d681811103835f9041e34d", "affiliation": "FNAL", "_fossil": "contributionParticipationMetadata", "fullName": "ANDRE, Jean-Marc Olivier", "id": "34"}, {"_type": "ContributionParticipation", "emailHash": "5ae3a32d3ffcbc90d737c183bebe4201", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MOROVIC, Srecko", "id": "35"}, {"_type": "ContributionParticipation", "emailHash": "d3be8d0289e184c2c754180e84f1638b", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ANASTASIOS, Andronidis", "id": "36"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/70", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "79", "speakers": [{"_type": "ContributionParticipation", "emailHash": "7b533a6fae96d13b8c65b524c2f13c38", "affiliation": "Austrian Academy of Sciences (AT)", "_fossil": "contributionParticipationMetadata", "fullName": "MATSUSHITA, Takashi", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "7b533a6fae96d13b8c65b524c2f13c38", "affiliation": "Austrian Academy of Sciences (AT)", "_fossil": "contributionParticipationMetadata", "fullName": "MATSUSHITA, Takashi", "id": "0"}], "title": "Software for implementing trigger algorithms on the upgraded CMS Global Trigger System", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T06:37:55.188607+00:00", "description": "", "title": "CHEP2015-ID79.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/79\/attachments\/578686\/796854\/chep2015.v4.pdf", "filename": "chep2015.v4.pdf", "content_type": "application\/pdf", "type": "file", "id": 796854, "size": 739087}], "title": "Slides", "default_folder": false, "id": 578686, "description": ""}], "_type": "Contribution", "description": "The Global Trigger is the final step of the CMS level-1 trigger and implements a trigger menu, a set of selection requirements applied to the final list of objects from calorimeter and muon triggers to meet the physics objectives. The conditions for trigger object selection, with possible topological requirements on multi-object triggers, are combined by simple combinatorial logic (AND-OR-NOT) to form the algorithms. The most basic algorithms consist of applying $E_T$ or $p_T$ threshold to single objects. The present Global Trigger is comprised of several VME modules with FPGAs.\r\n\r\nWhen the LHC resumes its operation in 2015, the collision-energy will be increased from 8 TeV to 13 TeV, with the luminosity expected to go up from 0.75 x 10$^{34}$ cm$^{\u22122}$s$^{\u22121}$ to 2 x 10$^{34}$ cm$^{\u22122}$s$^{\u22121}$. These operating environments will provide new challenges for the CMS trigger system. The CMS level-1 trigger system will be upgraded to improve its performance for selecting interesting physics events and to operate within the predefined data-acquisition rate.\r\n\r\nTogether with the upgrade of other level-1 trigger systems, the Global Trigger will be re-implemented on modern FPGAs on an Advanced Mezzanine Card in MicroTCA crate. The upgraded system will benefit from the ability to process complex algorithms with DSP slices and increased processing resources with optical links running at 10 Gbit\/s, enabling more algorithms at a time than previously possible and allowing CMS to be more flexible in how it handles the trigger bandwidth. CMS also will be able to match different objects, e.g. muons with jets, with higher resolution and efficiency and be able to calculate more sophisticated quantities such as the mass of a pair of objects. In 2015, CMS plans to keep the present triggers running and commission the new ones simultaneously. Detailed comparisons will be performed between both to test everything from technical implementation to whether the new triggers perform better, before the older system will be turned off.\r\n\r\nA software for handling trigger menu implementation on the present system is strongly coupled with the current hardware design. In order to handle the increased complexity of the trigger menu implemented on the upgraded Global Trigger, a set of new software has been developed. The software allows a physicist to define a menu with analysis-like triggers using intuitive user interface. The menu is then realised on FPGAs with further software processing, instantiating predefined firmware blocks. The menu plays a central role in trigger selection and is shared by the second level trigger, known as the High Level Trigger as well as a trigger emulating software in an offline software environment. The menu information is stored in either an XML file or in a database for sharing information with other systems.\r\n\r\nThe design and implementation of the software for preparing a menu for the upgraded CMS Global Trigger system will be presented.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578686", "resources": [{"_type": "LocalFile", "name": "CHEP2015-ID79.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/79\/attachments\/578686\/796854\/chep2015.v4.pdf", "fileName": "chep2015.v4.pdf", "_fossil": "localFileMetadata", "id": "796854", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/79", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "78", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2f4278bcf434f4395252c2c11b4a2dc4", "affiliation": "Texas A&M University at Qatar", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. ALI, Sheharyar", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2f4278bcf434f4395252c2c11b4a2dc4", "affiliation": "Texas A&M University at Qatar", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. ALI, Sheharyar", "id": "0"}], "title": "Parallelization and Computational Steering of the Electron Avalanche Simulations", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "Traditionally, the workflow of the simulation of the electron avalanches in a particle detector is to prepare input, execute the simulation, and then visualize the results as a post-processing step. Usually, these simulations are long running and computationally intensive. It is not unusual for a simulation to keep running for several days or even weeks. If the experiment leads to the conclusion that there is incorrect logic in the application, or input parameters were wrong, then simulation has to be restarted with correct parameters. Furthermore, these simulations are commonly run on the high performance supercomputer which is typically a shared resource among several other researchers. The supercomputer maintains a queue of researchers programs and executes them as time and priorities permit. If the simulation produces incorrect results and there is a need to restart it with different parameters, it may not be possible to restart it immediately. The simulation jobs have to wait in the queue until they are given a chance to execute again. It prolongs the scientific discovery process and hence reduces the researchers' productivity. In this presentation, we will describe the work done to address these problems with the help of parallelization and computational steering of the simulations. With the computational steering, the progress of the simulations can be monitored whenever the user wishes while it is running. In addition, the simulation can be restarted immediately on the fly with different parameters without requiring to resubmit the job on the supercomputer.", "track": "Track2: Offline software ", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "346c752fbd524f6d501b8a6d5c53e4d3", "affiliation": "Texas A&M University at Qatar", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. OTHMANE, Bouhali", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/78", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:15:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "546", "speakers": [{"_type": "ContributionParticipation", "emailHash": "068aa17b3f190fa9a201b9412fbc26f5", "affiliation": "Universita e INFN Genova (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "SCHIAVI, Carlo", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "068aa17b3f190fa9a201b9412fbc26f5", "affiliation": "Universita e INFN Genova (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "SCHIAVI, Carlo", "id": "0"}], "title": "ATLAS High-Level Trigger algorithms for Run-2 data-taking", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T02:47:19.093685+00:00", "description": "", "title": "CarloSchiavi_ATL-DAQ-SLIDE-2015-170.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/546\/attachments\/578687\/796855\/CarloSchiavi_ATL-DAQ-SLIDE-2015-170.pdf", "filename": "CarloSchiavi_ATL-DAQ-SLIDE-2015-170.pdf", "content_type": "application\/pdf", "type": "file", "id": 796855, "size": 1704335}], "title": "Slides", "default_folder": false, "id": 578687, "description": ""}], "_type": "Contribution", "description": "Following the successful Run-1 LHC data-taking, the long shutdown gave the opportunity for significant improvements in the ATLAS trigger capabilities, as a result of the introduction of new or improved Level-1 trigger hardware and significant restructuring of the DAQ infrastructure. To make use of these new capabilities, the High-Level trigger (HLT) software has been to a large extent rewritten, introducing in its turn a plethora of new features and improved algorithms for object reconstruction. The HLT algorithms rely heavily on the offline reconstruction algorithms with the aim to have an even larger efficiency for accepted events than in Run-1.\r\n\r\nA summary of the HLT algorithms for object reconstruction will be given with the focus on new and improved algorithms together with an outline of the strategy to combine them into a trigger menu. In addition, we will show examples of impressive code speedups and the expected trigger performance for Run-2 data-taking.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578687", "resources": [{"_type": "LocalFile", "name": "CarloSchiavi_ATL-DAQ-SLIDE-2015-170.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/546\/attachments\/578687\/796855\/CarloSchiavi_ATL-DAQ-SLIDE-2015-170.pdf", "fileName": "CarloSchiavi_ATL-DAQ-SLIDE-2015-170.pdf", "_fossil": "localFileMetadata", "id": "796855", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/546", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": null, "keywords": [], "id": "547", "speakers": [{"_type": "ContributionParticipation", "emailHash": "d41d8cd98f00b204e9800998ecf8427e", "affiliation": "OIST", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. DORFAN, Jonathan", "id": "0"}], "primaryauthors": [], "title": "Welcome Address", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [], "_type": "Contribution", "description": "", "track": null, "material": [], "coauthors": [], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/contribution\/547", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:15:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:30:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "545", "speakers": [{"_type": "ContributionParticipation", "emailHash": "851453156348a2291fe16147f9a3f493", "affiliation": "Universita e INFN, Bologna (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "PERROTTA, Andrea", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "851453156348a2291fe16147f9a3f493", "affiliation": "Universita e INFN, Bologna (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "PERROTTA, Andrea", "id": "1"}], "title": "Performance of the CMS High Level Trigger", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T06:28:43.810798+00:00", "description": "", "title": "HLT_Perf_AP-CHEP2015-upd.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/545\/attachments\/578688\/796856\/HLT_Perf_AP-CHEP2015-upd.pdf", "filename": "HLT_Perf_AP-CHEP2015-upd.pdf", "content_type": "application\/pdf", "type": "file", "id": 796856, "size": 4185240}], "title": "Slides", "default_folder": false, "id": 578688, "description": ""}], "_type": "Contribution", "description": "The CMS experiment has been designed with a 2-level trigger system. The first level is implemented using custom-designed electronics. The second level is the so-called High Level Trigger (HLT), a streamlined version of the CMS offline reconstruction software running on a computer farm. For Run II of the Large Hadron Collider, the increases in center-of-mass energy and luminosity will raise the event rate to a level challenging for the HLT algorithms. The increase in the number of interactions per bunch crossing: on average 25 in 2012, and expected to be around 40 in Run II will be an additional complication. We will present the performance of the main triggers used during the 2012 run and will also cover new approaches that have been developed since then to cope with the challenges of the new run. This includes improvements in HLT electron and photon reconstruction as well as better performing muon triggers. We will also present the performance of the improved tracking and \r\nvertexing algorithms, discussing their impact on the b-tagging performance as well as on the jet and missing energy reconstruction.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578688", "resources": [{"_type": "LocalFile", "name": "HLT_Perf_AP-CHEP2015-upd.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/545\/attachments\/578688\/796856\/HLT_Perf_AP-CHEP2015-upd.pdf", "fileName": "HLT_Perf_AP-CHEP2015-upd.pdf", "_fossil": "localFileMetadata", "id": "796856", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/545", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "8", "speakers": [{"_type": "ContributionParticipation", "emailHash": "fc1a7e97cb82f6dbc4dde837cfeb19ec", "affiliation": "Kent State University, USA", "_fossil": "contributionParticipationMetadata", "fullName": "SHANMUGANATHAN, Prashanth", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "fc1a7e97cb82f6dbc4dde837cfeb19ec", "affiliation": "Kent State University, USA", "_fossil": "contributionParticipationMetadata", "fullName": "SHANMUGANATHAN, Prashanth", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "60e98fb28af2edf9a8a6c39567ee11c7", "affiliation": "BROOKHAVEN NATIONAL LABORATORY", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LAURET, Jerome", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c540a9e507601f8b1fb08317986ea69e", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "ARKHIPKIN, Dmitry", "id": "2"}], "title": "Modular and scalable RESTful API to sustain STAR collaboration\u2019s record keeping", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "STAR collaboration\u2019s record system is a collection of heterogeneous and sparse information associated to each members and institutions. In its original incarnation, only flat information were stored revealing many restrictions such as the lack of historical change information, the inability to keep track of members leaving and re-joining or the ability to easily extend the saved information as new requirements appear.  \r\n\r\nIn mid-2013, a new project was launched covering for an extensive set of revisited requirements. The requirements led us to a design based on a RESTful API, back-end storage engine relying on key\/value pair data representation model coupled with a tiered architecture design. This design was motivated by the fact that unifying many STAR tools, relying on the same business logic and storage engine, was a key and central feature for the maintainability of records. This central service API would leave no ambiguities and provide easy service integration between STAR tools. \r\n\r\nThe new design stores the changes in records dynamically and allows tracking the changes chronologically. The storage engine is extensible as new field of information emerges (member specific or general) without affecting the presentation or the business logic layers. The new record system features a convenient administrative interface, fuzzy algorithms for data entry and search and provides basic statistics and graphs. Finally, this modular approach is supplemented with access control, allowing sensitive information and administrative operations away from public users. \r\n\r\nIn this contribution, we will review the requirements, present our design and its benefits as well as illustrate the power of the approach using practical examples for record keeping in a large collaboration like STAR.", "track": "Track6: Facilities, Infrastructure, Network", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/8", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "02:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "02:15:00"}, "duration": 15, "session": "Plenary", "keywords": [], "id": "548", "speakers": [{"_type": "ContributionParticipation", "emailHash": "d41d8cd98f00b204e9800998ecf8427e", "affiliation": "OIST", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PEACH, Ken", "id": "1"}], "primaryauthors": [], "title": "Welcome Address", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [], "_type": "Contribution", "description": "", "track": null, "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/548", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "02:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "03:00:00"}, "duration": 45, "session": "Plenary", "keywords": [], "id": "549", "speakers": [{"_type": "ContributionParticipation", "emailHash": "d41d8cd98f00b204e9800998ecf8427e", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. YAMAUCHI, Masanori", "id": "0"}], "primaryauthors": [], "title": "Future plan of KEK", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T23:08:56.865472+00:00", "description": "", "title": "CHEP-Yamauchi.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/549\/attachments\/578689\/796857\/CHEP-Yamauchi.pptx", "filename": "CHEP-Yamauchi.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796857, "size": 23799571}], "title": "Slides", "default_folder": false, "id": 578689, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578689", "resources": [{"_type": "LocalFile", "name": "CHEP-Yamauchi.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/549\/attachments\/578689\/796857\/CHEP-Yamauchi.pptx", "fileName": "CHEP-Yamauchi.pptx", "_fossil": "localFileMetadata", "id": "796857", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/549", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:15:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "120", "speakers": [{"_type": "ContributionParticipation", "emailHash": "64570537e9fce31fc2add6947b9869d1", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. JONES, Christopher", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "64570537e9fce31fc2add6947b9869d1", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. JONES, Christopher", "id": "0"}], "title": "Using the CMS Threaded Framework In A Production Environment", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T05:04:01.228724+00:00", "description": "", "title": "Threading_Production_CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/120\/attachments\/578690\/796858\/Threading_Production_CHEP2015.pdf", "filename": "Threading_Production_CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796858, "size": 952554}], "title": "Slides", "default_folder": false, "id": 578690, "description": ""}], "_type": "Contribution", "description": "During 2014, the CMS Offline and Computing Organization completed the necessary changes to use the CMS threaded framework in the full production environment. Running reconstruction workflows using the multi-threaded framework is a crucial element of CMS' 2015 and beyond production plan. We will briefly discuss the design of the CMS Threaded Framework, in particular how the design affects scaling performance. We will then cover the effort involved in getting both the CMSSW application software and the workflow management system ready for using multiple threads for production. Finally, we will present metrics on the performance of the application and workflow system as well as the difficulties which were uncovered. We will end with CMS' plans for using the threaded framework to do production for LHC Run 2.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578690", "resources": [{"_type": "LocalFile", "name": "Threading_Production_CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/120\/attachments\/578690\/796858\/Threading_Production_CHEP2015.pdf", "fileName": "Threading_Production_CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796858", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/120", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:30:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "121", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ecef85c95b0dddfc15be379651cd7344", "affiliation": "GSI DARMSTADT", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LINEV, Sergey", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ecef85c95b0dddfc15be379651cd7344", "affiliation": "GSI DARMSTADT", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LINEV, Sergey", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "70ece5ac68e53e7a3444076cb1e3a6fb", "affiliation": "GSI", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ADAMCZEWSKI-MUSCH, Joern", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "eed46903953b0bb844ea01fcbf69d311", "affiliation": "GSI", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KURZ, Nikolaus", "id": "2"}], "title": "Developments and applications of DAQ framework DABC v2", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-02T12:36:45.305074+00:00", "description": "", "title": "dabc_daq.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/121\/attachments\/578691\/796860\/dabc_daq.pdf", "filename": "dabc_daq.pdf", "content_type": "application\/pdf", "type": "file", "id": 796860, "size": 1174575}, {"_type": "attachment", "modified_dt": "2015-04-02T12:36:45.305074+00:00", "description": "", "title": "dabc_daq.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/121\/attachments\/578691\/796859\/dabc_daq.pptx", "filename": "dabc_daq.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796859, "size": 1257127}], "title": "Slides", "default_folder": false, "id": 578691, "description": ""}], "_type": "Contribution", "description": "The *Data Acquisition Backbone Core* (*DABC*) is a C++ software framework that can implement and run various data acquisition solutions on Linux platforms. In 2013 version 2 of *DABC* has been released with several improvements. These developments have taken into account lots of practical experiences of *DABC v1* with detector test beams and laboratory set-ups since first release in 2009. The plug-in interfaces for user code implementation, and configuration procedures have been simplified. Internally the framework has been enhanced by means of smart references and object cleanup mechanisms. Moreover, for monitoring and control a http web server, or a proprietary command channel access have been provided.\r\n\r\nIn May and August 2014, *DABC v2* was applied for production data taking of the *HADES* collaboration's pion beam time at *GSI*. It replaced the functionality of the established *HADES* event builder software *hadaq*, receiving frontend data via multiple Gigabit UDP streams, combining them, and storing them to RFIO tape and local disks. *DABC* was transparently integrated to the existing *HADES* DAQ environment, and the EPICS based control system. Additionally, the *HADES* quality monitoring process was fed online with data samples by a *DABC* \"streamserver\" socket instead of reading intermediate disk files, improving reaction time and stability. A maximum of 16 eventbuider processes on 4 Linux server machines were processing data of 32 subevent inputs. Each of the eventbuilder processes could be configured to run either dabc or hadaq software. In the first beamtime block just one single quality monitoring node was replaced by dabc; finally in August eventbuilding was completely handled by *DABC*.\r\n\r\n*GSI* standard DAQ systems often use PCIe boards *PEXOR*\/KINPEX that receive data via optical *gosip* protocol from various front-end hardware. For *DABC v2* corresponding plug-ins and drivers have been developed to handle reading out such systems on Linux platform. Functionalities of the previous DAQ solution with the established multi-platform framework *MBS* have been re-implemented with *DABC v2* and were compared with the same hardware against *MBS*.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578691", "resources": [{"_type": "LocalFile", "name": "dabc_daq.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/121\/attachments\/578691\/796860\/dabc_daq.pdf", "fileName": "dabc_daq.pdf", "_fossil": "localFileMetadata", "id": "796860", "_deprecated": true}, {"_type": "LocalFile", "name": "dabc_daq.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/121\/attachments\/578691\/796859\/dabc_daq.pptx", "fileName": "dabc_daq.pptx", "_fossil": "localFileMetadata", "id": "796859", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/121", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "122", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ab0e4b6abdb20488fe879b32b0460192", "affiliation": "University of Nebraska (US)", "_fossil": "contributionParticipationMetadata", "fullName": "ZVADA, Marian", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "587a395d4de88dcb8ecd493ba9926b45", "affiliation": "University of Nebraska (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BLOOM, Kenneth", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "ad27265638303547200b971ea77f7b31", "affiliation": "Universita di Pisa & INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "BOCCALI, Tommaso", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "e82c46a3a0d49dd555c888d113eace3a", "affiliation": "University of Nebraska (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BOCKELMAN, Brian Paul", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "0adea99967f91a47379750f6d80c3f33", "affiliation": "University of Wisconsin (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BRADLEY, Daniel Charles", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "4ab8343bbd6f01daf7807099c821c054", "affiliation": "University of Wisconsin (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. DASU, Sridhara", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "306a9d523d320766347c3ee70576d0ab", "affiliation": "Universita e INFN, Padova (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "FANZAGO, Federica", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "8984f616935b79e3c3c569c457c49a5f", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "SFILIGOI, Igor", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "1905f721940f9ff8a9b0b9b1e9f7eef9", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "TADEL, Matevz", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "062d1eaeaadf683ccde1fb8cb2daf5e0", "affiliation": "University of Wisconsin (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. VUOSALO, Carl", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "404ffd25f1af8b96d12533b8dcfc88dd", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "WUERTHWEIN, Frank", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "ab0e4b6abdb20488fe879b32b0460192", "affiliation": "University of Nebraska (US)", "_fossil": "contributionParticipationMetadata", "fullName": "ZVADA, Marian", "id": "1"}], "title": "CMS Experience with a World-Wide Data Federation", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-08T20:26:18.750725+00:00", "description": "", "title": "aaa-poster-chep2015-marian-ready.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/122\/attachments\/578692\/796861\/aaa-poster-chep2015-marian-ready.pdf", "filename": "aaa-poster-chep2015-marian-ready.pdf", "content_type": "application\/pdf", "type": "file", "id": 796861, "size": 8486192}], "title": "Poster", "default_folder": false, "id": 578692, "description": ""}], "_type": "Contribution", "description": "Over the past three years, the CMS Collaboration has developed the \u201cAny Data, Anytime, Anywhere\u201d technology to make use of a global data federation that is based on the XrootD protocol. The federation is now deployed across virtually all Tier-1 and Tier-2 sites in the CMS distributed computing system. This data federation gives workflows greater flexibility for location of execution, which has benefits at all scales of operation, from individual users accessing specific collision event records to organized production processing. In preparation for the coming LHC data run, CMS has been testing a number of applications of the data federation at increasingly large scales. In this presentation, we will discuss the applications in use and the results of the tests.", "track": "Track3: Data store and access ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578692", "resources": [{"_type": "LocalFile", "name": "aaa-poster-chep2015-marian-ready.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/122\/attachments\/578692\/796861\/aaa-poster-chep2015-marian-ready.pdf", "fileName": "aaa-poster-chep2015-marian-ready.pdf", "_fossil": "localFileMetadata", "id": "796861", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/122", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:45:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "123", "speakers": [{"_type": "ContributionParticipation", "emailHash": "068b9018b69ea40467e02d92d6a35844", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "HUFNAGEL, Dirk", "id": "7"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "068b9018b69ea40467e02d92d6a35844", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "HUFNAGEL, Dirk", "id": "7"}], "title": "Enabling opportunistic resources for CMS Computing Operations", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T21:37:45.208562+00:00", "description": "", "title": "dirk.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/123\/attachments\/578693\/796862\/dirk.pdf", "filename": "dirk.pdf", "content_type": "application\/pdf", "type": "file", "id": 796862, "size": 176822}], "title": "Slides", "default_folder": false, "id": 578693, "description": ""}], "_type": "Contribution", "description": "With the increased pressure on computing brought by the higher energy and luminosity from the LHC in Run 2, CMS Computing Operations expects to require the ability to utilize \u201copportunistic\u201d resources \u2014 resources not owned by, or a priori configured for CMS \u2014 to meet peak demands. In addition to our dedicated resources we look to add computing resources from non CMS grids, cloud resources, and national supercomputing centers. CMS uses the HTCondor\/glideinWMS job submission infrastructure for all its batch processing, so such resources will need to be transparently integrated into its glideinWMS pool. Bosco and parrot wrappers are used to enable access and bring the CMS environment into these non CMS resources. Here we describe our strategy to supplement our native capabilities with opportunistic resources and our experience so far using them.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578693", "resources": [{"_type": "LocalFile", "name": "dirk.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/123\/attachments\/578693\/796862\/dirk.pdf", "fileName": "dirk.pdf", "_fossil": "localFileMetadata", "id": "796862", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "c64735b9540bc92c4d230a2ca252b7cf", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GUTSCHE, Oliver", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "8ce8233ffe359f39d3fdd125371fe45b", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MASON, David Alexander", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "f3deb9637a78bebf20c5c69407f5b777", "affiliation": "University of Chicago (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MAMBELLI, Marco", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "1ab30f4066c48fc202816d553d46d1fe", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "TIRADANI, Anthony", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "579410e79e02e069dc197d8a22a258ae", "affiliation": "Fermi National Accelerator Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "MHASHILKAR, Parag", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "35f4f869ebf48e550006a86e20965547", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LARSON, Krista", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "21bc9e746b05c457c2f1ca200b016f7b", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HOLZMAN, Burt", "id": "6"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/123", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "124", "speakers": [{"_type": "ContributionParticipation", "emailHash": "3c0927d8d883c0c7337a19519dcd5670", "affiliation": "University of Notre Dame (US)", "_fossil": "contributionParticipationMetadata", "fullName": "WOODARD, Anna Elizabeth", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "3ef11477becf35256032b913ffa168f1", "affiliation": "University of Notre Dame (US)", "_fossil": "contributionParticipationMetadata", "fullName": "WOLF, Matthias", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "3c0927d8d883c0c7337a19519dcd5670", "affiliation": "University of Notre Dame (US)", "_fossil": "contributionParticipationMetadata", "fullName": "WOODARD, Anna Elizabeth", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "3ef11477becf35256032b913ffa168f1", "affiliation": "University of Notre Dame (US)", "_fossil": "contributionParticipationMetadata", "fullName": "WOLF, Matthias", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "5087a3b67988ed285dbd7b6a6f567313", "affiliation": "University of Notre Dame (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MUELLER, Charles Nicholas", "id": "2"}], "title": "Exploiting Volatile Opportunistic Computing Resources as a CMS User", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T01:15:07.220984+00:00", "description": "", "title": "awoodard_CHEP2015_lobster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/124\/attachments\/578694\/796863\/awoodard_CHEP2015_lobster.pdf", "filename": "awoodard_CHEP2015_lobster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796863, "size": 1573364}], "title": "Poster", "default_folder": false, "id": 578694, "description": ""}], "_type": "Contribution", "description": "Individual scientists in high energy physics experiments like the Compact Muon Solenoid (CMS) experiment at the Large Hadron Collider require extensive use of computing resources for analysis of massive data sets. The majority of this analysis work is done at dedicated grid-enabled CMS computing facilities. University campuses offer considerable additional computing resources, but these are not specifically configured to run CMS software. Furthermore, in many cases, the machines are available for general usage whenever they are idle, but opportunistic jobs can be terminated at any time, leading to a highly volatile computing environment.\r\n\r\nAs a joint effort involving computer scientists and CMS physicists at Notre Dame, we have developed an opportunistic workflow management tool, Lobster, to harvest available cycles from university campus computing pools. The Lobster framework consists of a management server, file server, and workers submitted to any available computing resource. Only standard user permissions are required to run the entire suite of processes, making it possible to use this tool with any resource on which the user has permission to run. Lobster makes use of the Work Queue system to perform task management, while the CMS specific software environment is provided via CVMFS and Parrot. Data is handled via Chirp and Hadoop for local data storage and XrootD for access to the CMS wide-area data federation. An extensive set of monitoring and diagnostic tools have been developed to facilitate system optimisation. The tool has been tested in a variety of environments, including within OSG Connect. We have tested it at large-scales using the 20,000-core cluster at Notre Dame, achieving approximately 8000 tasks running simultaneously, sustaining approximately 9 Gbit\/s of input data and 340 Mbit\/s of output data.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578694", "resources": [{"_type": "LocalFile", "name": "awoodard_CHEP2015_lobster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/124\/attachments\/578694\/796863\/awoodard_CHEP2015_lobster.pdf", "fileName": "awoodard_CHEP2015_lobster.pdf", "_fossil": "localFileMetadata", "id": "796863", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "ce4c7c690190f8beec9baf3a934b1f9e", "affiliation": "University of Notre Dame (US)", "_fossil": "contributionParticipationMetadata", "fullName": "TOVAR, Ben", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "5ce28a0313fbdef6cd4104c2b8b7bce7", "affiliation": "University of Notre Dame (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DONNELLY, Patrick", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "5a9bbd3a5000cf5136c941de547283e8", "affiliation": "University of Notre Dame (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BRENNER, Paul", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "5a50d093b4aaea68ae8c28872d092ff9", "affiliation": "University of Notre Dame (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LANNON, Kevin Patrick", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "2dcd8c5c21bc7166cc6d20d6c319a77b", "affiliation": "University of Notre Dame (US)", "_fossil": "contributionParticipationMetadata", "fullName": "HILDRETH, Mike", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "9e3e6500e854d82a8181a2589a2a9ce2", "affiliation": "University of Notre Dame", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. THAIN, Douglas", "id": "8"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/124", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "125", "speakers": [{"_type": "ContributionParticipation", "emailHash": "d77ba3ef219905889829b4c308d0ce82", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "KASIOUMIS, Nikos", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2c27a8e16567a817fe4b23711b4e333c", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. LE MEUR, Jean-Yves", "id": "0"}], "title": "Multimedia Content in the CERN Document Server", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T15:43:00.888694+00:00", "description": "", "title": "Multimedia_Content_in_the_CERN_Document_Server.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/125\/attachments\/578695\/796864\/Multimedia_Content_in_the_CERN_Document_Server.pdf", "filename": "Multimedia_Content_in_the_CERN_Document_Server.pdf", "content_type": "application\/pdf", "type": "file", "id": 796864, "size": 7134176}], "title": "Poster", "default_folder": false, "id": 578695, "description": ""}], "_type": "Contribution", "description": "The talk will focus on the recent developments done by the Multimedia team of the Digital Library Services to better acquire grand-public content captured at CERN and disseminate it to the general public. \r\n\r\nIn collaboration with the CERN communication unit and the Photo & Video Labs, the team has built new facilities to transfer, disseminate and archive multimedia content on the CERN Document Server Invenio-based platform. These original user-oriented features will be introduced for both the owners of multimedia material and the people willing to search for content. \r\n\r\nExplanations on how new technologies have been used behind the scene will follow, together with the details on how these developments have been carried out within the Invenio Open Source Digital Library community. The evolution of the front-end into a modern multimedia service will be explained as well as some on-going research work on automated face & object recognition, with direct image tagging functionalities. The re-architecture of the back engine to address the long-term preservation needs and the advantages of a new rich paradigm supporting images embedded into albums will be described.\r\n\r\nFinally, it will be shown the impressive peaks of use observed during major events and the recent increase in the needs for new collections. The example of the acquisition of 130'000 CERN diapos from the years 1960th and 1970th will give to all attendees the opportunity to discover not only the technical aspects of the process but also an overview of this unique content.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578695", "resources": [{"_type": "LocalFile", "name": "Multimedia_Content_in_the_CERN_Document_Server.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/125\/attachments\/578695\/796864\/Multimedia_Content_in_the_CERN_Document_Server.pdf", "fileName": "Multimedia_Content_in_the_CERN_Document_Server.pdf", "_fossil": "localFileMetadata", "id": "796864", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "958732ce04f9114da67768297a566253", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MARIAN, Ludmila", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "b83323124aca3478cacdebacc3ed03e6", "affiliation": "National Technical Univ. of Athens (GR)", "_fossil": "contributionParticipationMetadata", "fullName": "TZOVANAKIS, Charalampos", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/125", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:45:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "126", "speakers": [{"_type": "ContributionParticipation", "emailHash": "b3766e1bb47ff6f9cdf65d395d9c0ad8", "affiliation": "Lawrence Livermore Nat. Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LANGE, David", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "a7515791a515182e25805d5455cecec2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. IVANTCHENKO, Vladimir", "id": "0"}], "title": "CMS Full Simulation for Run-II", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T08:26:57.582573+00:00", "description": "", "title": "FullSimCMS1.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/126\/attachments\/578696\/796865\/FullSimCMS1.pdf", "filename": "FullSimCMS1.pdf", "content_type": "application\/pdf", "type": "file", "id": 796865, "size": 1138599}, {"_type": "attachment", "modified_dt": "2015-04-13T08:26:57.582573+00:00", "description": "", "title": "FullSimCMS1.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/126\/attachments\/578696\/796866\/FullSimCMS1.pptx", "filename": "FullSimCMS1.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796866, "size": 1196190}], "title": "Slides", "default_folder": false, "id": 578696, "description": ""}], "_type": "Contribution", "description": "This presentation will discuss new features of the CMS simulation for Run 2, where we have made considerable improvements during LHC shutdown to deal with the increased event complexity and rate for Run 2. For physics improvements migration from Geant4 9.4p03 to Geant4 10.0p02 has been performed. CPU performance was improved by introduction of the Russian roulette method inside CMS calorimeters, optimization of CMS simulation sub-libraries, and usage of statics build of the simulation executable. As a result of these efforts, CMS simulation was speeded up by about factor two. In this work we provide description of these updates and discuss different software components of CMS simulation. \r\n\r\nGeant4 version 10.0 is multi-threaded capable. This allows development of multi-threaded version of CMS simulation in parallel with mainstream sequential production version. For CMS multi-threaded Geant4 additional modules and manager classes were added. Geometry, magnetic field, physics, user actions, and sensitive detectors classes are the same for both sequential and multi-threaded versions of CMS simulation. In this work we report on details of implementation of CMS multi-threaded simulation including CPU and memory performance.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578696", "resources": [{"_type": "LocalFile", "name": "FullSimCMS1.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/126\/attachments\/578696\/796865\/FullSimCMS1.pdf", "fileName": "FullSimCMS1.pdf", "_fossil": "localFileMetadata", "id": "796865", "_deprecated": true}, {"_type": "LocalFile", "name": "FullSimCMS1.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/126\/attachments\/578696\/796866\/FullSimCMS1.pptx", "fileName": "FullSimCMS1.pptx", "_fossil": "localFileMetadata", "id": "796866", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "2dcd8c5c21bc7166cc6d20d6c319a77b", "affiliation": "University of Notre Dame (US)", "_fossil": "contributionParticipationMetadata", "fullName": "HILDRETH, Mike", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "b3766e1bb47ff6f9cdf65d395d9c0ad8", "affiliation": "Lawrence Livermore Nat. Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LANGE, David", "id": "2"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/126", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "127", "speakers": [{"_type": "ContributionParticipation", "emailHash": "8d0f43c4c4bbd707fe1a77354d2d250d", "affiliation": "Universidad de Oviedo (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GONZALEZ CABALLERO, Isidro", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "8d0f43c4c4bbd707fe1a77354d2d250d", "affiliation": "Universidad de Oviedo (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GONZALEZ CABALLERO, Isidro", "id": "0"}], "title": "PROOF Analysis Framework (PAF)", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T07:26:58.770210+00:00", "description": "", "title": "PROOF Analysis Framework.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/127\/attachments\/578697\/796867\/PAFPoster-6.pdf", "filename": "PAFPoster-6.pdf", "content_type": "application\/pdf", "type": "file", "id": 796867, "size": 713484}], "title": "Poster", "default_folder": false, "id": 578697, "description": ""}], "_type": "Contribution", "description": "The PROOF Analysis Framework (PAF) has been designed to improve the ability of the physicist to develop software for the final stages of an analysis where typically simple ROOT Trees are used and where the amount of data used is in the order of several terabytes. It hides the technicalities of dealing with PROOF leaving the scientist to concentrate on the analysis. PAF is capable of using available non specific resources on, for example, local batch systems, remote grid sites or clouds through the integration of other toolkit like PROOF Cluster or PoD. While it has been successfully used on LHC Run-1 data for some key analysis, including the H->WW dilepton channel, the higher instantaneous and integrated luminosity together with the increase of the center-of-mass energy foreseen for the LHC Run-2, which will increment the total size of the samples by a factor 6 to 20, will demand PAF to improve its scalability and to reduce the latencies as much as possible. In this paper we address the possible problems of processing such big data volumes with PAF and the solutions implemented to overcome them. We will also show the improvements in order to make PAF more modular and accessible to other communities.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578697", "resources": [{"_type": "LocalFile", "name": "PROOF Analysis Framework.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/127\/attachments\/578697\/796867\/PAFPoster-6.pdf", "fileName": "PAFPoster-6.pdf", "_fossil": "localFileMetadata", "id": "796867", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "3347c811fc9c40e87f66dd13b8fbfce1", "affiliation": "Universidad de Cantabria (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "RODRIGUEZ MARRERO, Ana", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "f3c6499490d2b925bdf55a1e83061c5c", "affiliation": "IFCA", "_fossil": "contributionParticipationMetadata", "fullName": "FERN\u00c1NDEZ, Enol", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "56b2dbf3e4ba2a1ee10e557f98d7c744", "affiliation": "Universidad de Oviedo", "_fossil": "contributionParticipationMetadata", "fullName": "DELGADO FERN\u00c1NDEZ, Javier", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/127", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:15:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:30:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "128", "speakers": [{"_type": "ContributionParticipation", "emailHash": "14c1288bca407f501ee1fa70d5f30369", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "WISSING, Christoph", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "efd528c4fecb23d4785370a2ba2989b2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MAGINI, Nicolo", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "c64735b9540bc92c4d230a2ca252b7cf", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GUTSCHE, Oliver", "id": "1"}], "title": "Pooling the resources of the CMS Tier-1 sites", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T22:28:21.937471+00:00", "description": "", "title": "chep2015_wissing.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/128\/attachments\/578698\/796868\/chep2015_wissing.pdf", "filename": "chep2015_wissing.pdf", "content_type": "application\/pdf", "type": "file", "id": 796868, "size": 4689632}], "title": "Slides", "default_folder": false, "id": 578698, "description": ""}], "_type": "Contribution", "description": "The CMS experiment at the LHC relies on 7 Tier-1 centres of the WLCG to perform the majority of its bulk processing activity, and to archive its data. During the first run of the LHC, these two functions were tightly coupled as each Tier-1 was constrained to process only the data archived on its hierarchical storage. This lack of flexibility in the assignment of processing workflows occasionally resulted in uneven resource utilisation and in an increased latency in the delivery of the results to the physics community.\r\n\r\nThe long shutdown of the LHC in 2013-2014 was an opportunity to revisit this mode of operations, disentangling the processing and archive functionalities of the Tier-1 centres. The storage services at the Tier-1s were redeployed breaking the traditional hierarchical model: each site now provides a large disk storage to host input and output data for processing, and an independent tape storage used exclusively for archiving. Movement of data between the tape and disk endpoints is not automated, but triggered externally through the WLCG transfer management systems.\r\n\r\nWith this new setup, CMS operations actively controls at any time which data is available on disk for processing and which data should be sent to archive. Thanks to the high-bandwidth connectivity guaranteed by the LHCOPN, input data can be freely transferred between disk endpoints as needed to take advantage of free CPU, turning the Tier-1s into a large pool of shared resources. The output data can be validated before archiving them permanently, and temporary data formats can be produced without wasting valuable tape resources. Finally, the data hosted on disk at Tier-1s can now be made available also for user analysis since there is no risk any longer of triggering chaotic staging from tape.\r\n\r\nIn this contribution, we describe the technical solutions adopted for the new disk and tape endpoints at the sites, and we report on the commissioning and scale testing of the service. We detail the procedures implemented by CMS computing operations to actively manage data on disk at Tier-1 sites, and we give examples of the benefits brought to CMS workflows by the additional flexibility of the new system.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578698", "resources": [{"_type": "LocalFile", "name": "chep2015_wissing.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/128\/attachments\/578698\/796868\/chep2015_wissing.pdf", "fileName": "chep2015_wissing.pdf", "_fossil": "localFileMetadata", "id": "796868", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "14c1288bca407f501ee1fa70d5f30369", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "WISSING, Christoph", "id": "2"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/128", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:45:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:00:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "129", "speakers": [{"_type": "ContributionParticipation", "emailHash": "84e347808f71c25608b52dc4bd77c2ad", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FRANK, Markus", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "84e347808f71c25608b52dc4bd77c2ad", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FRANK, Markus", "id": "0"}], "title": "DDG4 A Simulation Framework using the DD4hep Detector Description Toolkit", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T10:59:58.088355+00:00", "description": "Slides -- Open Office format", "title": "DDG4_id129.odp", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/129\/attachments\/578700\/796870\/CHEP2015-DDG4-id_129.odp", "filename": "CHEP2015-DDG4-id_129.odp", "content_type": "application\/vnd.oasis.opendocument.presentation", "type": "file", "id": 796870, "size": 2484860}], "title": "Slides -- Open Office format", "default_folder": false, "id": 578700, "description": ""}, {"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T11:00:35.586002+00:00", "description": "Slides -- PDF format", "title": "DDG4_id129.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/129\/attachments\/578699\/796869\/CHEP2015-DDG4-id_129.pdf", "filename": "CHEP2015-DDG4-id_129.pdf", "content_type": "application\/pdf", "type": "file", "id": 796869, "size": 1173501}], "title": "Slides -- PDF format", "default_folder": false, "id": 578699, "description": ""}], "_type": "Contribution", "description": "The detector description is an essential component that has to be used to analyse \r\nand simulate data resulting from particle collisions in high energy physics experiments. \r\nBased on the DD4hep detector description toolkit a flexible and data driven simulation \r\nframework was designed using the Geant4 tool-kit. We present this framework and describe \r\nthe guiding requirements and the architectural design, which was strongly driven by ease \r\nof use. The goal was, given an existing detector description, to simulate the detector \r\nresponse to particle collisions in high energy physics experiments with minimal \r\neffort, but not impose restrictions to support enhanced or improved behaviour.\r\nStarting from the ROOT based geometry implementation used by DD4hep an automatic \r\nconversion mechanism to Geant4 was developed. The physics response and the mechanism \r\nto input particle data from generators was highly formalized and can be instantiated \r\non demand using using known factory patterns. A palette of components to model the \r\ndetector response is provided by default, but improved or more sophisticated components \r\nmay easily be added using the factory pattern. Only the final configuration of the \r\ninstantiated components has to be provided by end-users using either C++ or python \r\nscripting or an XML based description.", "track": "Track2: Offline software ", "material": [{"_type": "Material", "title": "Slides -- Open Office format", "_fossil": "materialMetadata", "id": "578700", "resources": [{"_type": "LocalFile", "name": "DDG4_id129.odp", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/129\/attachments\/578700\/796870\/CHEP2015-DDG4-id_129.odp", "fileName": "CHEP2015-DDG4-id_129.odp", "_fossil": "localFileMetadata", "id": "796870", "_deprecated": true}], "_deprecated": true}, {"_type": "Material", "title": "Slides -- PDF format", "_fossil": "materialMetadata", "id": "578699", "resources": [{"_type": "LocalFile", "name": "DDG4_id129.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/129\/attachments\/578699\/796869\/CHEP2015-DDG4-id_129.pdf", "fileName": "CHEP2015-DDG4-id_129.pdf", "_fossil": "localFileMetadata", "id": "796869", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "f5dcc35395831a55f6722d12d4272492", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "GAEDE, Frank-Dieter", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "b5e94dc57d9ec82cc9dddec9b8706ed1", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "NIKIFOROU, Nikiforos", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "dcb99157c4dd3dfa639bfc1133a3f993", "affiliation": "Jozef Stefan Institute (SI)", "_fossil": "contributionParticipationMetadata", "fullName": "PETRIC, Marko", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "84f3f721370dba12b47a611b1a36081e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SAILER, Andre", "id": "4"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/129", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "414", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2de49c7a4fc7e80a5ffd18282c47a155", "affiliation": "Charles University in Prague", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. BILKA, Tadeas", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2de49c7a4fc7e80a5ffd18282c47a155", "affiliation": "Charles University in Prague", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. BILKA, Tadeas", "id": "0"}], "title": "Alignment and calibration of Belle II tracking detectors", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T23:03:31.590217+00:00", "description": "", "title": "chepBelle2AlignmentCalibration.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/414\/attachments\/578701\/796871\/chepBelle2AlignmentCalibration.pdf", "filename": "chepBelle2AlignmentCalibration.pdf", "content_type": "application\/pdf", "type": "file", "id": 796871, "size": 2051630}], "title": "Slides", "default_folder": false, "id": 578701, "description": ""}], "_type": "Contribution", "description": "The Belle II experiment will start taking data in 2017. The SuperKEKB accelerator will deliver a factor 40 higher luminosity in comparison to its predecessor, KEKB, to acquire a 50 times larger data sample of B-B\u0305 events. In order to manage higher occupancy and background, a new silicon vertex detector  consisting of two inner layers of DEPFET pixel sensors surrounded by four layers of double-sided strip sensors will be installed. The high target performance of the detector, in particular vertex and momentum resolution, motivated the development of reliable alignment and calibration procedures. Combined with calibration of the upgraded drift chamber tracker, this task requires advanced software design and considerable computing resources. We present track-based alignment and calibration of the vertex and tracking detector using the global approach of the Millepede II tool in combination with an advanced track parametrization by the General Broken Lines (GBL) model. The procedure is implemented within the software framework of the Belle II experiment and GBL has also been integrated into the experiment-independent track-fitting toolkit GENFIT.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578701", "resources": [{"_type": "LocalFile", "name": "chepBelle2AlignmentCalibration.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/414\/attachments\/578701\/796871\/chepBelle2AlignmentCalibration.pdf", "fileName": "chepBelle2AlignmentCalibration.pdf", "_fossil": "localFileMetadata", "id": "796871", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "87dcba0773a3d9a34f3f13bdf4bb7426", "affiliation": "Deutsches Elektronen Synchrotron (DESY)", "_fossil": "contributionParticipationMetadata", "fullName": "KLEINWORT, Claus", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "1422c0b10e722c94fb58f1e2b290af22", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "YASCHENKO, Sergey", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "de590eabce1adf1b94fbe186b2f605d8", "affiliation": "Charles University (CZ)", "_fossil": "contributionParticipationMetadata", "fullName": "KVASNICKA, Peter", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "33c516195d7b51b5db84d9e8765b993a", "affiliation": "Charles University (CZ)", "_fossil": "contributionParticipationMetadata", "fullName": "DOLEZAL, Zdenek", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/414", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "415", "speakers": [{"_type": "ContributionParticipation", "emailHash": "73c47f432ac574bc76aba1a48761b1ff", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "JUN, Soon Yung", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "73c47f432ac574bc76aba1a48761b1ff", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "JUN, Soon Yung", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "add066542804f14f0fb28b199f1636c5", "affiliation": "SLAC National Accelerator  Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DOTTI, Andrea", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "5a02abf7bf489401510b64b45e39a213", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "ELVIRA, Victor Daniel", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "c4ef0189b78622196af5389d6f2a9c7d", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FOLGER, Gunter", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "8f145e1619c011ffb9cc74a25519b640", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "GENSER, Krzysztof", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "6fac3d6f6a7e5fdfeca402e31f4c1868", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "KOWALKOWSKI, Jim", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "fe605a2b3b5c9c50970007d7ad02a6dd", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PATERNO, Marc", "id": "6"}], "title": "Geant4 Computing Performance Benchmarking and Monitoring", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T18:17:45.650083+00:00", "description": "", "title": "g4_performance.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/415\/attachments\/578702\/796872\/g4_performance.pdf", "filename": "g4_performance.pdf", "content_type": "application\/pdf", "type": "file", "id": 796872, "size": 3095898}, {"_type": "attachment", "modified_dt": "2015-04-09T18:17:45.650083+00:00", "description": "", "title": "g4_performance.ppt", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/415\/attachments\/578702\/796873\/g4_performance.ppt", "filename": "g4_performance.ppt", "content_type": "application\/vnd.ms-powerpoint", "type": "file", "id": 796873, "size": 3644416}], "title": "Slides", "default_folder": false, "id": 578702, "description": ""}], "_type": "Contribution", "description": "Performance evaluation and analysis of large-scale computing\r\napplications is essential for optimizing the use of resources. As detector\r\nsimulation is one of the most compute-intensive tasks and Geant4 is the\r\nsimulation toolkit most widely used in contemporary high energy\r\nphysics (HEP) experiments, it is important to monitor Geant4\r\nthrough its development cycle for changes in computing performance and\r\nto identify problems and opportunities for code improvements. All\r\ndevelopment and public releases are being profiled with a set of\r\napplications that utilize different input event samples, physics\r\nlists, and detector configurations. Results from multiple\r\nbenchmarking runs are compared to previous public and development\r\nreference releases to monitor CPU and memory usage. Observed changes are\r\nevaluated and correlated with code modifications. Besides the full\r\nsummary of call stack data and memory footprint, a detailed call graph\r\nanalysis is available to Geant4 developers for further analysis. The\r\nset of software tools used in the performance evaluation procedure,\r\nboth in sequential and multi-threaded modes, include FAST, IgProf and\r\nOpen$\\mid$Speedshop. The scalability of the CPU time and memory\r\nutilization of multi-threaded applications is evaluated by measuring\r\nevent throughput and memory usage as a function of the number of\r\nthreads for selected event samples. We will describe the procedure of\r\nGeant4 computing performance profiling and benchmarking and present\r\nrecent results.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578702", "resources": [{"_type": "LocalFile", "name": "g4_performance.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/415\/attachments\/578702\/796872\/g4_performance.pdf", "fileName": "g4_performance.pdf", "_fossil": "localFileMetadata", "id": "796872", "_deprecated": true}, {"_type": "LocalFile", "name": "g4_performance.ppt", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/415\/attachments\/578702\/796873\/g4_performance.ppt", "fileName": "g4_performance.ppt", "_fossil": "localFileMetadata", "id": "796873", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/415", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "416", "speakers": [{"_type": "ContributionParticipation", "emailHash": "74b9a7d2831526161755aca47a04ce55", "affiliation": "Universit\u00e0 degli Studi  e INFN Milano (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. VILLAPLANA PEREZ, Miguel", "id": "9"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "dda15ba6edeab44779adfc2e96161b6d", "affiliation": "Universit\u00e0 degli Studi e INFN Milano", "_fossil": "contributionParticipationMetadata", "fullName": "BARBERIS, Stefano", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "98d9a9661adbb9187d09e32f3ea8f754", "affiliation": "INFN Sezione di Milano (INFN)", "_fossil": "contributionParticipationMetadata", "fullName": "CARMINATI, Leonardo", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "fb0cfe4fda463cef9f1b75981ec08d83", "affiliation": "Universit\u00e0 degli Studi e INFN Milano (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "LEVERARO, Franco", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "e22982a6b59b835199fbc86df05cdaac", "affiliation": "Universit\u00e0 degli Studi  e INFN Milano (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "MAZZA, Simone Michele", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "6f6c030bd3b8abb11a7d9e738b1f215d", "affiliation": "Milan University and INFN", "_fossil": "contributionParticipationMetadata", "fullName": "PERINI, laura", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "c6f8dc98cff3ded41cb19185a6526e1f", "affiliation": "Universit\u00e0 degli Studi  e INFN Milano (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "PRELZ, Francesco", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "123f1bf502f209ea8f48f0624c9d7a18", "affiliation": "Universit\u00e0 degli Studi  e INFN Milano (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. REBATTO, David", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "9040cefd15f014f5f7bf91931ce360b7", "affiliation": "Universit\u00e0 degli Studi  e INFN Milano (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "TURRA, Ruggero", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "6ebc8d4beeb277878ba3f0061e95f6d3", "affiliation": "Universit\u00e0 degli Studi  e INFN Milano (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "VACCAROSSA, Luca", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "74b9a7d2831526161755aca47a04ce55", "affiliation": "Universit\u00e0 degli Studi  e INFN Milano (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. VILLAPLANA PEREZ, Miguel", "id": "9"}], "title": "Optimisation of the usage of LHC and local computing resources in a multidisciplinary physics department hosting a WLCG Tier-2 centre", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:06:57.019847+00:00", "description": "", "title": "Poster_CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/416\/attachments\/578703\/796874\/Poster_CHEP2015.pdf", "filename": "Poster_CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796874, "size": 1644933}], "title": "Poster", "default_folder": false, "id": 578703, "description": ""}], "_type": "Contribution", "description": "We present the approach of the University of Milan Physics Department and the local unit of INFN to allow and encourage the sharing among different research areas of computing, storage and networking resources (the largest ones being those composing the Milan WLCG Tier-2 centre and tailored to the needs of the ATLAS experiment).\r\n\r\nComputing resources are organised as independent HTCondor pools, with a global master in charge of monitoring them and optimising their usage. The configuration has to provide satisfactory throughput for both serial and parallel (multicore, MPI) jobs. A combination of local, remote and cloud storage options are available. The experience of users from different research areas operating on this shared infrastructure is discussed.\r\n\r\nThe promising direction of improving scientific computing throughput by federating access to distributed computing and storage also seems to fit very well with the objectives listed in the European Horizon 2020 framework for research and development.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578703", "resources": [{"_type": "LocalFile", "name": "Poster_CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/416\/attachments\/578703\/796874\/Poster_CHEP2015.pdf", "fileName": "Poster_CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796874", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/416", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:15:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "417", "speakers": [{"_type": "ContributionParticipation", "emailHash": "48f199a8ab13b82cdec81d35ec8b8e24", "affiliation": "The University of Chicago", "_fossil": "contributionParticipationMetadata", "fullName": "MANZOTTI, Alessandro", "id": "8"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "37bf07949718d2f2889355b32d085085", "affiliation": "urn:Google", "_fossil": "contributionParticipationMetadata", "fullName": "ZUNTZ, Joe", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "fe605a2b3b5c9c50970007d7ad02a6dd", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PATERNO, Marc", "id": "0"}], "title": "CosmoSIS: a system for MC parameter estimation", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-17T00:04:23.209949+00:00", "description": "", "title": "chep-talk_mod_16_9.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/417\/attachments\/578704\/796876\/chep-talk_mod_16_9.pdf", "filename": "chep-talk_mod_16_9.pdf", "content_type": "application\/pdf", "type": "file", "id": 796876, "size": 2792687}, {"_type": "attachment", "modified_dt": "2015-04-17T00:04:23.209949+00:00", "description": "", "title": "chep-talk_mod_4_3.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/417\/attachments\/578704\/796875\/chep-talk_mod_4_3.pdf", "filename": "chep-talk_mod_4_3.pdf", "content_type": "application\/pdf", "type": "file", "id": 796875, "size": 1360844}], "title": "Slides", "default_folder": false, "id": 578704, "description": ""}], "_type": "Contribution", "description": "CosmoSIS [http:\/\/arxiv.org\/abs\/1409.3409] is a modular system for\r\ncosmological parameter estimation, based on Markov Chain Monte Carlo\r\n(MCMC) and related techniques. It provides a series of samplers, which\r\ndrive the exploration of the parameter space, and a series of modules,\r\nwhich calculate the likelihood of the observed data for a given physical\r\nmodel, determined by the location of a sample in the parameter space.\r\nWhile CosmoSIS ships with a set of modules that calculate quantities of\r\ninterest to cosmologists, there is nothing about the framework itself,\r\nnor in the MCMC technique, that is specific to cosmology. Thus CosmoSIS\r\ncould be used for parameter estimation problems in other fields,\r\nincluding HEP.\r\n\r\nThis presentation will describe the features of CosmoSIS and show an\r\nexample of its use outside of cosmology. It will also discuss how\r\ncollaborative development strategies differ between two different\r\ncommunities: that of HEP physicists, accustomed to working in large\r\ncollaborations, and that of cosmologists, who have traditionally not\r\nworked in large groups. For example, because there is no collaboration\r\nto enforce a language choice, the framework supports programming in\r\nmultiple languages. Additionally, since scientists in the cosmology\r\ncommunity are used to working independently, a system was needed for\r\nhelping ensure that proper attribution is given to authors of\r\ncontributed algorithms.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578704", "resources": [{"_type": "LocalFile", "name": "chep-talk_mod_16_9.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/417\/attachments\/578704\/796876\/chep-talk_mod_16_9.pdf", "fileName": "chep-talk_mod_16_9.pdf", "_fossil": "localFileMetadata", "id": "796876", "_deprecated": true}, {"_type": "LocalFile", "name": "chep-talk_mod_4_3.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/417\/attachments\/578704\/796875\/chep-talk_mod_4_3.pdf", "fileName": "chep-talk_mod_4_3.pdf", "_fossil": "localFileMetadata", "id": "796875", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "1e4953e3099654fb18667dfd31b5638f", "affiliation": "fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. DODELSON, scott", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "5e13f931e2478322e49edbc5069ac7e7", "affiliation": "UCL", "_fossil": "contributionParticipationMetadata", "fullName": "BRIDLE, Sarah", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "930549df198c00aa7a21c7ea55890fd3", "affiliation": "urn:Google", "_fossil": "contributionParticipationMetadata", "fullName": "SEHRISH, Saba", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "6fac3d6f6a7e5fdfeca402e31f4c1868", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "KOWALKOWSKI, Jim", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "96b7615cc6e82850473ec3786023f904", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "JENNINGS, Elise", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "223d8993c3d818a64ffb6e742a44b926", "affiliation": "University of Chicago", "_fossil": "contributionParticipationMetadata", "fullName": "RUDD, Douglas", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "48f199a8ab13b82cdec81d35ec8b8e24", "affiliation": "The University of Chicago", "_fossil": "contributionParticipationMetadata", "fullName": "MANZOTTI, Alessandro", "id": "8"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/417", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "410", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e86323288b18387d48dc1af9718c14d6", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WILDISH, Tony", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "0d28138f3fc2216fb1d97810d9bd7e74", "affiliation": "University of Bologna", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. BONACORSI, Daniele", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "efd528c4fecb23d4785370a2ba2989b2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MAGINI, Nicolo", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e86323288b18387d48dc1af9718c14d6", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WILDISH, Tony", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "0d28138f3fc2216fb1d97810d9bd7e74", "affiliation": "University of Bologna", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. BONACORSI, Daniele", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "efd528c4fecb23d4785370a2ba2989b2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MAGINI, Nicolo", "id": "2"}], "title": "Monitoring data transfer latency in CMS computing operations", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T22:11:31.778500+00:00", "description": "", "title": "poster_hq.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/410\/attachments\/578705\/796877\/poster_hq.pdf", "filename": "poster_hq.pdf", "content_type": "application\/pdf", "type": "file", "id": 796877, "size": 11207490}], "title": "Poster", "default_folder": false, "id": 578705, "description": ""}], "_type": "Contribution", "description": "During the first LHC run, the CMS experiment collected tens of Petabytes of collision and simulated data, which need to be distributed among dozens of computing centres with low latency in order to make efficient use of the resources. While the desired level of throughput has been successfully achieved, it is still common to observe transfer workflows that cannot reach full completion in a timely manner due to a small fraction of stuck files which require operator intervention.\r\n\r\nFor this reason, in 2012 the CMS transfer management system, PhEDEx, was instrumented with a monitoring system to measure file transfer latencies, and to predict the completion time for the transfer of a data set. The operators can detect abnormal patterns in transfer latencies while the transfer is still in progress, and monitor the long-term performance of the transfer infrastructure to plan the data placement strategy.\r\n\r\nBased on the data collected with the latency monitoring system, we present a study on the different factors that contribute to transfer completion time. As case studies, we analyze several typical CMS transfer workflows, such as distribution of collision event data from CERN or upload of simulated event data from the Tier-2 centres to the archival Tier-1 centres. For each workflow, we present the typical patterns of transfer latencies that have been identified with the latency monitor.\r\n\r\nWe identify the areas in PhEDEx where a development effort can reduce the latency, and we show how we are able to detect stuck transfers which need operator intervention. We propose a set of metrics to alert about stuck subscriptions and prompt for manual intervention, with the aim of improving transfer completion times.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578705", "resources": [{"_type": "LocalFile", "name": "poster_hq.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/410\/attachments\/578705\/796877\/poster_hq.pdf", "fileName": "poster_hq.pdf", "_fossil": "localFileMetadata", "id": "796877", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "cac2e1f38368edbd76a41c1ec99fec22", "affiliation": "Ecole Polytechnique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "SARTIRANA, Andrea", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "ac2d516e3130b6efff7d45f6014fd230", "affiliation": "Cukurova University (TR)", "_fossil": "contributionParticipationMetadata", "fullName": "TAZE, Meric", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "e7d4750883fd28297f3b78f9f5b4ccf4", "affiliation": "University of Bologna", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. DIOTALEVI, Tommaso", "id": "5"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/410", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:45:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:00:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "411", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5b0402a9cd32d8915ceeb56718af64b6", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CANAL, Philippe", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "5b0402a9cd32d8915ceeb56718af64b6", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CANAL, Philippe", "id": "0"}], "title": "ROOT 6 and beyond: TObject, C++14 and many cores.", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T00:38:29.965196+00:00", "description": "", "title": "ROOT_CHEP_2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/411\/attachments\/578706\/796878\/ROOT_CHEP_2015.pdf", "filename": "ROOT_CHEP_2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796878, "size": 2018867}, {"_type": "attachment", "modified_dt": "2015-04-14T00:38:29.965196+00:00", "description": "", "title": "ROOT_CHEP_2015.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/411\/attachments\/578706\/796879\/ROOT_CHEP_2015.pptx", "filename": "ROOT_CHEP_2015.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796879, "size": 7555073}], "title": "Slides", "default_folder": false, "id": 578706, "description": ""}], "_type": "Contribution", "description": "Following the release of version 6, ROOT has entered a new area of development. It will leverage the industrial strength compiler library shipping in ROOT 6 and its support of the C++11\/14 standard, to significantly simplify and harden ROOT's interfaces and to clarify and substantially improve ROOT's support for multi-threaded environments.\r\n\r\nThis talk will also recap the most important new features and enhancements in ROOT in general, focusing on those allowed by the improved interpreter and better compiler support, including I\/O for smart pointers, easier type safe access to the content of TTrees and enhanced multi processor support.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578706", "resources": [{"_type": "LocalFile", "name": "ROOT_CHEP_2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/411\/attachments\/578706\/796878\/ROOT_CHEP_2015.pdf", "fileName": "ROOT_CHEP_2015.pdf", "_fossil": "localFileMetadata", "id": "796878", "_deprecated": true}, {"_type": "LocalFile", "name": "ROOT_CHEP_2015.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/411\/attachments\/578706\/796879\/ROOT_CHEP_2015.pptx", "fileName": "ROOT_CHEP_2015.pptx", "_fossil": "localFileMetadata", "id": "796879", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "6ea3743225ffb0457fc5cb31ebb0520e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "NAUMANN, Axel", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "28b03fed334cb75a99ee0a92ecd47530", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "VASILEV, Vasil Georgiev", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "e71a4f9c822c9af7b8ab70c18eb4e640", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "PIPARO, Danilo", "id": "3"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/411", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "412", "speakers": [{"_type": "ContributionParticipation", "emailHash": "acc8d4822e186517845e263497c16087", "affiliation": "Brunel University London", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. COLECCHIA, Federico", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "acc8d4822e186517845e263497c16087", "affiliation": "Brunel University London", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. COLECCHIA, Federico", "id": "0"}], "title": "Data-driven estimation of neutral pileup particle multiplicity in high-luminosity hadron collider environments", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-03-12T17:25:05.564972+00:00", "description": "", "title": "colecchia.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/412\/attachments\/578707\/796880\/colecchia.pdf", "filename": "colecchia.pdf", "content_type": "application\/pdf", "type": "file", "id": 796880, "size": 1253716}], "title": "Poster", "default_folder": false, "id": 578707, "description": ""}], "_type": "Contribution", "description": "The contamination from low-energy strong interactions is a major issue for data analysis at the Large Hadron Collider, particularly with reference to pileup, i.e. to proton-proton collisions from other bunch crossings. With a view to improving on the performance of pileup subtraction in higher-luminosity regimes, particle weighting methods have recently been proposed whereby the weights are used to rescale the particle four-momentum vectors. We describe an algorithm based on a different approach that instead employs the weights to reshape the particle-level kinematic distributions in the data. We have applied the algorithm to the task of estimating the number of neutral pileup particles in different kinematic regions inside individual simulated events. Because of the simplicity and parallelisation potential of this technique, we foresee the possibility of using it in conjunction with existing methods at the reconstruction level in future high-luminosity hadron collider environments.", "track": "Track2: Offline software ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578707", "resources": [{"_type": "LocalFile", "name": "colecchia.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/412\/attachments\/578707\/796880\/colecchia.pdf", "fileName": "colecchia.pdf", "_fossil": "localFileMetadata", "id": "796880", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/412", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:15:00"}, "duration": 15, "session": "Track 8 Session", "keywords": [], "id": "413", "speakers": [{"_type": "ContributionParticipation", "emailHash": "1fa592d680198988b1272a99a539aef8", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "CALLAND, Richard", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "1fa592d680198988b1272a99a539aef8", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "CALLAND, Richard", "id": "0"}], "title": "GPU Accelerated Event-by-event Reweighting for a T2K Neutrino Oscillation Analysis", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T13:25:40.654283+00:00", "description": "", "title": "rcalland_CHEP_2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/413\/attachments\/578708\/796881\/rcalland_CHEP_2015.pdf", "filename": "rcalland_CHEP_2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796881, "size": 3501996}], "title": "Slides", "default_folder": false, "id": 578708, "description": ""}], "_type": "Contribution", "description": "The Tokai-to-Kamioka (T2K) experiment is a second generation long baseline neutrino experiment, which uses a near detector to constrain systematic uncertainties for oscillation measurements with its far detector. Event-by-event reweighting of Monte Carlo (MC) events is applied to model systematic effects and construct PDFs describing predicted event distributions. However when analysing simultaneously several data samples from both near and far detectors, the computational overhead can become a limiting factor in an oscillation analysis. Because reweighting each MC event is an independent process, it can be parallelized using graphics processing units (GPUs). For a recent T2K analysis, several bottlenecking calculations were offloaded onto NVIDIA GPUs using CUDA: the calculation of oscillation probabilities with matter effects and the evaluation of non-linear parameter responses with cubic splines. Individually, these methods achieved 40-180x speed-ups in standalone benchmarks. When implemented into the analysis software suite, an improvement of ~20x was seen to the overall analysis. This talk will discuss the motivation and implementation of GPU reweighting into the T2K oscillation analysis, and prospects for further improvements using GPUs.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578708", "resources": [{"_type": "LocalFile", "name": "rcalland_CHEP_2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/413\/attachments\/578708\/796881\/rcalland_CHEP_2015.pdf", "fileName": "rcalland_CHEP_2015.pdf", "_fossil": "localFileMetadata", "id": "796881", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/413", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:15:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:30:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "498", "speakers": [{"_type": "ContributionParticipation", "emailHash": "466fa0065d83d1eb2e727a51109f864d", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "GASTHUBER, Martin", "id": "9"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "68f7b967f470ca55e1338183cba30850", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "STRUTZ, Marco", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "af371695cb0123d8c744071279bbb749", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "APLIN, Steven", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "8752fa4ff53ff75ecd6c6eb3cd406427", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. GASTHUBER, Martin", "id": "2"}], "title": "Architecture of a new data taking and analysis infrastructure and services for the next generation detectors of Petra3 at DESY", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T03:10:02.456112+00:00", "description": "", "title": "chep2015-asap3-from-keynote.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/498\/attachments\/578709\/796882\/chep2015-asap3-from-keynote.pdf", "filename": "chep2015-asap3-from-keynote.pdf", "content_type": "application\/pdf", "type": "file", "id": 796882, "size": 1846982}], "title": "Slides", "default_folder": false, "id": 578709, "description": ""}], "_type": "Contribution", "description": "Data taking and analysis infrastructures in HEP have evolved during many years to a well known problem domain. In contrast to HEP, third generations synchrotron light sources, existing and upcoming free electron laser are confronted an explosion in data rates which is primarily driven by recent developments in 2D pixel array detectors. The next generation will produce data in the region upwards of 50 Gbytes per second. At synchrotrons, data was traditionally taken away by users following data taking using portable media. This will clearly not scale at all.\r\n\r\nWe present first experiences of our new architecture and services underlying by results taken from the resumption of data taking in March 2015. Technology choices were undertaking over a period of twelve month. The work involved a close collaboration between central IT, beamline controls, and beamline support staff. In addition a cooperation was established between DESY IT and IBM to include industrial research and development experience and skills.\r\n\r\nIn technological terms the next generation detectors exceeds current generations in order of magnitudes by data volume, -rate as well as complexity at an exponential growth. We are challenging unpredictable data access demands, computing platform and OS version integration (i.e. Windows), but still requiring acceptable bandwidth for DAQ rate at the final storage system.\r\n\r\nOur approach integrates HPC technologies for storage systems and protocols. In particular, our solution uses a single filesystem instance with a multiple protocol access, while operating within a single namespace - ubiquitous NFS & SMB access to same repository. We are targeting a system supporting distributed parity, decent erasure codes - to allow very fast rebuilds and a high availability level for the capacity disk based resources, multiple cluster, asynchronous file replication, high speed networking - Infiniband FDR & 10\/40GE Ethernets, storage class memory (SSD & FlashDIMM) to run the high IOPS burst buffering layer within the architecture.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578709", "resources": [{"_type": "LocalFile", "name": "chep2015-asap3-from-keynote.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/498\/attachments\/578709\/796882\/chep2015-asap3-from-keynote.pdf", "fileName": "chep2015-asap3-from-keynote.pdf", "_fossil": "localFileMetadata", "id": "796882", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "7dbe60355c5a0763062b3a6e5832eec7", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. DIETRICH, Stefan", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "6c6329341dbb5e4332e61dba24d8e80b", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "LEWENDEL, Birgit", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "890ef28d8990373a88aec1dbf4f5aef8", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. ENSSLIN, Uwe", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "bf63b5305594c47600ad190114b08d07", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "GUELZOW, Volker", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "f3ce0ec4f040309085abbe055fe29d9d", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Mrs. KUHN, Manuela", "id": "7"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/498", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "418", "speakers": [{"_type": "ContributionParticipation", "emailHash": "fe605a2b3b5c9c50970007d7ad02a6dd", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PATERNO, Marc", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "930549df198c00aa7a21c7ea55890fd3", "affiliation": "urn:Google", "_fossil": "contributionParticipationMetadata", "fullName": "SEHRISH, Saba", "id": "0"}], "title": "Exploring Two Approaches for an End-to-End Scientific Analysis Workflow", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The scientific discovery process can be advanced by the integration of\r\nindependently-developed programs run on disparate computing facilities\r\ninto coherent workflows usable by scientists who are not experts in\r\ncomputing. For such advancement, we need a system which scientists can\r\nuse to formulate analysis workflows, to integrate new components to\r\nthese workflows, and to execute different components on resources that\r\nare best suited to run those components. In addition, we need to monitor\r\nthe status of the workflow as components get scheduled and executed, and\r\nto access the intermediate and final output for visual exploration and\r\nanalysis. Finally, it is important for scientists to be able to share\r\ntheir workflows with collaborators.\r\n\r\n\r\nWe are involved with a project to develop such an analysis framework for \r\nthe Large Synoptic Survey Telescope (LSST) Dark Energy Science Collaboration (DESC).\r\nFollowing upon the development of several detailed use cases for LSST DESC, \r\nwe have been working on two approaches for the framework; the first one is \r\nbased on the use and extension of Galaxy, a web-based portal for biomedical \r\nresearch, and the second one is based on a programming language, Python. \r\nThere are benefits to each approach as we discovered while implementing one example use case. \r\nBoth approaches allow scientists to run complicated workflows that involve the use of a\r\nvariety of computational resources (including grid resources,\r\nsupercomputing resources at NERSC, and local compute nodes) for the\r\nexecution of workflows on simulations of LSST images. \r\nAdding a new application in the Python-based workflow description is straight forward, \r\nhowever,\r\nadding new applications through the Galaxy interface requires expert knowledge of \r\nthe Galaxy system and interaction with Galaxy infrastructure.  \r\n\r\nIn this paper, we present a brief description of the two approaches,\r\ndescribe the kinds of extensions to the Galaxy system we have found necessary\r\nin order to support the wide variety of scientific analysis in the\r\ncosmology community, and discuss how similar efforts might be of benefit\r\nto the HEP community.\r\n", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "fe605a2b3b5c9c50970007d7ad02a6dd", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PATERNO, Marc", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "6fac3d6f6a7e5fdfeca402e31f4c1868", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "KOWALKOWSKI, Jim", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/418", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "419", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5152afe0ee224171ff18a8a6a0abb342", "affiliation": "National Research Centre Kurchatov Institute (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "RYABINKIN, Eygene", "id": "5"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "8bef009be2caf51729183fbf5e90ba79", "affiliation": "St.Petersburg State University", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. BATKOVICH, Dmitrii", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "7ae2c7e1352349229d8fc639552ea4d3", "affiliation": "St. Petersburg State University (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "KOMPANIETS, Mikhail", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "f5e7378e16e9fee96ad835deb6454086", "affiliation": "National Technical Univ. of Ukraine \"Kyiv Polytechnic Institute", "_fossil": "contributionParticipationMetadata", "fullName": "SHADURA, Oksana", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "cc3271196f967494971a2411d87d7c69", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "YURCHENKO, Volodymyr", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "93a73f45aa836f5a2930f289c12c5959", "affiliation": "St. Petersburg State University (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "ZAROCHENTSEV, Andrey", "id": "4"}], "title": "Integration of XRootD into the cloud infrastructure for ALICE data analysis", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T13:05:50.591483+00:00", "description": "", "title": "spbsu-poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/419\/attachments\/578710\/796883\/spbsu-poster.pdf", "filename": "spbsu-poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796883, "size": 816336}], "title": "Slides", "default_folder": false, "id": 578710, "description": ""}], "_type": "Contribution", "description": "Cloud technologies allow easy load balancing between different tasks and projects. From the viewpoint of the data analysis in the ALICE experiment, cloud allows to deploy software using Cern Virtual Machine (CernVM) and CernVM File System (CVMFS), to run different (including outdated) versions of software for long term data preservation and to dynamically allocate resources for different computing activities, e.g. grid site, ALICE Analysis Facility (AAF) and possible usage for local projects or other  LHC experiments.\r\n\r\nWe present a cloud solution for Tier-3 sites based on OpenStack and CEPH distributed storage with an integrated XRootD based storage element (SE).  One of the key features of the solution is that CEPH has been used  as a backend for Cinder Block Storage service for OpenStack and in the same time as a storage backend for XRootD, with redundancy and availability of data preserved by CEPH settings.\r\n\r\nFor faster and easier OpenStack deployment the PackStack solution, which is based on the Puppet configuration management system, is applied. CEPH installation and configuration chains are structured, converted to Puppet manifests and integrated into Packstack. This solution can be easily deployed, maintained and used even in small groups with limited computing resources  and small organisations which usually have lack of IT support.\r\nThe proposed infrastructure has been tested on two different clouds (SPbSU \\& BITP) and integrates succesfully with the ALICE data analysis model.\r\n\r\nThe present work is supported in part by Saint-Petersburg State University research grants 11.38.66.2012 and 11.38.197.2014.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578710", "resources": [{"_type": "LocalFile", "name": "spbsu-poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/419\/attachments\/578710\/796883\/spbsu-poster.pdf", "fileName": "spbsu-poster.pdf", "_fossil": "localFileMetadata", "id": "796883", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "5152afe0ee224171ff18a8a6a0abb342", "affiliation": "National Research Centre Kurchatov Institute (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "RYABINKIN, Eygene", "id": "5"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/419", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:15:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "319", "speakers": [{"_type": "ContributionParticipation", "emailHash": "56d8bec08e4b7dd9591b1bfc938bd36c", "affiliation": "FZ J\u00fclich GmbH", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. STOCKMANNS, Tobias", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "56d8bec08e4b7dd9591b1bfc938bd36c", "affiliation": "FZ J\u00fclich GmbH", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. STOCKMANNS, Tobias", "id": "0"}], "title": "Continuous Readout Simulation with FairRoot on the Example of the PANDA Experiment", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T01:24:57.321153+00:00", "description": "", "title": "TimeBasedSimulation_v5.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/319\/attachments\/578711\/796884\/TimeBasedSimulation_v5.pdf", "filename": "TimeBasedSimulation_v5.pdf", "content_type": "application\/pdf", "type": "file", "id": 796884, "size": 992426}, {"_type": "attachment", "modified_dt": "2015-04-14T01:24:57.321153+00:00", "description": "", "title": "TimeBasedSimulation_v5.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/319\/attachments\/578711\/796885\/TimeBasedSimulation_v5.pptx", "filename": "TimeBasedSimulation_v5.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796885, "size": 16713504}], "title": "Slides", "default_folder": false, "id": 578711, "description": ""}], "_type": "Contribution", "description": "Future particle physics experiments are searching more and more for rare decays which have similar signatures in the detector as the huge background.  For those events usually simple selection criteria do not exist, which makes it impossible to implement a hardware-trigger based on a small subset of detector data.\r\nTherefore all the detector data is read out continuously and processed on-the-fly to achieve a data reduction suitable for permanent storage and detailed analysis.\r\n\r\nTo cope with these requirements of a triggerless readout also the simulation software has to be adopted to add a continuous data production with pile-up effects and event overlapping in addition to the event wise simulation. This simulated data is of utmost importance to get a realistic detector simulation, to develop event-building algorithms and to determine the hardware requirements for the DAQ system of the experiments.\r\nThe possibility to simulate a continuous data stream was integrated into the FairRoot simulation framework. This running mode is called time-based simulation and a lot of effort was taken that one can switch seamlessly between event-based and a time-based simulation mode.\r\n\r\nOne experiment, which is using this new feature, is the PANDA experiment. It utilizes a quasi-continuous antiproton beam with a mean time between interactions of 50 ns. Because of the unbunched structure of the beam the interaction time follows a Poisson statistics with a high probability of events with short time distances. Depending on the time resolution of the sub-detectors this leads to an overlap of up to 20 events inside a sub-detector. This makes it an ideal test candidate for the time-based simulation.\r\n\r\nIn this talk the way the time-based simulation was implemented into FairRoot will be presented and examples of time-based simulations done for the PANDA experiment will be shown.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578711", "resources": [{"_type": "LocalFile", "name": "TimeBasedSimulation_v5.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/319\/attachments\/578711\/796884\/TimeBasedSimulation_v5.pdf", "fileName": "TimeBasedSimulation_v5.pdf", "_fossil": "localFileMetadata", "id": "796884", "_deprecated": true}, {"_type": "LocalFile", "name": "TimeBasedSimulation_v5.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/319\/attachments\/578711\/796885\/TimeBasedSimulation_v5.pptx", "fileName": "TimeBasedSimulation_v5.pptx", "_fossil": "localFileMetadata", "id": "796885", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/319", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "318", "speakers": [{"_type": "ContributionParticipation", "emailHash": "75f1c1dfb7a8f724166e67f2d40ae933", "affiliation": "INFN-CNAF (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SAPUNENKO, Vladimir", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "75f1c1dfb7a8f724166e67f2d40ae933", "affiliation": "INFN-CNAF (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SAPUNENKO, Vladimir", "id": "1"}], "title": "An integrated solution for remote data access", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T10:45:06.259151+00:00", "description": "", "title": "RemoteDataAccess_CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/318\/attachments\/578712\/796886\/RemoteDataAccess_CHEP2015.pdf", "filename": "RemoteDataAccess_CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796886, "size": 547807}], "title": "Poster", "default_folder": false, "id": 578712, "description": ""}], "_type": "Contribution", "description": "Data management constitutes one of the major challenges that a geographically-distributed data centre has to face, especially when remote data access is involved. We discuss an integrated solution which enables transparent and efficient access to online and nearline data through high latency networks. This is based on the joint use of the General Parallel File System (GPFS) and of the Tivoli Storage Manager (TSM). Both products, developed by IBM, are well known and extensively used in the HEP computing world. Owing to a new feature introduced in GPFS 3.5, so-called Active File Management (AFM), the definition of a single, geographically-distributed namespace, characterised by automated data flow management between different locations, becomes possible. As a practical example, we present the implementation of AFM-based remote data access between two data centres located in Bologna and Rome, demonstrating the validity of the solution for the use case of the AMS experiment.", "track": "Track3: Data store and access ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578712", "resources": [{"_type": "LocalFile", "name": "RemoteDataAccess_CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/318\/attachments\/578712\/796886\/RemoteDataAccess_CHEP2015.pdf", "fileName": "RemoteDataAccess_CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796886", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "e43cbedc9022e3c3d7335122469626db", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "D'URSO, Domenico", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "8efa0f7191c7b8ff186bbf8099f02e31", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "VAGNONI, Vincenzo", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "d04a5fe2a2ee8472d129c31df45ec1e5", "affiliation": "INFN-CNAF", "_fossil": "contributionParticipationMetadata", "fullName": "DELL'AGNELLO, Luca", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/318", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "313", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e4ea4c83ee2fa3fad4dbf535e57e2e80", "affiliation": "KiSTi Korea Institute of Science &  Technology Information (KR)", "_fossil": "contributionParticipationMetadata", "fullName": "PARK, Geun Chul", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e4ea4c83ee2fa3fad4dbf535e57e2e80", "affiliation": "KiSTi Korea Institute of Science &  Technology Information (KR)", "_fossil": "contributionParticipationMetadata", "fullName": "PARK, Geun Chul", "id": "0"}], "title": "Directory Search Performance Optimization of AMGA for the Belle II Experiment", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T06:40:42.806426+00:00", "description": "", "title": "AMGA-CHEP2015-gcpark-kisti-new.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/313\/attachments\/578713\/796887\/AMGA-CHEP2015-gcpark-kisti-new.pdf", "filename": "AMGA-CHEP2015-gcpark-kisti-new.pdf", "content_type": "application\/pdf", "type": "file", "id": 796887, "size": 595801}], "title": "Slides", "default_folder": false, "id": 578713, "description": ""}], "_type": "Contribution", "description": "AMGA (ARDA Metadata Grid Application) is a grid metadata catalog system that has been developed as a component of the EU FP7 EMI consortium based on the requirements of the HEP (High-Energy Physics) and the Biomed user communities. Currently, AMGA is exploited to manage the metadata in the gBasf2 framework at the Belle II which is one of the largest particle physics experiments in the world. \r\n\r\nIn this paper, we present our efforts to optimize the metadata query performance of AMGA to better support the massive MC Campaign of the Belle II experiment. While AMGA has been proved to show very competitive performance for a relatively small amount of data, as the number of directories created and the size of overall metadata increase (e.g. hundreds of thousands of directories) during the MC Campaign, it suffers from severe query processing performance degradations. To address this problem, we modified the query search mechanism and the database scheme of AMGA which results in dramatic improvements of metadata search performance and query response time. \r\n\r\nThroughout our comparative performance analysis of metadata search operations, we show that AMGA can be an optimal solution for a metadata catalog in a large-scale scientific experimental framework.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578713", "resources": [{"_type": "LocalFile", "name": "AMGA-CHEP2015-gcpark-kisti-new.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/313\/attachments\/578713\/796887\/AMGA-CHEP2015-gcpark-kisti-new.pdf", "fileName": "AMGA-CHEP2015-gcpark-kisti-new.pdf", "_fossil": "localFileMetadata", "id": "796887", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "c297ea0cd83410db2b176f68881d0f2b", "affiliation": "KISTI", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. HUH, Taesang", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "29f189f465a51b3d324b424a35a93dc4", "affiliation": "KISTI", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. KWAK, Jae-Hyuck Kwak", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "d872ae6e61ab96602690aabd64689cd3", "affiliation": "KISTI", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HWANG, Soonwook", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/313", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "312", "speakers": [{"_type": "ContributionParticipation", "emailHash": "f864c1c717f871856ce382c61432fa40", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. SUZUKI, Soh", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "f864c1c717f871856ce382c61432fa40", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. SUZUKI, Soh", "id": "0"}], "title": "Renovation of HEPnet-J for near-future experiments", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T09:06:14.106185+00:00", "description": "", "title": "poster-a1.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/312\/attachments\/578714\/796888\/poster-a1.pdf", "filename": "poster-a1.pdf", "content_type": "application\/pdf", "type": "file", "id": 796888, "size": 6060024}], "title": "Poster", "default_folder": false, "id": 578714, "description": ""}], "_type": "Contribution", "description": "Formerly most of HEP experiments in japan used\r\nthe centralized computing model.\r\n\r\nOriginally HEPnet-J had only one instance which is\r\nconnected to Internet, and recently it has many closed\r\nnetwork which connects domestic sites.\r\nAt that time, the network connectivity in Japan\r\nwas very poor and the main purpose of HEPnet-J was\r\nproviding enough connectivity for interactive use\r\nover domestic and international links funded by KEK.\r\nDuring last 10 years,\r\nthe domestic and international connectivity\r\nprovided by each university and NREN has been\r\ndramatically improved and it is enough for\r\nmanual transfer of typical mDST files.\r\n\r\nThe rapid growth of data volume\r\nmakes it unable to apply same model to new generation experiments.\r\nAs the LCG tier structure for LHC has proved that\r\nthe distributed computing model over collaboration sites\r\nis really applicable to the huge scale experiment,\r\nthe external connectivity for international collaboration sites\r\nshould be more faster, and more secure.\r\n\r\nFor example,\r\nthe Belle II experiment in KEK\r\nwill have huge data repositories in US and EU.\r\nThe expected throughput from KEK to US is 20Gbps,\r\nthus we have to bypass the security device like a firewall\r\nfor this data path.\r\nThe computing facility in KEK (KEKCC) is not enclosed\r\nin the firewall of KEK, but it has a dedicated firewall that\r\nis managed by the independent policy.\r\nIts throughput is sufficient to handle an ordinary activity\r\nover Grid VO.\r\nBut it is not enough for the mass-data transmission for Belle II,\r\nwe have to bypass it in the same way of LHCONE.\r\nNow bypass lines for Belle II are prepared and under tasting.\r\n\r\nWe will report how we renovate HEPnet-J for experiments\r\ndriven by international collaboration.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578714", "resources": [{"_type": "LocalFile", "name": "poster-a1.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/312\/attachments\/578714\/796888\/poster-a1.pdf", "fileName": "poster-a1.pdf", "_fossil": "localFileMetadata", "id": "796888", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "c6d5d53bc21839f56e68ac7bcfc7510a", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "YUASA, Fukuko", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "a6558e0be6a85bc95cec59bfdb8cb816", "affiliation": "High Energy Accelerator Research Organization (JP)", "_fossil": "contributionParticipationMetadata", "fullName": "NAKAMURA, Tomoaki", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "c8cf04fc5bd3457316e236c69a6b247e", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "KARITA, Yukio", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "ad59ece4e48448bc02511d8867861e3d", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "SCHRAM, Malachi", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "77129490a06056d8761c3ede7dea7289", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "HARA, Takanori", "id": "5"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/312", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:30:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "311", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6f56261b5ec69653be09d3709f5ea43c", "affiliation": "the University of Tokyo", "_fossil": "contributionParticipationMetadata", "fullName": "HIRAIDE, Katsuki", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6f56261b5ec69653be09d3709f5ea43c", "affiliation": "the University of Tokyo", "_fossil": "contributionParticipationMetadata", "fullName": "HIRAIDE, Katsuki", "id": "0"}], "title": "The data acquisition system of the XMASS experiment", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T11:05:49.869165+00:00", "description": "", "title": "hiraide-CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/311\/attachments\/578715\/796889\/hiraide-CHEP2015.pdf", "filename": "hiraide-CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796889, "size": 2251116}], "title": "Slides", "default_folder": false, "id": 578715, "description": ""}], "_type": "Contribution", "description": "XMASS is a multi-purpose low-background experiment with a large volume of liquid xenon scintillator at Kamioka in Japan. The first phase of the experiment aiming at direct detection of dark matter was commissioned in 2010 and is currently taking data.\r\n\r\n The detector uses ~830 kg of liquid xenon viewed by 642 photomultiplier tubes (PMTs). Signals from 642 PMTs are amplified and read out by 1 GS\/s digitizers (CAEN V1751) as well as ADC\/TDC modules. To reduce data size, we implemented an on-board data suppression algorithm in digitizers. The trigger is generated based on the number of hit PMTs within a 200 ns coincidence window. Recently, it is pointed out that the XMASS detector has also a great potential to detect supernova neutrino burst, and therefore several DAQ upgrades and system extensions are proceeding for this purpose.\r\n\r\n We will present the overall design and performance of the XMASS data acquisition system and the status of DAQ upgrade for supernova neutrino burst detection.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578715", "resources": [{"_type": "LocalFile", "name": "hiraide-CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/311\/attachments\/578715\/796889\/hiraide-CHEP2015.pdf", "fileName": "hiraide-CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796889", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/311", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:15:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "310", "speakers": [{"_type": "ContributionParticipation", "emailHash": "dd7018927651faf265e88113c88714a2", "affiliation": "STANFORD LINEAR ACCELERATOR CENTER", "_fossil": "contributionParticipationMetadata", "fullName": "HANUSHEVSKY, Andrew", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "dd7018927651faf265e88113c88714a2", "affiliation": "STANFORD LINEAR ACCELERATOR CENTER", "_fossil": "contributionParticipationMetadata", "fullName": "HANUSHEVSKY, Andrew", "id": "0"}], "title": "Accelerating Debugging In A Highly Distributed Environment", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T02:13:00.851345+00:00", "description": "", "title": "CHEP-DigFS.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/310\/attachments\/578716\/796890\/CHEP-DigFS.pdf", "filename": "CHEP-DigFS.pdf", "content_type": "application\/pdf", "type": "file", "id": 796890, "size": 614935}, {"_type": "attachment", "modified_dt": "2015-04-09T02:13:00.851345+00:00", "description": "", "title": "CHEP-DigFS.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/310\/attachments\/578716\/796891\/CHEP-DigFS.pptx", "filename": "CHEP-DigFS.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796891, "size": 123611}], "title": "Slides", "default_folder": false, "id": 578716, "description": ""}], "_type": "Contribution", "description": "As more experiments move to a federated model of data access the environment becomes highly distributed and decentralized. In many cases this may pose obstacles in quickly resolving site issues; especially given vast time-zone differences. Spurred by ATLAS needs, Release 4 of XRootD incorporates a special mode of access to provide remote debugging capabilities. Essentially, XRootD allows a site to grant secure access to specific individuals to view certain XRootD information (e.g. log files, configuration files, etc). In a virtual view all of the information is laid out in a site independent way regardless of how the site configured its system. This allows experts at other locations to assist in resolving issues and alleviates time-zone vagaries. The view is available through XRootd or, optionally, HTTP. This talk provides the motivation for developing the remote debugging facility, why it is essential in highly distributed environments, and what can actually be done with it.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578716", "resources": [{"_type": "LocalFile", "name": "CHEP-DigFS.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/310\/attachments\/578716\/796890\/CHEP-DigFS.pdf", "fileName": "CHEP-DigFS.pdf", "_fossil": "localFileMetadata", "id": "796890", "_deprecated": true}, {"_type": "LocalFile", "name": "CHEP-DigFS.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/310\/attachments\/578716\/796891\/CHEP-DigFS.pptx", "fileName": "CHEP-DigFS.pptx", "_fossil": "localFileMetadata", "id": "796891", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/310", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "317", "speakers": [{"_type": "ContributionParticipation", "emailHash": "17488d1ac2de0b46336dbfeca29abe85", "affiliation": "University of Tokyo \/ RIKEN", "_fossil": "contributionParticipationMetadata", "fullName": "NAKAI, Wataru", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "17488d1ac2de0b46336dbfeca29abe85", "affiliation": "University of Tokyo \/ RIKEN", "_fossil": "contributionParticipationMetadata", "fullName": "NAKAI, Wataru", "id": "0"}], "title": "Development of tracker alignment software for the J-PARC E16 experiment", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T11:59:36.759859+00:00", "description": "", "title": "chep2015_poster_nakai_rev2.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/317\/attachments\/578717\/796892\/chep2015_poster_nakai_rev2.pdf", "filename": "chep2015_poster_nakai_rev2.pdf", "content_type": "application\/pdf", "type": "file", "id": 796892, "size": 928306}], "title": "Slides", "default_folder": false, "id": 578717, "description": ""}], "_type": "Contribution", "description": "The J-PARC E16 experiment will be performed to measure the mass modification of vector mesons in nuclear matter at J-PARC in order to study the origin of hadron mass.\r\nIn the experiment, we will measure invariant mass spectra of vector mesons with the electron and positron decay channel.\r\nWe will use 30 GeV proton beam with an intensity of $1\\times10^{10}$ protons per pulse at High-momentum beam line, which is to be constructed at J-PARC Hadron Facility in early 2016.\r\n\r\n![A schematic view of the J-PARC E16 spectrometer.][1]\r\n\r\nThe design of the E16 spectrometer is shown as Figure 1.\r\nIt consists of several modules, and one module consists of three Gas Electron Multiplier Trackers (GTRs), a Hadron Blind Detector, which is a gas Cherenkov counter using CsI evaporated GEM, and Lead-Glass calorimeters. The sizes of GTRs are 100 mm $\\times$ 100 mm, 200 mm $\\times$ 200 mm, and 300 mm $\\times$ 300 mm, respectively. A full spectrometer consists of 26 modules. Nuclear targets such as CH$_2$, C, Cu, and Pb, are placed at the center of the spectrometer. The magnitude of magnetic field is about 1.8 T at the center.\r\n\r\nThe E16 experiment aims for the mass resolution of 5 MeV\/$c^2$ around $\\phi$ meson peak. In order to achieve this, the position resolution of 100 $\\mu$m is required.\r\nSo we should align GTRs with the accuracy of better than 100 $\\mu$m.\r\n\r\nDetector R&D has been well performed by the J-PARC E16 collaboration. \r\nOur GTR consists of a drift cathode, a triple GEM, and a readout strip board.\r\nWe chose a strip pitch of 350 $\\mu$m to achieve the required position resolution.\r\nBy several beam tests, the requirement is shown to be satisfied.\r\n\r\nThe photogrammetry system using a CCD camera will be conducted when detectors are installed, while the aimed precision will be finally achieved by using charged particle track-based software alignment.\r\nWe are developing its algorithm.\r\nWe will acquire calibration run data in which no magnetic field will be present and two additional wire targets will be used.\r\nIn the algorithm, we move tracking detectors to arrange wire target position and three hit positions of GTRs on a straight line.\r\nWe evaluate this algorithm using a Monte Carlo simulation.\r\nIn the simulation, tracks are generated from wire targets after misaligning detectors in random directions from designed places.\r\nFor example, distances between simulation true and corrected positions of detector centers are estimated.\r\n\r\nWe will report the R&D progress of the alignment software for the J-PARC E16 experiment.\r\n\r\n\r\n  [1]: http:\/\/high-p.kek.jp\/nakai\/e16_spec.pdf\r\n  [2]: http:\/\/high-p.kek.jp\/nakai\/e16_spec.eps\r\n  [3]: http:\/\/high-p.kek.jp\/nakai\/e16_spec.eps", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578717", "resources": [{"_type": "LocalFile", "name": "chep2015_poster_nakai_rev2.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/317\/attachments\/578717\/796892\/chep2015_poster_nakai_rev2.pdf", "fileName": "chep2015_poster_nakai_rev2.pdf", "_fossil": "localFileMetadata", "id": "796892", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "ba05cf23f1d8bafe174a36f98a11c8aa", "affiliation": "High Energy Accelerator Research Organization (KEK)", "_fossil": "contributionParticipationMetadata", "fullName": "AOKI, Kazuya", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c64b1d0a6f574b79ddb96d0271dbd6fc", "affiliation": "University of Tokyo \/ RIKEN", "_fossil": "contributionParticipationMetadata", "fullName": "KANNO, Koki", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "1606e50bea6b72d12b7ee332cb84d0e0", "affiliation": "RIKEN", "_fossil": "contributionParticipationMetadata", "fullName": "KAWAMA, Daisuke", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "ab14ea6c159f49606938a96327832a5a", "affiliation": "University of Tokyo", "_fossil": "contributionParticipationMetadata", "fullName": "KOMATSU, Yusuke", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "ebb136639dc1b94ba02b90fc5db93499", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "MORINO, Yuhei", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "35e99635c8f3a339e5943da42823f22d", "affiliation": "University of Tokyo", "_fossil": "contributionParticipationMetadata", "fullName": "MURAKAMI, Hikari", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "cc7c3f3a894576c3678e52e4632de850", "affiliation": "University of Tokyo", "_fossil": "contributionParticipationMetadata", "fullName": "OBARA, Yuki", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "9fef0eb8eef4e2453f1c3f7ab2638a6d", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "OZAWA, Kyoichiro", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "ad7cd8169726e930e5ac6d6ec11409ce", "affiliation": "Research Center for Nuclear Physics (RCNP), Osaka University", "_fossil": "contributionParticipationMetadata", "fullName": "TAKAHASHI, Tomonori", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "14fe8cb28e5f268fe0430955a2bb7980", "affiliation": "RIKEN", "_fossil": "contributionParticipationMetadata", "fullName": "YOKKAICHI, Satoshi", "id": "10"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/317", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "316", "speakers": [{"_type": "ContributionParticipation", "emailHash": "197e0464d81c774a40022f90d15a5626", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "HUSEJKO, Michal", "id": "4"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "197e0464d81c774a40022f90d15a5626", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "HUSEJKO, Michal", "id": "4"}], "title": "HPC in a HEP lab: lessons learned from setting up cost-effective HPC clusters", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-04T08:20:58.921748+00:00", "description": "", "title": "MHusejko_HPC_poster_v7.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/316\/attachments\/578718\/796893\/MHusejko_HPC_poster_v7.pdf", "filename": "MHusejko_HPC_poster_v7.pdf", "content_type": "application\/pdf", "type": "file", "id": 796893, "size": 1880528}], "title": "Poster", "default_folder": false, "id": 578718, "description": ""}], "_type": "Contribution", "description": "In this paper we present our findings gathered during the evaluation and testing of Windows Server High Performance Computing (Windows HPC) in view of potentially using it as a production HPC system for engineering applications. The Windows HPC package, an extension of Microsoft's Windows Server product,  provides all essential interfaces, utilities and management functionality for creating, operating and monitoring a Windows based HPC cluster infrastructure. The evaluation and test phase was focused on verifying the functionalities of Windows HPC, its performance, support of commercial tools and the integration with the users' work environment.\r\n\r\nWe will describe constraints imposed by the way the CERN Computer Centre is operated, licensing for engineering tools and scalability and behaviour of the HPC Engineering applications used at CERN. We will present an initial set of requirements, which were created based on above constraints and requests from the CERN engineering user community. \r\nWe will explain how we have configured Windows HPC clusters to provide job scheduling functionalities required to support the CERN engineering user community, quality of service, user and project based priorities, and fair access to limited resources. Finally, we will present several performance tests we carried out to verify Windows HPC performance and scalability.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578718", "resources": [{"_type": "LocalFile", "name": "MHusejko_HPC_poster_v7.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/316\/attachments\/578718\/796893\/MHusejko_HPC_poster_v7.pdf", "fileName": "MHusejko_HPC_poster_v7.pdf", "_fossil": "localFileMetadata", "id": "796893", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "4c3ca3bdf19ffeb02da6d8a393a1d00e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BAEHLER, Pierre", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "1660a0eef6860c343f5556dafe665946", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "EVANS, John", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "8eaa52c2690c8404ba595dd969b19de2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "HOIMYR, Nils", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "e79fbfa7642755008e862ffde72f0431", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MEINHARD, Helge", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "31957b62daca1ad1a2742c2f13b066da", "affiliation": "Aristotle Univ. of Thessaloniki (GR)", "_fossil": "contributionParticipationMetadata", "fullName": "AGTZIDIS, Ioannis", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "9883418e45feac4cb1fd99330e912e1c", "affiliation": "University of Wroclaw (PL)", "_fossil": "contributionParticipationMetadata", "fullName": "DUL, Tadeusz Jakub", "id": "6"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/316", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "315", "speakers": [{"_type": "ContributionParticipation", "emailHash": "197e0464d81c774a40022f90d15a5626", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "HUSEJKO, Michal", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "197e0464d81c774a40022f90d15a5626", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "HUSEJKO, Michal", "id": "2"}], "title": "Investigation of High-Level Synthesis tools' applicability to data acquisition systems design based on the CMS ECAL Data Concentrator Card example", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-04T08:21:39.317850+00:00", "description": "", "title": "MHusejko_HLS_poster_v7.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/315\/attachments\/578719\/796894\/MHusejko_HLS_poster_v7.pdf", "filename": "MHusejko_HLS_poster_v7.pdf", "content_type": "application\/pdf", "type": "file", "id": 796894, "size": 1985617}], "title": "Poster", "default_folder": false, "id": 578719, "description": ""}], "_type": "Contribution", "description": "High-Level Synthesis (HLS) for Field-Programmable Logic Array (FPGA) programming is becoming a practical alternative to well-established VHDL and Verilog languages. This paper describes a case study in the use of HLS tools to design an FPGA-based data acquisition systems (DAQ). We will present the implementation of the CERN CMS detector ECAL Data Concentrator Card (DCC) functionality in HLS and lessons learned from using HLS design flow.\r\n\r\nThe DCC functionality and a definition of the initial system-level performance requirements (latency, bandwidth, and throughput) will be presented .  We will describe how its packet processing control-centric algorithm was implemented with VHDL and Verilog languages. We will then show how the HLS flow could speed up design-space exploration by providing loose coupling between function\u2019s interface design and function\u2019s algorithm implementation.\r\n\r\nWe conclude with results of real-life hardware tests performed with the HLS flow-generated design with a DCC Tester system.", "track": "Track1: Online computing ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578719", "resources": [{"_type": "LocalFile", "name": "MHusejko_HLS_poster_v7.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/315\/attachments\/578719\/796894\/MHusejko_HLS_poster_v7.pdf", "fileName": "MHusejko_HLS_poster_v7.pdf", "_fossil": "localFileMetadata", "id": "796894", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "1660a0eef6860c343f5556dafe665946", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "EVANS, John", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "2a4c9032b40f2aed8f61900a0f3b9019", "affiliation": "LIP Laboratorio de Instrumentacao e Fisica Experimental de Part", "_fossil": "contributionParticipationMetadata", "fullName": "RASTEIRO DA SILVA, Jose Carlos", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/315", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "314", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5479333ca5a4bde353890f2a40fc3e73", "affiliation": "Nagoya Univ.", "_fossil": "contributionParticipationMetadata", "fullName": "HAYASAKA, Kiyoshi", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "5479333ca5a4bde353890f2a40fc3e73", "affiliation": "Nagoya Univ.", "_fossil": "contributionParticipationMetadata", "fullName": "HAYASAKA, Kiyoshi", "id": "0"}], "title": "Monitoring system for the Belle II distributed computing", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T10:18:21.277038+00:00", "description": "", "title": "chep2015_hayasaka_poster_v4.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/314\/attachments\/578720\/796895\/chep2015_hayasaka_poster_v4.pdf", "filename": "chep2015_hayasaka_poster_v4.pdf", "content_type": "application\/pdf", "type": "file", "id": 796895, "size": 1608056}], "title": "Slides", "default_folder": false, "id": 578720, "description": ""}], "_type": "Contribution", "description": "Belle II Experiment is the next generation of B factory at SuperKEKB in Japan.  A sample of 50 at${}^{-1}$  will be collected at the $\\Upsilon$ resonances. In addition a large Monte Carlo (MC) sample will be generated to optimize the event selection criteria. The large data samples are managed by a sophisticated distributed computing system.   To utilize the computing resources with a high efficiency, a monitoring system is absolutely needed.  The effective monitoring system for the central system and each site has been developed based on some experiences for the MC sample production that was performed at the world wide computing centers. Also, in order to detect troubles on each site, a job to collect the environment has been developed and is submitted periodically. The monitoring system for the Belle II distributed computing will be introduced.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578720", "resources": [{"_type": "LocalFile", "name": "chep2015_hayasaka_poster_v4.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/314\/attachments\/578720\/796895\/chep2015_hayasaka_poster_v4.pdf", "fileName": "chep2015_hayasaka_poster_v4.pdf", "_fossil": "localFileMetadata", "id": "796895", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "5984f6179c1328f2e8b6e50d2d0017fd", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "KATO, Yuji", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "77129490a06056d8761c3ede7dea7289", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "HARA, Takanori", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "7ae6773a40c49ae5776d96621f5c7ab5", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "MIYAKE, Hideki", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "3b85a4a025a30820faf4613b16c34fee", "affiliation": "University of Tokyo (JP)", "_fossil": "contributionParticipationMetadata", "fullName": "UEDA, I", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/314", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "139", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2cbb21e728f5ee0f6e1cb54ec854a4a7", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SALZBURGER, Andreas", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2cbb21e728f5ee0f6e1cb54ec854a4a7", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SALZBURGER, Andreas", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "5318f8857a7a9b9ec7e90991d0394197", "affiliation": "University of Innsbruck (AT)", "_fossil": "contributionParticipationMetadata", "fullName": "JANSKY, Roland", "id": "1"}], "title": "The ATLAS fast chain MC production project", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T08:38:31.135192+00:00", "description": "", "title": "fastmcchain_CHEP_v05.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/139\/attachments\/578721\/796896\/fastmcchain_CHEP_v05.pdf", "filename": "fastmcchain_CHEP_v05.pdf", "content_type": "application\/pdf", "type": "file", "id": 796896, "size": 3521675}], "title": "Slides", "default_folder": false, "id": 578721, "description": ""}], "_type": "Contribution", "description": "During the last years ATLAS has successfully deployed a new integrated simulation framework (ISF) which allows a flexible mixture of full and fast detector simulation techniques within the processing of one event. With the ISF, the simulation execution speed could be increased up to a factor 100, which makes subsequent digitisation and reconstruction processing the dominant contributions to the MC production CPU cost. The slowest components of both digitisation and reconstruction are within the Inner Detector due to the complex signal modelling needed in the emulation of the detector readout and in reconstruction due to the combinatorial nature of the problem to solve, respectively. Alternative fast approaches have been developed for these components: for the silicon based detectors a simpler geometrical clustering approach has been deployed replacing the charge drift emulation in the standard digitisation modules, and achieves a very high accuracy in describing the standard output. For the Inner Detector track reconstruction, a Monte Carlo truth based trajectory building has been deployed for bypassing the CPU intensive pattern recognition. All components have been, together with the ISF, integrated into a new fast MC production chain, aiming to produce fast MC simulated data with acceptable agreement with fully simulated and reconstructed data at a processing time of seconds per event.\u00a0", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578721", "resources": [{"_type": "LocalFile", "name": "fastmcchain_CHEP_v05.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/139\/attachments\/578721\/796896\/fastmcchain_CHEP_v05.pdf", "fileName": "fastmcchain_CHEP_v05.pdf", "_fossil": "localFileMetadata", "id": "796896", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/139", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:45:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "497", "speakers": [{"_type": "ContributionParticipation", "emailHash": "20ea92548b575c36277f712eaf666b20", "affiliation": "Jozef Stefan Institute (SI)", "_fossil": "contributionParticipationMetadata", "fullName": "BRACKO, Marko", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6cc249775bb40b9f478a29ad38170221", "affiliation": "Pacific Northwest National Laboratory, USA", "_fossil": "contributionParticipationMetadata", "fullName": "WOOD, Lynn", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "20ea92548b575c36277f712eaf666b20", "affiliation": "Jozef Stefan Institute (SI)", "_fossil": "contributionParticipationMetadata", "fullName": "BRACKO, Marko", "id": "1"}], "title": "The Belle II Conditions Database", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T05:07:53.718373+00:00", "description": "", "title": "Belle_II_Conditions_DB_CHEP_2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/497\/attachments\/578722\/796897\/Belle_II_Conditions_DB_CHEP_2015.pdf", "filename": "Belle_II_Conditions_DB_CHEP_2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796897, "size": 2450009}], "title": "Slides", "default_folder": false, "id": 578722, "description": ""}], "_type": "Contribution", "description": "The Belle II experiment, a next-generation B factory experiment at the KEK laboratory, Tsukuba, Japan, is expected to collect an experimental data sample fifty times larger than its predecessor, the Belle experiment. The data taking and processing rates are expected to be at least one order of magnitude larger as well.\r\n\r\nIn order to cope with these large data processing rates and huge data samples, the Conditions Database, which stores the time-dependent calibrations, have to be designed carefully for successful and efficient data processing and analysis by the worldwide Belle II Collaboration. The database system needs to offer fast response, has to enable storing experimental information with fine spatial and time granularity, and ensure the scalability and redundancy for robust operation.\r\n\r\nThe Conditions Database design and implementation details, together with future plans, will be presented in this talk.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578722", "resources": [{"_type": "LocalFile", "name": "Belle_II_Conditions_DB_CHEP_2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/497\/attachments\/578722\/796897\/Belle_II_Conditions_DB_CHEP_2015.pdf", "fileName": "Belle_II_Conditions_DB_CHEP_2015.pdf", "_fossil": "localFileMetadata", "id": "796897", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/497", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:30:00"}, "duration": 15, "session": "Track 8 Session", "keywords": [], "id": "3", "speakers": [{"_type": "ContributionParticipation", "emailHash": "81b1bf725f4540f8632673ac37508af9", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "HOLLOWELL, Christopher", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "81b1bf725f4540f8632673ac37508af9", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "HOLLOWELL, Christopher", "id": "0"}], "title": "The Effect of NUMA Tunings on CPU Performance", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-08T22:07:46.299081+00:00", "description": "", "title": "numa.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/3\/attachments\/578723\/796898\/numa.pdf", "filename": "numa.pdf", "content_type": "application\/pdf", "type": "file", "id": 796898, "size": 139185}], "title": "Slides", "default_folder": false, "id": 578723, "description": ""}], "_type": "Contribution", "description": "Non-uniform memory access (NUMA) is a memory architecture for symmetric\r\nmultiprocessing (SMP) systems where each processor is directly connected\r\nto separate memory.  Indirect access to other CPU's (remote) RAM is still\r\npossible, but such requests are slower as they must also pass through that \r\nmemory's controlling CPU.  In concert with a NUMA-aware operating\r\nsystem, the NUMA hardware architecture can help eliminate the memory\r\nperformance reductions generally seen in SMP systems when multiple\r\nprocessors simultaneously attempt to access memory.\r\n\r\nThe x86 CPU architecture has supported NUMA for a number of years.  Modern\r\noperating systems such as Linux support NUMA-aware scheduling, where the OS\r\nattempts to schedule a process to the CPU directly attached to the majority\r\nof its RAM.  In Linux, it is possible to further manually tune the NUMA\r\nsubsystem using the \"numactl\" utility.  With the release of Red\r\nHat Enterprise Linux (RHEL) 6.3, the \"numad\" daemon became available in\r\nthis distribution.  This daemon monitors a system's NUMA topology and utilization, \r\nand automatically makes adjustments to optimize locality.\r\n\r\nAs the number of cores in x86 servers continues to grow, efficient\r\nNUMA mappings of processes to CPUs\/memory will become increasingly\r\nimportant.  This presentation gives a brief overview of NUMA, and\r\ndiscusses the effects of manual tunings and numad on the performance\r\nof the HEPSPEC06 benchmark.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578723", "resources": [{"_type": "LocalFile", "name": "numa.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/3\/attachments\/578723\/796898\/numa.pdf", "fileName": "numa.pdf", "_fossil": "localFileMetadata", "id": "796898", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "851ef3cb50eeb935408ee9ae6e35ff00", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CARAMARCU, Costin", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "d02731e682c9e8307b844b566d0af6aa", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WONG, Tony", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "001b3e939ab534eee442ba2cbd6b82e4", "affiliation": "Brookhaven National Lab", "_fossil": "contributionParticipationMetadata", "fullName": "STRECKER-KELLOGG, William", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "2628a966cb60c5635940b139d5df8cb9", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. ZAYTSEV, Alexandr", "id": "4"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/3", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:45:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "368", "speakers": [{"_type": "ContributionParticipation", "emailHash": "244c8becc3b603c6e5722f14640d8e65", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "COUTURIER, Ben", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "0227951f2719a9a4069e664a2d4227fd", "affiliation": "IN2P3\/LAL", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BINET, Sebastien", "id": "0"}], "title": "docker & HEP: containerization of applications for development, distribution and preservation.", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T10:57:01.722523+00:00", "description": "", "title": "20150413-binet-docker-chep.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/368\/attachments\/578724\/796899\/20150413-binet-docker-chep.pdf", "filename": "20150413-binet-docker-chep.pdf", "content_type": "application\/pdf", "type": "file", "id": 796899, "size": 595425}], "title": "Slides", "default_folder": false, "id": 578724, "description": ""}], "_type": "Contribution", "description": "docker & HEP: containerization of applications for development, distribution and preservation.\r\n=================================================\r\n\r\nHEP software stacks are not shallow.\r\nIndeed, HEP experiments' software are usually many applications in one\r\n(reconstruction, simulation, analysis, ...) and thus require many\r\nlibraries - developed in-house or by third parties - to be properly\r\ncompiled and installed.\r\nMoreover, because of resource constraints, experiments' software is\r\nusually installed, tested, validated and deployed on a very narrow set\r\nof platforms, architectures, toolchains and operating systems. As a\r\nconsequence, bootstrapping a software environment on a developer\r\nmachine or deploying the software on production or user machines is\r\nusually perceived as tedious and iterative work, especially when one\r\nwants the native performances of bare metal.\r\n\r\n`Docker` containers provide an interesting avenue for packaging\r\napplications and development environment, relying on the Linux kernel\r\ncapabilities for process isolation, adding \"git\"-like capabilities to\r\nthe filesystem layer and providing (close to) native CPU, memory and\r\nI\/O performances.\r\n\r\nThis paper will introduce in more details the modus operandi of\r\n`Docker` containers and then focus on the `hepsw\/docks` containers\r\nwhich provide containerized software stacks for -among others-\r\n`LHCb`. The paper will then discuss various strategies experimented\r\nwith to package software (e.g. `CVMFS`, `RPM`s, from-source) and\r\napplied to optimize provisioning speed and disk usage, leveraging the\r\ncaching system of `Docker`.\r\n\r\nFinally, the paper will report on benchmarks comparing workloads on\r\nnative, VM and `Docker` containers setups.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578724", "resources": [{"_type": "LocalFile", "name": "20150413-binet-docker-chep.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/368\/attachments\/578724\/796899\/20150413-binet-docker-chep.pdf", "fileName": "20150413-binet-docker-chep.pdf", "_fossil": "localFileMetadata", "id": "796899", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "244c8becc3b603c6e5722f14640d8e65", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "COUTURIER, Ben", "id": "1"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/368", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "369", "speakers": [{"_type": "ContributionParticipation", "emailHash": "18f12d579cee5d8345191b00dae91d02", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "JUNG, Christopher", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "cbcc450bc203d110216c4c98878594df", "affiliation": "Karlsruhe Institute of Technology", "_fossil": "contributionParticipationMetadata", "fullName": "MEYER, J\u00f6rg", "id": "0"}], "title": "Evaluation of NoSQL database MongoDB for HEP analyses", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T07:34:21.711754+00:00", "description": "", "title": "CHEP_2015_MongoDB_HEP.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/369\/attachments\/578725\/796900\/CHEP_2015_MongoDB_HEP.pdf", "filename": "CHEP_2015_MongoDB_HEP.pdf", "content_type": "application\/pdf", "type": "file", "id": 796900, "size": 2555048}], "title": "Slides", "default_folder": false, "id": 578725, "description": ""}], "_type": "Contribution", "description": "Most analyses in experimental high-energy physics (HEP) are based on the data analysis framework ROOT. Therefore, simulated as well as measured events are stored in ROOT trees. A typical analysis loops over events in ROOT files and selects relevant events for further processing according to certain selection criteria.\r\nThe emergence of NoSQL databases provide a new mean for large scale data storage and analyses. NoSQL databases allow horizontal scaling and can be used as a new approach for HEP analyses.\r\nWe present a study that stores simulated top-pair-production events in the document-based NoSQL database MongoDB. We compare analysis steps using MongoDB as input source to the traditional approach using ROOT trees. We demonstrate that many analysis steps beyond the selection of events are supported by the database query language.\r\nThe advantages and limitations of the NoSQL approach are discussed in terms of functionality, performance, storage efficiency, and usability.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578725", "resources": [{"_type": "LocalFile", "name": "CHEP_2015_MongoDB_HEP.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/369\/attachments\/578725\/796900\/CHEP_2015_MongoDB_HEP.pdf", "fileName": "CHEP_2015_MongoDB_HEP.pdf", "_fossil": "localFileMetadata", "id": "796900", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "262687ebcd2c33b5380e2b0cf5203c5c", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "STREIT, Achim", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "18f12d579cee5d8345191b00dae91d02", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "JUNG, Christopher", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/369", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "366", "speakers": [{"_type": "ContributionParticipation", "emailHash": "778134ef9926a59b8edb08fce8eabc87", "affiliation": "Jozef Stefan Institute (SI)", "_fossil": "contributionParticipationMetadata", "fullName": "GORISEK, Andrej", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "af12697ddc94c57b4f03141f5ce1b1c4", "affiliation": "University of Toronto (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "MCGOLDRICK, Garrin", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "51c4022be42e68dd7c605d2f155ae746", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CERV, Matevz", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "778134ef9926a59b8edb08fce8eabc87", "affiliation": "Jozef Stefan Institute (SI)", "_fossil": "contributionParticipationMetadata", "fullName": "GORISEK, Andrej", "id": "0"}], "title": "Judith: A Software Package for Synchronised Analysis of Test-beam Data", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The Judith software performs pixel detector analysis tasks utilising two different data streams such as those produced by the reference and tested devices typically found in a testbeam. This software addresses and fixes problems arising from the desynchronization of the two simultaneously triggered heterogeneous data streams by detecting missed triggers in either of the streams. The software can perform all tasks required to generate particle tracks using multiple detector planes: it can align the planes, cluster hits and generate tracks from these clusters. This information can then be used to measure the properties of a particle detector with very fine spatial resolution. It was successfully used by the authors at DESY in a KarTel telescope with an ATLAS Diamond Beam Monitor module as a DUT as well as more recently by other groups testing pixel detectors at CERN PS and SPS.", "track": "Track2: Offline software ", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "af12697ddc94c57b4f03141f5ce1b1c4", "affiliation": "University of Toronto (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "MCGOLDRICK, Garrin", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "51c4022be42e68dd7c605d2f155ae746", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CERV, Matevz", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/366", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "367", "speakers": [{"_type": "ContributionParticipation", "emailHash": "0227951f2719a9a4069e664a2d4227fd", "affiliation": "IN2P3\/LAL", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BINET, Sebastien", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "0227951f2719a9a4069e664a2d4227fd", "affiliation": "IN2P3\/LAL", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BINET, Sebastien", "id": "0"}], "title": "fwk: a go-based\tconcurrent control framework", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:00:00+00:00", "description": "", "title": "http:\/\/talks.godoc.org\/github.com\/go-hep\/talks\/2015\/20150121-binet-fads-hsf\/go-fads.slide#1", "link_url": "http:\/\/talks.godoc.org\/github.com\/go-hep\/talks\/2015\/20150121-binet-fads-hsf\/go-fads.slide#1", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/367\/attachments\/578726\/796901\/go", "type": "link", "id": 796901}], "title": "Slides", "default_folder": false, "id": 578726, "description": ""}], "_type": "Contribution", "description": "`fwk`: a go-based concurrent control framework\r\n============================================\r\n\r\nCurrent HEP control frameworks have been designed and written in the\r\nearly 2000's, when multi-core architectures were not yet pervasive.\r\nAs a consequence, an inherently sequential event processing design\r\nemerged.\r\nEvolving current frameworks' APIs and data models encouraging global\r\nstates, non-reentrancy and non-thread-safety to a more concurrent\r\nfriendly environment, in an adiabatic way, is a major undertaking,\r\neven more so when relying on the building blocks provided by `C++`.\r\n\r\nThis paper reports on the development of `fwk`, a framework\r\ninvestigating and leveraging the built-in tools of the `Go` language to\r\nenable concurrency at the event- and sub-event-levels. The concurrency\r\nfeatures, code distribution and tooling of `Go` will be first presented\r\nto set the scene and explain why `Go` -and its ecosystem at large- is a\r\nnatural fit for a concurrent framework.\r\nThe aim for such a framework is to be usable as a big HEP experiment's\r\ncontrol framework. `fwk` is also meant to be a nimble application for\r\nanalyses which require quick development\/deployment cycles but without\r\npaying the price of a VM on the runtime performances side.\r\n\r\nThe paper will then discuss the design decisions applied to the\r\nvarious components of `fwk`, introduce its current capabilities\r\n(I\/O, histogramming, dataflow, ...) and highlight how the design\r\ndecisions of the `Go` language adequately paved the way for `fwk`.\r\nThen, `fads`, a fast detector simulation toolkit, will be introduced.\r\n'fads' is the port of a single-threaded fast detector simulation\r\ntoolkit (`Delphes`) to `fwk`.\r\nThe paper will present benchmarks (CPU, RSS, I\/O, scalability) of real\r\n`Delphes` use cases against `fads`, as well as comparisons of `fads`\r\nperformances with regard to other next-generation concurrent\r\nframeworks, using synthetic data.\r\nPerformance tools and concurrency debugging tools provided with the\r\n`Go` toolchain and used for this comparison will also be presented.\r\n\r\nFinally, the paper will present prospects and prototypes to fill the\r\ngap in the nascent `Go`-based HEP ecosystem and landscape - namely:\r\nhistograms interactive displays and interactive analysis tools.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578726", "resources": [{"_fossil": "linkMetadata", "url": "http:\/\/talks.godoc.org\/github.com\/go-hep\/talks\/2015\/20150121-binet-fads-hsf\/go-fads.slide#1", "_type": "Link", "name": "http:\/\/talks.godoc.org\/github.com\/go-hep\/talks\/2015\/20150121-binet-fads-hsf\/go-fads.slide#1", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/367", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:15:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "364", "speakers": [{"_type": "ContributionParticipation", "emailHash": "47b1db03d57c9513571947cb365de13a", "affiliation": "University of the West of England (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "SHAMDASANI, Jetendr", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "47b1db03d57c9513571947cb365de13a", "affiliation": "University of the West of England (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "SHAMDASANI, Jetendr", "id": "0"}], "title": "Analysis Traceability and Provenance for HEP", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T22:45:41.879401+00:00", "description": "", "title": "CHEP15_Presentation.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/364\/attachments\/578727\/796902\/CHEP15_Presentation.pdf", "filename": "CHEP15_Presentation.pdf", "content_type": "application\/pdf", "type": "file", "id": 796902, "size": 604369}], "title": "Slides", "default_folder": false, "id": 578727, "description": ""}], "_type": "Contribution", "description": "In complex data analyses it is increasingly important to capture information about the usage of data sets in addition to their preservation over time in order to ensure reproducibility of results, to verify the work of others and to ensure appropriate conditions data have been used for specific analyses. This so-called provenance data in the computer science world is defined as the history or derivation of a data (or process) artifact. Many scientific workflow based studies are beginning to realise the benefit of capturing the provenance of their data and the activities used to process, transform and carry out studies on that data. This is especially true in scientific disciplines where the collection of data through experiment is costly and\/or difficult to reproduce and where that data needs to be preserved over time. With the increase in importance of provenance data many different models have emerged to capture provenance such as PROV or the OPM models. However, these are more for interoperability of provenance information and do not focus on the capture of provenance related data. There is a clear and emerging requirement for systems to handle the provenance of data over extended timescales with a emphasis on preserving the analysis procedures themselves and the environment in which the analyses were conducted alongside the processed data sets.\r\n\r\nA provenance data system that has been built in house at CERN since early 2000 is called CRISTAL. CRISTAL was used to capture the provenance resulting from the design and construction of the CMS ECAL detector over the period 2002-2010. The CRISTAL Kernel (V3.0) has now been launched as open source under the LGPL (V3) licencing scheme and it is available for use by the wider communities including teams involved in the offline analysis of physics data, whether at CMS or other experiments. In addition, in the EC funded neuGRID and N4U projects the original developers have been using CRISTAL to capture the provenance of analyses for neuroscientists running complex pipelines of algorithms in the study of biomarkers for the onset of Alzheimer\u2019s disease. In this paper this application is presented with a focus on how its approach can be customised for use in the high energy physics data analysis community at large. The main focus of this is a set of analysis tools (persistency, browsing\/querying, visualising and analysis tracking services) which together with a generic analysis model backend can be used to capture the information required to support complex analyses. \r\nThe Analysis Tools comprise the following interfaces :\r\n\r\n - **The Analysis Web Serivce \u2013** Which is for advanced users, these users\r\n   are able to programmatically create analyses on the fly.\r\n - **The Analysis Command-line Interface \u2013** This is for users that are\r\n   intermediate\/advanced. It allows users to create analysis using a\r\n   shell like interface.\r\n - **The Analysis Web Portlet Interface \u2013** This is for novice users, it is\r\n   a visual interface which is portlet based and allows users to browse\r\n   datasets and pipelines. It also allows users to create and deploy\r\n   their analyses in a visual manner.\r\n - **The Analysis Core \u2013** These are a core set of objects used by the above\r\n   interfaces. These objects connect directly to a customised analysis\r\n   aware CRISTAL instance which is provenance aware.\r\n\r\nDuring the provenance capture phase the Analysis tools are able to capture :\r\n\r\n - **who** ran an analysis, this is a user name,\r\n - for **what** purpose,  what their analysis is supposed to achieve,\r\n - **when** they ran it this is a timestamp which denotes when it started\r\n   and when it finished,\r\n - **where** it was run this is GRID and Cloud related information,\r\n - **which** datasets and algorithms were used to create and run their\r\n   analyses,\r\n - **how** it was executed, this more detailed infrastructure information\r\n - and lastly **why** the analysis was run, this is a justification from the\r\n   user.\r\n\r\nAlso in this paper, we present the case for using the Analysis Services developed using CRISTAL as an avenue for long timescale data preservation. The tools are able to store the provenance metadata surrounding the analyses in a human readable form (XML). This is a light-weight and queryable manner of storing provenance as well as analysis results. In CRISTAL everything is recorded and nothing is thrown away. So another user would be able to replicate the experiment at a later date and time. Besides reproducibility of experiments, users can also share their experiments with other users using the provenance related information.\r\n\r\nThe analysis tools run currently in a GRID and a Cloud infrastructure. As well as collecting analysis provenance information, they are able to provenance of the infrastructure. There is strong novelty in this work which facilitates allows more precise reproducibility of experiments. This information is known as *infrastructure provenance*. It is currently being collected in the course of the N4U project. This infrastructure provenance can also be applied to HEP to aid in the reproducibility of results. For example, if performance is a factor it can be sent to the same compute node.\r\n\r\nConcerning the future of the analysis tools there is currently an emerging standard known as PROV which is used for *provenance interoperability*. In the near future, the analysis provenance information that we have collected with be exported to PROV. This work has already begun, we are looking for mapping patterns to aid in our cause. The reasoning for this is so that people can study and use provenance information in a standard and commonly understood format. This will also allow users to publish the provenance generated from their analyses onto the ever growing linked data cloud as well.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578727", "resources": [{"_type": "LocalFile", "name": "CHEP15_Presentation.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/364\/attachments\/578727\/796902\/CHEP15_Presentation.pdf", "fileName": "CHEP15_Presentation.pdf", "_fossil": "localFileMetadata", "id": "796902", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "97d37674e3c697321291e277d30b9878", "affiliation": "University of the West of England (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. MCCLATCHEY, Richard", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c873ab4fa4634a15e3902b42f319f3bf", "affiliation": "University of the West of England (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "BRANSON, Andrew", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "9d8538139802c1caa722e375f5acc49a", "affiliation": "University of the West of England (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "KOVACS, Zsolt", "id": "3"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/364", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:15:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "365", "speakers": [{"_type": "ContributionParticipation", "emailHash": "543ad8ae46ecd1be94327a51c4495526", "affiliation": "Acad. of Sciences of the Czech Rep. (CZ)", "_fossil": "contributionParticipationMetadata", "fullName": "CHUDOBA, Jiri", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "543ad8ae46ecd1be94327a51c4495526", "affiliation": "Acad. of Sciences of the Czech Rep. (CZ)", "_fossil": "contributionParticipationMetadata", "fullName": "CHUDOBA, Jiri", "id": "0"}], "title": "Distributed Computing for Pierre Auger Observatory", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T00:21:56.541082+00:00", "description": "", "title": "auger_chep2015_chudoba.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/365\/attachments\/578728\/796903\/auger_chep2015_chudoba.pdf", "filename": "auger_chep2015_chudoba.pdf", "content_type": "application\/pdf", "type": "file", "id": 796903, "size": 2215956}, {"_type": "attachment", "modified_dt": "2015-04-12T00:21:56.541082+00:00", "description": "", "title": "auger_chep2015_chudoba.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/365\/attachments\/578728\/796904\/auger_chep2015_chudoba.pptx", "filename": "auger_chep2015_chudoba.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796904, "size": 1364071}], "title": "Slides", "default_folder": false, "id": 578728, "description": ""}], "_type": "Contribution", "description": "Pierre Auger Observatory operates the largest system of detectors for ultra-high energy cosmic ray measurements. Comparison of theoretical models of interactions with recorded data requires thousands of computing cores for Monte Carlo simulations. Since 2007 distributed resources connected via EGI grid are succesfully used. The first and the second versions of production system based on bash scripts and MySQL database were able to submit jobs to all reliable sites supporting Virtual Organization auger. Many years VO auger belongs to top ten of EGI users based on the total used computing time.\r\n\r\nMigration of the production system to DIRAC interware started in 2014. Pilot jobs improve efficiency of computing jobs and  eliminate problems with small and less reliable sites used for the bulk production. The new system has also possibility to use available resources in clouds. Dirac File Catalog replaced LFC for new files, which are organized in datasets defined via metadata. CVMFS is used for software distribution since 2014. In the presentation we give a comparison of the old and new production system and report the experience from migration to the new system.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578728", "resources": [{"_type": "LocalFile", "name": "auger_chep2015_chudoba.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/365\/attachments\/578728\/796903\/auger_chep2015_chudoba.pdf", "fileName": "auger_chep2015_chudoba.pdf", "_fossil": "localFileMetadata", "id": "796903", "_deprecated": true}, {"_type": "LocalFile", "name": "auger_chep2015_chudoba.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/365\/attachments\/578728\/796904\/auger_chep2015_chudoba.pptx", "fileName": "auger_chep2015_chudoba.pptx", "_fossil": "localFileMetadata", "id": "796904", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/365", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "362", "speakers": [{"_type": "ContributionParticipation", "emailHash": "da4cac912389521f0838486a2a6ebd61", "affiliation": "STFC", "_fossil": "contributionParticipationMetadata", "fullName": "DE WITT, Shaun", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "da4cac912389521f0838486a2a6ebd61", "affiliation": "STFC", "_fossil": "contributionParticipationMetadata", "fullName": "DE WITT, Shaun", "id": "0"}], "title": "Storage Interface Usage at a Large, Multi-Experiment Teir1", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T08:19:28.477853+00:00", "description": "", "title": "Usage.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/362\/attachments\/578729\/796905\/Usage.pdf", "filename": "Usage.pdf", "content_type": "application\/pdf", "type": "file", "id": 796905, "size": 1677420}, {"_type": "attachment", "modified_dt": "2015-04-16T08:19:28.477853+00:00", "description": "", "title": "Usage.ppt", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/362\/attachments\/578729\/796906\/Usage.ppt", "filename": "Usage.ppt", "content_type": "application\/vnd.ms-powerpoint", "type": "file", "id": 796906, "size": 5042176}], "title": "Slides", "default_folder": false, "id": 578729, "description": ""}], "_type": "Contribution", "description": "Within WLCG much has been discussed concerning the possible demise of the Storage Resource Manager (SRM) and replacing it with different technologies such as XrootD and WebDAV.  Each of these storage interfaces presents different functionalities and experiments currently make use of all of these at different sites.  At the RAL Tier-1 we have been monitoring the usage of both SRM and XrootD by all the major experiments to assess when and if some of the nodes hosting the SRM should be redeployed to support XRootD.  \r\nInitial results were presented at ISGC2014 which showed the SRM still handles the majority of requests issued by most experiments (ALICE do not and have never used the SRM), but with an increasing usage of XrootD, particularly by ATLAS.  This poster will update these results based on several months of additional data and show that the SRM is still widely used by all the major WLCG VOs and all smaller experiments such as T2K and NA62.  We break down this usage according by read\/write and by geographic source to show how a large Tier 1 is used globally.  We also analyse usage by \u2018use case\u2019 (archival storage, persistent storage and scratch storage) and show how different experiments make use of the SRM.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578729", "resources": [{"_type": "LocalFile", "name": "Usage.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/362\/attachments\/578729\/796905\/Usage.pdf", "fileName": "Usage.pdf", "_fossil": "localFileMetadata", "id": "796905", "_deprecated": true}, {"_type": "LocalFile", "name": "Usage.ppt", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/362\/attachments\/578729\/796906\/Usage.ppt", "fileName": "Usage.ppt", "_fossil": "localFileMetadata", "id": "796906", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "145e08666b34652377403da49b8b2033", "affiliation": "STFC", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. REGGLER, Matthew", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/362", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:45:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "363", "speakers": [{"_type": "ContributionParticipation", "emailHash": "c510480795e217117b6157fb2c208fa7", "affiliation": "Forschungszentrum J\u00fclich", "_fossil": "contributionParticipationMetadata", "fullName": "BIANCHI, Ludovico", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c510480795e217117b6157fb2c208fa7", "affiliation": "Forschungszentrum J\u00fclich", "_fossil": "contributionParticipationMetadata", "fullName": "BIANCHI, Ludovico", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "70e8f64fb45d780c10e88165f40877ed", "affiliation": "Forschungszentrum J\u00fclich", "_fossil": "contributionParticipationMetadata", "fullName": "HERTEN, Andreas", "id": "1"}], "title": "Online tracking with GPUs at PANDA", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T19:59:32.093956+00:00", "description": "v1", "title": "bianchi-chep2015-v1", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/363\/attachments\/578730\/796907\/ebc698c41db8973b6f86dc6aa6e5c6dd.pdf", "filename": "ebc698c41db8973b6f86dc6aa6e5c6dd.pdf", "content_type": "application\/pdf", "type": "file", "id": 796907, "size": 21088400}], "title": "Slides", "default_folder": false, "id": 578730, "description": ""}], "_type": "Contribution", "description": "The PANDA experiment is a next generation particle detector planned for operation at the FAIR facility, currently under construction in Darmstadt, Germany. PANDA will detect events generated by colliding an antiproton beam on a fixed proton target, allowing studies in hadron spectroscopy, hypernuclei production, open charm and nucleon structure. \r\n\r\nThe nature of hadronic collisions means that signal and background events will look very similar, making a conventional approach, based on a hardware trigger signal generated by a subset of the detectors to start the data acquisition, unfeasible. \r\nInstead, data coming from the detector are acquired continuously, and all online selection is performed in real-time. A rejection factor of about 1000 is needed to reduce the data rate for offline storage, making the data acquisition system computationally very challenging. \r\n\r\nAdoption of Graphical Processing Units (GPUs) in many computing applications is increasing, due to their cost-effectiveness, performance, and accessible and versatile development using high-level programming paradigms such as CUDA or OpenCL. Applications of GPU within HEP include Monte Carlo production, analysis, low- and high-level trigger. \r\n\r\nOnline track reconstruction of charged particles plays an essential part in the event reconstruction and selection process. Our activity within the PANDA collaboration is centered on the development and implementation of particle tracking algorithms on GPUs, and on studying the possibility of performing online tracking using a multi-GPU architecture. Three algorithms are currently under development, using information from the PANDA tracking system: a Hough Transform; a Riemann Track Finder; and a Triplet Finder algorithm, a novel approach finely tuned for the PANDA STT detector. The algorithms are implemented on the GPU in the CUDA C language, utilizing low-level optimizations and non-trivial data packaging in order to exploit to the maximum the capabilities of GPUs. \r\n\r\nThis talk will present details of the implementation of these algorithms, together with first performance results, and solutions for data transfer to and from GPUs based on message queues for a deeper integration of the algorithms with the FairRoot and PandaRoot frameworks, both for online and offline applications.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578730", "resources": [{"_type": "LocalFile", "name": "bianchi-chep2015-v1", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/363\/attachments\/578730\/796907\/ebc698c41db8973b6f86dc6aa6e5c6dd.pdf", "fileName": "ebc698c41db8973b6f86dc6aa6e5c6dd.pdf", "_fossil": "localFileMetadata", "id": "796907", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "ec339599ef65410006f7ef7bba91ac25", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "RITMAN, James", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "56d8bec08e4b7dd9591b1bfc938bd36c", "affiliation": "Forschungszentrum J\u00fclich GmbH", "_fossil": "contributionParticipationMetadata", "fullName": "STOCKMANNS, Tobias", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "44c170cb0f219701d195868c32256066", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PLEITER, Dirk", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "b6b83e77816bdf127bdbf5beba6e9cb7", "affiliation": "J\u00fclich Supercomputing Centre, Forschungszentrum J\u00fclich", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ADINETZ, Andrew", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "ef33d261ca85a2bb721f60e324390c13", "affiliation": "NVIDIA GmbH, Germany", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. KRAUS, Jiri", "id": "6"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/363", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "360", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a11b77fc54a5a3fd19cf040a3c41e953", "affiliation": "Imperial College London", "_fossil": "contributionParticipationMetadata", "fullName": "MARTYNIAK, Janusz", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "a11b77fc54a5a3fd19cf040a3c41e953", "affiliation": "Imperial College London", "_fossil": "contributionParticipationMetadata", "fullName": "MARTYNIAK, Janusz", "id": "0"}], "title": "A Grid-based Batch Reconstruction Framework for MICE", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T10:44:52.887081+00:00", "description": "", "title": "poster_chep2015v2.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/360\/attachments\/578731\/796908\/poster_chep2015v2.pdf", "filename": "poster_chep2015v2.pdf", "content_type": "application\/pdf", "type": "file", "id": 796908, "size": 415647}, {"_type": "attachment", "modified_dt": "2015-04-11T10:44:52.887081+00:00", "description": "", "title": "poster_chep2015v2.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/360\/attachments\/578731\/796909\/poster_chep2015v2.pptx", "filename": "poster_chep2015v2.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796909, "size": 327273}], "title": "Slides", "default_folder": false, "id": 578731, "description": ""}], "_type": "Contribution", "description": "The international Muon Ionisation Cooling Experiment (MICE) is designed\r\nto demonstrate the principle of muon ionisation cooling for the first\r\ntime, for application to a future Neutrino Factory or Muon Collider. The\r\nexperiment is currently under construction at the ISIS synchrotron at\r\nthe Rutherford Appleton Laboratory, UK. As presently envisaged, the\r\nprogramme is divided into three Steps: characterisation of the muon\r\nbeams (complete), characterisation of the Cooling Channel and Absorbers\r\n(data-taking restarting in 2015-2016), and demonstration of Ionisation\r\nCooling (2017-2018).\r\n\r\nThe MICE Batch Reconstruction framework reconstructs all MICE data\r\nrecorded to date for a particular MICE Step, using a specified MAUS\r\nversion. Our job manager process holds a local SQLite database of MICE runs\r\nalready reconstructed with a given MAUS version; it repeatedly checks\r\nagainst the MICE Metadata DB and on identifying data runs that have not\r\nyet been reconstructed it will submit a job to the Grid to run MAUS\r\nagainst that data run. For this we select from those Tier2 sites that hold\r\na copy of the raw data. The reconstructed data are stored on the SE at the site\r\nwhere the job runs and registered in the LFC. On successful\r\nreconstruction, the Grid job registers an FTS transfer request with a\r\nseparate File Transfer Controller (Web Service based) to copy the reconstructed\r\ndata to the Castor SE at RAL, where they are stored on tape. This decouples the FTS\r\ntransfer request and monitoring from the Grid job. After the  output\r\nhas been transferred to the Castor SE at RAL, a corresponding record is\r\nadded to the MICE Metadata DB by the File Transfer Controller.\r\nThe MICE Metadata DB can then co-ordinate the further distribution of the\r\nreconstructed data across the Grid, including to HTTP-enabled SE's to\r\nallow MICE collaborators to access the data directly from any web browser.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578731", "resources": [{"_type": "LocalFile", "name": "poster_chep2015v2.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/360\/attachments\/578731\/796908\/poster_chep2015v2.pdf", "fileName": "poster_chep2015v2.pdf", "_fossil": "localFileMetadata", "id": "796908", "_deprecated": true}, {"_type": "LocalFile", "name": "poster_chep2015v2.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/360\/attachments\/578731\/796909\/poster_chep2015v2.pptx", "fileName": "poster_chep2015v2.pptx", "_fossil": "localFileMetadata", "id": "796909", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "94121529d24254dc4043ca0e31625a05", "affiliation": "Brunel University", "_fossil": "contributionParticipationMetadata", "fullName": "NEBRENSKY, Henry", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/360", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Track 8 Session", "keywords": [], "id": "361", "speakers": [{"_type": "ContributionParticipation", "emailHash": "573555c7eea26b662289bd639076bd77", "affiliation": "Brown University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "PIPEROV, Stefan", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "573555c7eea26b662289bd639076bd77", "affiliation": "Brown University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "PIPEROV, Stefan", "id": "0"}], "title": "Operational Experience with Opportunistic Use of Allocation Based HPC Facilities for CMS Data Processing.", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [], "_type": "Contribution", "description": "We describe the operational experience gained by the CMS experiment at CERN in integrating allocation based HPC resources into its computing infrastructure. Utilization of these resources allows us to perform tasks not necessarily suited - due to higher CPU or memory needs, for example - to CMS native resources. We have begun expanding our computing capabilities through the use of allocation-based super-computing facilities around the world. With the support of Brown University and Fermilab, we have obtained an allocation at DOE's NERSC super-computer facility at LBNL, and with UCSD - an allocation on the Gordon XSEDE cluster at SDSC. CMS used Bosco to access these resources, and a parrot glidein wrapper to bring in the CMS environment. With this the resources were made available to the CMS HTCondor\/glideinWMS pools in a way that allows additional future allocations at these centers to be readily used. The experience gained through building this capability for CMS, and its use for a variety of workflows is described here.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "068b9018b69ea40467e02d92d6a35844", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "HUFNAGEL, Dirk", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "8ce8233ffe359f39d3fdd125371fe45b", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MASON, David Alexander", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "f3deb9637a78bebf20c5c69407f5b777", "affiliation": "(Fermi National Accelerator Lab. (US))", "_fossil": "contributionParticipationMetadata", "fullName": "MAMBELLI, Marco", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "4feedad7c3fc85ee79b3b7413b634407", "affiliation": "Brown University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "NARAIN, Meenakshi", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "f41885e7dd46192ac2ccb0e10da21cb9", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "TIRADANI, Anthony Richard", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "e08dc867fcec64c9a59155301b0c2dce", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MALTA RODRIGUES, Alan", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "5ef39cdc5ffe3205efeb348aed6b90d5", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BAUERDICK, Lothar A.T.", "id": "7"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/361", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "380", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2d3197cea2de7705cbaf2df989cdf1da", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CLEMENCIC, Marco", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2d3197cea2de7705cbaf2df989cdf1da", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CLEMENCIC, Marco", "id": "0"}], "title": "Implementing a Domain Specific Language to configure and run LHCb Continuous Integration builds", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-08T13:47:26.131801+00:00", "description": "", "title": "clemencic_1.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/380\/attachments\/578732\/796910\/clemencic_1.pdf", "filename": "clemencic_1.pdf", "content_type": "application\/pdf", "type": "file", "id": 796910, "size": 1332803}], "title": "Poster", "default_folder": false, "id": 578732, "description": ""}], "_type": "Contribution", "description": "The new LHCb nightly build system described at CHEP 2013 was limited by the use of JSON files for its configuration. JSON had been chosen as a temporary solution to maintain backward compatibility towards the old XML format by means of a translation function.\r\n\r\nModern languages like Python leverage on meta-programming techniques to enable the development of Domain Specific Languages (DSLs).\r\n\r\nIn this contribution we will present the advantages of such techniques and how they have been used to implement a DSL that can be used to both describe the configuration of the LHCb Nightly Builds and actually operate them.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578732", "resources": [{"_type": "LocalFile", "name": "clemencic_1.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/380\/attachments\/578732\/796910\/clemencic_1.pdf", "fileName": "clemencic_1.pdf", "_fossil": "localFileMetadata", "id": "796910", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "244c8becc3b603c6e5722f14640d8e65", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "COUTURIER, Ben", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/380", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:30:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:45:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "381", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e71a4f9c822c9af7b8ab70c18eb4e640", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "PIPARO, Danilo", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e71a4f9c822c9af7b8ab70c18eb4e640", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "PIPARO, Danilo", "id": "0"}], "title": "ROOT6: a quest for performance", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T10:16:22.864011+00:00", "description": "", "title": "CHEP2015_ROOT6AQUESTFORPERFORMANCE.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/381\/attachments\/578733\/796911\/CHEP2015_ROOT6AQUESTFORPERFORMANCE.pdf", "filename": "CHEP2015_ROOT6AQUESTFORPERFORMANCE.pdf", "content_type": "application\/pdf", "type": "file", "id": 796911, "size": 7897916}], "title": "Slides", "default_folder": false, "id": 578733, "description": ""}], "_type": "Contribution", "description": "The sixth release cycle of ROOT is characterised by a radical modernisation in\r\nthe core software technologies the tookit relies on: language standard,\r\ninterpreter, hardware exploitation mechanisms.\r\nIf on the one hand, the change offered the opportunity of consolidating the\r\nexisting codebase, in presence of such innovations, maintaing the balance\r\nbetween full backward compatibility and software performance was not easy.\r\nIn this contribution we review the challenges and the solutions identified and\r\nimplemented in the area of CPU and memory consumption as well as I\/O\r\ncapabilities in terms of patterns.\r\nMoreover, we present some of the new ROOT components which are offered to the\r\nusers to improve the performance of third party applications.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578733", "resources": [{"_type": "LocalFile", "name": "CHEP2015_ROOT6AQUESTFORPERFORMANCE.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/381\/attachments\/578733\/796911\/CHEP2015_ROOT6AQUESTFORPERFORMANCE.pdf", "fileName": "CHEP2015_ROOT6AQUESTFORPERFORMANCE.pdf", "_fossil": "localFileMetadata", "id": "796911", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/381", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "382", "speakers": [{"_type": "ContributionParticipation", "emailHash": "74ee34b2c6310009f3cfe86b83279393", "affiliation": "Universite de Geneve (CH)", "_fossil": "contributionParticipationMetadata", "fullName": "DAPONTE, Vincenzo", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "74ee34b2c6310009f3cfe86b83279393", "affiliation": "Universite de Geneve (CH)", "_fossil": "contributionParticipationMetadata", "fullName": "DAPONTE, Vincenzo", "id": "1"}], "title": "HLT configuration management system", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T08:49:38.411731+00:00", "description": "", "title": "CHEP_2015_V4_CMS_rev4.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/382\/attachments\/578734\/796912\/CHEP_2015_V4_CMS_rev4.pdf", "filename": "CHEP_2015_V4_CMS_rev4.pdf", "content_type": "application\/pdf", "type": "file", "id": 796912, "size": 2492223}], "title": "Slides", "default_folder": false, "id": 578734, "description": ""}], "_type": "Contribution", "description": "The CMS High Level Trigger (HLT) is implemented running a streamlined version of the CMS offline reconstruction software running on thousands of CPUs. The CMS software is written mostly in C++, using Python as its configuration language through an embedded CPython interpreter. The configuration of each process is made up of hundreds of \"modules\", organized in \"sequences\" and \"paths\". As an example, the HLT configurations used for 2011 data taking comprised over 2200 different modules, organized in more than 400 independent trigger paths. The complexity of the HLT configurations and the large number of configuration produced require the design of a suitable data management system. The present work describes the designed solution to manage the considerable number of configurations developed and to assist the editing of new configurations. The system is required to be remotely accessible and OS-independent as well as easly maintainable easy to use. To meet these requirements a three-layers architecture has been choosen. On top of the \"ConfDB\" database a business logic manager has been introduced to handle the database operations, to perform the read and write rights check and to send a configuration to the user in a suitable format for the user interface. The graphical user interface (GUI) provides the features to display, modify and manage the configurations. The GUI design was carried out first by exposing paper sketches to the end-users and based on the their feedbacks a software mockup was implemented. At the end of the development process usability test will be carried out in order to measure the impact that the new GUI has on the development of configurations for the CMS-HLT.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578734", "resources": [{"_type": "LocalFile", "name": "CHEP_2015_V4_CMS_rev4.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/382\/attachments\/578734\/796912\/CHEP_2015_V4_CMS_rev4.pdf", "fileName": "CHEP_2015_V4_CMS_rev4.pdf", "_fossil": "localFileMetadata", "id": "796912", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/382", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "383", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ba3d551ec6b8db97e365962817383888", "affiliation": "Boston University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "RICHARDSON, Clint Allan", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ba3d551ec6b8db97e365962817383888", "affiliation": "Boston University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "RICHARDSON, Clint Allan", "id": "1"}], "title": "CMS High Level Trigger Timing Measurements", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T06:40:02.290655+00:00", "description": "", "title": "CHEP_2015_draft5.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/383\/attachments\/578735\/796913\/CHEP_2015_draft5.pdf", "filename": "CHEP_2015_draft5.pdf", "content_type": "application\/pdf", "type": "file", "id": 796913, "size": 1187743}], "title": "Slides", "default_folder": false, "id": 578735, "description": ""}], "_type": "Contribution", "description": "The two-level trigger system employed by CMS consists of the Level 1 (L1) Trigger, which is implemented using custom-built electronics, and the High Level Trigger (HLT), a farm of commercial cpus running a streamlined version of the offline CMS reconstruction software. The operational L1 output rate of 100 kHz, together with the number of cpus in the HLT farm, imposes a fundamental constraint on the amount of time available for the HLT to process events. Exceeding this limit impacts the experiment's ability to collect data efficiently. Hence, there is a critical need to characterize the performance of the HLT farm as well as the algorithms run prior to startup in order to ensure optimal data taking. Additional complications arise from the fact that the HLT farm consists of multiple generations of hardware and there can be subtleties in machine performance. We will present our methods of measuring the timing performance of the CMS HLT, including the challenges of making such measurements. Results for the performance of various intel Xeon architectures from 2009-2014 and different data taking scenarios will also be presented.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578735", "resources": [{"_type": "LocalFile", "name": "CHEP_2015_draft5.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/383\/attachments\/578735\/796913\/CHEP_2015_draft5.pdf", "fileName": "CHEP_2015_draft5.pdf", "_fossil": "localFileMetadata", "id": "796913", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/383", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "384", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2d3197cea2de7705cbaf2df989cdf1da", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CLEMENCIC, Marco", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e3f494a3dbd0bb0dc6f7a70d69427810", "affiliation": "National and Kapodistrian University of Athens (GR)", "_fossil": "contributionParticipationMetadata", "fullName": "KARACHALIOU, Anastasia", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "2d3197cea2de7705cbaf2df989cdf1da", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CLEMENCIC, Marco", "id": "1"}], "title": "Using DD4Hep through Gaudi for new experiments and LHCb", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T07:57:22.058753+00:00", "description": "", "title": "clemencic_2.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/384\/attachments\/578736\/796914\/clemencic_2.pdf", "filename": "clemencic_2.pdf", "content_type": "application\/pdf", "type": "file", "id": 796914, "size": 1556485}], "title": "Poster", "default_folder": false, "id": 578736, "description": ""}], "_type": "Contribution", "description": "The LHCb Software Framework Gaudi is a C++ software framework for HEP applications used by several experiments.\r\n\r\nAlthough Gaudi is extremely flexible and extensible, its adoption is limited by the lack of certain components that are fundamental for the software framework of an experiment, in particular a detector description framework, whose implementation is delegated to the adopters.\r\n\r\nTo enable future experiments to quickly adopt Gaudi, we integrated the DD4Hep toolkit in the existing software framework, and, as a proof of concept, we used it with the LHCb software applications, from simulation to reconstruction and analysis.\r\n\r\nWe will describe how the DD4Hep toolkit can be used by a new experiment, as well as how we can migrate an existing detector description framework to the new toolkit.", "track": "Track2: Offline software ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578736", "resources": [{"_type": "LocalFile", "name": "clemencic_2.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/384\/attachments\/578736\/796914\/clemencic_2.pdf", "fileName": "clemencic_2.pdf", "_fossil": "localFileMetadata", "id": "796914", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/384", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "385", "speakers": [{"_type": "ContributionParticipation", "emailHash": "244c8becc3b603c6e5722f14640d8e65", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "COUTURIER, Ben", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "2d3197cea2de7705cbaf2df989cdf1da", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CLEMENCIC, Marco", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "bb9d3f23d0402d48c1d8175b04bc4174", "affiliation": "National and Kapodistrian University of Athens (GR)", "_fossil": "contributionParticipationMetadata", "fullName": "QIRJAZI, Sofia", "id": "0"}], "title": "Improved interface for the LHCb Continuous Integration System", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-08T13:23:35.633235+00:00", "description": "", "title": "PosterCHEP-385.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/385\/attachments\/578737\/796915\/PosterCHEP-385.pdf", "filename": "PosterCHEP-385.pdf", "content_type": "application\/pdf", "type": "file", "id": 796915, "size": 2632315}], "title": "Slides", "default_folder": false, "id": 578737, "description": ""}], "_type": "Contribution", "description": "The purpose of this paper is to describe the steps that led to an improved interface for LHCb's Nightly Builds Dashboard. The goal was to have an efficient application that meets the needs of both the project developers, by providing them with a user friendly interface, as well as those of the computing team supporting the system by providing them with a dashboard allowing for better monitoring of build job themselves. In line with what is already used by LHCb, the web interface is implemented with the Flask Python framework for future maintainability and code clarity. The Database chosen to host the data is the schema-less CouchDB, serving the purpose of flexibility in document form changes. To improve the user experience, we use JavaScript libraries such as JQuery.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578737", "resources": [{"_type": "LocalFile", "name": "PosterCHEP-385.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/385\/attachments\/578737\/796915\/PosterCHEP-385.pdf", "fileName": "PosterCHEP-385.pdf", "_fossil": "localFileMetadata", "id": "796915", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "244c8becc3b603c6e5722f14640d8e65", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "COUTURIER, Ben", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "2d3197cea2de7705cbaf2df989cdf1da", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CLEMENCIC, Marco", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/385", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "386", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a6215b4b9fd5985eb6a95b24a590adcd", "affiliation": "INFN - Pisa", "_fossil": "contributionParticipationMetadata", "fullName": "Mrs. AREZZINI, Silvia", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "77adff23b18e3eb3eee03730a3853584", "affiliation": "INFN - Pisa", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. CARUSO, Giuseppe", "id": "0"}], "title": "Clusteralive", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [], "title": "Poster", "default_folder": false, "id": 578738, "description": ""}], "_type": "Contribution", "description": "\"CLUSTERALIVE\"\r\n\r\n\"Clusteralive\" is an integrated system developed in order to monitor and manage few important tasks in our HPC environment.\r\nWe have also other management systems, but now, with \u201cClusteralive\u201d we can know immediately, just seeing our screen, if Clusters are up and running and we are sure that the most important functionality are well instanced. \r\n\"Clusteralive\" is a php scripts suite able to monitor and perform tasks of automatic management about services, parameters and basic activities related in particular to computing nodes of the HPC clusters but also of the entire computing infrastructure. \r\nAt this link: http:\/\/farmsmon.pi.infn.it\/clusteralive\/monitoring.php you have a look at the specific application of \"Clusteralive\" dedicated to our principal projects. \r\nIn particular \"Clusteralive\" controls the cluster (dedicated to theoretical physics) called 'Zefiro' and funded by the SUMA project (SUper MAssive computing project, link: http:\/\/vh2.pi.infn.it\/ a special project approved by Italian Research Ministery). Zefiro (2048 cores total: AMD Opteron 6380-2.5GHz) consists of 32 machines each one with 512 GB of RAM and 4 processors (16 cores for processors fo 64 cores total, grouped into 2 jobslot). Nodes are linked via Infiniband QDR connections operated with Mellanox IS5100 switch with 108 ports. The accesses are regulated by the IBM LSF (V.9) scheduler. \r\n\"Clusteralive\" has also been recently extended to all the resources of the HPC in Pisa used for academic and industrial collaborations (more than 4000 computing cores total). \r\nThe monitoring system allows, via web browser, the view of essential informations about the status of each compute nodes and about a specific service and status of the entire HPC infrastructure. \r\nFor each computing node are displayed few informations like: the state (used \/ free \/ closed for maintenance), users that have assigned resources on it (used or reserved), information about the communication between the machines (ping) via Ethernet and via Infiniband, the percentage of used disk space, the used or reserved resources, the CPU load, the loked processes called zombie (due to a termination or to a bug in the application), daemons for user's autentication (nscd \/ nslcd), communication daemons (sshd) and node status daemons (gmond \/ hsflowd). \r\nFor the entire cluster  the status of jobs on the specific queues are shown , specifically the running\/pending and suspended jobs end in last two cases, it is possible to know reasons \r\nfor the specific status. The monitoring system also shows the physically turned off and turned on computing nodes and the nodes with the file system unmounted. \r\nThe automatic management system, has been developed for the recovery of some services in case of their malfunction. Specifically, an automatic restart of stopped and blocked services and an automatic closing system for all compute nodes in case of malfunction of the filesystem which supports the entire cluster HPC has been implemented togheter a system of automatic recovery of all the services necessary for communication between computing nodes and user identification. \r\nAn additional feature is going to be implemented and will permit the automatic cleaning of processes (zombie) after an unexpected termination or at the end of specific job.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "08569756a84e5a4bf3f8ae48b93d0aba", "affiliation": "INFN - Pisa", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CIAMPA, Alberto", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "a6215b4b9fd5985eb6a95b24a590adcd", "affiliation": "INFN - Pisa", "_fossil": "contributionParticipationMetadata", "fullName": "Mrs. AREZZINI, Silvia", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "11df57041761dd7dafbc48c8cec7bdff", "affiliation": "INFN - Pisa", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. FORMUSO, Antonino", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/386", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "387", "speakers": [{"_type": "ContributionParticipation", "emailHash": "64b7dcc505ab28db9fcbac74b1678ea3", "affiliation": "I.N.F.N. TORINO", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BAGNASCO, Stefano", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "64b7dcc505ab28db9fcbac74b1678ea3", "affiliation": "I.N.F.N. TORINO", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BAGNASCO, Stefano", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "2430dd1261f2b71f3c9c0b87ab014d22", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "VALLERO, Sara", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "adea190d287fd96f2a0245f8be8654b3", "affiliation": "INFN-TO", "_fossil": "contributionParticipationMetadata", "fullName": "LUSSO, Stefano", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "44593af2702121e8d18f7ac3492ccb16", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "MASERA, Massimo", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "69b973a64a9f8889a446059e1db57cff", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BERZANO, Dario", "id": "4"}], "title": "Managing competing elastic Grid and Cloud scientific computing applications using OpenNebula", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T01:00:17.687534+00:00", "description": "", "title": "posterCHEP2015_TorinoPRIN.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/387\/attachments\/578739\/796917\/posterCHEP2015_TorinoPRIN.pdf", "filename": "posterCHEP2015_TorinoPRIN.pdf", "content_type": "application\/pdf", "type": "file", "id": 796917, "size": 707389}], "title": "Poster", "default_folder": false, "id": 578739, "description": ""}], "_type": "Contribution", "description": "Elastic cloud computing applications, i.e. applications that automatically scale according to computing needs, work on the ideal assumption of infinite resources. While large public cloud infrastructures may be a reasonable approximation of this condition, scientific computing centres like WLCG Grid sites usually work in a saturated regime, in which applications compete for scarce resources through queues, priorities and scheduling policies, and keeping a fraction of the computing cores idle to allow for headroom is usually not an option.\r\nIn our particular environment one of the applications (a WLCG Tier-2 Grid site) is much larger than all the others, so a possible strategy is to make it shrink and release resources when smaller, higher priority ones require them.\r\nThe implementation of said model in our infrastructure, based on the OpenNebula cloud stack, will be described and the very\u00a0first operational experiences with a small number of strategies for timely allocation and release of resources will be\u00a0discussed.\r\nSuch strategies, which aim to be non-invasive with respect to our most common class of applications, i.e. Grid jobs, include for example the tuning of virtual Worker Node parameters (number of cores and lifetime) to match the statistical distribution of job durations, or the balance between the amount of resources statically pinned to an application and the amount left available for competitive seizing by elastic applications.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578739", "resources": [{"_type": "LocalFile", "name": "posterCHEP2015_TorinoPRIN.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/387\/attachments\/578739\/796917\/posterCHEP2015_TorinoPRIN.pdf", "fileName": "posterCHEP2015_TorinoPRIN.pdf", "_fossil": "localFileMetadata", "id": "796917", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/387", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "388", "speakers": [{"_type": "ContributionParticipation", "emailHash": "d2481eb1f11b08f40d4e1ef3cdb920dc", "affiliation": "University of Edinburgh (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "WASHBROOK, Andrew John", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "7c85d37398152148d60e922d4eda76d7", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "CROOKS, David", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "d2481eb1f11b08f40d4e1ef3cdb920dc", "affiliation": "University of Edinburgh (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "WASHBROOK, Andrew John", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "7c85d37398152148d60e922d4eda76d7", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "CROOKS, David", "id": "1"}], "title": "Development of site-oriented Analytics for Grid computing centres", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T10:35:50.352553+00:00", "description": "", "title": "CHEPAnalytics.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/388\/attachments\/578740\/796918\/CHEPAnalytics.pdf", "filename": "CHEPAnalytics.pdf", "content_type": "application\/pdf", "type": "file", "id": 796918, "size": 1071338}], "title": "Poster", "default_folder": false, "id": 578740, "description": ""}], "_type": "Contribution", "description": "The field of analytics, the process of analysing data to visualise meaningful patterns and trends, has become increasingly important to a wide range of scientific applications as the volume and variety of accessible data available to process (so called Big Data) has significantly increased. There are a number of scalable analytic platforms and services which have risen in prominence (such as Elasticsearch) which enable unstructured data from numerous sources to be gathered, curated and visualised through a single extensible interface. There is ongoing work in the HEP community evaluating these tools, for example in the augmentation of system management at regional computing centres. In this context the provisioning of analytic solutions for computing sites pledging resources to the Worldwide LHC Computing Grid (WLCG) is an area of considerable interest.\r\n \r\nEach Grid computing centre generates a wealth of monitoring data from multiple sources as part of their ongoing operations. These include system logging, Grid middleware services, LRMS scheduling information, network and storage utilisation and workload performance. This rich set of data is available for exploitation using analytics tools to enable post-facto diagnostics and a more comprehensive understanding of site systems, extending existing work on site monitoring. A site-oriented analytics portal would allow administrators to more easily leverage their available logging and monitoring data to determine the causes in variations in workload performance that may be unclear from a single data source.\r\n\r\nIn this study we will explore the components necessary for a WLCG site-oriented analytics platform. A corpus of relevant time-series based monitoring and logging data collected at two UK Grid computing centres (ECDF and Glasgow) will be categorised and stored. We will then explore the use of this data as part of a distributed analytics system. A necessary part of this work will be an examination of the appropriate level of visibility for different categories of site data. Furthermore, we will explore the extent to which machine learning techniques could be harnessed to provide predictive capability in error detection by using curated site data as a continuous training set.\r\n\r\nThis model is being developed with a particular focus on providing a solution for Grid computing centres rather than attempting to cover all data sources generated by a Virtual Organisation (VO).  Such an approach is intended to complement analytic development from larger VOs (such as the LHC experiments) whilst benefiting smaller VOs who may not have the resources available to develop these types of tools. Based on this work we will look towards areas of best practice in developing systems of this nature.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578740", "resources": [{"_type": "LocalFile", "name": "CHEPAnalytics.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/388\/attachments\/578740\/796918\/CHEPAnalytics.pdf", "fileName": "CHEPAnalytics.pdf", "_fossil": "localFileMetadata", "id": "796918", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "da6806efb2f919f20a8d4d30e47c621c", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "ROY, Gareth Douglas", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "b120fdf9f04a123ea43e83d2718cd2ef", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SKIPSEY, Samuel Cadellin", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "ec2d238aa9dcfee23b8c6dcb3cc714a4", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "QIN, Gang", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "9036c001235c207315f2f1cb8f8524b7", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. BRITTON, David", "id": "5"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/388", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:00:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "389", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2430dd1261f2b71f3c9c0b87ab014d22", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "VALLERO, Sara", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "64b7dcc505ab28db9fcbac74b1678ea3", "affiliation": "I.N.F.N. TORINO", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BAGNASCO, Stefano", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "2430dd1261f2b71f3c9c0b87ab014d22", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "VALLERO, Sara", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "adea190d287fd96f2a0245f8be8654b3", "affiliation": "INFN-TO", "_fossil": "contributionParticipationMetadata", "fullName": "LUSSO, Stefano", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "69b973a64a9f8889a446059e1db57cff", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BERZANO, Dario", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "44593af2702121e8d18f7ac3492ccb16", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "MASERA, Massimo", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "abd6e829ec54ae84aeb0447b91d40d2b", "affiliation": "Istituto Nazionale Fisica Nucleare (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "GUARISE, Andrea", "id": "5"}], "title": "Integrated Monitoring-as-a-service for Scientific Computing Cloud applications using the Elasticsearch ecosystem", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T10:01:29.311065+00:00", "description": "", "title": "Monitoring.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/389\/attachments\/578741\/796919\/Monitoring.pdf", "filename": "Monitoring.pdf", "content_type": "application\/pdf", "type": "file", "id": 796919, "size": 20237225}], "title": "Slides", "default_folder": false, "id": 578741, "description": ""}], "_type": "Contribution", "description": "The INFN computing centre in Torino hosts a private Cloud, which is managed with the OpenNebula cloud controller. The infrastructure offers IaaS services to different scientific computing applications. The main stakeholders of the facility are a grid Tier-2 site for the ALICE collaboration at LHC, an interactive analysis facility for the same experiment and a separate grid Tier-2 site for the BES-III collaboration, plus an increasing number of other smaller tenants. The dynamic allocation of resources to tenants is partially automated. This feature  requires detailed monitoring and accounting of the resource usage. \r\nWe set up a monitoring framework to inspect the site activities both in terms of IaaS and applications running on the hosted virtual instances. For this purpose we used the Elasticsearch, Logstash and Kibana stack. The infrastructure relies on an SQL database back-end for data preservation and to ensure flexibility to switch to a different monitoring solution if needed. The heterogeneous accounting information is transferred from the database to the Elasticsearch engine via a custom Logstash plugin. Each use-case is indexed separately in Elasticsearch and we setup a set of Kibana dashboards with pre-defined queries in order to monitor the relevant information in each case. \r\nFor the IaaS metering, we developed sensors for the OpenNebula API. The IaaS level information gathered through the API is sent to the MySQL database through an ad-hoc developed RESTful web service. Moreover, we have developed a billing system for our private Cloud, which relies on the RabbitMQ message queue for asynchronous communication to the database and on the ELK stack for its graphical interface.\r\nConcerning the application level, we used the Root plugin TProofMonSenderSQL to collect accounting data from the interactive analysis facility. The BES-III virtual instances used to be monitored with Zabbix, as a proof of concept we also retrieve the information contained in the Zabbix database. \r\nFinally, we have defined a model for monitoring-as-a-service, based on the tools described above, which the Cloud tenants can easily configure to suit their needs.\r\nIn this way we have achieved a uniform monitoring interface for both the IaaS and the scientific applications, mostly leveraging off-the-shelf tools.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578741", "resources": [{"_type": "LocalFile", "name": "Monitoring.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/389\/attachments\/578741\/796919\/Monitoring.pdf", "fileName": "Monitoring.pdf", "_fossil": "localFileMetadata", "id": "796919", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/389", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "02:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "02:15:00"}, "duration": 15, "session": "Plenary", "keywords": [], "id": "579", "speakers": [{"_type": "ContributionParticipation", "emailHash": "d41d8cd98f00b204e9800998ecf8427e", "affiliation": "OIST", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. DORFAN, Jonathan", "id": "0"}], "primaryauthors": [], "title": "HEP Computing: A Tradition of Scientific Leadership", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [], "_type": "Contribution", "description": "", "track": null, "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/579", "roomFullname": null}, {"startDate": {"date": "2015-04-17", "tz": "Europe\/Zurich", "time": "05:50:00"}, "endDate": {"date": "2015-04-17", "tz": "Europe\/Zurich", "time": "05:55:00"}, "duration": 5, "session": "Plenary", "keywords": [], "id": "578", "speakers": [{"_type": "ContributionParticipation", "emailHash": "c0f60cc34b7b1742a9071b2a2c685d26", "affiliation": "University of Tokyo (JP)", "_fossil": "contributionParticipationMetadata", "fullName": "SAKAMOTO, Hiroshi", "id": "0"}], "primaryauthors": [], "title": "Closing Address", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-17T01:22:55.037441+00:00", "description": "", "title": "closing.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/578\/attachments\/578742\/796920\/closing.pdf", "filename": "closing.pdf", "content_type": "application\/pdf", "type": "file", "id": 796920, "size": 5856530}, {"_type": "attachment", "modified_dt": "2015-04-17T01:22:55.037441+00:00", "description": "", "title": "closing.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/578\/attachments\/578742\/796921\/closing.pptx", "filename": "closing.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796921, "size": 6016891}], "title": "Slides", "default_folder": false, "id": 578742, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578742", "resources": [{"_type": "LocalFile", "name": "closing.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/578\/attachments\/578742\/796920\/closing.pdf", "fileName": "closing.pdf", "_fossil": "localFileMetadata", "id": "796920", "_deprecated": true}, {"_type": "LocalFile", "name": "closing.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/578\/attachments\/578742\/796921\/closing.pptx", "fileName": "closing.pptx", "_fossil": "localFileMetadata", "id": "796921", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/578", "roomFullname": null}, {"startDate": {"date": "2015-04-17", "tz": "Europe\/Zurich", "time": "02:50:00"}, "endDate": {"date": "2015-04-17", "tz": "Europe\/Zurich", "time": "03:15:00"}, "duration": 25, "session": "Plenary", "keywords": [], "id": "573", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2b4e5274e6b37fc772c4189e542d2d12", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BETEV, Latchezar", "id": "0"}], "primaryauthors": [], "title": "Track3 Summary", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T21:25:43.665143+00:00", "description": "", "title": "summary_track3_chep_2015_lb.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/573\/attachments\/578743\/796922\/summary_track3_chep_2015_lb.pdf", "filename": "summary_track3_chep_2015_lb.pdf", "content_type": "application\/pdf", "type": "file", "id": 796922, "size": 2198389}, {"_type": "attachment", "modified_dt": "2015-04-16T21:25:43.665143+00:00", "description": "", "title": "summary_track3_chep_2015_lb.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/573\/attachments\/578743\/796923\/summary_track3_chep_2015_lb.pptx", "filename": "summary_track3_chep_2015_lb.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796923, "size": 6170363}], "title": "Slides", "default_folder": false, "id": 578743, "description": ""}], "_type": "Contribution", "description": "", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578743", "resources": [{"_type": "LocalFile", "name": "summary_track3_chep_2015_lb.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/573\/attachments\/578743\/796922\/summary_track3_chep_2015_lb.pdf", "fileName": "summary_track3_chep_2015_lb.pdf", "_fossil": "localFileMetadata", "id": "796922", "_deprecated": true}, {"_type": "LocalFile", "name": "summary_track3_chep_2015_lb.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/573\/attachments\/578743\/796923\/summary_track3_chep_2015_lb.pptx", "fileName": "summary_track3_chep_2015_lb.pptx", "_fossil": "localFileMetadata", "id": "796923", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/573", "roomFullname": null}, {"startDate": {"date": "2015-04-17", "tz": "Europe\/Zurich", "time": "02:25:00"}, "endDate": {"date": "2015-04-17", "tz": "Europe\/Zurich", "time": "02:50:00"}, "duration": 25, "session": "Plenary", "keywords": [], "id": "572", "speakers": [{"_type": "ContributionParticipation", "emailHash": "4476ecb76f14659d9de3008f3371c157", "affiliation": "Johann-Wolfgang-Goethe Univ. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KISEL, Ivan", "id": "0"}], "primaryauthors": [], "title": "Track2 Summary", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T23:56:09.420273+00:00", "description": "", "title": "Kisel_CHEP2015_Track_2_Summary.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/572\/attachments\/578744\/796924\/Kisel_CHEP2015_Track_2_Summary.pdf", "filename": "Kisel_CHEP2015_Track_2_Summary.pdf", "content_type": "application\/pdf", "type": "file", "id": 796924, "size": 23480442}], "title": "Slides", "default_folder": false, "id": 578744, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578744", "resources": [{"_type": "LocalFile", "name": "Kisel_CHEP2015_Track_2_Summary.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/572\/attachments\/578744\/796924\/Kisel_CHEP2015_Track_2_Summary.pdf", "fileName": "Kisel_CHEP2015_Track_2_Summary.pdf", "_fossil": "localFileMetadata", "id": "796924", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/572", "roomFullname": null}, {"startDate": {"date": "2015-04-17", "tz": "Europe\/Zurich", "time": "02:00:00"}, "endDate": {"date": "2015-04-17", "tz": "Europe\/Zurich", "time": "02:25:00"}, "duration": 25, "session": "Plenary", "keywords": [], "id": "571", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a0edda8c32b7a5c82a1cf049171b0206", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BOCCI, Andrea", "id": "0"}], "primaryauthors": [], "title": "Track1 Summary", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T23:41:08.687780+00:00", "description": "", "title": "Track_1_summary.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/571\/attachments\/578745\/796925\/Track_1_summary.pdf", "filename": "Track_1_summary.pdf", "content_type": "application\/pdf", "type": "file", "id": 796925, "size": 14450734}], "title": "Slides", "default_folder": false, "id": 578745, "description": ""}], "_type": "Contribution", "description": "", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578745", "resources": [{"_type": "LocalFile", "name": "Track_1_summary.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/571\/attachments\/578745\/796925\/Track_1_summary.pdf", "fileName": "Track_1_summary.pdf", "_fossil": "localFileMetadata", "id": "796925", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/571", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "10:30:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "10:55:00"}, "duration": 25, "session": "Plenary", "keywords": [], "id": "570", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e71a4f9c822c9af7b8ab70c18eb4e640", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "PIPARO, Danilo", "id": "0"}], "primaryauthors": [], "title": "Track8 Summary", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T06:16:46.136031+00:00", "description": "", "title": "Track8Summary.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/570\/attachments\/578746\/796926\/Track8Summary.pdf", "filename": "Track8Summary.pdf", "content_type": "application\/pdf", "type": "file", "id": 796926, "size": 6449349}], "title": "Slides", "default_folder": false, "id": 578746, "description": ""}], "_type": "Contribution", "description": "", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578746", "resources": [{"_type": "LocalFile", "name": "Track8Summary.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/570\/attachments\/578746\/796926\/Track8Summary.pdf", "fileName": "Track8Summary.pdf", "_fossil": "localFileMetadata", "id": "796926", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/570", "roomFullname": null}, {"startDate": {"date": "2015-04-17", "tz": "Europe\/Zurich", "time": "05:30:00"}, "endDate": {"date": "2015-04-17", "tz": "Europe\/Zurich", "time": "05:50:00"}, "duration": 20, "session": "Plenary", "keywords": [], "id": "577", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a0fcddc3759379b2bdf0e359fabd7e04", "affiliation": "SLAC National Accelerator  Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MOUNT, Richard Philip", "id": "0"}], "primaryauthors": [], "title": "Invitation to CHEP2016", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-17T00:24:05.664427+00:00", "description": "", "title": "Mount-CHEP2016-Invitation-v02c.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/577\/attachments\/578747\/796927\/Mount-CHEP2016-Invitation-v02c.pdf", "filename": "Mount-CHEP2016-Invitation-v02c.pdf", "content_type": "application\/pdf", "type": "file", "id": 796927, "size": 2841223}], "title": "Slides", "default_folder": false, "id": 578747, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578747", "resources": [{"_type": "LocalFile", "name": "Mount-CHEP2016-Invitation-v02c.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/577\/attachments\/578747\/796927\/Mount-CHEP2016-Invitation-v02c.pdf", "fileName": "Mount-CHEP2016-Invitation-v02c.pdf", "_fossil": "localFileMetadata", "id": "796927", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/577", "roomFullname": null}, {"startDate": {"date": "2015-04-17", "tz": "Europe\/Zurich", "time": "04:35:00"}, "endDate": {"date": "2015-04-17", "tz": "Europe\/Zurich", "time": "05:30:00"}, "duration": 55, "session": "Plenary", "keywords": [], "id": "576", "speakers": [{"_type": "ContributionParticipation", "emailHash": "105ddff0ca7c9692f4c25ca934c68b5d", "affiliation": "Tokyo Institute of Technology (JP)", "_fossil": "contributionParticipationMetadata", "fullName": "KOBAYASHI, Dai", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "84f3f721370dba12b47a611b1a36081e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SAILER, Andre", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "7d07f865b38767d4d2ed9895ebae0697", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "HAEN, Christophe", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "5984f6179c1328f2e8b6e50d2d0017fd", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "KATO, Yuji", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "3c0927d8d883c0c7337a19519dcd5670", "affiliation": "University of Notre Dame (US)", "_fossil": "contributionParticipationMetadata", "fullName": "WOODARD, Anna Elizabeth", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "7bc813d53aa9b6d7a3c4fca8b5484de9", "affiliation": "Georg-August-Universitaet Goettingen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. MAGRADZE, Erekle", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "fd06959b93c405340e9900abf962beb7", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "LAHIFF, Andrew David", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "c529a955e937a762b4f9e978e65fafee", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SANTOGIDIS, Aram", "id": "7"}], "primaryauthors": [], "title": "Lightning Talks", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [], "_type": "Contribution", "description": "", "track": null, "material": [], "coauthors": [], "subContributions": [{"folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:00:00+00:00", "description": "", "title": "http:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/55", "link_url": "http:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/55", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/0\/attachments\/578749\/796929\/go", "type": "link", "id": 796929}], "title": "Poster", "default_folder": false, "id": 578749, "description": ""}, {"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-17T02:20:04.953579+00:00", "description": "", "title": "CHEP20150417_2.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/0\/attachments\/578748\/796928\/CHEP20150417_2.pdf", "filename": "CHEP20150417_2.pdf", "content_type": "application\/pdf", "type": "file", "id": 796928, "size": 5168593}], "title": "Slides", "default_folder": false, "id": 578748, "description": ""}], "speakers": [{"_type": "SubContribParticipation", "emailHash": "105ddff0ca7c9692f4c25ca934c68b5d", "affiliation": "Tokyo Institute of Technology (JP)", "_fossil": "contributionParticipationMetadata", "fullName": "KOBAYASHI, Dai", "id": "0"}], "title": "Poster award winner's lightning talk - Track 1", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578748", "resources": [{"_type": "LocalFile", "name": "CHEP20150417_2.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/0\/attachments\/578748\/796928\/CHEP20150417_2.pdf", "fileName": "CHEP20150417_2.pdf", "_fossil": "localFileMetadata", "id": "796928", "_deprecated": true}], "_deprecated": true}, {"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578749", "resources": [{"_fossil": "linkMetadata", "url": "http:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/55", "_type": "Link", "name": "http:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/55", "_deprecated": true}], "_deprecated": true}], "_type": "SubContribution", "note": {}, "_fossil": "subContributionMetadata", "duration": 5, "id": "0"}, {"folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:00:00+00:00", "description": "", "title": "http:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/290", "link_url": "http:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/290", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/1\/attachments\/578751\/796931\/go", "type": "link", "id": 796931}], "title": "Poster", "default_folder": false, "id": 578751, "description": ""}, {"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-17T00:51:08.757907+00:00", "description": "", "title": "Track2_Sailer_DD4hepAndLC.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/1\/attachments\/578750\/796930\/Track2_Sailer_DD4hepAndLC.pdf", "filename": "Track2_Sailer_DD4hepAndLC.pdf", "content_type": "application\/pdf", "type": "file", "id": 796930, "size": 5520187}], "title": "Slides", "default_folder": false, "id": 578750, "description": ""}], "speakers": [{"_type": "SubContribParticipation", "emailHash": "84f3f721370dba12b47a611b1a36081e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SAILER, Andre", "id": "0"}], "title": "Poster award winner's lightning talk - Track 2", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578751", "resources": [{"_fossil": "linkMetadata", "url": "http:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/290", "_type": "Link", "name": "http:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/290", "_deprecated": true}], "_deprecated": true}, {"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578750", "resources": [{"_type": "LocalFile", "name": "Track2_Sailer_DD4hepAndLC.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/1\/attachments\/578750\/796930\/Track2_Sailer_DD4hepAndLC.pdf", "fileName": "Track2_Sailer_DD4hepAndLC.pdf", "_fossil": "localFileMetadata", "id": "796930", "_deprecated": true}], "_deprecated": true}], "_type": "SubContribution", "note": {}, "_fossil": "subContributionMetadata", "duration": 5, "id": "1"}, {"folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:00:00+00:00", "description": "", "title": "http:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/325", "link_url": "http:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/325", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/2\/attachments\/578753\/796933\/go", "type": "link", "id": 796933}], "title": "Poster", "default_folder": false, "id": 578753, "description": ""}, {"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-17T01:02:40.731770+00:00", "description": "", "title": "Track3_CHEP2015-LHCbDMS-Poster-Slides.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/2\/attachments\/578752\/796932\/Track3_CHEP2015-LHCbDMS-Poster-Slides.pptx", "filename": "Track3_CHEP2015-LHCbDMS-Poster-Slides.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796932, "size": 860879}], "title": "Slides", "default_folder": false, "id": 578752, "description": ""}], "speakers": [{"_type": "SubContribParticipation", "emailHash": "7d07f865b38767d4d2ed9895ebae0697", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "HAEN, Christophe", "id": "0"}], "title": "Poster award winner's lightning talk - Track 3", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578753", "resources": [{"_fossil": "linkMetadata", "url": "http:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/325", "_type": "Link", "name": "http:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/325", "_deprecated": true}], "_deprecated": true}, {"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578752", "resources": [{"_type": "LocalFile", "name": "Track3_CHEP2015-LHCbDMS-Poster-Slides.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/2\/attachments\/578752\/796932\/Track3_CHEP2015-LHCbDMS-Poster-Slides.pptx", "fileName": "Track3_CHEP2015-LHCbDMS-Poster-Slides.pptx", "_fossil": "localFileMetadata", "id": "796932", "_deprecated": true}], "_deprecated": true}], "_type": "SubContribution", "note": {}, "_fossil": "subContributionMetadata", "duration": 5, "id": "2"}, {"folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:00:00+00:00", "description": "", "title": "http:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/337", "link_url": "http:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/337", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/3\/attachments\/578755\/796935\/go", "type": "link", "id": 796935}], "title": "Poster", "default_folder": false, "id": 578755, "description": ""}, {"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T23:44:13.632760+00:00", "description": "", "title": "chep2015_lightning_yujikato.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/3\/attachments\/578754\/796934\/chep2015_lightning_yujikato.pdf", "filename": "chep2015_lightning_yujikato.pdf", "content_type": "application\/pdf", "type": "file", "id": 796934, "size": 747464}], "title": "Slides", "default_folder": false, "id": 578754, "description": ""}], "speakers": [{"_type": "SubContribParticipation", "emailHash": "5984f6179c1328f2e8b6e50d2d0017fd", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "KATO, Yuji", "id": "0"}], "title": "Poster award winner's lightning talk - Track 4", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578754", "resources": [{"_type": "LocalFile", "name": "chep2015_lightning_yujikato.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/3\/attachments\/578754\/796934\/chep2015_lightning_yujikato.pdf", "fileName": "chep2015_lightning_yujikato.pdf", "_fossil": "localFileMetadata", "id": "796934", "_deprecated": true}], "_deprecated": true}, {"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578755", "resources": [{"_fossil": "linkMetadata", "url": "http:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/337", "_type": "Link", "name": "http:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/337", "_deprecated": true}], "_deprecated": true}], "_type": "SubContribution", "note": {}, "_fossil": "subContributionMetadata", "duration": 5, "id": "3"}, {"folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:00:00+00:00", "description": "", "title": "http:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/124", "link_url": "http:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/124", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/4\/attachments\/578757\/796937\/go", "type": "link", "id": 796937}], "title": "Poster", "default_folder": false, "id": 578757, "description": ""}, {"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-17T01:48:54.195879+00:00", "description": "", "title": "CHEP2015_talk.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/4\/attachments\/578756\/796936\/CHEP2015_talk.pdf", "filename": "CHEP2015_talk.pdf", "content_type": "application\/pdf", "type": "file", "id": 796936, "size": 612103}], "title": "Slides", "default_folder": false, "id": 578756, "description": ""}], "speakers": [{"_type": "SubContribParticipation", "emailHash": "3c0927d8d883c0c7337a19519dcd5670", "affiliation": "University of Notre Dame (US)", "_fossil": "contributionParticipationMetadata", "fullName": "WOODARD, Anna Elizabeth", "id": "0"}], "title": "Poster award winner's lightning talk - Track 5", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578756", "resources": [{"_type": "LocalFile", "name": "CHEP2015_talk.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/4\/attachments\/578756\/796936\/CHEP2015_talk.pdf", "fileName": "CHEP2015_talk.pdf", "_fossil": "localFileMetadata", "id": "796936", "_deprecated": true}], "_deprecated": true}, {"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578757", "resources": [{"_fossil": "linkMetadata", "url": "http:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/124", "_type": "Link", "name": "http:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/124", "_deprecated": true}], "_deprecated": true}], "_type": "SubContribution", "note": {}, "_fossil": "subContributionMetadata", "duration": 5, "id": "4"}, {"folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:00:00+00:00", "description": "", "title": "http:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/18", "link_url": "http:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/18", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/5\/attachments\/578759\/796939\/go", "type": "link", "id": 796939}], "title": "Poster", "default_folder": false, "id": 578759, "description": ""}, {"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-19T05:13:42.842849+00:00", "description": "", "title": "ErekleMagradzeCHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/5\/attachments\/578758\/796938\/ErekleMagradzeCHEP2015.pdf", "filename": "ErekleMagradzeCHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796938, "size": 1964631}], "title": "Slides", "default_folder": false, "id": 578758, "description": ""}], "speakers": [{"_type": "SubContribParticipation", "emailHash": "7bc813d53aa9b6d7a3c4fca8b5484de9", "affiliation": "Georg-August-Universitaet Goettingen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. MAGRADZE, Erekle", "id": "0"}], "title": "Poster award winner's lightning talk - Track 6", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578758", "resources": [{"_type": "LocalFile", "name": "ErekleMagradzeCHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/5\/attachments\/578758\/796938\/ErekleMagradzeCHEP2015.pdf", "fileName": "ErekleMagradzeCHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796938", "_deprecated": true}], "_deprecated": true}, {"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578759", "resources": [{"_fossil": "linkMetadata", "url": "http:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/18", "_type": "Link", "name": "http:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/18", "_deprecated": true}], "_deprecated": true}], "_type": "SubContribution", "note": {}, "_fossil": "subContributionMetadata", "duration": 5, "id": "5"}, {"folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:00:00+00:00", "description": "", "title": "http:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/452", "link_url": "http:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/452", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/6\/attachments\/578761\/796941\/go", "type": "link", "id": 796941}], "title": "Poster", "default_folder": false, "id": 578761, "description": ""}, {"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-17T05:50:33.470356+00:00", "description": "", "title": "BatchCloudSameResources_Lightning.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/6\/attachments\/578760\/796940\/BatchCloudSameResources_Lightning.pdf", "filename": "BatchCloudSameResources_Lightning.pdf", "content_type": "application\/pdf", "type": "file", "id": 796940, "size": 548582}], "title": "Slides", "default_folder": false, "id": 578760, "description": ""}], "speakers": [{"_type": "SubContribParticipation", "emailHash": "fd06959b93c405340e9900abf962beb7", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "LAHIFF, Andrew David", "id": "0"}], "title": "Poster award winner's lightning talk - Track 7", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578760", "resources": [{"_type": "LocalFile", "name": "BatchCloudSameResources_Lightning.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/6\/attachments\/578760\/796940\/BatchCloudSameResources_Lightning.pdf", "fileName": "BatchCloudSameResources_Lightning.pdf", "_fossil": "localFileMetadata", "id": "796940", "_deprecated": true}], "_deprecated": true}, {"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578761", "resources": [{"_fossil": "linkMetadata", "url": "http:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/452", "_type": "Link", "name": "http:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/452", "_deprecated": true}], "_deprecated": true}], "_type": "SubContribution", "note": {}, "_fossil": "subContributionMetadata", "duration": 5, "id": "6"}, {"folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:00:00+00:00", "description": "", "title": "http:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/27", "link_url": "http:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/27", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/7\/attachments\/578763\/796943\/go", "type": "link", "id": 796943}], "title": "Poster", "default_folder": false, "id": 578763, "description": ""}, {"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-17T02:15:40.538381+00:00", "description": "", "title": "Track8_Aram_Santogidis.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/7\/attachments\/578762\/796942\/Track8_Aram_Santogidis.pptx", "filename": "Track8_Aram_Santogidis.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796942, "size": 1292913}], "title": "Slides", "default_folder": false, "id": 578762, "description": ""}], "speakers": [{"_type": "SubContribParticipation", "emailHash": "c529a955e937a762b4f9e978e65fafee", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SANTOGIDIS, Aram", "id": "0"}], "title": "Poster award winner's lightning talk - Track 8", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578763", "resources": [{"_fossil": "linkMetadata", "url": "http:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/27", "_type": "Link", "name": "http:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/27", "_deprecated": true}], "_deprecated": true}, {"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578762", "resources": [{"_type": "LocalFile", "name": "Track8_Aram_Santogidis.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576\/7\/attachments\/578762\/796942\/Track8_Aram_Santogidis.pptx", "fileName": "Track8_Aram_Santogidis.pptx", "_fossil": "localFileMetadata", "id": "796942", "_deprecated": true}], "_deprecated": true}], "_type": "SubContribution", "note": {}, "_fossil": "subContributionMetadata", "duration": 5, "id": "7"}], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/576", "roomFullname": null}, {"startDate": {"date": "2015-04-17", "tz": "Europe\/Zurich", "time": "04:10:00"}, "endDate": {"date": "2015-04-17", "tz": "Europe\/Zurich", "time": "04:35:00"}, "duration": 25, "session": "Plenary", "keywords": [], "id": "575", "speakers": [{"_type": "ContributionParticipation", "emailHash": "cbb0d28d39b1ca6e5d447a3e19e0e3c7", "affiliation": "Centro de Investigaciones Energ. Medioambientales y Tecn. - (ES", "_fossil": "contributionParticipationMetadata", "fullName": "FLIX MOLINA, Jose", "id": "0"}], "primaryauthors": [], "title": "Track6 Summary", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-17T00:51:08.121730+00:00", "description": "", "title": "SummaryTrack6.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/575\/attachments\/578764\/796944\/SummaryTrack6.pdf", "filename": "SummaryTrack6.pdf", "content_type": "application\/pdf", "type": "file", "id": 796944, "size": 11011350}], "title": "Slides", "default_folder": false, "id": 578764, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578764", "resources": [{"_type": "LocalFile", "name": "SummaryTrack6.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/575\/attachments\/578764\/796944\/SummaryTrack6.pdf", "fileName": "SummaryTrack6.pdf", "_fossil": "localFileMetadata", "id": "796944", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/575", "roomFullname": null}, {"startDate": {"date": "2015-04-17", "tz": "Europe\/Zurich", "time": "03:45:00"}, "endDate": {"date": "2015-04-17", "tz": "Europe\/Zurich", "time": "04:10:00"}, "duration": 25, "session": "Plenary", "keywords": [], "id": "574", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2d3197cea2de7705cbaf2df989cdf1da", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CLEMENCIC, Marco", "id": "0"}], "primaryauthors": [], "title": "Track4 Summary", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-17T00:03:18.148616+00:00", "description": "", "title": "Track_4_Summary.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/574\/attachments\/578765\/796945\/Track_4_Summary.pdf", "filename": "Track_4_Summary.pdf", "content_type": "application\/pdf", "type": "file", "id": 796945, "size": 5877243}], "title": "Slides", "default_folder": false, "id": 578765, "description": ""}], "_type": "Contribution", "description": "", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578765", "resources": [{"_type": "LocalFile", "name": "Track_4_Summary.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/574\/attachments\/578765\/796945\/Track_4_Summary.pdf", "fileName": "Track_4_Summary.pdf", "_fossil": "localFileMetadata", "id": "796945", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/574", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "60", "speakers": [{"_type": "ContributionParticipation", "emailHash": "4daea3ac567431a3fc62a5ca910a9e74", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CERMINARA, Gianluca", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "4daea3ac567431a3fc62a5ca910a9e74", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CERMINARA, Gianluca", "id": "0"}], "title": "Automated workflows for critical time-dependent calibrations at the CMS experiment.", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T13:26:51.964348+00:00", "description": "", "title": "2015-04_Poster_CHEP_v3.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/60\/attachments\/578766\/796946\/2015-04_Poster_CHEP_v3.pdf", "filename": "2015-04_Poster_CHEP_v3.pdf", "content_type": "application\/pdf", "type": "file", "id": 796946, "size": 4150236}], "title": "Poster", "default_folder": false, "id": 578766, "description": ""}], "_type": "Contribution", "description": "Fast and efficient methods for the calibration and the alignment of the detector are a key asset to exploit the physics potential of the Compact Muon Solenoid (CMS) detector and to ensure timely preparation of results for conferences and publications.\r\n\r\nTo achieve this goal, the CMS experiment has set up a powerful framework. This includes automated workflows in the context of a prompt calibration concept, which allows for a quick turnaround of the calibration process following as fast as possible any change in running conditions.\r\n\r\nThe presentation will review the design and operational experience of these workflows and the related monitoring system during the LHC RunI and focus on the development, deployment and commissioning in preparation of RunII.", "track": "Track2: Offline software ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578766", "resources": [{"_type": "LocalFile", "name": "2015-04_Poster_CHEP_v3.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/60\/attachments\/578766\/796946\/2015-04_Poster_CHEP_v3.pdf", "fileName": "2015-04_Poster_CHEP_v3.pdf", "_fossil": "localFileMetadata", "id": "796946", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/60", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:15:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "61", "speakers": [{"_type": "ContributionParticipation", "emailHash": "0ccb5305e98ea446dc37370c1c25d9db", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FERREIRA, Pedro", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "0ccb5305e98ea446dc37370c1c25d9db", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FERREIRA, Pedro", "id": "0"}], "title": "Indico - the road to 2.0", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T08:54:33.025098+00:00", "description": "", "title": "indico_chep2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/61\/attachments\/578767\/796947\/indico_chep2015.pdf", "filename": "indico_chep2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796947, "size": 5927369}], "title": "Slides", "default_folder": false, "id": 578767, "description": ""}], "_type": "Contribution", "description": "Indico has come a long way since it was first used to organize CHEP 2004.\r\nMore than ten years of development have brought new features and projects, widening the application's feature set and enabling event organizers to work even more efficiently. While this has boosted the tool's usage and facilitated its adoption by a remarkable 300,000 events (at CERN only), it has also generated a whole new range of challenges, which have been the target of the team's attention for the last 2 years. One of them was that of scalability and the maintainability of the current database solution (ZODB).\r\n \r\nAfter careful consideration, the decision was taken to move away from ZODB to PostgreSQL, a relational and widely-adopted solution that will permit the development of a more ambitious feature set as well as improved performance and scalability. A change of this type is by no means trivial in nature and requires the refactoring of most backend code as well as the full rewrite of significant portions of it. We are taking this opportunity to modernize Indico, by employing standard web modules, technologies and concepts that not only make development and maintenance easier but also constitute an upgrade to Indico's stack. The first results are already visible since August 2014, with the full migration of the Room Booking module to the new paradigm.\r\n \r\nIn this paper we explain what has been done so far in the context of this ambitious migration, what have been the main findings and challenges, as well as the main technologies and concepts that will constitute the foundation of the resultant Indico 2.0.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578767", "resources": [{"_type": "LocalFile", "name": "indico_chep2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/61\/attachments\/578767\/796947\/indico_chep2015.pdf", "fileName": "indico_chep2015.pdf", "_fossil": "localFileMetadata", "id": "796947", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/61", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:15:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:30:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "62", "speakers": [{"_type": "ContributionParticipation", "emailHash": "7b0a240f7a59d03e34101d2917c82149", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. CORREIA FERNANDES, Joao", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "7b0a240f7a59d03e34101d2917c82149", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. CORREIA FERNANDES, Joao", "id": "0"}], "title": "Vidyo@CERN: A Service Update", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T05:18:28.407416+00:00", "description": "", "title": "CERN_Vidyo_CHEP_2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/62\/attachments\/578768\/796948\/CERN_Vidyo_CHEP_2015.pdf", "filename": "CERN_Vidyo_CHEP_2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796948, "size": 2148446}, {"_type": "attachment", "modified_dt": "2015-04-15T05:18:28.407416+00:00", "description": "", "title": "CERN_Vidyo_CHEP_2015.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/62\/attachments\/578768\/796949\/CERN_Vidyo_CHEP_2015.pptx", "filename": "CERN_Vidyo_CHEP_2015.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796949, "size": 4739823}], "title": "Slides", "default_folder": false, "id": 578768, "description": ""}], "_type": "Contribution", "description": "We will present an overview of the current real-time video service offering for the LHC, in particular the operation of the CERN Vidyo service will be described in terms of consolidated performance and scale: The service is an increasingly critical part of the daily activity of the LHC collaborations, topping recently more than 50 million minutes of communication in one year, with peaks of up to 852 simultaneous connections. We will elaborate on the improvement of some front-end key features such as the integration with CERN Indico, or the enhancements of the Unified Client and also on new ones, released or in the pipeline, such as a new WebRTC client and CERN SSO\/Federated SSO integration. An overview of future infrastructure improvements, such as virtualization techniques of Vidyo routers and geo-location mechanisms for load-balancing and optimum user distribution across the service infrastructure will also be discussed. The work done by CERN to improve the monitoring of its Vidyo network will also be presented and demoed.\r\nAs a last point, we will touch the roadmap and strategy established by CERN and Vidyo with a clear objective of optimizing the service both on the end client and backend infrastructure to make it truly universal, to serve Global Science; that includes the introduction of the multi-tenant concept to serve different communities, as the follow up of CERN\u2019s decision to offer the Vidyo service currently operated for the LHC, to other Sciences, Institutions and Virtual Organizations beyond HEP that might express interest for it.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578768", "resources": [{"_type": "LocalFile", "name": "CERN_Vidyo_CHEP_2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/62\/attachments\/578768\/796948\/CERN_Vidyo_CHEP_2015.pdf", "fileName": "CERN_Vidyo_CHEP_2015.pdf", "_fossil": "localFileMetadata", "id": "796948", "_deprecated": true}, {"_type": "LocalFile", "name": "CERN_Vidyo_CHEP_2015.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/62\/attachments\/578768\/796949\/CERN_Vidyo_CHEP_2015.pptx", "fileName": "CERN_Vidyo_CHEP_2015.pptx", "_fossil": "localFileMetadata", "id": "796949", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/62", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:45:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "259", "speakers": [{"_type": "ContributionParticipation", "emailHash": "b2b26eb62c8cf5580930e0784a6020ea", "affiliation": "BNL", "_fossil": "contributionParticipationMetadata", "fullName": "YU, David", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "b2b26eb62c8cf5580930e0784a6020ea", "affiliation": "BNL", "_fossil": "contributionParticipationMetadata", "fullName": "YU, David", "id": "0"}], "title": "Deep Storage for Big Scientific Data", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T01:16:52.994984+00:00", "description": "", "title": "CHEP2015_BNL_DSFBSD_v7.ppt", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/259\/attachments\/578769\/796950\/CHEP2015_BNL_DSFBSD_v7.ppt", "filename": "CHEP2015_BNL_DSFBSD_v7.ppt", "content_type": "application\/vnd.ms-powerpoint", "type": "file", "id": 796950, "size": 29568512}], "title": "Slides", "default_folder": false, "id": 578769, "description": ""}], "_type": "Contribution", "description": "Brookhaven National Lab (BNL)\u2019s RHIC and Atlas Computing Facility (RACF), is supporting science experiments such as RHIC as its Tier-0 center and the U.S. ATLAS\/LHC as a Tier-1 center. Scientific data is still growing exponentially after each upgrade. The RACF currently manages over 50 petabytes of data on robotic tape libraries, and we expect a 50% increase in data next year. Not only do we have to address the issue of efficiently archiving high bandwidth data to our tapes, but we also have to face the problem of randomly restoring files from tapes. In addition, we have to manage tape resource usage and technology migration, which is moving data from low-capacity media to newer, high-capacity tape media, in order to free space within a tape library. BNL\u2019s mass storage system is managed by a software called IBM HPSS. To restore files from HPSS, we have developed a file retrieval scheduling software, called TSX. TSX provides dynamic HPSS resource management, schedules jobs efficiently, and enhances visibility of real-time staging activities and advanced error handling to maximize the tape staging performance.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578769", "resources": [{"_type": "LocalFile", "name": "CHEP2015_BNL_DSFBSD_v7.ppt", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/259\/attachments\/578769\/796950\/CHEP2015_BNL_DSFBSD_v7.ppt", "fileName": "CHEP2015_BNL_DSFBSD_v7.ppt", "_fossil": "localFileMetadata", "id": "796950", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "c918e3836529ec26b3c3b6fc738578cb", "affiliation": "BNL", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. NOVAKOV, Ognian", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "796fae3811832333c657eb48d5ee63b8", "affiliation": "BNL", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. CHOU, Tim", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "6709171323f81bd346f8ad977cee0fd9", "affiliation": "BNL", "_fossil": "contributionParticipationMetadata", "fullName": "Mrs. CHE, Guangwei", "id": "3"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/259", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "64", "speakers": [{"_type": "ContributionParticipation", "emailHash": "96cb4a6a9a1a85aa56b1d32a3426a372", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CANO, Eric", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "96cb4a6a9a1a85aa56b1d32a3426a372", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CANO, Eric", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "060cad0e10f381cb4dad9bd98b9f82b5", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MURRAY, Steven", "id": "1"}], "title": "The new CERN tape software - getting ready for total performance", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T14:05:39.144190+00:00", "description": "", "title": "CHEP_2015_CERN_Tape_Sofware.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/64\/attachments\/578770\/796951\/CHEP_2015_CERN_Tape_Sofware.pdf", "filename": "CHEP_2015_CERN_Tape_Sofware.pdf", "content_type": "application\/pdf", "type": "file", "id": 796951, "size": 2606387}], "title": "Poster", "default_folder": false, "id": 578770, "description": ""}], "_type": "Contribution", "description": "CASTOR (the CERN Advanced STORage system) is used to store the custodial copy of all of the physics data collected from the CERN experiments, both past and present.  CASTOR is a hierarchical storage management system that has a disk-based front-end and a tape-based back-end.  The software responsible for controlling the tape back-end has been redesigned and redeveloped over the last year and shall be put into production at the beginning of 2015.  This paper summarises the motives behind the redesign, describes in detail the redevelopment work  and concludes with the short and long-term benefits.\r\n \r\nModern tape drives achieve 250 MB\/s and speeds up to 1GB\/s are on the roadmaps. To achieve this performance and to drop support for obsolete requirements, a new tape server software has been designed from the ground up, with a fully pipelined architecture. Disk and tape transfers, instruction fetches and result reporting are all running in independent threads communicating through queues to prevent external latencies from impacting tape drive performance.\r\n \r\nThe software has been developed in C++, facilitating modularity, unit testing, and faster development turnaround to face future challenges. The main developments included a novel methodology for SCSI development, polymorphic drive classes and factories to minimize extra coding, a tape file layer re-implementing the existing file format, polymorphic disk file access methods for easy extension to new protocols, thread-safe FIFO containers, easy to use threading building blocks and a memory management system for buffering data between disk and tape transfers.\r\n \r\nDevelopment was backed by a continuous integration system that included unit testing. Thanks to the use of mock objects, such as simulated tape drives, the unit tests covered complex use cases comparable with full production sessions. The unit tests were also validated by memory leak and race condition detectors.\r\n \r\nThanks to strict adherence to unit testing methodologies, the project went very smoothly. The development started on the 31st May 2013. The first read-only test version was delivered to tape operators mid-August 2014 for running against production stagers and the read-write version followed in mid-September. The project took the equivalent 2 FTEs for 5 people involved.\r\n \r\nStarting from this solid base, the CERN tape infrastructure will evolve to address future needs such as long term data preservation, striped file systems access for higher throughput and session pre-emption to improve the granularity of drive allocations and to ensure for the full utilization of all drives at all times.", "track": "Track3: Data store and access ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578770", "resources": [{"_type": "LocalFile", "name": "CHEP_2015_CERN_Tape_Sofware.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/64\/attachments\/578770\/796951\/CHEP_2015_CERN_Tape_Sofware.pdf", "fileName": "CHEP_2015_CERN_Tape_Sofware.pdf", "_fossil": "localFileMetadata", "id": "796951", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "327c77b88e6cb7778e3ca35affe5295d", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "KRUSE, Daniele Francesco", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "23bd498511a79e564b947aa22be33628", "affiliation": "CERN\/ISAE-Supa\u00e9ro", "_fossil": "contributionParticipationMetadata", "fullName": "COME, David", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "ce7a3e667e6c384e1911f89ffa110e74", "affiliation": "Institute for High Energy Physics  (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "KOTLIAR, Viktor", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/64", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "65", "speakers": [{"_type": "ContributionParticipation", "emailHash": "91282c80c1b507645d357c37f93421fd", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MARTIN MARQUEZ, Manuel", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "91282c80c1b507645d357c37f93421fd", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MARTIN MARQUEZ, Manuel", "id": "0"}], "title": "Big Data Analytics as a Service Infrastructure: Challenges, Desired Properties and Solutions", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T14:14:17.930583+00:00", "description": "", "title": "DataAnalyticsChep_PreprintFinal.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/65\/attachments\/578771\/796952\/DataAnalyticsChep_PreprintFinal.pdf", "filename": "DataAnalyticsChep_PreprintFinal.pdf", "content_type": "application\/pdf", "type": "file", "id": 796952, "size": 10261424}], "title": "Poster", "default_folder": false, "id": 578771, "description": ""}], "_type": "Contribution", "description": "CERN\u2019s accelerator complex is an extreme data generator, every second an important amount of comprehensively heterogeneous data coming from control equipment and monitoring agents is persisted and needs to be analysed. Over the decades, CERN\u2019s researching and engineering teams have applied different approaches, techniques and technologies. This situation has minimized the necessary collaboration and more relevantly the cross data analytics over different domains. These two factors are essential to unlock hidden insights and correlations between the underlying processes, which enable better and more efficient daily-based accelerators operations and more informed decisions.\r\nThe proposed Big Data Analytics as a Service Infrastructure aims to: (1) Integrate the existing developments. (2) Centralize and standardize the complex data analytics needs for the wide CERN research and engineering community. (3) Deliver real time and batch data analytics capabilities and (4) provide transparent access and extraction-transformation-load, ETL, mechanisms to the different and critical-mission existing data repositories.\r\nThis paper reflects the desired properties resulting from the analysis on CERN\u2019s data analytics requirements; the main challenges: technological, collaborative and educational; and finally potential solutions and lessons learned.", "track": "Track3: Data store and access ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578771", "resources": [{"_type": "LocalFile", "name": "DataAnalyticsChep_PreprintFinal.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/65\/attachments\/578771\/796952\/DataAnalyticsChep_PreprintFinal.pdf", "fileName": "DataAnalyticsChep_PreprintFinal.pdf", "_fossil": "localFileMetadata", "id": "796952", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/65", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:30:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "66", "speakers": [{"_type": "ContributionParticipation", "emailHash": "bfe764c773b0dfc3bfad1541d90511f5", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CERATI, Giuseppe", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "bfe764c773b0dfc3bfad1541d90511f5", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CERATI, Giuseppe", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "6da9b999bf385b0fb91e8d3ec0077e60", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MACNEILL, Ian", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "1905f721940f9ff8a9b0b9b1e9f7eef9", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "TADEL, Matevz", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "d96e4b1f6996132e6576cf8a02eed0c6", "affiliation": "UCSD", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. WUERTHWEIN, Frank", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "07802bad2c67bf8870bb8788a0b446d3", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. YAGIL, Avi", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "9a8ce9aebaabdde7956acf3f73f3b7a5", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ELMER, Peter", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "04fa2b329a1924ca9f5ac34c369f9b77", "affiliation": "Cornell University", "_fossil": "contributionParticipationMetadata", "fullName": "LANTZ, Steven", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "54403a0606d70b6b3e960407dc910138", "affiliation": "Cornell University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MCDERMOTT, Kevin", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "3dd3d828a5d1f33aac3f2ba9aca7cf1d", "affiliation": "Cornell University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. RILEY, Daniel Sherman", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "b2f3d7efcaf4cafbf84b474f44151bbd", "affiliation": "Cornell University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "WITTICH, Peter", "id": "9"}], "title": "Kalman Filter Tracking on Parallel Architectures", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T03:22:51.585804+00:00", "description": "", "title": "mictrk-cerati-CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/66\/attachments\/578772\/796953\/mictrk-cerati-CHEP2015.pdf", "filename": "mictrk-cerati-CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796953, "size": 2888186}], "title": "Slides", "default_folder": false, "id": 578772, "description": ""}], "_type": "Contribution", "description": "Power density constraints are limiting the performance improvements of modern CPUs. To address this we have seen the introduction of lower-power, multi-core processors, but the future will be even more exciting. In order to stay within the power density limits but still obtain Moore's Law performance\/price gains, it will be necessary to parallelize algorithms to exploit larger numbers of lightweight cores and specialized functions like large vector units. Example technologies today include Intel's Xeon Phi and GPGPUs.\r\n\r\nTrack finding and fitting is one of the most computationally challenging problems for event reconstruction in particle physics. At the High Luminosity LHC, for example, this will be by far the dominant problem. The need for greater parallelism has driven investigations of very different track finding techniques including Cellular Automata or returning to Hough Transform. The most common track finding techniques in use today are however those based on the Kalman Filter. Significant experience has been accumulated with these techniques on real tracking detector systems, both in the trigger and offline. They are known to provide high physics performance, are robust and are exactly those being used today for the design of the tracking system for HL-LHC. \r\n\r\nOur previous investigations showed that, using optimized data structures, track fitting with Kalman Filter can achieve large speedup both with Intel Xeon and Xeon Phi. We report here our further progress towards an end-to-end track reconstruction algorithm fully exploiting vectorization and parallelization techniques in a realistic simulation setup.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578772", "resources": [{"_type": "LocalFile", "name": "mictrk-cerati-CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/66\/attachments\/578772\/796953\/mictrk-cerati-CHEP2015.pdf", "fileName": "mictrk-cerati-CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796953", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/66", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:30:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:45:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "67", "speakers": [{"_type": "ContributionParticipation", "emailHash": "91282c80c1b507645d357c37f93421fd", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MARTIN MARQUEZ, Manuel", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "91282c80c1b507645d357c37f93421fd", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MARTIN MARQUEZ, Manuel", "id": "0"}], "title": "Data Science for Improving CERN\u2019s Accelerator Complex Control Systems", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T04:52:44.552445+00:00", "description": "", "title": "CHEP_2015DataScienceFinal.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/67\/attachments\/578773\/796954\/CHEP_2015DataScienceFinal.pdf", "filename": "CHEP_2015DataScienceFinal.pdf", "content_type": "application\/pdf", "type": "file", "id": 796954, "size": 2498093}, {"_type": "attachment", "modified_dt": "2015-04-15T04:52:44.552445+00:00", "description": "", "title": "CHEP_2015DataScienceFinal.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/67\/attachments\/578773\/796955\/CHEP_2015DataScienceFinal.pptx", "filename": "CHEP_2015DataScienceFinal.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796955, "size": 7241416}], "title": "Slides", "default_folder": false, "id": 578773, "description": ""}], "_type": "Contribution", "description": "Data science is about unlocking valuable insights and obtaining deep knowledge out of the data. Its application enables more efficient daily-based operations and more intelligent decision-making processes. CERN has been very successful on developing custom data-driven control and monitoring systems. Several millions of control devices: sensors, front-end equipment, etc., make up these critical-mission services and have lead to a significant investment in terms of data persistency. Exploiting CERN\u2019s historical investment on data and evolve the controls and monitoring infrastructures to intelligent, predictive and proactive control and monitoring systems has become an essential task to overcome some the most important challenges to be faced in the coming years. CERN\u2019s Engineering, Beams and Information Technology departments have documented this fact. This paper describes how applied data science: classification, clustering, time series forecasting and information discovery techniques, allows us to improve the efficiency of CERN\u2019s control and monitoring systems by performing efficient and intelligent predictive maintenance on control equipment, detecting potential anomalies and spotting hidden root causes. Future developments and applications are also discussed.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578773", "resources": [{"_type": "LocalFile", "name": "CHEP_2015DataScienceFinal.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/67\/attachments\/578773\/796954\/CHEP_2015DataScienceFinal.pdf", "fileName": "CHEP_2015DataScienceFinal.pdf", "_fossil": "localFileMetadata", "id": "796954", "_deprecated": true}, {"_type": "LocalFile", "name": "CHEP_2015DataScienceFinal.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/67\/attachments\/578773\/796955\/CHEP_2015DataScienceFinal.pptx", "fileName": "CHEP_2015DataScienceFinal.pptx", "_fossil": "localFileMetadata", "id": "796955", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/67", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "68", "speakers": [{"_type": "ContributionParticipation", "emailHash": "f1a132bd8360c35d1c6f69dfed11f194", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. VON HALLER, Barthelemy", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "f1a132bd8360c35d1c6f69dfed11f194", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. VON HALLER, Barthelemy", "id": "0"}], "title": "The new ALICE DQM client: a web access to ROOT-based objects", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T01:34:11.176061+00:00", "description": "", "title": "posterv2.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/68\/attachments\/578774\/796956\/posterv2.pdf", "filename": "posterv2.pdf", "content_type": "application\/pdf", "type": "file", "id": 796956, "size": 7910544}], "title": "Diapositives", "default_folder": false, "id": 578774, "description": ""}], "_type": "Contribution", "description": "A Large Ion Collider Experiment (ALICE) is the heavy-ion detector designed to study the physics of strongly interacting matter and the quark-gluon plasma at the CERN Large Hadron Collider (LHC).  The online Data Quality Monitoring (DQM) plays an essential role in the experiment operation by providing shifters with immediate feedback on the data being recorded in order to quickly identify and overcome problems.\r\n\r\nAn immediate access to the DQM results is needed not only by shifters in the control room but also by detector experts worldwide. \r\nAs a consequence, a new web application has been developed to dynamically display and manipulate the ROOT-based objects produced by the DQM system in a flexible and user friendly interface. \r\n\r\nThis paper describes the architecture and design of the tool, its main features and the technologies that were used, both on the server and the client side. In particular, we detail how we took advantage of the most recent ROOT JavaScript I\/O and web server library to give interactive access to ROOT objects stored in a database. We describe as well the use of modern web techniques  and packages such as AJAX, DHTMLX and Jquery, which  has been instrumental in the successful implementation of a reactive and efficient application.\r\n\r\nWe finally present the performance of this application under normal and peak load and how code quality was ensured. We conclude with a roadmap for future technical and functional developments.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Diapositives", "_fossil": "materialMetadata", "id": "578774", "resources": [{"_type": "LocalFile", "name": "posterv2.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/68\/attachments\/578774\/796956\/posterv2.pdf", "fileName": "posterv2.pdf", "_fossil": "localFileMetadata", "id": "796956", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "c11e82553cc2afee8efae7d6b228eede", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. CARENA, Franco", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "ff901e1570c97fe6b5feb219bbd4a2a8", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "SIMONETTI, Giuseppe", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "4d050ca3f3e136051d38fabbdf1cad97", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SOOS, Csaba", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "33a2f7e17267454ac7f2b96ef380dcae", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "TELESCA, Adriana", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "adad1c096b61f79e930480c8d88632ea", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "VANDE VYVRE, Pierre", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "ca50027eec50873b1ca5d70532ec74a3", "affiliation": "Warsaw University of Technology (PL)", "_fossil": "contributionParticipationMetadata", "fullName": "WEGRZYNEK, Adam", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "a9da0cbe0688c66379f09e9dc7448564", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CARENA, Wisla", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "b6a438aa14369eee8583315267b7df18", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHAPELAND, Sylvain", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "7b73f8fdf0c59c3804fe739200f59505", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "COSTA, Filippo", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "36f550a5103510fcefe10e69e4f79530", "affiliation": "Ministere des affaires etrangeres et europeennes (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "DELORT, Charles", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "bd80461aa0bf421068751c8bb4c3d49f", "affiliation": "Hungarian Academy of Sciences (HU)", "_fossil": "contributionParticipationMetadata", "fullName": "DENES, Ervin", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "bc36c7de437a6462dd9c7911aaf6a677", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DIVIA, Roberto", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "fc6c33957505ddef67034062f9d3e51e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FUCHS, Ulrich", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "51ace6af57c950acce7a9e7e3bdb1acf", "affiliation": "Warsaw University of Technology (PL)", "_fossil": "contributionParticipationMetadata", "fullName": "NIEDZIELA, Jeremi", "id": "14"}, {"_type": "ContributionParticipation", "emailHash": "52ec300b2a63fdf7d0f86e212cf98126", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHIBANTE BARROSO, Vasco", "id": "15"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/68", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:45:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "69", "speakers": [{"_type": "ContributionParticipation", "emailHash": "81fe17d9c65fc2548425c4a782d47341", "affiliation": "Beihang University (CN)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SHAN, Baosong", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "49724283a36cf0a82f1c085a237b2786", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CHOUTKO, vitali", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "81fe17d9c65fc2548425c4a782d47341", "affiliation": "Beihang University (CN)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SHAN, Baosong", "id": "1"}], "title": "COMPUTING STRATEGY OF THE AMS-02 EXPERIMENT", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T06:14:07.414804+00:00", "description": "", "title": "chep2015-AMS.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/69\/attachments\/578775\/796957\/chep2015-AMS.pdf", "filename": "chep2015-AMS.pdf", "content_type": "application\/pdf", "type": "file", "id": 796957, "size": 1841042}, {"_type": "attachment", "modified_dt": "2015-04-12T06:14:07.414804+00:00", "description": "", "title": "chep2015-AMS.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/69\/attachments\/578775\/796958\/chep2015-AMS.pptx", "filename": "chep2015-AMS.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796958, "size": 2799050}], "title": "Slides", "default_folder": false, "id": 578775, "description": ""}], "_type": "Contribution", "description": "The Alpha Magnetic Spectrometer (AMS) is a high energy physics experiment installed and operating on board of the International Space Station (ISS) from May 2011 and expected to last through Year 2024 and beyond. The computing strategy of the AMS experiment is discussed in the paper, including software design, data processing and modelling details, simulation of the detector performance and overall computing organization.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578775", "resources": [{"_type": "LocalFile", "name": "chep2015-AMS.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/69\/attachments\/578775\/796957\/chep2015-AMS.pdf", "fileName": "chep2015-AMS.pdf", "_fossil": "localFileMetadata", "id": "796957", "_deprecated": true}, {"_type": "LocalFile", "name": "chep2015-AMS.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/69\/attachments\/578775\/796958\/chep2015-AMS.pptx", "fileName": "chep2015-AMS.pptx", "_fossil": "localFileMetadata", "id": "796958", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "41983ca188100b1f3cbefe41a9f07f7c", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "EGOROV, Alexander", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "45747a35ad4d6352f5a5d8942a5edcb2", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "ELINE, Alexandre", "id": "3"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/69", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "250", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2431482769fdd885d86593de986412b9", "affiliation": "Georg-August-Universitaet Goettingen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "GLASER, Fabian", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2431482769fdd885d86593de986412b9", "affiliation": "Georg-August-Universitaet Goettingen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "GLASER, Fabian", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "7115b6ceb356151a501b527f4284a8e2", "affiliation": "Georg-August-Universitaet Goettingen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "NADAL SERRANO, Jordi", "id": "1"}], "title": "ATLAS user analysis on private cloud resources at GoeGrid", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T09:32:49.098364+00:00", "description": "", "title": "poster_ccgoegrid.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/250\/attachments\/578776\/796959\/poster_ccgoegrid.pdf", "filename": "poster_ccgoegrid.pdf", "content_type": "application\/pdf", "type": "file", "id": 796959, "size": 1928806}], "title": "Poster", "default_folder": false, "id": 578776, "description": ""}], "_type": "Contribution", "description": "User analysis job demands can exceed available computing resources, especially before major conferences. ATLAS physics results might be slowed down due to this lack of resources available. For these reasons, cloud R&D activities are now included in the skeleton of the ATLAS computing model, which has been extended by using resources from commercial and private cloud providers to satisfy the demand. However, most of these activities are focused on Monte-Carlo production jobs, extending the resources at Tier-2. To evaluate the suitability of the cloud-computing model for user analysis jobs, we developed a framework to launch an ATLAS user analysis cluster in a cloud infrastructure on demand and evaluated two solutions. The first solution is totally integrated in the Grid infrastructure by using the same mechanism, which is already in use at Tier-2: A designated Panda-Queue is monitored and additional worker nodes are launched in a cloud environment and assigned to a corresponding HTCondor queue according to the demand. Thereby, the use of cloud resources is totally transparent to the user. However, using this approach, submitted user analysis jobs might still suffer from a certain delay introduced by waiting in the queue. Therefore, our second solution offers the possibility to easily  deploy a totally private analysis cluster, i.e., batch or PROOF, on private cloud resources belonging to the university. Thereby, the private analysis cluster is connected to the ATLAS data-management system (DDM) to read and write input\/output files.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578776", "resources": [{"_type": "LocalFile", "name": "poster_ccgoegrid.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/250\/attachments\/578776\/796959\/poster_ccgoegrid.pdf", "fileName": "poster_ccgoegrid.pdf", "_fossil": "localFileMetadata", "id": "796959", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "02ca6423f165ed8da4a50ae68abe9d7e", "affiliation": "Georg-August-Universitaet Goettingen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "QUADT, Arnulf", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "aa02815c579b4c29ba5e46efd7457926", "affiliation": "Georg-August-Universitaet Goettingen", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. GRABOWSKI, Jens", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/250", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:00:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "251", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6bb6dd3105e4f5628e4868479399cf83", "affiliation": "Ministere des affaires etrangeres et europeennes (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "OTTO, Adam Jedrzej", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "4b2dee02cc9c077ec5c48080350a3b29", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CAMPORA PEREZ, Daniel Hugo", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "9f655ee0d367b6896f4dd3a37228be6f", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "NEUFELD, Niko", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "6bb6dd3105e4f5628e4868479399cf83", "affiliation": "Ministere des affaires etrangeres et europeennes (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "OTTO, Adam Jedrzej", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "2b14525e680c8cd085d5b92f54ddd6a0", "affiliation": "Universita e INFN, Roma I (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "PISANI, Flavio", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "e6e328b9cafbe9711a68c6fae2da818f", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SCHWEMMER, Rainer", "id": "4"}], "title": "A first look at 100 Gbps LAN technologies, with an emphasis on future DAQ applications", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T05:13:04.639984+00:00", "description": "", "title": "A_first_look_at_100_Gbps_LAN_technologies.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/251\/attachments\/578777\/796960\/A_first_look_at_100_Gbps_LAN_technologies.pdf", "filename": "A_first_look_at_100_Gbps_LAN_technologies.pdf", "content_type": "application\/pdf", "type": "file", "id": 796960, "size": 980625}], "title": "Slides", "default_folder": false, "id": 578777, "description": ""}], "_type": "Contribution", "description": "The LHCb experiment is preparing a major upgrade of both the detector and the data acquisition system. A system capable of transporting up to 50 Tbps of data will be required. This can only be achieved in a manageable way using 100 Gbps links. Such links recently became available also in the servers, while they have been available between switches already for a while.\r\n\r\nWe present first measurements with such links (InfiniBand EDR, Ethernet 100 GbE) both using standard benchmarks and using a prototype event-building application. We analyse the CPU load effects by using Remote DMA technologies, and we also show comparison with previous tests on 40 G equipment.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578777", "resources": [{"_type": "LocalFile", "name": "A_first_look_at_100_Gbps_LAN_technologies.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/251\/attachments\/578777\/796960\/A_first_look_at_100_Gbps_LAN_technologies.pdf", "fileName": "A_first_look_at_100_Gbps_LAN_technologies.pdf", "_fossil": "localFileMetadata", "id": "796960", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/251", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:30:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "256", "speakers": [{"_type": "ContributionParticipation", "emailHash": "cc2584799132e172e17dc9666e9e9f09", "affiliation": "Rheinisch-Westfaelische Tech. Hoch. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "URBAN, Martin", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "cc2584799132e172e17dc9666e9e9f09", "affiliation": "Rheinisch-Westfaelische Tech. Hoch. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "URBAN, Martin", "id": "0"}], "title": "The VISPA Internet Platform for Scientific Research, Outreach and Education", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T06:25:43.753528+00:00", "description": "", "title": "Talk_murban.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/256\/attachments\/578778\/796961\/Talk_v3.pdf", "filename": "Talk_v3.pdf", "content_type": "application\/pdf", "type": "file", "id": 796961, "size": 2935514}], "title": "Slides", "default_folder": false, "id": 578778, "description": ""}], "_type": "Contribution", "description": "VISPA provides a graphical front-end to computing infrastructures giving its users all functionality needed for working conditions comparable to a personal computer. It is a framework that can be extended with custom applications to support individual needs, e.g. graphical interfaces for experiment-specific software. By design, VISPA serves as a multi-purpose platform for many disciplines and experiments as demonstrated in the following different use-cases. A GUI to the analysis framework OFFLINE of the Pierre Auger collaboration, submission and monitoring of computing jobs, university teaching of hundreds of students, and outreach activity, especially in CERN's open data initiative.  \r\nServing heterogeneous user groups and applications gave us lots of experience. This helps us in maturing the system, i.e. improving the robustness and responsiveness, and the interplay of the components. Among the lessons learned are the choice of a file system, the implementation of websockets, efficient load balancing, and the fine-tuning of existing technologies like the RPC over SSH. We present in detail the improved server setup and report on the performance, the user acceptance and the realized applications of the system.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578778", "resources": [{"_type": "LocalFile", "name": "Talk_murban.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/256\/attachments\/578778\/796961\/Talk_v3.pdf", "fileName": "Talk_v3.pdf", "_fossil": "localFileMetadata", "id": "796961", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "47529bb3cd8d8691733f79044e702941", "affiliation": "Rheinisch-Westfaelische Tech. Hoch. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "VAN ASSELDONK, Daniel", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "19f8bc0594410c4d7717cdb2660a9536", "affiliation": "Rheinisch-Westfaelische Tech. Hoch. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. WELLING, Christoph", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "433102ab5c267e28efc1ce05f8d41882", "affiliation": "Rheinisch-Westfaelische Tech. Hoch. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "ERDMANN, Martin", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "0d2004b8d6b8d0f1802034817c842050", "affiliation": "Rheinisch-Westfaelische Tech. Hoch. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. FISCHER, Benjamin", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "ecca023b01a7f3582d9cff88a1540a20", "affiliation": "Rheinisch-Westfaelische Tech. Hoch. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "FISCHER, Robert", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "1f7216ef4d6f44eef1144f32e0d826ab", "affiliation": "Rheinisch-Westfaelische Tech. Hoch. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "GLASER, Christian", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "720b1ee57d4ee05eb433c601358127c8", "affiliation": "Rheinisch-Westfaelische Tech. Hoch. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. HEIDEMANN, Fabian", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "8893ce569a3ce88f99c90e802a8a1817", "affiliation": "Rheinisch-Westfaelische Tech. Hoch. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. MUELLER, Gero", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "52be32cd037b789131f812491c0dc3e5", "affiliation": "Rheinisch-Westfaelische Tech. Hoch. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. QUAST, Thorben", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "2649d10aea72ee92f56468d4dc217b91", "affiliation": "Rheinisch-Westfaelische Tech. Hoch. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "RIEGER, Marcel", "id": "10"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/256", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "257", "speakers": [{"_type": "ContributionParticipation", "emailHash": "baffd2fcf7094ce6229a02eab8691018", "affiliation": "IHEP", "_fossil": "contributionParticipationMetadata", "fullName": "SHI, Jingyan", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "baffd2fcf7094ce6229a02eab8691018", "affiliation": "IHEP", "_fossil": "contributionParticipationMetadata", "fullName": "SHI, Jingyan", "id": "0"}], "title": "Design and development of a Virtual Computing Platform Integrated with Batch Job System", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "Batch system is a common way for a local cluster to schedule jobs running on work nodes. In some cases, some jobs have to stay in queue without suitable work nodes while some job slots have to keep free without suitable jobs running. The reasons for such case might be various. One of the main reasons is that operating system running on the free work nodes is different from the one that jobs in queues wanted. Another reason to block jobs running on free job slots is the complicated job scheduling policies.\r\nVirtual machine has been proved to run some type of jobs with low penalty, and it could provide more flexible Operating system the job wanted. \r\nWe designed and developed a virtual computing platform based on openstack and integrated with batch job system. The same submit command and job script could be used without any changes. The jobs submitted to the virtual computing platform would be run on virtual machines with the correct operating system. All of these are transparent to users. The platform is composed of four parts, Batch Info Collector, Job Matcher, Virtual Machine Manager and Virtual Job Manager. After consults with the info from Batch Info Collector and Virtual Machine Manager, Job Matcher would send request to the Virtual Machine Manager to start virtual machines for the jobs submitted to the virtual queue. All the jobs running on virtual machine are monitored and controlled by virtual job manager.", "track": "Track7: Clouds and virtualization", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "41c9211fa0c4d3b49e193b906880a756", "affiliation": "IHEP", "_fossil": "contributionParticipationMetadata", "fullName": "Ms. KAN, Bowen", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "d52242b35abd1ae1e3a0545442a1eadc", "affiliation": "Institute of High Energy Physics, Beijing", "_fossil": "contributionParticipationMetadata", "fullName": "HUANG, qiulan", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "263b9b545ef46a68fc212b9ea89d3f4e", "affiliation": "IHEP", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LI, Haibo", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "06fd23e56b502a5633e1b498d9fa22f3", "affiliation": "ihep", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. SUN, Zhenyu", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/257", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "254", "speakers": [{"_type": "ContributionParticipation", "emailHash": "9004fd367d357871828de260587c178b", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "MILLAR, Paul", "id": "6"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2f89590a83d4f4b4a573f2d8d9fb76eb", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "BERNARDT, Christian", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "ede687cc1fd66698a4d3625c52758c3d", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "SCHWANK, Karsten", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "81df8b23e099068d71b62875bea6e24e", "affiliation": "ANSL (Yerevan Physics Institute) (AM)", "_fossil": "contributionParticipationMetadata", "fullName": "HAYRAPETYAN, Arsen", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "e8f5f6811898914eda13848d629d48b0", "affiliation": "Kalrsruhe Institute of Technology", "_fossil": "contributionParticipationMetadata", "fullName": "HARDT, Marcus", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "8d26ed6854ded78641202361191acea7", "affiliation": "NDGF", "_fossil": "contributionParticipationMetadata", "fullName": "BEHRMANN, Gerd", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "6400c2cc73bf4ffb1d6d4985b52d6786", "affiliation": "FNAL", "_fossil": "contributionParticipationMetadata", "fullName": "LITVINTSEV, Dmitry", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "9004fd367d357871828de260587c178b", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "MILLAR, Paul", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "a198240a6b21354217ead633867b3e31", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. FUHRMANN, Patrick", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "6a5c244279eea68fc59848571e6b9f8b", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "MKRTCHYAN, Tigran", "id": "8"}], "title": "Unlocking data: federated identity with LSDMA and dCache", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T03:33:26.513173+00:00", "description": "", "title": "federated-identity.svg", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/254\/attachments\/578779\/796962\/federated-identity.svg", "filename": "federated-identity.svg", "content_type": "image\/svg+xml", "type": "file", "id": 796962, "size": 3383858}], "title": "Poster", "default_folder": false, "id": 578779, "description": ""}], "_type": "Contribution", "description": "X.509, the dominant identity system from grid computing, has proved\r\nunpopular for many user communties.  More popular alternatives\r\ngenerally assume the user is interacting via their web-browser.  Such\r\nalternatives allow a user to authenticate with many services with the\r\nsame credentials (username and password).  They also allow users\r\nfrom different organisations form collaborations quickly and simply.\r\n\r\nScientists generally require that their custom analysis software has\r\ndirect access to the data.  Such direct access is not currently\r\nsupported by alternatives to X.509, as they require the use of a\r\nweb-browser.\r\n\r\nVarious approaches to solve this issue are being investigated as part\r\nof the Large Scale Data Management and Analysis (LSDMA) project, a\r\nGerman funded national R&D project.  These involve dynamic credential\r\ntranslation (creating an X.509 credential) to allow backwards\r\ncompatibility in addition to direct SAML- and OpenID Connect-based\r\nauthentication.\r\n\r\nWe present a summary of the current state of art and the current\r\nstatus of the federated identity work funded by the LSDMA project\r\nalong with the future road map.", "track": "Track3: Data store and access ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578779", "resources": [{"_type": "LocalFile", "name": "federated-identity.svg", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/254\/attachments\/578779\/796962\/federated-identity.svg", "fileName": "federated-identity.svg", "_fossil": "localFileMetadata", "id": "796962", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/254", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:30:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "255", "speakers": [{"_type": "ContributionParticipation", "emailHash": "9004fd367d357871828de260587c178b", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "MILLAR, Paul", "id": "6"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2f89590a83d4f4b4a573f2d8d9fb76eb", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "BERNARDT, Christian", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "ede687cc1fd66698a4d3625c52758c3d", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "SCHWANK, Karsten", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "76a65b8706379e29923a2c824459b935", "affiliation": "HTW Berlin", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. HE\u00dfLING, Hermann", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "34f245a5e75bf92030309413606df57b", "affiliation": "HTW Berlin", "_fossil": "contributionParticipationMetadata", "fullName": "Ms. WESCHENFELDER, Jana", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "8d26ed6854ded78641202361191acea7", "affiliation": "NDGF", "_fossil": "contributionParticipationMetadata", "fullName": "BEHRMANN, Gerd", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "6400c2cc73bf4ffb1d6d4985b52d6786", "affiliation": "FNAL", "_fossil": "contributionParticipationMetadata", "fullName": "LITVINTSEV, Dmitry", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "9004fd367d357871828de260587c178b", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "MILLAR, Paul", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "a198240a6b21354217ead633867b3e31", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. FUHRMANN, Patrick", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "43b6bc536b1c55643d004555a8403865", "affiliation": "urn:Google", "_fossil": "contributionParticipationMetadata", "fullName": "MKRTCHYAN, Tigran", "id": "9"}], "title": "New adventures in storage: cloud storage  and CDMI", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T02:33:08.024684+00:00", "description": "", "title": "dcache-cdmi.odp", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/255\/attachments\/578780\/796963\/dcache-cdmi.odp", "filename": "dcache-cdmi.odp", "content_type": "application\/vnd.oasis.opendocument.presentation", "type": "file", "id": 796963, "size": 3325113}, {"_type": "attachment", "modified_dt": "2015-04-14T02:33:08.024684+00:00", "description": "", "title": "dcache-cdmi.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/255\/attachments\/578780\/796964\/dcache-cdmi.pdf", "filename": "dcache-cdmi.pdf", "content_type": "application\/pdf", "type": "file", "id": 796964, "size": 1505376}], "title": "Slides", "default_folder": false, "id": 578780, "description": ""}], "_type": "Contribution", "description": "Traditionally storage systems have had well understood\r\nresponsibilities and behaviour, codified by the POSIX standards.  More\r\nsophisticated systems (such as dCache) support additional\r\nfunctionality, such as storing data on media with different latencies\r\n(SSDs, HDDs, tapes).  From a user's perspective, this forms a\r\nrelatively simple adjunct to POSIX: providing optional\r\nquality-of-service values when writing data and optionally requesting\r\ndata be staged from tape ahead of use.\r\n\r\nThe CDMI protocol provides a standard mechanism for clients to\r\ndiscover and use many advanced features.  Such features include\r\nstoring and querying metadata, searching for files matching metadata\r\npredicates, controlling a file's quality-of-service and retention\r\npolicies, providing an object store and alternative protocol\r\ndiscovery.\r\n\r\nA CDMI enabled storage has the potential for greatly simplifying a\r\nmore general service as some high-level functionality can be delegated\r\nto the storage system.  This reduces and may remove the need to run\r\nadditional services, which makes it easier for sites to support their\r\nusers.\r\n\r\nBy implementing the CDMI standard, dCache can expose new features in a\r\nstandards compliant fashion.  Here, various scenarios are presented\r\nwhere CDMI provides an advantage and the road-map for CDMI support in\r\ndCache is explored.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578780", "resources": [{"_type": "LocalFile", "name": "dcache-cdmi.odp", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/255\/attachments\/578780\/796963\/dcache-cdmi.odp", "fileName": "dcache-cdmi.odp", "_fossil": "localFileMetadata", "id": "796963", "_deprecated": true}, {"_type": "LocalFile", "name": "dcache-cdmi.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/255\/attachments\/578780\/796964\/dcache-cdmi.pdf", "fileName": "dcache-cdmi.pdf", "_fossil": "localFileMetadata", "id": "796964", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/255", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "508", "speakers": [{"_type": "ContributionParticipation", "emailHash": "c2b0ccd41ef0fa777b5db04b2e9bf6f5", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "XIE, Zhen", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c2b0ccd41ef0fa777b5db04b2e9bf6f5", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "XIE, Zhen", "id": "0"}], "title": "The CMS BRIL Data Acquisition system", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-03-26T09:52:05.832196+00:00", "description": "", "title": "brildaqposter_chep05_v1.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/508\/attachments\/578781\/796965\/brildaqposter_chep05_v1.pdf", "filename": "brildaqposter_chep05_v1.pdf", "content_type": "application\/pdf", "type": "file", "id": 796965, "size": 464707}], "title": "Poster", "default_folder": false, "id": 578781, "description": ""}], "_type": "Contribution", "description": "(BRIL) Beam Radiation Instrumentation and Luminosity is a new project within CMS. It consists of several independent sub-detectors for measuring the luminosity, monitoring the beam conditions, and the protection of CMS from serious radiation damage. It is beneficial for the project in the long run to use a single software infrastructure for data acquisition.\r\n\r\nSimilar to CMS central daq, BRIL daq manages distributed and heterogeneous subsystems.\r\nThe difference is that it has no  event building and requires less data throughput.\r\nWe present the design of the BRIL DAQ system build on the XDAQ  and RCMS framework in CMS.\r\n\r\nXDAQ is the C++ framework for distributed data acquisition system. Its event-driven architecture enables large numbers of loosely coupled software components and services to exchange information in near real-time.\r\n\r\nA BRIL DAQ component is a XDAQ process containing several applications. There are three tiers of BRIL DAQ components: sources, processors and central processors. A source is the hardware readout unit, a processor is responsible for local data aggregation and reduction, and a central processing component aggregates and further reduces data from all subsystems. Different central processors serve different purposes such as data storage, luminosity selection or online data quality monitoring.\r\n\r\nXDAQ b2in eventing in publish\/subscribe mode is used as data transport mechanism between components. The publish\/subscribe model is chosen because it guarantees  maximum decoupling of data senders and receivers.\r\n\r\nRCMS is the run-control framework in CMS. It is written in Java and with built-in web and database supports. To write a RCMS component means to implement a function manager(FM) with a state machine, state transitions and event callbacks. Some state inputs are visible to the run control and monitoring web page allowing external intervention. State inputs, configurations or other parameters of choice can be stored in the database.\r\n\r\nThere are about 15 to 20 XDAQ processes running in the BRIL linux cluster. The global configuration and control of them is achieved by a single generic function manager driven by several configuration groups. The division of such groups is decided by the life cycle pattern of the processes. This design is simpler than function manager hierarchy used in the CMS central daq but is sufficient to control BRIDAQ processes which are stateless.\r\n\r\nThe fact that each XDAQ process containing a HTTP server allows the data acquisition and the control system to collaborate seamlessly via SOAP messaging.\r\n\r\nFinally, we present some first experience of using the new BRIL daq system.", "track": "Track1: Online computing ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578781", "resources": [{"_type": "LocalFile", "name": "brildaqposter_chep05_v1.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/508\/attachments\/578781\/796965\/brildaqposter_chep05_v1.pdf", "fileName": "brildaqposter_chep05_v1.pdf", "_fossil": "localFileMetadata", "id": "796965", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "6bb1fc7507f1acf34ef998af2cf6bc35", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DABROWSKI, Anne", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "9f86e39a65ea0dea8edc486f6ce6c241", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "STICKLAND, David Peter", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "7cdd48f784a087c6d9b6f6003cfc03f7", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ORSINI, Luciano", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "1951a3ffe6d5cf1393f669f572be4751", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "PETRUCCI, Andrea", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/508", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "509", "speakers": [{"_type": "ContributionParticipation", "emailHash": "9e7dd8f84b1524d686815744ec524fea", "affiliation": "University of Manchester (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "MCNAB, Andrew", "id": "5"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "d1fd9bd7eef7fa3e8b739711bdb88c69", "affiliation": "University of Manchester (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "CHEN, Shanzhen", "id": "0"}], "title": "Discovering matter-antimatter asymmetries with GPUs", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T01:12:04.785777+00:00", "description": "", "title": "CHEP2015_poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/509\/attachments\/578782\/796966\/CHEP2015_poster.pdf", "filename": "CHEP2015_poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796966, "size": 527968}], "title": "Slides", "default_folder": false, "id": 578782, "description": ""}], "_type": "Contribution", "description": "The LHCb experiment has recorded the world\u2019s largest sample of charmed meson decays. The search for matter-antimatter asymmetries in charm sector requires high precision analysis and thus intensive computing. This contribution will present a powerful method to measure matter-antimatter asymmetries in multi-body decays where GPU systems have been successfully exploited. In this method, local asymmetries in phase-space distributions were explored with an unbinned approach, and the parallelisation of GPU makes this approach feasible for the first time. The performance including on GPUs on the grid will be discussed in detail. With this new method, the world\u2019s best sensitivities to particular decay channels have been achieved.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578782", "resources": [{"_type": "LocalFile", "name": "CHEP2015_poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/509\/attachments\/578782\/796966\/CHEP2015_poster.pdf", "fileName": "CHEP2015_poster.pdf", "_fossil": "localFileMetadata", "id": "796966", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "e54ba10f3b1369bc0ee7854f1727cc22", "affiliation": "University of Manchester (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "BRODZICKA, Jolanta", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "edccb2e689e83f9d3ee51e2c2d88834e", "affiliation": "University of Manchester (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GERSABECK, Marco", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "fcf37dffe9ce64f02c3e0802696367dd", "affiliation": "University of Manchester (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "PARKES, Chris", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "9e7dd8f84b1524d686815744ec524fea", "affiliation": "University of Manchester (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "MCNAB, Andrew", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/509", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:15:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "506", "speakers": [{"_type": "ContributionParticipation", "emailHash": "55154d5947f3225c410325be9cb08ece", "affiliation": "Czech Technical University (CZ)", "_fossil": "contributionParticipationMetadata", "fullName": "NOVY, Josef", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "55154d5947f3225c410325be9cb08ece", "affiliation": "Czech Technical University (CZ)", "_fossil": "contributionParticipationMetadata", "fullName": "NOVY, Josef", "id": "0"}], "title": "Pilot run of the new DAQ of the COMPASS experiment", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T14:18:59.664936+00:00", "description": "", "title": "CHEP2015_FPGA_DAQ_NOVY.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/506\/attachments\/578783\/796967\/CHEP2015_FPGA_DAQ_NOVY.pdf", "filename": "CHEP2015_FPGA_DAQ_NOVY.pdf", "content_type": "application\/pdf", "type": "file", "id": 796967, "size": 5407192}], "title": "Slides", "default_folder": false, "id": 578783, "description": ""}], "_type": "Contribution", "description": "This contribution focuses on the deployment and first results of the new data acquisition system (DAQ) of the COMPASS experiment utilizing FPGA-based event builder. The new DAQ system is developed under name RCCARS (run control, configuration, and readout system).\r\n\r\nCOMPASS is a high energy physics experiment situated at the SPS particle accelerator at CERN laboratory in Geneva, Switzerland. After two years of preparations and commissioning, the physics data taking started in 2002. The original DAQ consisted of several layers and was based on software event building paradigm. The detector frontend electronics continuously preprocess and digitize data in approximately 300000 channels, the data are readout when trigger signal arrives and are concentrated into 250 custom VME modules. These modules were connected to the event building network using 90 Slinks. The network consisted of two types of servers: readout buffers and event builders. Readout buffers served for data reception and buffering which allowed to distribute the load over the entire SPS accelerator cycle. The collected event fragments were transferred over the switched gigabit Ethernet to the event builders that assembled full events. Full events were written into the local disk space and afterwards send to the central CERN storage facility CASTOR. The system was controlled by adapted ALICE DATE package which implemented run control, event sampling, monitoring, run keeping, and configuration functionality.\r\n\r\nSince 2002, number of channels increased from 190000 to approximately 300000, trigger rate increase from 5 kHz to 30 kHz; the average event size remained roughly 35 kB. In order to handle the increased data rates and mainly cope with aging of the system, it has been decided to develop a new DAQ system during technical shutdown of CERN accelerator in 2013-2014. Custom FPGA based data handling cards (DHC) are responsible for building of events in the new system, thus replacing the event building network. The cards have been designed in Compact AMC form factor and they feature 16 high speed serial links, 4GB of DDR3 memory, Gigabit Ethernet connection, and COMPASS Trigger Control System receiver. There are two different versions of firmware: multiplexer and switch. The multiplexer card combines data from 15 incoming Slinks send them into one outgoing, whereas the switch combines data from up to 8 multiplexers and distributes the full events to the readout engine servers equipped by spillbuffer PCI-Express cards that receive the data. DHC cards memory allows to store date of one spill and to distribute the load over the accelerator cycle period. Readout engine servers are also used for monitoring of data consistency and data quality checks.\r\n\r\nAs the DHC cards perform data flow control and event building, the software serves for configuration, run control, and monitoring. For these purposes, we have developed special software package. The main part of the software is implemented in the C++ language with the Qt framework; JavaScript, PHP, TCL, Python languages are used for support tasks. MySQL database has been selected as storage of system configuration and logs. Communication between processes in the system is implemented using the DIM library. Several types of processes are present in the system. The master is the most important process; it exchanges information and commands between user interface and slave processes. Slaves monitor and configure the DHC cards, process data, and provide interface for data access. User interface can run either in control or in viewer mode. However, only one instance of user interface may be present in the control mode. Message logger collects messages from all processes involved in the RCCARS and stores them in the database. Message browser is a graphical tool which allows to display and to filter these logs. The RCCARS is configured trough the web interface.\r\n\r\nThe RCCARS has been deployed for the pilot run starting from the September 2014. In the paper, we present performance and stability results of the new DAQ architecture, we compare it with the original system in more details.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578783", "resources": [{"_type": "LocalFile", "name": "CHEP2015_FPGA_DAQ_NOVY.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/506\/attachments\/578783\/796967\/CHEP2015_FPGA_DAQ_NOVY.pdf", "fileName": "CHEP2015_FPGA_DAQ_NOVY.pdf", "_fossil": "localFileMetadata", "id": "796967", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "c42f2e84acb4f795be4950c7281641eb", "affiliation": "Charles University (CZ)", "_fossil": "contributionParticipationMetadata", "fullName": "BODLAK, Martin", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "a144cbe9f02dbc8dfb576d0a00dc6581", "affiliation": "Czech Technical University (CZ)", "_fossil": "contributionParticipationMetadata", "fullName": "JARY, Vladimir", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "2693a1e6297ba28b42c3a3371010840e", "affiliation": "Technische Universitaet Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "KONOROV, Igor", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "1c85d93abed3063c485a4549b8241601", "affiliation": "Czech Technical University (CZ)", "_fossil": "contributionParticipationMetadata", "fullName": "VIRIUS, Miroslav", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "0b046a9b1029fc321d6586f4603d21c9", "affiliation": "Charles University (CZ)", "_fossil": "contributionParticipationMetadata", "fullName": "TOMSA, Jan", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "9a84207df3be0c4d589e2466d3e38a5c", "affiliation": "Czech Technical University (CZ)", "_fossil": "contributionParticipationMetadata", "fullName": "KVETON, Antonin", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "8cf44283937ad699328a017de0e6b590", "affiliation": "Technische Universitaet Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "LEVIT, Dmytro", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "32483e26766d205331eb0d14eb87529c", "affiliation": "Technische Universitaet Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "HUBER, Stefan", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "a7b0fe30c2bf585f68d5fe623889990f", "affiliation": "Joint Inst. for Nuclear Research (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "FROLOV, Vladimir", "id": "9"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/506", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "507", "speakers": [{"_type": "ContributionParticipation", "emailHash": "aa9728e673a0fbad9278dcb8be80e145", "affiliation": "Department of Atomic Energy (IN)", "_fossil": "contributionParticipationMetadata", "fullName": "SINGHAL, Vikas", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "aa9728e673a0fbad9278dcb8be80e145", "affiliation": "Department of Atomic Energy (IN)", "_fossil": "contributionParticipationMetadata", "fullName": "SINGHAL, Vikas", "id": "0"}], "title": "Event Building Process for Time streamed data", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The Compressed Baryonic Matter (CBM) experiment at the Facility for Anti-Proton and Ion Research (FAIR)in Darmstadt, Germany, is going to produce about 1 TByte per second of raw data at an interaction rate of 10 MHz for the measurement of very rare particles. Until now, all the HEP experiments are based on traditional hardware trigger approach; therefore all simulation and reconstruction software frameworks have been working on the basis of event by event data processing. In contrast, CBM is based on a triggerless DAQ concept.  This introduces a real challenge for the online computing process, which has to suppress the raw data volume by three orders of magnitude in real-time. In this respect major work has been going in the direction of changing the software framework such that it produces a continuous, time-sorted stream of digitized data packed in fixed time intervals  (\"time slice\") which will be similar to the actual raw data output from the different detectors of the experiment. In this time-sliced data there is no event separation; consequently, the available simulation and reconstruction software cannot be straightforwardly used.\r\n\r\nAs a first step towards the reconstruction of such free-streaming data, we introduced an event-building process which tags physical events based on the time information of the raw data. The process is based on identifying and analyzing dips in the continuous data stream. It was developed on the example of the muon detecton system. The process measures the raw data rate on a nanosecond level; data between two dips indicate event candidates, which are then further analysed with respect to a number of parameters like minimum and maximum number of hits per event, event duration etc. After event definiton in software, all reconstruction algorithms, working on an event-by-event base, can be used without any modification.\r\n\r\nIn this paper, we will present and discuss the event selection process and its further development like the implementation on heterogeneous platforms using different parallel computing paradigms like OpenMP, MPI, OpenCL and CUDA, as well as its extension to the other CBM detector systems in order to reach at a global event definition.", "track": "Track2: Offline software ", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "a6ec9b4d0a7fb58c3b687869949e6c6c", "affiliation": "GSI Darmstadt", "_fossil": "contributionParticipationMetadata", "fullName": "FRIESE, Volker", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "78a7a96a5484ed34f87e92b79de3954d", "affiliation": "Department of Atomic Energy (IN)", "_fossil": "contributionParticipationMetadata", "fullName": "CHATTOPADHYAY, Subhasis", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/507", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "504", "speakers": [{"_type": "ContributionParticipation", "emailHash": "829a0cfa985b7f1688471e078f88e2c7", "affiliation": "Czech Technical University Department of Physics", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KOSEJK, Vladislav", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "829a0cfa985b7f1688471e078f88e2c7", "affiliation": "Czech Technical University Department of Physics", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KOSEJK, Vladislav", "id": "0"}], "title": "Processing of data from innovative parabolic strip telescope.", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T21:50:28.569995+00:00", "description": "", "title": "Kosejk_Poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/504\/attachments\/578784\/796968\/Kosejk_Poster.pdf", "filename": "Kosejk_Poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796968, "size": 2268946}], "title": "Poster", "default_folder": false, "id": 578784, "description": ""}], "_type": "Contribution", "description": "This paper presents innovative telescope design based on usage of parabolic strip as objective.\r\n\r\n \r\nIsaac Newton was the first one to solve problem of chromatic aberration, which is caused by difference in refractive index in lens. This problem was solved by new kind of telescope with mirror used as objective.\r\nThere are many different kind of telescopes.\r\nThe most basic one is lens telescope. This kind of telescope uses set of lenses.\r\nThe next type is mirror telescope. Objective of such telescope can be concave mirror, spherical parabolic mirror or hyperbolically shaped mirror. Lens speed is depending directly on surface of the mirror.\r\nBoth kinds can be combined. Such telescope composes of at least two mirrors and set of lenses.  Light is bounced of the primary mirror to the secondary one and then to the lens system. This type has smaller sizes, but also smaller lens speed.\r\n\r\n\r\nThe telescope design presented in this paper have parabolic strip as objective.\r\nObserved objects are projected as line in picture plane. Each line, which size is equal to size of strip, corresponds to sum of intensities of light coming perpendicular to objective from observed object. Series of pictures taken with different rotation and processed by special reconstruction algorithm is needed to get 2D pictures. The telescope can be also used for fast detection of objects. The rotation and multiple pictures are not need in this mode, just one picture of mirror is needed.\r\n\r\nThe greatest advantage of the parabolic strip telescope is its simpler and cheaper construction. Just two holders with cut-out for parabolic strip and strip from reflective flexible material are needed to create the objective. Tension in the strip material guarantees stability and precision of the reflective surface shape. \r\nMain idea of reconstruction is based on the same principle as CT (Computer Tomography); reconstruction of 2D image by usage of Radon transformation principle from 1D samples. Four algorithms were implemented and tested. \r\nThe most basic one is summation algorithm. It is based on sequential summation of projections (adjusted pictures taken during rotation of the telescope). It algorithm is very fast and very useful for verification of basic functionality of the whole setup.\r\nMultiplicative algorithm is improved version of summation algorithm. It is based on mutual multiplication of picture matrixes. Main advantage of this algorithm is primarily in ability to easily clean dark point from final matrix and thus suppress noise in the reconstructed 2D picture. Disadvantage is complicated weighting of matrixes during multiplication. \r\n The third one is iterative algorithm. This approach is base on multiplication in steps. Pictures are divided into subsets. These subsets are multiplicated and  until the reconstructed image is created. Idea of subsets came from CT image processing.\r\nThe last algorithm, for processing of pictures from parabolic strip telescope, is Filtered Back Projection method. It is not possible to directly use inverse radon transformation despite the fact that telescope is based on principle of Radon transformation. It is mainly because radon transformation does not have guaranteed analytic solution, thus approximation of this algorithm is used. This approximation is known as Filtered Back Projection and was originally used for reconstruction of the picture from CT. Final picture processed by this method is usually of very good quality and algorithm itself is very well adjustable for particular problem thanks to wide variety of filters. \r\n\r\n\r\nConstruction of whole telescope is relatively simple. It composes of four parts. The base is multi-axis astronomical tripod with automatic Earth movement corrections. The second part is firm holder for servomotor. This servomotor is responsible for rotation of the objective of telescope. Furthermore the CDD camera is attached to the telescope in focal point. Image from objective is projected directly on the CCD chip without any other optical elements. The support elements are made of aluminum and duralaluminum. Whole surface is anodized to black color to minimize light reflection. Mirror holders are made of hardened plastic. \r\n\r\nThe building of many observatories all around the world is possible thanks to low cost of this design. Observation of large part of the sky would be possible by such system of telescopes with big objective and angular freedom of about 15 degrees. This system is also fit for usage in satellite construction, thanks to the low weight. Best place for such telescope, is an orbit of the Earth, where all the adverse effects of the atmosphere are absent and construction of hundreds meters large objective can be made. Telescope of this magnitude can make observation of distant stars and search for exoplanets more precise and easier. Resolution of the 120 cm large parabolic strip telescope based on preliminary calculation, equal those of parabolic telescope with 150 cm diameter.\r\nThis design has yet another very unique usage. Telescope can be used for observation outside of visible spectrum, more precisely wavelength of X-Ray and gama, by exchange of parabolic strip material.", "track": "Track2: Offline software ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578784", "resources": [{"_type": "LocalFile", "name": "Kosejk_Poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/504\/attachments\/578784\/796968\/Kosejk_Poster.pdf", "fileName": "Kosejk_Poster.pdf", "_fossil": "localFileMetadata", "id": "796968", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "55154d5947f3225c410325be9cb08ece", "affiliation": "Czech Technical University (CZ)", "_fossil": "contributionParticipationMetadata", "fullName": "NOVY, Josef", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "17fb20b2b299893834b94a56ed568aa9", "affiliation": "Czech Technical University Department of Physics", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. CHADZITASKOS, Goce", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/504", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "505", "speakers": [{"_type": "ContributionParticipation", "emailHash": "9a8ce9aebaabdde7956acf3f73f3b7a5", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ELMER, Peter", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "9a8ce9aebaabdde7956acf3f73f3b7a5", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ELMER, Peter", "id": "0"}], "title": "Exploiting heterogeneity on the Grid for fun and profit", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [], "_type": "Contribution", "description": "In recent years large scientific projects such as the Large Hadron\r\nCollider (LHC) have developed extensive distributed computing\r\nsystems, involving many computer centers around the globe. At first\r\nglance the resulting computing platform appears to be based on a\r\nhomogeneous deployment of Linux on commodity x86 processors.  In\r\npractice, however, many generations of processors are deployed in\r\nthe computing centers, typically corresponding to 4 or 5 years worth\r\nof purchases.  Depending on where an application is run and details\r\nof what it is doing, it will see some variation in achievable compute \r\nperformance stemming directly from the processor heterogeneity.\r\nIn the future, this heterogeneity will increase significantly. Due\r\nto power density limitations in processors, alternate processor\r\narchitectures are now being considered, including low power cores (ARM,\r\nATOM) and specialized coprocessors (GPUs, Xeon Phi).  If the\r\ndistributed computing system evolves to include clusters with\r\nalternate technologies, the resulting heterogeneity will mean that\r\nboth the performance and the performance\/Watt achievable by any\r\ngiven application will vary significantly.\r\n\r\nThis heterogeneity and the resulting variations in application\r\nperformance and energy efficiency can in principle provide information\r\nusable for job scheduling to improve net throughput or to reduce\r\npower consumption or cost. We report our studies into potential \r\nperformance and power efficiency\/cost gains resulting for inclusion\r\nof this kind of information in scheduling choices.", "track": "Track5: Computing activities and Computing models", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "7a6c8f49981d093b41b365a4b522378d", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. EULISSE, Giulio", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/505", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:15:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:30:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "502", "speakers": [{"_type": "ContributionParticipation", "emailHash": "83231eb6b32f41dda19d6528e53efdb9", "affiliation": "Johann-Wolfgang-Goethe Univ. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "KRZEWICKI, Mikolaj", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "6233e92da587b0c3279f739014a7e2d9", "affiliation": "Johann-Wolfgang-Goethe Univ. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "KRZEWICKI, Mikolaj", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "83231eb6b32f41dda19d6528e53efdb9", "affiliation": "Johann-Wolfgang-Goethe Univ. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "KRZEWICKI, Mikolaj", "id": "0"}], "title": "The ALICE High Level Trigger, status and plans", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T05:22:55.120951+00:00", "description": "", "title": "M.Krzewicki-HLT-CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/502\/attachments\/578785\/796969\/M.Krzewicki-HLT-CHEP2015.pdf", "filename": "M.Krzewicki-HLT-CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796969, "size": 3142444}], "title": "Slides", "default_folder": false, "id": 578785, "description": ""}], "_type": "Contribution", "description": "The ALICE High Level Trigger (HLT) is an online reconstruction, triggering and data compression system used in the ALICE experiment at CERN. Unique among the LHC experiments, it extensively uses modern coprocessor technologies like general purpose graphic processing units (GPGPU) and field programmable gate arrays (FPGA) in the data flow. Real-time data compression is performed using a cluster finder algorithm implemented on FPGA boards. These data, instead of raw clusters, are used in the subsequent processing and storage, resulting in a compression factor of around 4. Track finding is performed using a cellular automaton and a Kalman filter algorithm on GPGPU hardware, where both CUDA and OpenCL technologies can be used interchangeably.\r\nThe ALICE upgrade requires further development of online concepts to include detector calibration and stronger data compression. The current HLT farm will be used as a test bed for online calibration and both synchronous and asynchronous processing frameworks already before the upgrade, during Run 2.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578785", "resources": [{"_type": "LocalFile", "name": "M.Krzewicki-HLT-CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/502\/attachments\/578785\/796969\/M.Krzewicki-HLT-CHEP2015.pdf", "fileName": "M.Krzewicki-HLT-CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796969", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/502", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:45:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "503", "speakers": [{"_type": "ContributionParticipation", "emailHash": "7b8b9569d76344a0c1440aec3a9e6caf", "affiliation": "SLAC National Accelerator  Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ASAI, Makoto", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "7b8b9569d76344a0c1440aec3a9e6caf", "affiliation": "SLAC National Accelerator  Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ASAI, Makoto", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "14f8ac6f7f0f671ea8b2a1b274ca8949", "affiliation": "Ecole Polytechnique (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. VERDERI, Marc", "id": "1"}], "title": "Geant4 Version 10 Series", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T07:27:05.601001+00:00", "description": "", "title": "Geant4Version10Series_CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/503\/attachments\/578786\/796970\/Geant4Version10Series_CHEP2015.pdf", "filename": "Geant4Version10Series_CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796970, "size": 14786261}], "title": "Slides", "default_folder": false, "id": 578786, "description": ""}], "_type": "Contribution", "description": "The Geant4 Collaboration released a new generation of the Geant4\r\nsimulation toolkit (version 10.0) in December 2013, and continues \r\nto improve its physics, computing performance and usability. This \r\npresentation will cover the major improvements made since version \r\n10.0. The physics evolutions include improvement of the Fritiof \r\nhadronics model, extension of the INCL++ model to higher energy, \r\nextension of the data-driven low-energy neutron model approach to \r\nproton and light ions, extension of radioactive decay modes to\r\ninclude proton emission and double beta decay, introduction of \r\nphonon physics, a first implementation of particle-matter \r\ninteractions dealing with crystal lattice structure, improvements \r\nof multiple-scattering for displacement at volume boundary, and \r\nextension of low energy electromagnetic processes in DNA scale. \r\nExtension and improvement of the unified solid library will provide \r\nmore functionality and better computing performance. The continued \r\neffort to reduce memory consumption per thread allows for massive \r\nparallelism of large applications in the multithreaded mode. \r\nToolkit usability is improved with an evolved real-time \r\nvisualization in multithreaded mode, a new web-based visualization \r\ndriver, enhancements in histogramming tool in particular in \r\nmultithreaded mode, and additional APIs for easier integration with \r\nIntel TBB and MPI. In addition, the Collaboration has extended its \r\nplatform support to the Xeon Phi coprocessors. A redesigned physics \r\nvalidation tool based on modern web technologies will ease users' \r\naccess to routinely performed validations. We will also discuss \r\nour short and long term development perspectives.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578786", "resources": [{"_type": "LocalFile", "name": "Geant4Version10Series_CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/503\/attachments\/578786\/796970\/Geant4Version10Series_CHEP2015.pdf", "fileName": "Geant4Version10Series_CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796970", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/503", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:15:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "500", "speakers": [{"_type": "ContributionParticipation", "emailHash": "9a8ce9aebaabdde7956acf3f73f3b7a5", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ELMER, Peter", "id": "4"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "7a6c8f49981d093b41b365a4b522378d", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. EULISSE, Giulio", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "5ed09727f43058232b24b8ad13f529a8", "affiliation": "Vilnius University (LT)", "_fossil": "contributionParticipationMetadata", "fullName": "ABDURACHMANOV, David", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "e82c46a3a0d49dd555c888d113eace3a", "affiliation": "University of Nebraska (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BOCKELMAN, Brian Paul", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "b63b642f0d515365e8d562be36ae7a4f", "affiliation": "Princeton University", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KNIGHT, Robert", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "9a8ce9aebaabdde7956acf3f73f3b7a5", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ELMER, Peter", "id": "4"}], "title": "Building a Tier-3 Based on ARMv8 64-bit Server-on-Chip for the WLCG", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T04:46:33.725585+00:00", "description": "", "title": "20150413-chep15-elmer-tier3.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/500\/attachments\/578787\/796971\/20150413-chep15-elmer-tier3.pdf", "filename": "20150413-chep15-elmer-tier3.pdf", "content_type": "application\/pdf", "type": "file", "id": 796971, "size": 1402332}], "title": "Slides", "default_folder": false, "id": 578787, "description": ""}], "_type": "Contribution", "description": "Deploying the Worldwide LHC Computing Grid (WLCG) was greatly\r\nfacilitated by the convergence, around the year 2000, on Linux and\r\ncommodity x86 processors as a standard scientific computing platform.\r\nThis homogeneity enabled a relatively simple \"build once, run\r\nanywhere\" model for applications. A number of factors are now driving\r\ninterest in alternative platforms. Power limitations at the level\r\nof individual processors, and in aggregate in computer centers,\r\nplace greater emphasis on power efficiency issues. The rise of\r\nmobile computing, based primarily on ARM processors with a different\r\nintellectual property model, has also created interest in ARM as\r\nan potential general purpose architecture for the server market.\r\n\r\nWe report our experience building a demonstrator Grid cluster using\r\nsystems with ARMv8 64-bit processors, capable of running production-style\r\nworkflows. We present what we have learned regarding the use of\r\nboth application and Open Science Grid (OSG) software on ARMv8\r\n64-bit processors, as well as issues related to the need to manage\r\nheterogeneous mixes of x86_64 and ARMv8 64-bit processors in a Grid\r\nenvironment.  We also report the hardware experience we have gained\r\nwhile building the cluster.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578787", "resources": [{"_type": "LocalFile", "name": "20150413-chep15-elmer-tier3.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/500\/attachments\/578787\/796971\/20150413-chep15-elmer-tier3.pdf", "fileName": "20150413-chep15-elmer-tier3.pdf", "_fossil": "localFileMetadata", "id": "796971", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/500", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "501", "speakers": [{"_type": "ContributionParticipation", "emailHash": "aaedaf0b6923da9472cbc33726464d1d", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "PETZOLD, Andreas", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "aaedaf0b6923da9472cbc33726464d1d", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "PETZOLD, Andreas", "id": "0"}], "title": "Building a bridge between cloud storage and GPFS", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T01:36:08.815831+00:00", "description": "", "title": "CHEP_2015_Cloud_GPFS_Poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/501\/attachments\/578788\/796972\/CHEP_2015_Cloud_GPFS_Poster.pdf", "filename": "CHEP_2015_Cloud_GPFS_Poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796972, "size": 258637}], "title": "Slides", "default_folder": false, "id": 578788, "description": ""}], "_type": "Contribution", "description": "The possibilities of cloud storage for use in HEP computing have been the topic of many studies and trials. The typical cloud storage values, easily accessible and expandable, relatively cheap and with a light weight interface have become available for local storage as well. Initially as part of larger environments like Open Nebula or OpenStack Swift, vendors now offer value storage with integrated object based interfaces. Although cloud storage itself can be accessed with little effort, the integration with existing storage is done at the application level. However, this approach ties the storage to the application and more importantly, is not usable for all applications because these require regular POSIX semantics. The German Tier 1 GridKa as well as the Large Scale Data Facility (LSDF) at KIT, uses GPFS as the underlying file system for most of its storage. In recent versions GPFS offers a built-in rule and policy engine that allows complex data management operations including migrating data to external storage systems. The policy engine can drive hierarchical storage management (HSM) in which a condition that matches a defined characteristic triggers an external event such as the copy of a file or the movement of files from and to the file system. Interfaced to GPFS, cloud storage has the potential to be used transparently for all applications.\r\n\r\nIn this paper we present the first results of experiments where data is migrated from and recalled to a GPFS system that serves as storage for an xrootd disk pool. The data moves between the GPFS and local cloud storage and as a result, the xrootd system operates transparently on disk-based virtual storage that is several times larger than the actual file system. The results of the presented solution are compared with the traditional disk to tape HSM in use at GridKa. The experiences with the automated migration will be very helpful for the design of large and cost effective storage systems.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578788", "resources": [{"_type": "LocalFile", "name": "CHEP_2015_Cloud_GPFS_Poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/501\/attachments\/578788\/796972\/CHEP_2015_Cloud_GPFS_Poster.pdf", "fileName": "CHEP_2015_Cloud_GPFS_Poster.pdf", "_fossil": "localFileMetadata", "id": "796972", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "2474f49629543575f00823f5238b9e36", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "VAN WEZEL, Jos", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/501", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "469", "speakers": [{"_type": "ContributionParticipation", "emailHash": "20bb69b559e97a92f35eb4aae4b5b3bd", "affiliation": "LMU M\u00fcnchen", "_fossil": "contributionParticipationMetadata", "fullName": "SCHL\u00dcTER, Tobias", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "20bb69b559e97a92f35eb4aae4b5b3bd", "affiliation": "LMU M\u00fcnchen", "_fossil": "contributionParticipationMetadata", "fullName": "SCHL\u00dcTER, Tobias", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "6c1156b6b57c1aa339bf2e6132ff1d5a", "affiliation": "Technische Universit\u00e4t M\u00fcnchen", "_fossil": "contributionParticipationMetadata", "fullName": "RAUCH, Johannes", "id": "1"}], "title": "The GENFIT Library for Track Fitting and its Performance in Belle II", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "GENFIT is an experiment-independent, universal track-fitting package, available under a free software license.  It implements a variety of track-fitting algorithms and provides the surrounding functionality needed by particle physics experiments: general handling of detector hits, supplemented with example implementations for various detector types; track extrapolation code; a track-data model allowing for flexible storage and track-level operations such as the chaining of tracks from different subdetectors; residual calculation; interfaces to alignment codes; track and fit visualization, which can also serve didactical purposes.  The package has been significantly overhauled and extended in the course of the development of the Belle II software framework, taking into account lessons learned also in the COMPASS, FOPI and PANDA experiments.  GENFIT is particularly suited e.g. for new experiments that want to be able to use track fits of simulated data already at early stages in order to more realistically assess the impact of design choices.  We will report on the current status of the project, performance of the code within Belle II, and share some of\r\nthe lessons learned during the work on the project.", "track": "Track2: Offline software ", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/469", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "468", "speakers": [{"_type": "ContributionParticipation", "emailHash": "cd25500c313b389a485e4973a4d08310", "affiliation": "University of Melbourne", "_fossil": "contributionParticipationMetadata", "fullName": "HSU, Chia-Ling", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "cd25500c313b389a485e4973a4d08310", "affiliation": "University of Melbourne", "_fossil": "contributionParticipationMetadata", "fullName": "HSU, Chia-Ling", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "31a2e16ea0419e091f98907026634fb9", "affiliation": "University of Melbourne (AU)", "_fossil": "contributionParticipationMetadata", "fullName": "SEVIOR, Martin", "id": "1"}], "title": "The Belle II analysis on Grid", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T11:35:27.103204+00:00", "description": "", "title": "chep2015_hsu.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/468\/attachments\/578789\/796973\/CHEP_HSU_0408.pdf", "filename": "CHEP_HSU_0408.pdf", "content_type": "application\/pdf", "type": "file", "id": 796973, "size": 6964171}], "title": "Poster", "default_folder": false, "id": 578789, "description": ""}], "_type": "Contribution", "description": "The basf2 software framework has been developed the Belle II\r\nexperiment, the next generation B-factory experiment at the KEK\r\nLaboratory. Belle II will collect 50 times more data than the previous\r\nBelle experiment and has a commensurate increase in computing\r\nrequirements.\r\nConsequently Belle II has adopted a distributed computing solution to\r\nprovide the computing resources required of the experiment. The\r\ninterface is called gbasf2 and is designed to provide an easy\r\ntransition from an analysis done with basf2 to an analysis performed\r\non the grid. The output of the raw data processing are mDST files,\r\nconsisting of all relevant information for physics analyses. Another\r\nfile format called $\\mu$DST has been proposed which also records the\r\nreconstructed particle information.\r\nWe present a study of physics analysis with gbasf2 and compare the performance\r\nof the analysis methods which employ mDST and $\\mu$DST files on grid\r\nsites and on local resources.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578789", "resources": [{"_type": "LocalFile", "name": "chep2015_hsu.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/468\/attachments\/578789\/796973\/CHEP_HSU_0408.pdf", "fileName": "CHEP_HSU_0408.pdf", "_fossil": "localFileMetadata", "id": "796973", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/468", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "465", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6ba1f95da55f3a0622e4785d12b5cc25", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "NORMAN, Andrew", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6ba1f95da55f3a0622e4785d12b5cc25", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "NORMAN, Andrew", "id": "0"}], "title": "Large Scale Monte Carlo Simulation of neutrino interactions using the Open Science Grid and Commercial Clouds", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T19:22:49.110619+00:00", "description": "", "title": "nova_mc_osg_aws_chep2015_poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/465\/attachments\/578790\/796974\/nova_mc_osg_aws_chep2015_poster.pdf", "filename": "nova_mc_osg_aws_chep2015_poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796974, "size": 8468795}], "title": "Poster", "default_folder": false, "id": 578790, "description": ""}], "_type": "Contribution", "description": "Modern long baseline neutrino experiments like the NOvA experiment at Fermilab, require large scale, compute intensive simulations of their neutrino beam fluxes and backgrounds induced by cosmic rays.  The amount of simulation required to keep the systematic uncertainties in the simulation from dominating the final physics results is often 10x to 100x that of the actual detector exposure.  For the first physics results from NOvA this has meant the simulation of more than 2 billion cosmic ray events in the far detector and more than 300 million NuMI beam spill simulations.  Performing these high statistics levels of simulation have been made possible for NOvA through the use of the Open Science Grid and through large scale runs on commercial clouds like Amazon EC2.\r\n\r\nThis paper details the challenges in performing large scale simulation in these environments and how the computing infrastructure for the NOvA experiment has been adapted to seamlessly support the routing of different simulation and data processing tasks to these resources.  We discuss the optimization of the simulation computing model, data movement and computation to match the commercial computing environment.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578790", "resources": [{"_type": "LocalFile", "name": "nova_mc_osg_aws_chep2015_poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/465\/attachments\/578790\/796974\/nova_mc_osg_aws_chep2015_poster.pdf", "fileName": "nova_mc_osg_aws_chep2015_poster.pdf", "_fossil": "localFileMetadata", "id": "796974", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "fb301d8355628f15b8301c7992ca22fd", "affiliation": "Iowa State University", "_fossil": "contributionParticipationMetadata", "fullName": "DAVIES, Gavin", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "66391efd180489cb4b61725469458858", "affiliation": "University of Sussex", "_fossil": "contributionParticipationMetadata", "fullName": "TAMSETT, Matthew", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "5736aa29351bae70605b834f6af249c0", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "TIMM, Steven", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "c06196a1fdeb4395a54918aa9d91ed07", "affiliation": "Tufts University", "_fossil": "contributionParticipationMetadata", "fullName": "MAYER, Nathan", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "99b5f0be25ba5de19c37365d55205e60", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "FLUMERFELT, Eric", "id": "5"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/465", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:30:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "464", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6ba1f95da55f3a0622e4785d12b5cc25", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. NORMAN, Andrew", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6ba1f95da55f3a0622e4785d12b5cc25", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. NORMAN, Andrew", "id": "0"}], "title": "Large Scale Management of Physicist\u2019s Personal Analysis Data", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T02:48:24.442496+00:00", "description": "", "title": "sam_for_users_chep2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/464\/attachments\/578791\/796975\/sam_for_users_chep2015.pdf", "filename": "sam_for_users_chep2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796975, "size": 2033520}, {"_type": "attachment", "modified_dt": "2015-04-13T02:48:24.442496+00:00", "description": "", "title": "sam_for_users_chep2015.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/464\/attachments\/578791\/796976\/sam_for_users_chep2015.pptx", "filename": "sam_for_users_chep2015.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796976, "size": 33595265}], "title": "Slides", "default_folder": false, "id": 578791, "description": ""}], "_type": "Contribution", "description": "The ability of modern HEP experiments to acquire and process unprecedented amounts of data and simulation have led to an explosion in the volume of information that individual scientists deal with on a daily basis.  This explosion has resulted in a need for individuals to generate and keep large \u201cpersonal analysis\u201d data sets which represent the skimmed portions of official data collections pertaining to their specific analysis.  These personal analysis and simulation sets  represent a significant reduction in size compared to the original data, but they can still be many terabytes or tens of terabytes in size and consist of tens of thousands of files.  When this personal data is aggregated across the many physicists in a single analysis group or experiment it can represent data volumes on par with or exceeding the official \u201cproduction\u201d samples which require special data handling techniques and storage systems to deal with effectively.\r\n\r\nIn this paper we explore the toolsets, analysis models and changes to the Fermilab computing infrastructure which have been developed and deployed by the NOvA experiment to allow experimenters to effectively manage their personal analysis data and other data that falls outside of the typically centrally managed production chains.  In particular we describe the models and tools that are being used to allow NOvA to leverage Fermilab storage resources that are sufficient to meet their analysis needs, without imposing management burdens of specific quotas on users or groups of users, without relying on traditional central disk facilities and without having to constantly police individuals users usage.  We discuss the storage mechanisms and the caching algorithms that are being used as well as the toolkits that have been developed to allow the users to easily operate with terascale+ datasets.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578791", "resources": [{"_type": "LocalFile", "name": "sam_for_users_chep2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/464\/attachments\/578791\/796975\/sam_for_users_chep2015.pdf", "fileName": "sam_for_users_chep2015.pdf", "_fossil": "localFileMetadata", "id": "796975", "_deprecated": true}, {"_type": "LocalFile", "name": "sam_for_users_chep2015.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/464\/attachments\/578791\/796976\/sam_for_users_chep2015.pptx", "fileName": "sam_for_users_chep2015.pptx", "_fossil": "localFileMetadata", "id": "796976", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "336e636659f12d5fe7aede25fd33f666", "affiliation": "University of Virginia", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GROUP, Robert", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "66391efd180489cb4b61725469458858", "affiliation": "University of Sussex", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. TAMSETT, Matthew", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "21f186c57f345bba6eabf3bec24fbb1b", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ILLINGWORTH, Robert", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "004a1bf6b35a678030d9d18e46aa7de6", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "MENGEL, Marc", "id": "4"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/464", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "467", "speakers": [{"_type": "ContributionParticipation", "emailHash": "495be4f5790553fa157ed05c614db307", "affiliation": "FNAL", "_fossil": "contributionParticipationMetadata", "fullName": "SLYZ, Marko", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "9bf0fda1db06f391c8e7b54d68c0b8ce", "affiliation": "FNAL", "_fossil": "contributionParticipationMetadata", "fullName": "BUCKLEY-GEER, Elizabeth", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "495be4f5790553fa157ed05c614db307", "affiliation": "FNAL", "_fossil": "contributionParticipationMetadata", "fullName": "SLYZ, Marko", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "739b9134e35d3b477c26820ff8cfe19a", "affiliation": "FNAL", "_fossil": "contributionParticipationMetadata", "fullName": "YANNY, Brian", "id": "2"}], "title": "Dark Energy Survey Computing on FermiGrid", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The Dark Energy Survey (DES) uses a CCD camera installed in the Blanco\r\ntelescope in Cerro Tololo, Chile. The goal of the survey is to study\r\nthe effect known as Dark Energy.\r\n\r\nDES uses Fermigrid for nightly processing, for quality assessement of\r\nimages, and for the detection of type 1A Super Novae. Nighly\r\nprocessing needs to be carried out for each of the 105 nights in a\r\nseason that DES acquires data, and must be completed before\r\nobservations begin on the following night. This was seen as feasible\r\non Fermigrid because the requirements for memory and CPU were similar\r\nto those of HEP jobs. Fermigrid used some HEP computing techniques --\r\namong them, the CernVM File System (CVMFS) for storing software, and\r\ndisks on the worker nodes for each job's scratch space -- that were\r\nnovel to cosmology experiments accustomed to using HPC machines.\r\n\r\nAt the same time, we learned of other compute requirements which were\r\nnot well served by the the existing model, but were still well suited\r\nto the basic approach of loosely-coupled, high throughput computing\r\nused in HEP. We are working to support workflows with large memory\r\nrequirements, workflows that require multi-core cpus, and workflows\r\nthat cache calibration data on the worker nodes.\r\n\r\nDES started running production on Fermigrad in August of 2014. We\r\npresent how we are addressing some notable problems with the current\r\nsystem, including the variable amount of time it takes for files to be\r\nvisible on worker nodes after they're first uploaded to CVMFS, low CPU\r\nefficiency, and jobs hanging while finishing.", "track": "Track5: Computing activities and Computing models", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/467", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "466", "speakers": [{"_type": "ContributionParticipation", "emailHash": "29f189f465a51b3d324b424a35a93dc4", "affiliation": "KISTI", "_fossil": "contributionParticipationMetadata", "fullName": "KWAK, Jae-Hyuck", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "29f189f465a51b3d324b424a35a93dc4", "affiliation": "KISTI", "_fossil": "contributionParticipationMetadata", "fullName": "KWAK, Jae-Hyuck", "id": "0"}], "title": "Improvement of AMGA Python Client Library for the Belle II Experiment", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T06:14:09.928189+00:00", "description": "", "title": "CHEP2015_poster-kwak-0410.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/466\/attachments\/578792\/796977\/CHEP2015_poster-kwak-0410.pdf", "filename": "CHEP2015_poster-kwak-0410.pdf", "content_type": "application\/pdf", "type": "file", "id": 796977, "size": 889393}], "title": "Poster", "default_folder": false, "id": 578792, "description": ""}], "_type": "Contribution", "description": "This paper describes the recent improvement of AMGA python client library for the Belle II Experiment. We were drawn to the action items about library improvement after in-depth discussions with the developer of the Belle II distributed computing group. It includes GSI support, client-side metadata federation support and atomic operation support. Some of the improvements were already applied to AMGA python client library bundled with the Belle II distributed computing software. The recent mass Monte-Carlo production campaign shows AMGA python client library operates in a reliable stability.", "track": "Track3: Data store and access ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578792", "resources": [{"_type": "LocalFile", "name": "CHEP2015_poster-kwak-0410.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/466\/attachments\/578792\/796977\/CHEP2015_poster-kwak-0410.pdf", "fileName": "CHEP2015_poster-kwak-0410.pdf", "_fossil": "localFileMetadata", "id": "796977", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "e4ea4c83ee2fa3fad4dbf535e57e2e80", "affiliation": "KISTI", "_fossil": "contributionParticipationMetadata", "fullName": "PARK, Geun Chul", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c297ea0cd83410db2b176f68881d0f2b", "affiliation": "KISTI", "_fossil": "contributionParticipationMetadata", "fullName": "HUH, Tae Sang", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "d872ae6e61ab96602690aabd64689cd3", "affiliation": "KISTI", "_fossil": "contributionParticipationMetadata", "fullName": "HWANG, Soonwook", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/466", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:30:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:45:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "461", "speakers": [{"_type": "ContributionParticipation", "emailHash": "69b973a64a9f8889a446059e1db57cff", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BERZANO, Dario", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "69b973a64a9f8889a446059e1db57cff", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BERZANO, Dario", "id": "0"}], "title": "Lightweight scheduling of elastic analysis containers in a competitive cloud environment: a Docked Analysis Facility for ALICE", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T08:54:13.884252+00:00", "description": "", "title": "20150416-chep2015-dberzano-alice_cernvm_containers.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/461\/attachments\/578793\/796978\/20150416-chep2015-dberzano-alice_cernvm_containers.pdf", "filename": "20150416-chep2015-dberzano-alice_cernvm_containers.pdf", "content_type": "application\/pdf", "type": "file", "id": 796978, "size": 10559254}], "title": "Slides", "default_folder": false, "id": 578793, "description": ""}], "_type": "Contribution", "description": "During the last years, several Grid computing centers chose virtualization as a better way to manage diverse use cases with self-consistent environments on the same bare infrastructure. The maturity of control interfaces (such as OpenNebula and OpenStack) opened the possibility to easily change the amount of resources assigned to each use case by simply turning on and off virtual machines. Some of those private clouds use, in production, copies of the Virtual Analysis Facility, a fully virtualized and self-contained batch analysis cluster capable of expanding and shrinking automatically upon need: however, resources starvation occurs frequently as expansion has to compete with other virtual machines running long-living batch jobs. Such batch nodes cannot relinquish their resources in a timely fashion: the more jobs they run, the longer it takes to drain them and shut off, and making one-job virtual machines introduces a non-negligible virtualization overhead. By improving several components of the Virtual Analysis Facility we have realized an experimental \u201cDocked\u201d Analysis Facility for ALICE, which leverages containers instead of virtual machines for providing performance and security isolation. We will present the techniques we have used to address practical problems, such as software provisioning through CVMFS, as well as our considerations on the maturity of containers for High Performance Computing. As the abstraction layer is thinner, our Docked Analysis Facilities may feature a more fine-grained sizing, down to single-job node containers: we will show how this approach will positively impact automatic cluster resizing by deploying lightweight pilot containers instead of replacing central queue polls.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578793", "resources": [{"_type": "LocalFile", "name": "20150416-chep2015-dberzano-alice_cernvm_containers.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/461\/attachments\/578793\/796978\/20150416-chep2015-dberzano-alice_cernvm_containers.pdf", "fileName": "20150416-chep2015-dberzano-alice_cernvm_containers.pdf", "_fossil": "localFileMetadata", "id": "796978", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "771122777d10e8f8dc1c3aa7156922e8", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BLOMER, Jakob", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "3f41c6112486480c3ebd506d39003e9e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BUNCIC, Predrag", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "8c3829aeb9782b9da4c2474b6257cbab", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHARALAMPIDIS, Ioannis", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "5464ee52c2740264980ed36d4553e207", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GANIS, Gerardo", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "b7670e16208d58f21cb7eff8f0036404", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MEUSEL, Rene", "id": "5"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/461", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "460", "speakers": [{"_type": "ContributionParticipation", "emailHash": "69b973a64a9f8889a446059e1db57cff", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BERZANO, Dario", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "69b973a64a9f8889a446059e1db57cff", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BERZANO, Dario", "id": "0"}], "title": "A virtual validation cluster for ALICE software releases based on CernVM", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T08:56:06.892286+00:00", "description": "", "title": "20150415-chep2015-dberzano-alice_release_validation.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/460\/attachments\/578794\/796979\/20150415-chep2015-dberzano-alice_release_validation.pdf", "filename": "20150415-chep2015-dberzano-alice_release_validation.pdf", "content_type": "application\/pdf", "type": "file", "id": 796979, "size": 1468740}], "title": "Slides", "default_folder": false, "id": 578794, "description": ""}], "_type": "Contribution", "description": "One of the most important steps of software lifecycle is Quality Assurance: this process comprehends both automatic tests and manual reviews, and all of them must pass successfully before the software is approved for production. Some tests, such as source code static analysis, are executed on a single dedicated service: in High Energy Physics, a full simulation and reconstruction chain on a distributed computing environment, backed with a sample \u201cgolden\u201d dataset, is also necessary for the quality sign-off. The ALICE experiment uses dedicated and virtualized computing infrastructures for the Release Validation in order not to taint the production environment (i.e. CVMFS and the Grid) with non-validated software and validation jobs: the ALICE Release Validation cluster is a disposable virtual cluster appliance based on CernVM and the Virtual Analysis Facility, capable of deploying on-demand, and with a single command, a dedicated virtual HTCondor cluster with an automatically scalable number of virtual workers on any cloud supporting the standard EC2 interface. Input and output data are externally stored on EOS, and a dedicated CVMFS service is used to provide the software to be validated. We will show how the Release Validation Cluster deployment and disposal are completely transparent for the Release Manager, who simply triggers the validation from the ALICE build system\u2019s web interface. CernVM 3, based entirely on CVMFS, permits to boot any snapshot of the operating system in time: we will show how this allows us to certify each ALICE software release for an exact CernVM snapshot, addressing the problem of Long Term Data Preservation by ensuring a consistent environment for software execution and data reprocessing in the future.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578794", "resources": [{"_type": "LocalFile", "name": "20150415-chep2015-dberzano-alice_release_validation.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/460\/attachments\/578794\/796979\/20150415-chep2015-dberzano-alice_release_validation.pdf", "fileName": "20150415-chep2015-dberzano-alice_release_validation.pdf", "_fossil": "localFileMetadata", "id": "796979", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "83231eb6b32f41dda19d6528e53efdb9", "affiliation": "Johann-Wolfgang-Goethe Univ. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "KRZEWICKI, Mikolaj", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/460", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:15:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:30:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "463", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6ba1f95da55f3a0622e4785d12b5cc25", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. NORMAN, Andrew", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "004a1bf6b35a678030d9d18e46aa7de6", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "MENGEL, Marc", "id": "0"}], "title": "Replacing the Engines without Stopping The Train; How A Production Data Handling System was Re-engineered and Replaced without anyone Noticing.", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:30:28.461608+00:00", "description": "", "title": "sam_upgrade_CHEP_2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/463\/attachments\/578795\/796980\/sam_upgrade_CHEP_2015.pdf", "filename": "sam_upgrade_CHEP_2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796980, "size": 1406293}], "title": "Slides", "default_folder": false, "id": 578795, "description": ""}], "_type": "Contribution", "description": "As high energy physics experiments have grown, their operational needs and requirements they place on computing systems change.  These changes often require new technical solutions to meet the increased demands and functionalities of the science.  How do you affect sweeping change to core infrastructure, without causing major interruptions to the scientific programs?\r\n\r\nThis paper explores the operational challenges, procedures and techniques that were used to completely replace the core data handling infrastructure for the Fermilab experimental program, while continuing to store and deliver more than 1PB of data per month to the analysis and computing efforts of the experiments.  It discusses designs patterns like the Command Pattern, the Fa\u00e7ade and Delegation that were employed at different stages of the project and how they worked and didn\u2019t work to hide the underlying changes that were being made.  We discuss how parallel production and parasitic integration systems were used to perform \u201cat scale\u201d performance testing and data validation prior to switch overs and how this allowed for more robust test environments which would not have been possible to replicate in traditional test settings.  We describe how the experimenters were engaged in the transition process and how they were used to propagate subtle but necessary changes to the client interfaces over a period of 18 months.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578795", "resources": [{"_type": "LocalFile", "name": "sam_upgrade_CHEP_2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/463\/attachments\/578795\/796980\/sam_upgrade_CHEP_2015.pdf", "fileName": "sam_upgrade_CHEP_2015.pdf", "_fossil": "localFileMetadata", "id": "796980", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "9d455a80b2cf39ee576e6fec6abca560", "affiliation": "F", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. DIESBURG, Michael", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "6ba1f95da55f3a0622e4785d12b5cc25", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. NORMAN, Andrew", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "437f94b0a1fcddf44d09c43cd4b1f7ca", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LYON, Adam", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "21f186c57f345bba6eabf3bec24fbb1b", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ILLINGWORTH, Robert", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "8d310af91de2117be771c8d1aa323b59", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "GHEITH, Michael", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "005245be8af5223a3e5efd8f5a42197b", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "WHITE, Steve", "id": "6"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/463", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:15:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "462", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6ba1f95da55f3a0622e4785d12b5cc25", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. NORMAN, Andrew", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6ba1f95da55f3a0622e4785d12b5cc25", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. NORMAN, Andrew", "id": "0"}], "title": "Archiving Scientific Data outside of the traditional High Energy Physics  Domain, using the National Archive Facility at Fermilab", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T06:37:52.540108+00:00", "description": "", "title": "chep2015_archiving_fts.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/462\/attachments\/578796\/796981\/chep2015_archiving_fts.pdf", "filename": "chep2015_archiving_fts.pdf", "content_type": "application\/pdf", "type": "file", "id": 796981, "size": 1614569}, {"_type": "attachment", "modified_dt": "2015-04-13T06:37:52.540108+00:00", "description": "", "title": "chep2015_archiving_fts.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/462\/attachments\/578796\/796982\/chep2015_archiving_fts.pptx", "filename": "chep2015_archiving_fts.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796982, "size": 3322439}], "title": "Slides", "default_folder": false, "id": 578796, "description": ""}], "_type": "Contribution", "description": "Many experiments in the HEP and Astrophysics communities generate large extremely valuable datasets, which need to be efficiently cataloged and recorded to archival storage.  These datasets, both new and legacy, are often structured in a manner that is not conducive to storage and cataloging with modern data handling systems and large file archive facilities.  In this paper we discuss in detail how we have created a robust toolset and simple portal into the Fermilab Archive Facility, which allows for scientific data to be quickly imported, organized and retrieved from the 0.650 Exabyte facility.\r\n\r\nIn particular we discuss how the data from the Sudbury Neutrino Observatory (SNO) for the COUPP dark matter detector was aggregated, cataloged, archived and re-organized to permit it to be retrieved and analyzed using modern distributed computing resources both at Fermilab and on the Open Science Grid.  We pay particular attention to the methods that were employed to \u201cuniquify\u201d the namespaces for the data, derive metadata for the over 460,000 image series taken by the COUP experiment and what was required to map that information into coherent datasets that could be stored and retrieved using the large scale archives systems.\r\n\r\nWe describe the data transfer and cataloging engines that are used for data importation and how these engines have been setup to import data from the data acquisition systems of ongoing experiments at non-Fermilab remote sites including the Laboratori Nazionali del Gran Sasso and the Ash River Laboratory in Orr, Minnesota.  We also describe how large University computing sites around the world are using the system to store and retrieve large volumes of simulation and experiment data for physics analysis.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578796", "resources": [{"_type": "LocalFile", "name": "chep2015_archiving_fts.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/462\/attachments\/578796\/796981\/chep2015_archiving_fts.pdf", "fileName": "chep2015_archiving_fts.pdf", "_fossil": "localFileMetadata", "id": "796981", "_deprecated": true}, {"_type": "LocalFile", "name": "chep2015_archiving_fts.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/462\/attachments\/578796\/796982\/chep2015_archiving_fts.pptx", "fileName": "chep2015_archiving_fts.pptx", "_fossil": "localFileMetadata", "id": "796982", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "9d455a80b2cf39ee576e6fec6abca560", "affiliation": "F", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. DIESBURG, Michael", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "21f186c57f345bba6eabf3bec24fbb1b", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ILLINGWORTH, Robert", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "437f94b0a1fcddf44d09c43cd4b1f7ca", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LYON, Adam", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "004a1bf6b35a678030d9d18e46aa7de6", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "MENGEL, Marc", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "8d310af91de2117be771c8d1aa323b59", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "GHEITH, Michael", "id": "5"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/462", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "168", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e944f196404337975d14b895283f17fd", "affiliation": "Budker Institute of Nuclear Physics (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "ANISENKOV, Alexey", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e944f196404337975d14b895283f17fd", "affiliation": "Budker Institute of Nuclear Physics (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "ANISENKOV, Alexey", "id": "0"}], "title": "AGIS: Evolution of Distributed Computing information system for ATLAS", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T00:14:55.007054+00:00", "description": "", "title": "AGIS_CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/168\/attachments\/578797\/796983\/AGIS_CHEP2015.pdf", "filename": "AGIS_CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796983, "size": 967654}], "title": "Slides", "default_folder": false, "id": 578797, "description": ""}], "_type": "Contribution", "description": "The variety of the ATLAS Computing Infrastructure requires a central information system to define the topology of computing resources and to store the different parameters and configuration data which are needed by the various ATLAS software components.\r\nThe ATLAS Grid Information System (AGIS) is the system designed to integrate configuration and status information about resources, services and topology of the computing infrastructure used by ATLAS Distributed Computing applications and services. Being an intermediate middleware system between clients and external information sources (like central BDII, GOCDB, MyOSG), AGIS defines the relations between experiment specific used resources and physical distributed computing capabilities.\r\nWe describe the evolution and the recent developments of AGIS functionalities, including new bulk update implementation for user interfaces. The improvements of information model are also shown, in particular the consolidation of computing resources definition for the ATLAS workload management system (PanDA). We will explain how the AGIS flexibility and scalability allow the transparent definition of virtual resources like Cloud and HPC capabilities recently become widely used by ATLAS.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578797", "resources": [{"_type": "LocalFile", "name": "AGIS_CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/168\/attachments\/578797\/796983\/AGIS_CHEP2015.pdf", "fileName": "AGIS_CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796983", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "a0dd1a3a7589f1e2a04cc967f2ae6c39", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DI GIROLAMO, Alessandro", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/168", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "169", "speakers": [{"_type": "ContributionParticipation", "emailHash": "12c18494664f7a1a64e606d3a72e883e", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "BRASOLIN, Franco", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "12c18494664f7a1a64e606d3a72e883e", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "BRASOLIN, Franco", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "defcbbd9878e1424b343d5bd3014e63f", "affiliation": "Institute for High Energy Physics  (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "ZAYTSEV, Alexandre", "id": "1"}], "title": "Design, Results, Evolution and Status of the ATLAS simulation in Point1 project.", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-03-31T07:26:53.344831+00:00", "description": "", "title": "CHEP2015_SP1_poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/169\/attachments\/578798\/796984\/CHEP2015_SP1_poster.pdf", "filename": "CHEP2015_SP1_poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796984, "size": 2012449}], "title": "Poster", "default_folder": false, "id": 578798, "description": ""}], "_type": "Contribution", "description": "During the LHC long shutdown period (LS1), that started in 2013,  the simulation in Point1 (Sim@P1) project takes advantage in an opportunistic way of the trigger and data acquisition (TDAQ) farm of the ATLAS experiment. The farm provides more than 1500 computer nodes, and they are particularly suitable for running event generation and Monte Carlo production jobs that are mostly CPU and not I\/O bound. It is capable of running up to 2500 virtual machines (VM) provided with 8 CPU cores each, for a total of up to 20000 parallel running jobs. This contribution gives a thorough review of the design, the results and the evolution of the Sim@P1 project operating a large scale Openstack based virtualized platform deployed on top of the ATLAS TDAQ farm computing resources. During LS1, Sim@P1 was one of the most productive GRID sites: it delivered more than 50 million CPU-hours and it generated more than 1.7 billion Monte Carlo events to various analysis communities within the ATLAS collaboration. \r\nThe particular design aspects are presented: the virtualization platform exploited by the Sim@P1 project permits to avoid interferences with TDAQ operations and, more important, it guarantees the security and the usability of the ATLAS private network. The Cloud infrastructure allows to decouple the needed support on both infrastructural (hardware, virtualization layer) and logical (Grid site support and handling the job lifecycle) levels. In particular in this note we focus on the operational aspects of such a large system for the upcoming LHC Run 2 period: customized, simple, reliable and efficient tools are needed to quickly switch from Sim@P1 to TDAQ mode and vice versa to exploit the TDAQ resources when they are not used for the data acquisition, even for short period. We also describe the evolution of the central Openstack infrastructure as it was upgraded from Folsom to Icehouse release and the scalability issues we have addressed.\r\nThe success of the Sim@P1 project is due to the continuous combined efforts of the ATLAS TDAQ SysAdmins and NetAdmins teams, CERN IT and the RHIC & ATLAS Computing Facility (RACF) at BNL.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578798", "resources": [{"_type": "LocalFile", "name": "CHEP2015_SP1_poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/169\/attachments\/578798\/796984\/CHEP2015_SP1_poster.pdf", "fileName": "CHEP2015_SP1_poster.pdf", "_fossil": "localFileMetadata", "id": "796984", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "af730dbe979e72aa88ba4063a0dc7c51", "affiliation": "University of Johannesburg (ZA)", "_fossil": "contributionParticipationMetadata", "fullName": "BALLESTRERO, Sergio", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "d758774bbd3d12133a6861082ec8c2d3", "affiliation": "Polytechnic University of Bucharest (RO)", "_fossil": "contributionParticipationMetadata", "fullName": "CONTESCU, Cristian", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "2d6446f1d79006ce6dc10bba0cef2859", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FAZIO, Daniel", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "a0dd1a3a7589f1e2a04cc967f2ae6c39", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DI GIROLAMO, Alessandro", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "d9649b94178b18e38f1f945d86a13425", "affiliation": "University of Johannesburg (ZA)", "_fossil": "contributionParticipationMetadata", "fullName": "LEE, Christopher Jon", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "150631322bd9dfbef0d9fcc8a2d2cc99", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "POZO ASTIGARRAGA, Mikel Eukeni", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "ab3fe249ad2023c49cd56c4051abdaf1", "affiliation": "University of California Irvine (US)", "_fossil": "contributionParticipationMetadata", "fullName": "SCANNICCHIO, Diana", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "d4eaa2c0cd5e7e8b1704f5c3c19261ed", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "WANG, Fuqiang", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "dc1e820c954e530d1fced8685740a83c", "affiliation": "Universitat Aut\u00f2noma de Barcelona", "_fossil": "contributionParticipationMetadata", "fullName": "SEDOV, Alexey", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "9322fc958fe01fe63ff2db9334336e5b", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. FRESSARD-BATRANEANU, Silvia-Maria", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "2fa117faa146ac537e42ffd3950d8cce", "affiliation": "University of Washington (US)", "_fossil": "contributionParticipationMetadata", "fullName": "TWOMEY, Matthew Shaun", "id": "12"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/169", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:30:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "164", "speakers": [{"_type": "ContributionParticipation", "emailHash": "43742ae033cddcfe719e23c6ca360497", "affiliation": "University of Oslo (NO)", "_fossil": "contributionParticipationMetadata", "fullName": "CATMORE, James", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "3034074301a29ed1439e6f48e828ad6a", "affiliation": "University of Liverpool (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "LAYCOCK, Paul James", "id": "0"}], "title": "A New Petabyte-scale Data Derivation Framework for ATLAS", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-01T07:59:10.161141+00:00", "description": "", "title": "ATL-COM-SOFT-2015-020.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/164\/attachments\/578799\/796985\/ATL-COM-SOFT-2015-020.pdf", "filename": "ATL-COM-SOFT-2015-020.pdf", "content_type": "application\/pdf", "type": "file", "id": 796985, "size": 6660961}], "title": "Slides", "default_folder": false, "id": 578799, "description": ""}], "_type": "Contribution", "description": "During the Long shutdown of the LHC, the ATLAS collaboration overhauled its analysis model based on experience gained during Run 1.\u00a0 A significant component of the model is a \"Derivation Framework\" that takes the Petabyte-scale AOD output from ATLAS reconstruction and produces samples, typically Terabytes in size, targeted at specific analyses.\u00a0 The framework incorporates all of the functionality of the core reconstruction software, while producing outputs that are simply configured.\u00a0 Event selections are specified via strings, including support for logical operations.\u00a0 The output content can be highly optimised to minimise disk requirements, while maintaining the same C++ interface.\u00a0 The framework includes an interface to the late-stage physics analysis tools, ensuring that the final outputs are consistent with tool requirements.\u00a0 Finally, the framework allows several outputs to be produced for the same input, providing the possibility to optimise configurations to computing resources.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578799", "resources": [{"_type": "LocalFile", "name": "ATL-COM-SOFT-2015-020.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/164\/attachments\/578799\/796985\/ATL-COM-SOFT-2015-020.pdf", "fileName": "ATL-COM-SOFT-2015-020.pdf", "_fossil": "localFileMetadata", "id": "796985", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "43742ae033cddcfe719e23c6ca360497", "affiliation": "University of Oslo (NO)", "_fossil": "contributionParticipationMetadata", "fullName": "CATMORE, James", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "546ff539087e1a69e44940051172474e", "affiliation": "University of Texas at Arlington", "_fossil": "contributionParticipationMetadata", "fullName": "Ms. OZTURK, Nurcan", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "77cd36f169488fb132c45718a20945f4", "affiliation": "University of Cambridge (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "GILLAM, Thomas", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "87a100f703ae951e1cd6f8f0c55b8582", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CRANSHAW, Jack", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "041b592e938be4f6f24dd8e7bfee54cb", "affiliation": "University of Oslo (NO)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GRAMSTAD, Eirik", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "9f60b1ecea45598b615f77b2b41150f1", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "STEWART, Graeme", "id": "6"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/164", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:15:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "165", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6aef683082e2cae7cc6fb0e086c1b9be", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "TSULAIA, Vakho", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6aef683082e2cae7cc6fb0e086c1b9be", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "TSULAIA, Vakho", "id": "0"}], "title": "Running ATLAS workloads within massively parallel distributed applications using Athena Multi-Process framework (AthenaMP)", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T03:07:35.540729+00:00", "description": "", "title": "ATLAS-AthenaMP.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/165\/attachments\/578800\/796986\/ATLAS-AthenaMP.pdf", "filename": "ATLAS-AthenaMP.pdf", "content_type": "application\/pdf", "type": "file", "id": 796986, "size": 1069408}], "title": "Slides", "default_folder": false, "id": 578800, "description": ""}], "_type": "Contribution", "description": "AthenaMP is a multi-process version of the ATLAS reconstruction and data analysis framework Athena. By leveraging Linux fork and copy-on-write, it allows the sharing of memory pages between event processors running on the same compute node with little to no change in the application code. Originally targeted to optimize the memory footprint of reconstruction jobs, AthenaMP has demonstrated that it can reduce the memory usage of certain confugurations of ATLAS production jobs by a factor of 2. AthenaMP has also evolved to become the parallel event-processing core of the recently developed ATLAS infrastructure for fine-grained event processing (Event Service) which allows to run AthenaMP inside massively parallel distributed applications on hundreds of compute nodes simultaneously. We present the architecture of AthenaMP, various strategies implemented by AthenaMP for scheduling workload to worker processes (for example: Shared Event Queue and Shared Distributor of Event Tokens) and the usage of AthenaMP in the diversity of ATLAS event processing workloads on various computing resources: Grid, opportunistic resources and HPC.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578800", "resources": [{"_type": "LocalFile", "name": "ATLAS-AthenaMP.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/165\/attachments\/578800\/796986\/ATLAS-AthenaMP.pdf", "fileName": "ATLAS-AthenaMP.pdf", "_fossil": "localFileMetadata", "id": "796986", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "c3b302a4fc2fd1ca19138cb3a3e5479a", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CALAFIURA, Paolo", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "287eff907e8b88ac28442324d4258129", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LEGGETT, Charles", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "5408e1e18dffa5d9d8bbaa52be69ba44", "affiliation": "TRIUMF (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "SEUSTER, Rolf", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "3883f9db68feb16e68792b1483a250cd", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. VAN GEMMEREN, Peter", "id": "4"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/165", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:00:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "166", "speakers": [{"_type": "ContributionParticipation", "emailHash": "287eff907e8b88ac28442324d4258129", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LEGGETT, Charles", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "287eff907e8b88ac28442324d4258129", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LEGGETT, Charles", "id": "0"}], "title": "Development of a Next Generation Concurrent Framework for the ATLAS Experiment", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T01:13:04.591967+00:00", "description": "", "title": "ATLAS-Hive.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/166\/attachments\/578801\/796987\/ATLAS-Hive.pdf", "filename": "ATLAS-Hive.pdf", "content_type": "application\/pdf", "type": "file", "id": 796987, "size": 4800542}], "title": "Slides", "default_folder": false, "id": 578801, "description": ""}], "_type": "Contribution", "description": "The ATLAS experiment has successfully used its Gaudi\/Athena software framework for data taking and analysis during the first LHC run, with billions of events successfully processed. However, the design of Gaudi\/Athena dates from early 2000 and the software and the physics code has been written using a single threaded, serial design. This programming model has increasing difficulty in exploiting the potential of current CPUs, which offer their best performance only through taking full advantage of multiple cores and wide vector registers. Future CPU evolution will intensify this trend, with core counts increasing and memory per core falling. With current memory consumption for 64 bit ATLAS reconstruction in a high luminosity environment approaching 4GB, it will become impossible to fully occupy all cores in a machine without exhausting available memory.  However, since maximising performance per watt will be a key metric, a mechanism must be found to use all cores as efficiently as possible.\r\n\r\nIn this paper we report on our progress with a practical demonstration of the use of multi-threading in the ATLAS reconstruction software, using the GaudiHive framework. We have expanded support to Calorimeter, Inner Detector, and Tracking code, discussing what changes were necessary in order to allow the serially designed ATLAS code to run, both to the framework and to the tools and algorithms used. We report on both the performance gains, and what general lessons were learned about the code patterns that had been employed in the software and which patterns were identified as particularly problematic for multi-threading. We also present our findings on implementing a hybrid multi-threaded \/ multi-process framework, to take advantage of the strengths of each type of concurrency, while avoiding some of their corresponding limitations.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578801", "resources": [{"_type": "LocalFile", "name": "ATLAS-Hive.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/166\/attachments\/578801\/796987\/ATLAS-Hive.pdf", "fileName": "ATLAS-Hive.pdf", "_fossil": "localFileMetadata", "id": "796987", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "c3b302a4fc2fd1ca19138cb3a3e5479a", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CALAFIURA, Paolo", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "b0300a2ba72bacc349647c1313f4ba21", "affiliation": "University of Arizona (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LAMPL, Walter", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "9308ae80f0f8aaf14aef2fa1243f3fec", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MALON, David", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "7ef73b47dd59178dccf508dc1030e573", "affiliation": "University of Glasgow", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. STEWART, Graeme A", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "9f60b1ecea45598b615f77b2b41150f1", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "STEWART, Graeme", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "e3a9c215d966fc5e240cdb5f16cc8116", "affiliation": "University of Edinburgh (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "WYNNE, Benjamin Michael", "id": "6"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/166", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:00:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "167", "speakers": [{"_type": "ContributionParticipation", "emailHash": "db8f2733d62b979849b152bc3e130cfd", "affiliation": "Univ. Federal do Rio de Janeiro (BR)", "_fossil": "contributionParticipationMetadata", "fullName": "LANGE RAMOS, Bruno", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "0f0c464655b7a2b6c916cb50679d37a7", "affiliation": "Univ. Federal do Rio de Janeiro (BR)", "_fossil": "contributionParticipationMetadata", "fullName": "LANGE RAMOS, Bruno", "id": "5"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "db8f2733d62b979849b152bc3e130cfd", "affiliation": "Univ. Federal do Rio de Janeiro (BR)", "_fossil": "contributionParticipationMetadata", "fullName": "LANGE RAMOS, Bruno", "id": "0"}], "title": "An object-oriented approach to generating highly configurable Web interfaces for the ATLAS experiment", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T06:31:35.876478+00:00", "description": "", "title": "brunolange_CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/167\/attachments\/578802\/796988\/brunolange_CHEP2015.pdf", "filename": "brunolange_CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796988, "size": 8297581}], "title": "Slides", "default_folder": false, "id": 578802, "description": ""}], "_type": "Contribution", "description": "In order to manage a heterogeneous and worldwide collaboration, the ATLAS experiment developed web systems that range from supporting the process of publishing scientific papers to monitoring equipment radiation levels. These systems are vastly supported by Glance, a technology that was set forward in 2004 to create an abstraction layer on top of different databases; it automatically recognizes their modelling and generates web search interfaces. Fence (Front ENd ENgine for glaNCE) assembles classes to build applications by making extensive use of configuration files. It produces templates of the core JSON files on top of which it is possible to create Glance-compliant search interfaces. Once the database, its schemas and tables are defined using Glance, its records can be incorporated into the templates by escaping the returned values with a reference to the column identifier wrapped around double enclosing brackets. \r\nThe developer may also expand on available configuration files to create HTML forms and securely interact with the database. A token is issued within each deployed form as a random string of characters which must then be matched whenever it is posted. Additionally, once the user is authenticated through CERN's Shibboleth single sign-on, Fence assigns them roles and permissions as stored in the database. Clearance attributes can then be bound to individual inputs within their own JSON description so that whenever they are submitted, the resulting system verifies whether the user has the necessary permissions to edit them. Input validation is primarily carried out on the server side with PHP but, following progressive enhancement guidelines, verification routines may be additionally entrusted to the client side by enabling specific HTML5 data attributes which are then handed over to the jQuery validation plugin. User monitoring is accomplished by logging URL requests along with any POST data. The documentation is automatically published from the source code using the Doxygen tool and made accessible in a web interface. Fence, therefore, speeds up the implementation of Web software products while minimizing maintenance overhead and facilitating the comprehension of embedded rules and requirements.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578802", "resources": [{"_type": "LocalFile", "name": "brunolange_CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/167\/attachments\/578802\/796988\/brunolange_CHEP2015.pdf", "fileName": "brunolange_CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796988", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "fe96040e410a1df490d0452b81646714", "affiliation": "Univ. Federal do Rio de Janeiro (BR)", "_fossil": "contributionParticipationMetadata", "fullName": "MAIDANTCHIK, Carmen", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "7c7a58c3dbc6a08747afc9618c9d9903", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "POMMES, Kathy", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "cc1ec084a1188f56e517e0791b836ed1", "affiliation": "Univ. Federal do Rio de Janeiro (BR)", "_fossil": "contributionParticipationMetadata", "fullName": "VIEIRA AROSA, Breno", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "d7bd54adc433f4a39eb60f79d20e7e82", "affiliation": "Univ. Federal do Rio de Janeiro (BR)", "_fossil": "contributionParticipationMetadata", "fullName": "PAVANI NETO, Varlen", "id": "4"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/167", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": null, "keywords": [], "id": "160", "speakers": [{"_type": "ContributionParticipation", "emailHash": "73f9d874a0a0bd47c5e380afc1d800bf", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WALKER, Rodney", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "73f9d874a0a0bd47c5e380afc1d800bf", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WALKER, Rodney", "id": "0"}], "title": "ATLAS experience of the ARC CE", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The ARC Compute Element(CE) is a critical component of the NorduGrid middleware. Its unique ability is to manage the input and output data for jobs submitted, but it also has all the functionality of the Cream CE, as used in other parts of the WLCG. We therefore present a direct comparison with the Cream CE, for the CondorG submission of Panda production and analysis pilots. Performance, reliability and scaling are considered, but also ease of installation, configuration and maintenance have a large impact on administration manpower and are thus put on the same footing. Lastly, the data handling capability of the ARC CE, is compared to the usual ATLAS Distributed Data Management (DDM) method of delivering jobs input and output.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "fd06959b93c405340e9900abf962beb7", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "LAHIFF, Andrew David", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/contribution\/160", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "161", "speakers": [{"_type": "ContributionParticipation", "emailHash": "040e4690e862ef70aead477af54c707c", "affiliation": "Universitaet Bern (CH)", "_fossil": "contributionParticipationMetadata", "fullName": "SCIACCA, Francesco Giovanni", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "02bd81dc701ab62d43cb1a80954a5396", "affiliation": "Universitaet Bern (CH)", "_fossil": "contributionParticipationMetadata", "fullName": "HOSTETTLER, Michi", "id": "0"}], "title": "The ATLAS ARC ssh back-end to HPC", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T08:04:26.852250+00:00", "description": "", "title": "ATLAS-ARC-SSH-CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/161\/attachments\/578803\/796989\/ATLAS-ARC-SSH-CHEP2015.pdf", "filename": "ATLAS-ARC-SSH-CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796989, "size": 3771677}], "title": "Poster", "default_folder": false, "id": 578803, "description": ""}], "_type": "Contribution", "description": "The current distributed computing resources used for simulating and processing collision data collected by the LHC experiments are largely based on dedicated Linux clusters. Job control and software provisioning mechanisms are quite different from the common concept of self-contained HPC applications run by particular users on specific HPC systems.\u00a0This poster reports on the\u00a0development and\u00a0the\u00a0usage of a ssh back-end to the Advanced Resource Connector (ARC) middleware to enable HPC compliant access and on the corresponding software provisioning mechanisms.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578803", "resources": [{"_type": "LocalFile", "name": "ATLAS-ARC-SSH-CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/161\/attachments\/578803\/796989\/ATLAS-ARC-SSH-CHEP2015.pdf", "fileName": "ATLAS-ARC-SSH-CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796989", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "11d0d3621ad6265a31d681a3580e1ccc", "affiliation": "Universitaet Bern (CH)", "_fossil": "contributionParticipationMetadata", "fullName": "HAUG, Sigve", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "040e4690e862ef70aead477af54c707c", "affiliation": "Universitaet Bern (CH)", "_fossil": "contributionParticipationMetadata", "fullName": "SCIACCA, Francesco Giovanni", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/161", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "162", "speakers": [{"_type": "ContributionParticipation", "emailHash": "40eda71617669e056433e9c4682916ad", "affiliation": "Universita e INFN, Napoli (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "SANCHEZ PINEDA, Arturos", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6ccc07fd015ee291538d306b2a1f82dd", "affiliation": "Universita di Napoli Federico II-Universita e INFN", "_fossil": "contributionParticipationMetadata", "fullName": "SANCHEZ PINEDA, Arturo", "id": "0"}], "title": "Exploring JavaScript and ROOT technologies to create Web-based ATLAS analysis and monitoring tools", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T08:55:55.131964+00:00", "description": "", "title": "Poster_CHEP2015_v2.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/162\/attachments\/578804\/796990\/Poster_CHEP2015_v2.pdf", "filename": "Poster_CHEP2015_v2.pdf", "content_type": "application\/pdf", "type": "file", "id": 796990, "size": 5600274}], "title": "Poster", "default_folder": false, "id": 578804, "description": ""}], "_type": "Contribution", "description": "\u200eWe explore the potentialities of current web applications to create online interfaces that allow the visualization, interaction and real physics cut-based analysis and monitoring of processes trough a web browser. The project consists in the initial development of web-based and cloud computing services to allow students and researches to perform fast and very useful cut-based analysis on a browser, reading and using real data and official Monte-Carlo simulations stored in ATLAS computing facilities. Several tools are considered: ROOT, JavaScript and HTML. Our study case is the current cut-based \u200eH->ZZ->llqq analysis of the ATLAS experiment. Preliminary but satisfactory results have been obtained online; this presentation describes the tests and plans and future upgrades.", "track": "Track2: Offline software ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578804", "resources": [{"_type": "LocalFile", "name": "Poster_CHEP2015_v2.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/162\/attachments\/578804\/796990\/Poster_CHEP2015_v2.pdf", "fileName": "Poster_CHEP2015_v2.pdf", "_fossil": "localFileMetadata", "id": "796990", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "8193b5347d932796bdb266311437fae3", "affiliation": "C", "_fossil": "contributionParticipationMetadata", "fullName": "MOSKALETS, Tetiana", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/162", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "163", "speakers": [{"_type": "ContributionParticipation", "emailHash": "f93e30ef38e68512dad5beb650c2ba19", "affiliation": "Lancaster University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "JONES, Roger", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "43742ae033cddcfe719e23c6ca360497", "affiliation": "University of Oslo (NO)", "_fossil": "contributionParticipationMetadata", "fullName": "CATMORE, James", "id": "9"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "70768e8c55e20a0b81debbc89cfc688a", "affiliation": "The Hong Kong University of Science and Technology (HK)", "_fossil": "contributionParticipationMetadata", "fullName": "PROKOFIEV, Kirill", "id": "0"}], "title": "ATLAS strategy for primary vertex reconstruction during run-II of the LHC", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T02:57:27.143801+00:00", "description": "", "title": "ATL-SOFT-SLIDE-2015-169.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/163\/attachments\/578805\/796991\/ATL-SOFT-SLIDE-2015-169.pdf", "filename": "ATL-SOFT-SLIDE-2015-169.pdf", "content_type": "application\/pdf", "type": "file", "id": 796991, "size": 1137897}], "title": "Slides", "default_folder": false, "id": 578805, "description": ""}], "_type": "Contribution", "description": "Based on experience gained from run-I of the LHC, the ATLAS vertex reconstruction group has developed a refined primary vertex reconstruction strategy for run-II. \u00a0With instantaneous luminosity exceeding 10^34 cm-2 s-1, an average of  40 to 50 pp collisions per bunch crossing are expected. Together with the increase of the center-of-mass collision energy from 8 TeV to 13 TeV, this will create a challenging environment for primary vertex pattern recognition.\r\n\r\nThis contribution explains the ATLAS strategy for primary vertex reconstruction in high pile-up conditions. \u00a0The new approach is based on vertex seeding with a medical-imaging algorithm, adaptive reconstruction of vertex positions, and iterative recombination of occasional split vertices. The mathematical foundation and software implementation of the method are described in detail. Monte Carlo-based estimates of vertex reconstruction performance for LHC run-II are presented.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578805", "resources": [{"_type": "LocalFile", "name": "ATL-SOFT-SLIDE-2015-169.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/163\/attachments\/578805\/796991\/ATL-SOFT-SLIDE-2015-169.pdf", "fileName": "ATL-SOFT-SLIDE-2015-169.pdf", "_fossil": "localFileMetadata", "id": "796991", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "0cb8b52748e6f6a6eec3c8dd68c0a5f2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ARNAEZ, Olivier", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "495a888272e42dd4d030d8722bd19d72", "affiliation": "University of Toronto (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "RUDOLPH, Matthew Scott", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "8f41556b1f9149106389b96dcf2b5e60", "affiliation": "University of Copenhagen (DK)", "_fossil": "contributionParticipationMetadata", "fullName": "PEDERSEN, Lars Egholm", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "3c2764d6b7fa9c6cf0c0c7d6bf9f1cd3", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "PAGAN GRISO, Simone", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "68780e6845f3697b3950df7c5af04aff", "affiliation": "Lancaster University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "GRIMM, Kathryn", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "a0c178b458d4d1246c9016cd074d3b4f", "affiliation": "Lancaster University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BORISSOV, Guennadi", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "76e5b789e88c981ce01d91c02c5a4c91", "affiliation": "Lancaster University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "WHARTON, Andrew Mark", "id": "7"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/163", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "9", "speakers": [{"_type": "ContributionParticipation", "emailHash": "bd874829965cfbcd049d745b274e6cff", "affiliation": "Bayerische Julius Max. Universitaet Wuerzburg (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "ZIBELL, Andre", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "9630d8c730a700164f5ce40db8bda3b8", "affiliation": "Aristotle Univ. of Thessaloniki (GR)", "_fossil": "contributionParticipationMetadata", "fullName": "SIDIROPOULOU, Ourania", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "efe43d1460c77dc0ce3e841d65ceebf7", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BIANCO, Michele", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "653c56e2033473bd6f60113092a8e57a", "affiliation": "IFIN-HH Bucharest (RO)", "_fossil": "contributionParticipationMetadata", "fullName": "MARTOIU, Sorin", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "bd874829965cfbcd049d745b274e6cff", "affiliation": "Bayerische Julius Max. Universitaet Wuerzburg (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "ZIBELL, Andre", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "6d7266dc5485bf92bb8e475d262af247", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "LOESEL, Philipp Jonathan", "id": "4"}], "title": "Development and test of the DAQ system for a Micromegas prototype installed into the ATLAS experiment", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T14:08:34.162500+00:00", "description": "", "title": "Andre_Zibell_CHEP2015_poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/9\/attachments\/578806\/796992\/Andre_Zibell_CHEP2015_poster.pdf", "filename": "Andre_Zibell_CHEP2015_poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 796992, "size": 1483733}], "title": "Poster", "default_folder": false, "id": 578806, "description": ""}], "_type": "Contribution", "description": "Ourania Sidiropoulou \r\n                 on behalf of the ATLAS Muon Collaboration\r\n\r\nA Micromegas (MM) quadruplet prototype with an active area of {0.5 m$^2$} that adopts the general design foreseen for the upgrade of the innermost forward muon tracking systems (Small Wheels) of the ATLAS detector in 2018-2019, has been built at CERN and is going to be tested in the ATLAS cavern environment during the LHC RUN-II period 2015-2017. \r\n\r\nThe integration of this prototype detector into the ATLAS data acquisition system using custom ATCA equipment is presented. An ATLAS compatible ReadOutDriver (ROD) based on the Scalable Readout System (SRS), the Scalable Readout Unit (SRU), will be used in order to transmit the data after generating valid event fragments to the high-level Read Out System (ROS). The SRU will be synchronized with the LHC bunch crossing clock (40.08 MHz) and will receive the Level-1 trigger signals from the Central Trigger Processor (CTP) through the TTCrx receiver ASIC. The configuration of the system will be driven directly from the ATLAS Run Control System. By using the ATLAS TDAQ Software, a dedicated Micromegas segment has been implemented, in order to include the detector inside the main ATLAS DAQ partition. A full set of tests, on the hardware and software aspects, is presented.", "track": "Track1: Online computing ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578806", "resources": [{"_type": "LocalFile", "name": "Andre_Zibell_CHEP2015_poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/9\/attachments\/578806\/796992\/Andre_Zibell_CHEP2015_poster.pdf", "fileName": "Andre_Zibell_CHEP2015_poster.pdf", "_fossil": "localFileMetadata", "id": "796992", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/9", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": null, "keywords": [], "id": "357", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5195928bdf7011f0708dc7eac2b93e66", "affiliation": "IKP - Forschngszentrum Juelich", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PRENCIPE, Elisabetta", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "5195928bdf7011f0708dc7eac2b93e66", "affiliation": "IKP - Forschngszentrum Juelich", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PRENCIPE, Elisabetta", "id": "0"}], "title": "Kalman Filter based algorithms for PANDA @ FAIR", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "PANDA at the future FAIR facility at Darmstadt, Germany, is an experiment\r\nwith a cooled antiproton beam in a range between 1.5 and 15 GeV\/c,\r\nallowing a wide physics program in nuclear and particle physics. High\r\naverage reaction rates up to 2 * 10^7 interactions\/s are expected. Panda\r\nis the only experiment worldwide, which combines a solenoid field and a\r\ndipole field in an experiment with a fixed target topology. As a\r\nconsequence, the tracking system must be able to reconstruct high momenta\r\nin the laboratory frame, up to several GeV\/c.\r\n\r\nThe tracking system of PANDA involves the presence of a high performance\r\nsilicon vertex detector, a GEM detector, a Straw-Tubes central tracker, a\r\nforward tracking system, and a luminosity monitor. The first three of\r\nthose, are inserted in a solenoid homogeneous magnetic field (B=2T), the\r\nlatter two are inside a dipole magnetic field (B=2Tm),\r\n\r\nThe offline tracking algorithm is developed within the PandaRoot\r\nframework, which is a part of the FAIRROOT project. The algorithm is based\r\non a tool containing the Kalman-Filter-equations and a deterministic\r\nannealing filter [GENFIT]. The Kalman-Filter-based routines can\r\nperform extrapolations of track parameters and covariance matrices.\r\n\r\nWe present first results of an implementation of GENFIT2 in\r\nPandaRoot. In GENFIT2, different track representations are available, such\r\nas a helix representation for the solenoid field and a parabola\r\nrepresentation for the dipole field. In the intermediate field region,\r\nRunge-Kutta-equations are used to take into account B field\r\ninhomogeneities. Resolutions and efficiencies for different beam momenta\r\nand different track hypotheses will be shown.", "track": "Track2: Offline software ", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/contribution\/357", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:30:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "356", "speakers": [{"_type": "ContributionParticipation", "emailHash": "d2481eb1f11b08f40d4e1ef3cdb920dc", "affiliation": "University of Edinburgh (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "WASHBROOK, Andrew John", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "da6806efb2f919f20a8d4d30e47c621c", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "ROY, Gareth Douglas", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "d2481eb1f11b08f40d4e1ef3cdb920dc", "affiliation": "University of Edinburgh (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "WASHBROOK, Andrew John", "id": "3"}], "title": "Evaluation of containers as a virtualisation alternative for HEP workloads", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T15:11:47.676772+00:00", "description": "", "title": "ajw-Containers-CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/356\/attachments\/578807\/796993\/ajw-Containers-CHEP2015.pdf", "filename": "ajw-Containers-CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796993, "size": 6327716}], "title": "Slides", "default_folder": false, "id": 578807, "description": ""}], "_type": "Contribution", "description": "Cloud computing enables ubiquitous, convenient and on-demand access to a shared pool of configurable computing resources that can be rapidly provisioned with minimal management effort. The flexible and scalable nature of the cloud computing model is attractive to both industry and academia. In HEP, the use of the \u201ccloud\u201d has become more prevalent with LHC experiments making use of standard Cloud technologies to take advantages of elastic resources in both private and commercial computing environments. \r\n\r\nA key software technology that has eased transition to a cloud environment is the Virtual Machine (VM). VM\u2019s can be dynamically provisioned, managed and run a variety of Operating Systems tailored to user requirements. From a resource utilisation perspective however, VM's are a considered a heavyweight solution. Upon instantiation a VM will contain a complete copy of an operating system and all associated services leading to an increase in resource consumption when compared to standard \"bare metal\" deployment. This level of virtualisation is not required by the majority of workloads processed in HEP and can lead to increases in execution time on workloads that performs intensive I\/O file operations such as LHC data analysis.\r\n\r\nAn alternative solution which is gaining rapid traction within industry is containerisation. Here the Linux Kernel itself can virtualise and isolate a user-land level instance of the operating system in which applications can run. Less resources are needed compared to a VM because only shared system libraries and files needed by the application are virtualised. Additionally, as the virtualisation takes place via namespaces (a mechanism provided by the Linux Kernel giving an isolated view of a global system resource) performance is close to that of the physical hardware with minimal tuning.\r\n\r\nIn this study the use of containers will be assessed as a mechanism for running LHC experiment application payloads. Using currently available software tools (Docker) deployment strategies will be investigated by the use of a distributed WLCG Tier-2 facility as an example computing site. The relative performance of Containers and VM\u2019s when compared with native execution will be detailed using hardware benchmarking tools such as the HEPSPEC suite and Bonnie++. System-level resource profiling will also be performed on a selection of representative LHC workloads in order to determine the relative efficiency of hardware utilisation in each scenario. \r\n\r\nAs part of this deployment alternative methods of providing resources to WLCG similar to those found in \"Cloud\u201d solutions will be investigated. The integration of Containers in existing Cloud platforms (OpenStack) will be explored as well as new and emerging platforms (CoreOS, Kubernetes). Additionally, the possibility of hosting Grid Services in containers to ease Middleware deployment will also be assessed.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578807", "resources": [{"_type": "LocalFile", "name": "ajw-Containers-CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/356\/attachments\/578807\/796993\/ajw-Containers-CHEP2015.pdf", "fileName": "ajw-Containers-CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "796993", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "ec2d238aa9dcfee23b8c6dcb3cc714a4", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "QIN, Gang", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "9036c001235c207315f2f1cb8f8524b7", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. BRITTON, David", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "7c85d37398152148d60e922d4eda76d7", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "CROOKS, David", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "b120fdf9f04a123ea43e83d2718cd2ef", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SKIPSEY, Samuel Cadellin", "id": "5"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/356", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "355", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a9db592454003edc4ee459a7c7da7af6", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PIA, Maria Grazia", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "a9db592454003edc4ee459a7c7da7af6", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PIA, Maria Grazia", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "e5a67a7c37370877a3f34467d297922b", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. BASAGLIA, Tullio", "id": "1"}], "title": "Scientometrics of Monte Carlo simulation: lessons learned and how HEP can profit from them", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T10:37:58.724701+00:00", "description": "", "title": "poster_scientometric.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/355\/attachments\/578808\/796994\/poster_scientometric.pdf", "filename": "poster_scientometric.pdf", "content_type": "application\/pdf", "type": "file", "id": 796994, "size": 248084}], "title": "Slides", "default_folder": false, "id": 578808, "description": ""}], "_type": "Contribution", "description": "An extensive scientometric assessment of the literature is presented, which documents the prominent role achieved by Monte Carlo methods, and simulation in general, in particle physics and related fields (nuclear physics, astrophysics, medical physics etc.). As an example of their pervasiveness, one can remark that currently approximately 50% of the papers published in major, multi-disciplinary nuclear technology journals mention simulation methods. Scientometric data objectively evidence their use not only in traditional academic research domains, but also in industry, hospitals and other disciplines not directly related to high energy physics.\r\n\r\nThe analysis of scientometric data identifies some relevant features related to Monte Carlo methods and codes: it quantifies their impact on HEP itself, on scientific communities other than HEP, on industry and on social aspects of life sciences. Both widely used, general purpose Monte Carlo codes and specialized codes that address specific needs or user communities are examined.\r\n\r\nThe presentation highlights the contribution of HEP to this scientific domain, and discusses how the HEP community could profit from these achievements by improved communication methods addressed to the civil society, to funding agencies and to government bodies.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578808", "resources": [{"_type": "LocalFile", "name": "poster_scientometric.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/355\/attachments\/578808\/796994\/poster_scientometric.pdf", "fileName": "poster_scientometric.pdf", "_fossil": "localFileMetadata", "id": "796994", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "bd2d6f0ace48e4250cc994ae4e39f7cc", "affiliation": "ORNL", "_fossil": "contributionParticipationMetadata", "fullName": "BELL, Zane", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "9a56bb36951c282078af3fdd59331f18", "affiliation": "IEEE", "_fossil": "contributionParticipationMetadata", "fullName": "DRESSENDORFER, Paul", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/355", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "354", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6cccef6d6cebb7a770526f8954e4483f", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "HUFFMAN, Adam", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "b4e7a4ea1d679a57a1467fc9ae74c443", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "FAYER, Simon", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6cccef6d6cebb7a770526f8954e4483f", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "HUFFMAN, Adam", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "b4e7a4ea1d679a57a1467fc9ae74c443", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "FAYER, Simon", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "cd17961312c904e3bb4b71723c8e87ac", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. COLLING, David", "id": "2"}], "title": "The GridPP DIRAC Project - Site Monitoring of DIRAC", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "DIRAC (Distributed Infrastructure with Remote Agent Control) is a workload and data management framework that was originally developed for the LHCb experiment. Thanks to its flexible architecture, it is not restricted to this use case and is now being adopted much more widely, to serve communities with different requirements. With the move away from the traditional EMI-WMS service, many smaller VOs in particular (such as T2K and COMET) are looking to DIRAC as their preferred method of job submission for the future. We present here a set of tools to monitor the health of the DIRAC server itself, in an automated manner, to help prevent problems, and to aid fault diagnosis. We consider that such provision is essential if DIRAC is to be a service on which many VOs and other communities can rely for their day-to-day requirements.\r\n\r\nWe have chosen to implement these tools mainly in the form of plugins for Nagios, a monitoring system relied on widely by Grid sites and VOs.  In this way they should also be usable on compatible products such as Icinga and Naemon, thereby maximising the number of sites that can benefit from this work. In some cases, changes are required in the upstream DIRAC code, so we publish these changes as open source contributions.\r\n\r\nOur approach is to take a component-by-component view of the modules in DIRAC. In our description we start with lower-level tests and move on to checks of higher-level functionality. Services within the DIRAC framework use different ports, so we include a basic \u2018ping\u2019 check of these services via their respective ports. Where services do not already offer this facility, we add this ourselves in the form of patches to DIRAC. Beyond this most basic test of responsiveness, we include functional tests of individual services and we build on the existing DIRAC Monitoring service by parsing the logs for errors and warnings. In the latter case, to avoid some security drawbacks, again we implement this in the form of patches. We also include Nagios tests of the values reported by this service, to ensure they are not exceeding nominal ranges. Moving higher up the stack, we test the full job workflow, from submission to execution and collection of job output. File replication tests are included as well as checking of the Proxy Manager service. We describe the development and use of this new monitoring infrastructure in the production DIRAC service hosted at Imperial College.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/354", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:00:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "353", "speakers": [{"_type": "ContributionParticipation", "emailHash": "32469ad582447e084625f1c255d9f6df", "affiliation": "GSI - Helmholtzzentrum fur Schwerionenforschung GmbH (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "RYBALCHENKO, Alexey", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "32469ad582447e084625f1c255d9f6df", "affiliation": "GSI - Helmholtzzentrum fur Schwerionenforschung GmbH (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "RYBALCHENKO, Alexey", "id": "0"}], "title": "Efficient time frame building for online data reconstruction in ALICE experiment", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T08:34:55.611325+00:00", "description": "", "title": "rybalchenko_chep2015_slides.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/353\/attachments\/578809\/796995\/rybalchenko_chep2015_slides.pdf", "filename": "rybalchenko_chep2015_slides.pdf", "content_type": "application\/pdf", "type": "file", "id": 796995, "size": 366596}], "title": "Slides", "default_folder": false, "id": 578809, "description": ""}], "_type": "Contribution", "description": "After Long Shutdown 2, the upgraded ALICE detector at the LHC will produce more than a terabyte of data per second. The data, constituted from a continuous un-triggered stream data, have to be distributed from about 250 First Level Processor nodes (FLPs) to O(1000) Event Processing Nodes (EPNs). Each FLP receives a small subset of the detector data that is chopped in sub-timeframes. One EPN needs all the fragments from the 250 FLPs to build a full timeframe. An algorithm is being implemented on the FLPs with the aim of optimizing the usage of the network connecting the FLPs and EPNs. The algorithm minimizes contention when several FLPs are sending to the same EPN. An adequate traffic shaping is implemented by delaying the sending time of each FLP by a unique offset. The payloads are stored in a buffer large enough to accommodate the delay provoked by the maximum number of FLPs. As the buffers are queued for sending, the FLPs can operate with the highest efficiency. Using the time information embedded in the data any further FLP synchronization can be avoided. Moreover, \u201czero-copy\u201d and multipart messages of ZeroMQ are used to create full timeframes on the EPNs without the overhead of copying the payloads. The concept and the performance measurement of the implementation on a computing cluster are presented.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578809", "resources": [{"_type": "LocalFile", "name": "rybalchenko_chep2015_slides.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/353\/attachments\/578809\/796995\/rybalchenko_chep2015_slides.pdf", "fileName": "rybalchenko_chep2015_slides.pdf", "_fossil": "localFileMetadata", "id": "796995", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "7553d4db8d9901e91d5bbb73ec902007", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. AL-TURANY, Mohammad", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "6d753b60ed28dc6fb4ce7d2c8541cbe6", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "KOUZINOPOULOS, Charalampos", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "9fd1d2994ca0c8a1118e7fb3d6053807", "affiliation": "GSI", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. WINCKLER, Nicolas", "id": "3"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/353", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:00:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "352", "speakers": [{"_type": "ContributionParticipation", "emailHash": "1f3693dfc29524cf67ff9b5b25fb1466", "affiliation": "Japan Synchrotron Radiation Research Institute", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SUGIMOTO, Takashi", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "1f3693dfc29524cf67ff9b5b25fb1466", "affiliation": "Japan Synchrotron Radiation Research Institute", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SUGIMOTO, Takashi", "id": "0"}], "title": "Data-analysis scheme and infrastructure at the X-ray free electron laser facility, SACLA", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T16:30:39.784740+00:00", "description": "", "title": "CHEP2015_Track5-20150413-1545_sugimoto_ver042.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/352\/attachments\/578810\/796996\/CHEP2015_Track5-20150413-1545_sugimoto_ver042.pdf", "filename": "CHEP2015_Track5-20150413-1545_sugimoto_ver042.pdf", "content_type": "application\/pdf", "type": "file", "id": 796996, "size": 3462168}, {"_type": "attachment", "modified_dt": "2015-04-11T16:30:39.784740+00:00", "description": "", "title": "CHEP2015_Track5-20150413-1545_sugimoto_ver042.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/352\/attachments\/578810\/796997\/CHEP2015_Track5-20150413-1545_sugimoto_ver042.pptx", "filename": "CHEP2015_Track5-20150413-1545_sugimoto_ver042.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 796997, "size": 25926363}], "title": "Slides", "default_folder": false, "id": 578810, "description": ""}], "_type": "Contribution", "description": "An X-ray free electron laser (XFEL) facility, SACLA, is generating ultra-short, high peak brightness, and full-spatial-coherent X-ray pulses [1].  The unique characteristics of the X-ray pulses, which have never been obtained with conventional synchrotron orbital radiation, are now opening new opportunities in a wide range of scientific fields such as atom, molecular and optical physics, ultrafast science, material science, and life science.  More than 100 experiments have been performed since the first X-ray delivery to experimental users in March 2012.  In this paper, we present an overview of SACLA data acquisition (DAQ) and analysis system with a special emphasis on the analysis scheme and its infrastructure.  In the case of serial femotosecond protein crystallography experiments [2], a typical experiment collects diffraction image patterns of order of $10^6$, which demands heavy load to the data analysis system.  Each pattern is recorded by a $2000 \\times 2000$ pixel detector that consists of eight Multiport Charge-Coupled Device (MPCCD) sensors [3].  The resolution of the single MPCCD sensor is $1024 \\times 512$ pixel and data depth of each pixel is 16 bits.  The DAQ system consists of detector front-ends [4], data-handling servers, hardware-based event-tag distribution system [5], event-synchronized database [6], two cache storages, tape archive system, and physically-segregated two network system [7].  The DAQ has data bandwidth of maximum 6 Gbps to support other experiment setups with various detector configuration of up to twelve MPCCD sensors.  In addition to the currently operational beamline BL3, BL2 will operate concurrently through a fast-switching operation mode in 2015.  To support this operation mode, the cache storage with capacities of 200 TB (250 TB) is assigned to the beamline BL2 (BL3) respectively [8].  These capacities correspond to the accumulated data size for one week operation.  Experimental data are periodically moved into the tape archive system.  The tape archive system has a capacity of 7 PB, and extendable up to 26 PB by installing additional tape cartridges.\r\n\r\n  The analysis section has two functions: one is run-by-run analysis to monitor the experimental conditions, and the other is off-line analysis.  To implement these functions, the analysis system consists of a PC cluster and a supercomputer.  The PC cluster is based on x86_64 processors and has a computing power of 14 TFLOPS.  A 160 TB storage is connected to the PC cluster via Infiniband QDR network.  To pick up raw image data, the PC cluster is connected to the cache storages and the tape archive system via 10 Gigabit Ethernet.  The run-by-run analysis is performed using the PC cluster.  The results are saved on the storage in HDF5 format [9].  The PC cluster is also used for off-line analysis, using analysis code developed by the scientific community, such as CrystFEL [10] and SITENNO [11].  The supercomputer with 90 TFLOPS SPARC-based processors (Fujitsu FX10) was installed for the data analysis that requires higher computing power.  Storage of the supercomputer is 100+500 TB Lustre-based file system (Fujitsu FEFS).  Another 1 PB Lustre file system is also connected to both the supercomputer and the PC cluster to interaccess the experimental data from the two systems.  Data analysis that requires much higher computing power is foreseen.  For these cases, we are developing a joint analysis mode using both the supercomputer and the 10-PFLOPS K computer [12].  The results of the feasibility study on data transfer and quasi-realtime job submission to the K computer will also be discussed.\r\n\r\n[1] T. Ishikawa et al., \"A compact X-ray free-electron laser emitting in the sub-angstrom region\", Nature Photonics 6, 540-544 (2012).\r\n\r\n[2] M. Sugahara et al., Nature Methods submitted.\r\n\r\n[3] T. Kameshima et al., \"Development of an X-ray pixel detector with multi-port charge-coupled device for X-ray free-electron laser experiments\", Rev. Sci. Instrum. 85, 033110 (2014).\r\n\r\n[4] A. Kiyomichi, A. Amselem, et al., \"Development of Image Data Acquisition System for 2D Detector at SACLA\", Proceedings of ICALEPCS2011, WEPMN028 (2011).\r\n\r\n[5] T. Abe et al., \"DEVELOPMENT OF NEW TAG SUPPLY SYSTEM FOR DAQ FOR SACLA USER EXPERIMENTS\", Proceedings of IPAC2014, TUPRI108 (2013).\r\n\r\n[6] M. Yamaga et al., \"Event-Synchronized Data Acquisition System of 5 Giga-bps Data Rate for User Experiment at the XFEL Facility, SACLA\", Proceedings of ICALEPCS2011, TUCAUST06 (2011).\r\n\r\n[7] T. Sugimoto et al., \"Large-bandwidth Data-acquisition Network for XFEL Facility, SACLA\", Proceedings of ICALEPCS2011, WEBHAUST03 (2011).\r\n\r\n[8] K. Okada, et al., \"UPGRADE OF SACLA DAQ SYSTEM ADAPTS TO MULTI-BEAMLINE OPERATION\", Proceedings of PCaPAC2014, WCO205 (2014).\r\n\r\n[9] The HDF Group, http:\/\/www.hdfgroup.org\/HDF5\/.\r\n\r\n[10] T. A. White, et al.. \"CrystFEL: a software suite for snapshot serial crystallography\". J. Appl. Cryst. 45, pp.335\u2013341 (2012).\r\n\r\n[11] Y. Sekiguchi et al., \"Data processing software suite SITENNO for coherent X-ray diffraction imaging using X-ray free electron laser SACLA\", Journal of Synchrotron Radiation  21\/ 4, 600-612 (2014).\r\n\r\n[12] M. Yokokawa, et al., \"The K computer: Japanese next-generation supercomputer development project\", ISLPED '11 Proceedings of the 17th IEEE\/ACM international symposium on Low-power electronics and design, pp.371-372 (2011).", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578810", "resources": [{"_type": "LocalFile", "name": "CHEP2015_Track5-20150413-1545_sugimoto_ver042.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/352\/attachments\/578810\/796996\/CHEP2015_Track5-20150413-1545_sugimoto_ver042.pdf", "fileName": "CHEP2015_Track5-20150413-1545_sugimoto_ver042.pdf", "_fossil": "localFileMetadata", "id": "796996", "_deprecated": true}, {"_type": "LocalFile", "name": "CHEP2015_Track5-20150413-1545_sugimoto_ver042.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/352\/attachments\/578810\/796997\/CHEP2015_Track5-20150413-1545_sugimoto_ver042.pptx", "fileName": "CHEP2015_Track5-20150413-1545_sugimoto_ver042.pptx", "_fossil": "localFileMetadata", "id": "796997", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "58d2ca04c5b6b120e2f4b475650d169c", "affiliation": "Japan Synchrotron Radiation Research Institute", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ABE, Toshinori", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "3b1c1cdee53a1647102ab856ec7a1719", "affiliation": "Japan Synchrotron Radiation Research Institute", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. JOTI, Yasumasa", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "22d4b34b853c7637d5cec0182d0eedeb", "affiliation": "Japan Synchrotron Radiation Research Institute", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KAMESHIMA, Takashi", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "65209dee0a3bbccd31cf34bb20a95a2c", "affiliation": "Japan Synchrotron Radiation Research Institute", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. OKADA, Kensuke", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "34a4d7717a71f7fd74aa9eaafbed54ce", "affiliation": "Japan Synchrotron Radiation Research Institute", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. YAMAGA, Mitsuhiro", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "dd0627aa115aed90ccb9a268c97a6f62", "affiliation": "Japan Synchrotron Radiation Research Institute", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. TANAKA, Ryotaro", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "639b30bb57c9417fabd525ff0ce7f5c8", "affiliation": "RIKEN SPring-8 Center", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HATSUI, Takaki", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "e386973fedcf9637eb833dc13aefecd3", "affiliation": "RIKEN SPring-8 Center", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. YABASHI, Makina", "id": "8"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/352", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "351", "speakers": [{"_type": "ContributionParticipation", "emailHash": "22a1a3e2ab267299b1754df69296edaf", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "DAL PRA, Stefano", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "22a1a3e2ab267299b1754df69296edaf", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "DAL PRA, Stefano", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "894b9c337a7360d1a35371598eebdfc0", "affiliation": "Istituto Nazionale Fisica Nucleare (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CIASCHINI, Vincenzo", "id": "1"}], "title": "Dynamic partitioning as a way to exploit new computing paradigms: the cloud usecase.", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-08T12:44:18.608476+00:00", "description": "", "title": "poster_dynpart_chep2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/351\/attachments\/578811\/796998\/poster_dynpart_chep2015.pdf", "filename": "poster_dynpart_chep2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 796998, "size": 83796}], "title": "Poster", "default_folder": false, "id": 578811, "description": ""}], "_type": "Contribution", "description": "The WLCG community and many groups in the HEP community have based\r\ntheir computing strategy on the Grid paradigm, which proved successful\r\nand still ensues its goals. However, Grid technology has not spread\r\nmuch over other communities; in the commercial world, the cloud\r\nparadigm is the emerging way to provide computing services.\r\n\r\nWLCG experiments aim to achieve integration of their existing current\r\ncomputing model with cloud deployments and take advantage of the\r\nso-called opportunistic resources (including HPC facilities) which are\r\nusually not Grid compliant. One missing feature in the most common\r\ncloud frameworks, is the concept of job scheduler, which plays\r\na key role in a traditional computing centre, by enabling a fairshare\r\nbased access at the resources to the the experiments in a scenario\r\nwhere demand greatly outstrips availability.\r\n\r\nAt CNAF we have opened started, as a preproduction service, the\r\npossibility to access the Tier-1 computing resources as an OpenStack\r\nbased cloud service. The system, exploiting the dynamic partitioning\r\nmechanism already being used to enable Multicore computing, allowed us\r\nto avoid a static splitting of the computing resources in the Tier-1 farm,\r\nwhile permitting a share friendly approach.\r\n\r\nThe hosts in a dynamically partitioned farm may be moved to or from\r\nthe partition, according to suitable policyes for request and release\r\nof computing resources. Nodes being requested in the partition switch\r\ntheir role and become available to play a different one. In the cloud\r\nuse case hosts may switch from acting as Worker Node in the Batch\r\nsystem farm to cloud compute node member, made available to tenants.\r\n\r\nIn this paper we describe the dynamic partitioning concept, its\r\nimplementation and integration with our current batch system, LSF. We\r\nthen present results for the dynamic cloud usecase.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578811", "resources": [{"_type": "LocalFile", "name": "poster_dynpart_chep2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/351\/attachments\/578811\/796998\/poster_dynpart_chep2015.pdf", "fileName": "poster_dynpart_chep2015.pdf", "_fossil": "localFileMetadata", "id": "796998", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "f505fcb069596925095571572bf1bf39", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "DELL'AGNELLO, Luca", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/351", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:15:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "350", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6794b20ff1a79867a414c13ad66a52f2", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "RAND, Duncan", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6794b20ff1a79867a414c13ad66a52f2", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "RAND, Duncan", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "cd17961312c904e3bb4b71723c8e87ac", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. COLLING, David", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "b4e7a4ea1d679a57a1467fc9ae74c443", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "FAYER, Simon", "id": "2"}], "title": "Possibilities for Named Data Networking in HEP", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T03:37:52.332391+00:00", "description": "", "title": "chep-ndn.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/350\/attachments\/578812\/796999\/chep-ndn.pdf", "filename": "chep-ndn.pdf", "content_type": "application\/pdf", "type": "file", "id": 796999, "size": 371978}], "title": "Slides", "default_folder": false, "id": 578812, "description": ""}], "_type": "Contribution", "description": "Named Data Networks (NDN) are an emerging network technology based around requesting data from a network rather than a specific host. Intermediate routers in the network cache the data. Each data packet must be signed to allow its provenance to be verified. Data blocks are addressed by a unique name which consists of a hierarchical path, a name and attributes. An example of a valid address could be \"\/ndn\/uk\/ac\/imperial\/ph\/hep\/data\/somefile\/1\". \r\n\r\nThe provision for in-network caching makes NDN an ideal choice for transferring large data sets where different endpoints are likely to make use of the same data within close succession. The naming of data rather than nodes also means that a data request could potentially be satisfied by multiple geographically disparate sites allowing for failover and load-balancing.\r\n\r\nWe believe that the delegation of robustness and reliability to the network itself offers significant  possibilities for computing in HEP. For example, the LHC experiments currently pre-place data and have more recently started making use of storage namespace federation and caching through the xrootd protocol. NDN offers interesting possibilities to simplify many of the the experiments\u2019 data placement systems and to extend the data caching approach. Another advantage is that in the future it is likely that large commercial content delivery providers will be using NDN and will contribute effort into developing and maintaining the technology. The deployment of NDN will be a slow process, however it can run over the existing IP infrastructure allowing for a phased, non-disruptive parallel rollout. \r\n\r\nWe will discuss results from an HEP NDN testbed, prototype *GFAL 2* and *root* NDN plugins, aspects of packet signing and security and implications for the LHC computing models.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578812", "resources": [{"_type": "LocalFile", "name": "chep-ndn.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/350\/attachments\/578812\/796999\/chep-ndn.pdf", "fileName": "chep-ndn.pdf", "_fossil": "localFileMetadata", "id": "796999", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/350", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "359", "speakers": [{"_type": "ContributionParticipation", "emailHash": "264c0fb3f0384c895005df174eeed532", "affiliation": "STFC (RAL) GB", "_fossil": "contributionParticipationMetadata", "fullName": "DAVIES, Brian", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "264c0fb3f0384c895005df174eeed532", "affiliation": "STFC (RAL) GB", "_fossil": "contributionParticipationMetadata", "fullName": "DAVIES, Brian", "id": "0"}], "title": "Filesize distribution of WLCG data at the Rutherford Appleton Laboratory Tier1", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T00:45:25.954082+00:00", "description": "", "title": "CHEP_2015_filesystem_poster-Brian_Davies.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/359\/attachments\/578813\/797000\/CHEP_2015_filesystem_poster-Brian_Davies.pdf", "filename": "CHEP_2015_filesystem_poster-Brian_Davies.pdf", "content_type": "application\/pdf", "type": "file", "id": 797000, "size": 1352462}], "title": "Poster", "default_folder": false, "id": 578813, "description": ""}], "_type": "Contribution", "description": "The Rutherford Appleton Laboratory (RAL) operates the UK WLCG Tier1 facility on behalf of GridPP. Tier 1's provide persistent archival storage (on tape at RAL) and\r\nonline storage for fast access data analysis. RAL is one of the few Tier-1s which\r\nsupports data management for all the major LHC experiments, as well as a number of \r\nsmaller Virtual Organisations.  This allows us to compare usage of the infrastructure\r\nacross many different experiments and use cases.\r\n\r\nIn this post, we look at the distribution of file sizes between VOs and across\r\ndifferent storage work flows within the same VO.  We also report on how this \r\ndistribution impacts standard operational procedures such as tape usage and \r\ndecommissioning of storage nodes. \r\n\r\nFurther, we also show how a hypothetical change on data storage policy of log files \r\nwould change the profile of filesize distribution of a significant subset of data \r\ncurrently being stored at RAL and how this change would impact operations to the\r\nadvantage of the user communities.", "track": "Track3: Data store and access ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578813", "resources": [{"_type": "LocalFile", "name": "CHEP_2015_filesystem_poster-Brian_Davies.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/359\/attachments\/578813\/797000\/CHEP_2015_filesystem_poster-Brian_Davies.pdf", "fileName": "CHEP_2015_filesystem_poster-Brian_Davies.pdf", "_fossil": "localFileMetadata", "id": "797000", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "da4cac912389521f0838486a2a6ebd61", "affiliation": "STFC", "_fossil": "contributionParticipationMetadata", "fullName": "DE WITT, Shaun", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/359", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "358", "speakers": [{"_type": "ContributionParticipation", "emailHash": "264c0fb3f0384c895005df174eeed532", "affiliation": "STFC (RAL) (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "DAVIES, Brian", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "264c0fb3f0384c895005df174eeed532", "affiliation": "STFC (RAL) (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "DAVIES, Brian", "id": "0"}], "title": "Deployment and usage of perfSonar Networking tools for non-HEP communities", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T00:42:10.006221+00:00", "description": "", "title": "CHEP_2015_Perfsonar-Brian_Davies.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/358\/attachments\/578814\/797001\/CHEP_2015_Perfsonar-Brian_Davies.pdf", "filename": "CHEP_2015_Perfsonar-Brian_Davies.pdf", "content_type": "application\/pdf", "type": "file", "id": 797001, "size": 2353556}], "title": "Poster", "default_folder": false, "id": 578814, "description": ""}], "_type": "Contribution", "description": "perfSonar is a network monitoring tool set which enables performance of wide area communications to be analysed and eases problem identification across distributed centres.  It has been widely used within WLCG since 2012 and has been crucial in identifying network problems and confirming network changes have the desired effect.  We report on examples of this within this presentation.\r\n\r\nIn addition, at Rutherford Appleton Laboratory (RAL), we have been extending the use of perfSonar to communities outside of High Energy Physics (HEP) such as photonics and climatology.  We describe how the knowledge base gained from our experience in WLCG has been used to help these non-HEP communities to deploy perfSonar hosts and incorporate these into the current test infrastructure. We demonstrate improvements that have resulted from the monitoring information obtained such as identification of packet loss across the firewall which had not previously been identified by any other tools used.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578814", "resources": [{"_type": "LocalFile", "name": "CHEP_2015_Perfsonar-Brian_Davies.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/358\/attachments\/578814\/797001\/CHEP_2015_Perfsonar-Brian_Davies.pdf", "fileName": "CHEP_2015_Perfsonar-Brian_Davies.pdf", "_fossil": "localFileMetadata", "id": "797001", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "a3b7c6542a6829f69fc1b2fef00e8f57", "affiliation": "DLS Ltd", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. WHITE, Alex", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "65c85e76704e917f7f593c6b04dc4702", "affiliation": "Unknown-Unknown-Unknown", "_fossil": "contributionParticipationMetadata", "fullName": "DEL CANO NOVALES, Cristina", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/358", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "216", "speakers": [{"_type": "ContributionParticipation", "emailHash": "0cb6ba9ae68e30d8bf3ef4a3e0f294c2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GIORDANO, Domenico", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "0cb6ba9ae68e30d8bf3ef4a3e0f294c2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GIORDANO, Domenico", "id": "2"}], "title": "Accessing commercial cloud resources within the European Helix Nebula cloud marketplace", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-08T16:34:01.096466+00:00", "description": "", "title": "CHEP2015_Helix_Nebula.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/216\/attachments\/578815\/797002\/CHEP2015_Helix_Nebula.pdf", "filename": "CHEP2015_Helix_Nebula.pdf", "content_type": "application\/pdf", "type": "file", "id": 797002, "size": 2855195}], "title": "Slides", "default_folder": false, "id": 578815, "description": ""}], "_type": "Contribution", "description": "Helix Nebula \u2013 the Science Cloud Initiative is a public-private-partnership between Europe's leading scientific research organisations (notably CERN, EMBL and ESA) and European IT cloud providers, that aims to establish a cloud-computing platform for data intensive science within Europe.\r\n\r\nOver the past two years, Helix Nebula has built a federated cloud framework \u2013 the Helix Nebula Marketplace (HNX) - to provision cloud services from a range of commercial cloud providers and public e-infrastructures, such as the EGI and GEANT. HNX delivers efficient and scalable access to cloud resources through an innovative broker technology deployed within the Helix Nebula project. In addition it complies with EU regulations and legislation to provide trusted cloud services.\r\n\r\nCERN contributed to this initiative by providing a flagship use case: the integration of Helix Nebula cloud resources within the workload management system of the ATLAS experiment. Aiming to gain experience in managing and monitoring large-scale deployments as well as in benchmarking the cloud resources, a sizable Monte Carlo production was realized using the Helix Nebula platform. Thousands of concurrent virtual machines were run for several months in order to be comparable with the capacity supplied by a typical WLCG Tier-2 site.\r\n\r\nThis contribution describes the Helix Nebula initiative and summarizes the CERN experience and the lessons learned in deploying experiment applications with large cloud setups involving several commercial providers. Details about the procurement process, the cost analysis and the cloud benchmark are provided.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578815", "resources": [{"_type": "LocalFile", "name": "CHEP2015_Helix_Nebula.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/216\/attachments\/578815\/797002\/CHEP2015_Helix_Nebula.pdf", "fileName": "CHEP2015_Helix_Nebula.pdf", "_fossil": "localFileMetadata", "id": "797002", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "a0dd1a3a7589f1e2a04cc967f2ae6c39", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DI GIROLAMO, Alessandro", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "74f37452da18d70a445f84d900eff716", "affiliation": "Universita e INFN, Roma I (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "DE SALVO, Alessandro", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "cdbcbb10b476cf643a54c4a3d38f8a83", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "JONES, Bob", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "6049ebfaa694e419b61cfd107f553289", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FIELD, Laurence", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "1d7caa59ddd1371760449102910a7dc0", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DOMINGUES CORDEIRO, Cristovao Jose", "id": "5"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/216", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:00:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "217", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5ae3a32d3ffcbc90d737c183bebe4201", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MOROVIC, Srecko", "id": "14"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2d10c567422b53eb7fccd0134c728788", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MESCHI, Emilio", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "5ae3a32d3ffcbc90d737c183bebe4201", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MOROVIC, Srecko", "id": "37"}], "title": "A scalable monitoring for the CMS Filter Farm based on elasticsearch", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T07:52:43.836951+00:00", "description": "", "title": "Morovic_CHEP2015_v7.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/217\/attachments\/578816\/797003\/Morovic_CHEP2015_v7.pdf", "filename": "Morovic_CHEP2015_v7.pdf", "content_type": "application\/pdf", "type": "file", "id": 797003, "size": 1168508}], "title": "Slides", "default_folder": false, "id": 578816, "description": ""}], "_type": "Contribution", "description": "A flexible monitoring system has been designed for the CMS File-based Filter Farm making use of modern data mining and analytics components. All the metadata and monitoring information concerning data flow and execution of the HLT are generated locally in the form of small \u201cdocuments\u201d using the JSON encoding. These documents are indexed into a hierarchy of elasticsearch (es) clusters along with process and system log information. Elasticsearch is a search server based on Apache Lucene. It provides a distributed, multitenant-capable search and aggregation engine. Since es is schema-free, any new information can be added seamlessly and the unstructured information can be queried in non-predetermined ways. The leaf es clusters consist of the very same nodes that form the Filter Farm thus providing \u201cnatural\u201d horizontal scaling. A separate \u201ccentral\" es cluster is used to collect and index aggregated information. The fine-grained information, all the way to individual processes, remains available in the leaf clusters. The central es cluster provides quasi-real-time high-level monitoring information to any kind of client. Historical data can be retrieved to analyse past problems or correlate them with external information. We discuss the design and performance of this system in the context of the CMS DAQ commissioning for LHC Run 2.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578816", "resources": [{"_type": "LocalFile", "name": "Morovic_CHEP2015_v7.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/217\/attachments\/578816\/797003\/Morovic_CHEP2015_v7.pdf", "fileName": "Morovic_CHEP2015_v7.pdf", "_fossil": "localFileMetadata", "id": "797003", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "321d291a26425f7ba000c1227f961f93", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "ANDRE, Jean-Marc Olivier", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "ddaeb584ae450aae5064a948e08ead3c", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. DOBSON, Marc", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "8b7c922625aaf91ee706a1518921a755", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DUPONT, Aymeric Arnaud", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "b03db255046e47e29d52f585a312fecf", "affiliation": "Univ. of California Los Angeles (US)", "_fossil": "contributionParticipationMetadata", "fullName": "ERHAN, Samim", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "4694ee2e44416e0163f30d20408ccad0", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GIGI, Dominique", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "6fb6d2d7f55ab188239ef3702f47965a", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GLEGE, Frank", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "37e8bb7f1971a51948ad4fce29334bf7", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GOMEZ CEBALLOS RETUERTO, Guillelmo", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "0686dcacfe13161c58c9e8a9fd9d65d2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HEGEMAN, Jeroen", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "98493644fe4b7f32b27b9e10cf03a340", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "HOLZNER, Andre Georg", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "932403f3327b0e3b2903995e513382a2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MASETTI, Lorenzo", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "82e7a21a5011f52cb8606f0f0601f8d0", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MEIJERS, Frans", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "d3be8d0289e184c2c754180e84f1638b", "affiliation": "Aristotle Univ. of Thessaloniki (GR)", "_fossil": "contributionParticipationMetadata", "fullName": "ANDRONIDIS, Anastasios", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "060d7d9521c75cc21082d88edbe36aae", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MOMMSEN, Remi", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "7e2c5e676f7ba0f9af6e617d71de2a44", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "NUNEZ BARRANCO FERNANDEZ, Carlos", "id": "15"}, {"_type": "ContributionParticipation", "emailHash": "b22d1baa246adbc92c9a6050219aeea8", "affiliation": "Fermi National Accelerator Laboratory (FNAL)", "_fossil": "contributionParticipationMetadata", "fullName": "O'DELL, Vivian", "id": "16"}, {"_type": "ContributionParticipation", "emailHash": "7cdd48f784a087c6d9b6f6003cfc03f7", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ORSINI, Luciano", "id": "17"}, {"_type": "ContributionParticipation", "emailHash": "c20242a86672d5fc162e300100556857", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "PAUS, Christoph", "id": "18"}, {"_type": "ContributionParticipation", "emailHash": "1951a3ffe6d5cf1393f669f572be4751", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "PETRUCCI, Andrea", "id": "19"}, {"_type": "ContributionParticipation", "emailHash": "88f4d725c22d118782d55d91895473a9", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "PIERI, Marco", "id": "20"}, {"_type": "ContributionParticipation", "emailHash": "dea4bd4e0305108f26ad10d80d8a923c", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. RACZ, Attila", "id": "21"}, {"_type": "ContributionParticipation", "emailHash": "4288d38b57c8078dd7a98743244aafa0", "affiliation": "Nottingham Trent University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "ROBERTS, Penelope Amelia", "id": "22"}, {"_type": "ContributionParticipation", "emailHash": "148e6617a9a734113e8ef6b1f86cb767", "affiliation": "University of Wisconsin (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BAWEJ, Tomasz Adrian", "id": "23"}, {"_type": "ContributionParticipation", "emailHash": "e61604d9f35ae72b3531378bca0e6216", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SAKULIN, Hannes", "id": "24"}, {"_type": "ContributionParticipation", "emailHash": "01ca3cdd821be71f01a7fa78f1cd3e2c", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SCHWICK, Christoph", "id": "25"}, {"_type": "ContributionParticipation", "emailHash": "f458e4333696558cf9533ea62f29fc06", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "STIEGER, Benjamin", "id": "26"}, {"_type": "ContributionParticipation", "emailHash": "7b150004ac09fb1883e427cdc460cedc", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "SUMOROK, Konstanty", "id": "27"}, {"_type": "ContributionParticipation", "emailHash": "0b3372b1a406c07547c0658d54373e3a", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. VEVERKA, Jan", "id": "28"}, {"_type": "ContributionParticipation", "emailHash": "d8f0435f2ab3190ac7faaf3007671c08", "affiliation": "Sezione di Pisa (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "ZAZA, Salvatore", "id": "29"}, {"_type": "ContributionParticipation", "emailHash": "842f03deb43d34961e29b5c1ee95392e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ZEJDL, Petr", "id": "30"}, {"_type": "ContributionParticipation", "emailHash": "d33e3ae247733829a0d0c9511973c44e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHAZE, Olivier", "id": "31"}, {"_type": "ContributionParticipation", "emailHash": "3a7e6bfcd4dbb638710fce76e52d2342", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BRANSON, James Gordon", "id": "32"}, {"_type": "ContributionParticipation", "emailHash": "34b7170966da6ae240c3a7c096c08295", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "BEHRENS, Ulf", "id": "33"}, {"_type": "ContributionParticipation", "emailHash": "779d4951aab39403b13ffbc89e9f0c90", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CITTOLIN, Sergio", "id": "34"}, {"_type": "ContributionParticipation", "emailHash": "14823d25b1099daf48e1923cddbe3811", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DARLEA, Georgiana Lavinia", "id": "35"}, {"_type": "ContributionParticipation", "emailHash": "c5db4520580bcc09fe8df060c043fc7b", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DELDICQUE, Christian", "id": "36"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/217", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "214", "speakers": [{"_type": "ContributionParticipation", "emailHash": "df6f12602a0e37a3a6aafcc9eaab0c42", "affiliation": "University of Cincinnati", "_fossil": "contributionParticipationMetadata", "fullName": "AURISANO, Adam", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "df6f12602a0e37a3a6aafcc9eaab0c42", "affiliation": "University of Cincinnati", "_fossil": "contributionParticipationMetadata", "fullName": "AURISANO, Adam", "id": "0"}], "title": "Data Handling with SAM and ART at the NOvA Experiment", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T05:17:55.398951+00:00", "description": "", "title": "aurisano_dh_sam_nova.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/214\/attachments\/578817\/797004\/aurisano_dh_sam_nova.pdf", "filename": "aurisano_dh_sam_nova.pdf", "content_type": "application\/pdf", "type": "file", "id": 797004, "size": 1084305}], "title": "Poster", "default_folder": false, "id": 578817, "description": ""}], "_type": "Contribution", "description": "During operations, NOvA produces between 5,000 and 7,000 raw files per day with peaks in excess of 12,000. These files must be processed in several stages to produce fully calibrated and reconstructed analysis files. In addition, many simulated neutrino interactions must be produced and processed through the same stages as data. To accommodate the large volume of data and Monte Carlo, production must be possible both on the Fermilab grid and on off-site farms, such as the ones accessible through the Open Science Grid.\r\n\r\nTo handle the challenge of cataloging these files and to facilitate their off-line processing, we have adopted the SAM system developed at Fermilab. SAM indexes files according to metadata, keeps track of each file's physical locations, provides dataset management facilities, and facilitates data transfer to off-site grids.\r\n\r\nTo integrate SAM with the Fermilab's ART software framework and the NOvA production workflow, we have developed methods to embed metadata into our configuration files, ART files, and standalone ROOT files. A module in the ART framework propagates the embedded information from configuration files into ART files, and from input ART files to output ART files, allowing us to maintain a complete processing history within our files. Embedding metadata in configuration files also allows configuration files indexed in SAM to be used as inputs to Monte Carlo production jobs. Further, SAM keeps track of the input files used to create each output file. Parentage information enables the construction of self-draining datasets which have become the primary production paradigm used at NOvA. We will present an overview of SAM at NOvA and how it has transformed the file production framework used by the experiment.", "track": "Track3: Data store and access ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578817", "resources": [{"_type": "LocalFile", "name": "aurisano_dh_sam_nova.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/214\/attachments\/578817\/797004\/aurisano_dh_sam_nova.pdf", "fileName": "aurisano_dh_sam_nova.pdf", "_fossil": "localFileMetadata", "id": "797004", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "16bdf91b327521302d812ff7c61549ed", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "BACKHOUSE, Christopher", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "fb301d8355628f15b8301c7992ca22fd", "affiliation": "Iowa State University", "_fossil": "contributionParticipationMetadata", "fullName": "DAVIES, Gavin", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "21f186c57f345bba6eabf3bec24fbb1b", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ILLINGWORTH, Robert", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "c06196a1fdeb4395a54918aa9d91ed07", "affiliation": "Tufts University", "_fossil": "contributionParticipationMetadata", "fullName": "MAYER, Nathan", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "004a1bf6b35a678030d9d18e46aa7de6", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "MENGEL, Marc", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "6ba1f95da55f3a0622e4785d12b5cc25", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "NORMAN, Andrew", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "3a31a8ae057f17e06a6156066d7bd968", "affiliation": "University of Minnesota", "_fossil": "contributionParticipationMetadata", "fullName": "ROCCO, Dominick", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "a03131261466f88a56cf5516acacef47", "affiliation": "University of Minnesota", "_fossil": "contributionParticipationMetadata", "fullName": "ZIRNSTEIN, Jan", "id": "8"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/214", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:15:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:30:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "215", "speakers": [{"_type": "ContributionParticipation", "emailHash": "64a54294729a6872c2f675402592e854", "affiliation": "Universiteit van Amsterdam", "_fossil": "contributionParticipationMetadata", "fullName": "BALZER, Arnim", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "64a54294729a6872c2f675402592e854", "affiliation": "Universiteit van Amsterdam", "_fossil": "contributionParticipationMetadata", "fullName": "BALZER, Arnim", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "878298dc22e48b00978e59cddedc9c38", "affiliation": "DESY Zeuthen", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. F\u00dc\u00dfLING, Matthias", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c487851f4395c36becd07f59c9304218", "affiliation": "Max-Planck-Institut f\u00fcr Kernphysik", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HOFVERBERG, Petter", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "ca5be45665cf3edd8146949bf43d399e", "affiliation": "Max-Planck-Institut f\u00fcr Kernphysik", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PARSONS, Robert Daniel", "id": "3"}], "title": "The Performance of the H.E.S.S. Target of Opportunity Alert System", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T23:28:11.566669+00:00", "description": "", "title": "Balzer_HESS_DAQ.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/215\/attachments\/578818\/797005\/Balzer_HESS_DAQ.pdf", "filename": "Balzer_HESS_DAQ.pdf", "content_type": "application\/pdf", "type": "file", "id": 797005, "size": 6119492}], "title": "Slides", "default_folder": false, "id": 578818, "description": ""}], "_type": "Contribution", "description": "The High Energy Stereoscopic System (H.E.S.S.) is an array of five imaging atmospheric Cherenkov telescopes located in the Khomas Highland in Namibia. Very high energy gamma rays are detected using the Imaging Atmospheric Cherenkov Technique. It separates the Cherenkov light emitted by the background of mostly hadronic air showers from the light emitted by air showers induced by gamma rays. Using the fifth, larger telescope of the array with a huge mirror area of $600~\\text{m}^2$, it was possible to lower the energy threshold down to \u2248 30 GeV. With this unique ability to observe large amounts of gamma rays in the high energy gamma-ray regime (< 100 GeV) by using the large effective area of the fifth telescope at these energies, the H.E.S.S. experiment is ideally suited to observe short time scale transient events like gamma-ray bursts (GRBs). Originally detected by the Vela satellites in 1967, GRBs are among the most energetic processes in the known Universe. Extrapolating the spectrum of long duration GRBs (i.e. a GRB duration of the order of a few seconds or above) measured by current satellite experiments like Fermi, which measured gamma rays up to 95 GeV for GRB 130427A, a detection of these phenomena with the H.E.S.S. array is possible.\r\nThis presentation will give an update on the H.E.S.S. Target of Opportunity (ToO) alert system. It is used for an immediate and fully automatic response to a prompt GRB alert received via the Gamma-Ray Coordinates Network (GCN). The key feature of this system is a fast repointing of the whole array to a new observation position. The recent decrease of the response time to a ToO alert of more than 50 % achieved by improvements in hard- & software as well as the overall performance of the system will be discussed.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578818", "resources": [{"_type": "LocalFile", "name": "Balzer_HESS_DAQ.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/215\/attachments\/578818\/797005\/Balzer_HESS_DAQ.pdf", "fileName": "Balzer_HESS_DAQ.pdf", "_fossil": "localFileMetadata", "id": "797005", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "d54c202551c883d66e25d23de3a7ae72", "affiliation": "H.E.S.S. Site Namibia", "_fossil": "contributionParticipationMetadata", "fullName": "H.E.S.S., Collaboration", "id": "4"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/215", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:00:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "212", "speakers": [{"_type": "ContributionParticipation", "emailHash": "41c9211fa0c4d3b49e193b906880a756", "affiliation": "Institute of High Physics Chinese Academy of Sciences", "_fossil": "contributionParticipationMetadata", "fullName": "Ms. KAN, Bowen", "id": "4"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "79eec8fa2cf30e30bbc8ec86de7a4ea5", "affiliation": "Chinese Academy of Sciences (CN)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HUANG, Qiulan", "id": "0"}], "title": "BESIII physical offline data analysis on virtualization platform", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T11:07:56.451807+00:00", "description": "", "title": "BESIII_computing_in_virtual_platform.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/212\/attachments\/578819\/797006\/BESIII_computing_in_virtual_platform.pdf", "filename": "BESIII_computing_in_virtual_platform.pdf", "content_type": "application\/pdf", "type": "file", "id": 797006, "size": 2792089}, {"_type": "attachment", "modified_dt": "2015-04-10T11:07:56.451807+00:00", "description": "", "title": "BESIII_computing_in_virtual_platform.ppt", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/212\/attachments\/578819\/797007\/BESIII_computing_in_virtual_platform.ppt", "filename": "BESIII_computing_in_virtual_platform.ppt", "content_type": "application\/vnd.ms-powerpoint", "type": "file", "id": 797007, "size": 1788416}], "title": "Slides", "default_folder": false, "id": 578819, "description": ""}], "_type": "Contribution", "description": "Mass data processing and analysis contribute much to the development and discoveries of a new generation of High Energy Physics. The BESIII experiment of IHEP(Institute of High Energy Physics, Beijing, China) studies particles  in the tau-charm energy region ranges from 2 GeV to 4.6 GeV, and requires massive storage and computing resources, which is a typical kind of data intensive application. With the rapid growth of experimental data, the data processing system encounters many problems, such as low resource utilization, complex migration and so on, which makes it urgent to transplant the data analysis system to a virtualization platform. However, offline software design, resource allocation and job scheduling of BESIII experiment are all based on physical machine. To solve those problems, we bring the virtualization technology of Openstack and KVM to BESIII computing system. \r\n\r\nIn this contribution we present an ongoing work which aims to make BESIII physical analysis work on virtualized resources to achieve higher resource utilization, dynamic resource management and higher job operating efficiency. Particularly, we discuss the architecture of BESIII offline software and the way to optimize the offline software to reduce the performance loss in virtualized environment by creating event index(event metadata) and do event pre-selection based on index, which significantly reduces the IO throughput and event numbers that need to do analysis, and then greatly improves the job processing efficiency. We also report the optimization of KVM from various factors in hardware and kernel including EPT (Extended Page Tables) and CPU affinity. Experimental results show the CPU performance penalty of KVM can be decreased to about 3%. This work is validated through real use cases of production BESIII jobs by working on physical slots and virtualized slots. In addition, the performance comparison between KVM and physical machines in aspect of CPU, disk IO and network IO is also presented. Finally, we describe our development work of adaptive cloud scheduler, which allocates and reclaims VMs dynamically according to the status of TORQUE queue and the size of resource pool to improve resource utilization and job processing efficiency.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578819", "resources": [{"_type": "LocalFile", "name": "BESIII_computing_in_virtual_platform.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/212\/attachments\/578819\/797006\/BESIII_computing_in_virtual_platform.pdf", "fileName": "BESIII_computing_in_virtual_platform.pdf", "_fossil": "localFileMetadata", "id": "797006", "_deprecated": true}, {"_type": "LocalFile", "name": "BESIII_computing_in_virtual_platform.ppt", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/212\/attachments\/578819\/797007\/BESIII_computing_in_virtual_platform.ppt", "fileName": "BESIII_computing_in_virtual_platform.ppt", "_fossil": "localFileMetadata", "id": "797007", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "263b9b545ef46a68fc212b9ea89d3f4e", "affiliation": "Chinese Academy of Sciences", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LI, haibo", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "8f42bcf5a598f1e145ad66dda0e23c3f", "affiliation": "Chinese Academy of Sciences", "_fossil": "contributionParticipationMetadata", "fullName": "Ms. LEI, xiaofeng", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "41c9211fa0c4d3b49e193b906880a756", "affiliation": "Institute of High Physics Chinese Academy of Sciences", "_fossil": "contributionParticipationMetadata", "fullName": "Ms. KAN, Bowen", "id": "4"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/212", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:15:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "213", "speakers": [{"_type": "ContributionParticipation", "emailHash": "df6f12602a0e37a3a6aafcc9eaab0c42", "affiliation": "University of Cincinnati", "_fossil": "contributionParticipationMetadata", "fullName": "AURISANO, Adam", "id": "6"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "df6f12602a0e37a3a6aafcc9eaab0c42", "affiliation": "University of Cincinnati", "_fossil": "contributionParticipationMetadata", "fullName": "AURISANO, Adam", "id": "6"}], "title": "The NOvA Simulation Chain", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T02:43:46.233134+00:00", "description": "", "title": "aurisano_sim_nova.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/213\/attachments\/578820\/797008\/aurisano_sim_nova.pdf", "filename": "aurisano_sim_nova.pdf", "content_type": "application\/pdf", "type": "file", "id": 797008, "size": 1387165}], "title": "Slides", "default_folder": false, "id": 578820, "description": ""}], "_type": "Contribution", "description": "The NOvA experiment is a two-detector, long-baseline neutrino experiment operating in the recently upgraded NuMI muon neutrino beam. Simulating neutrino interactions and backgrounds requires many steps including: the simulation of the neutrino beam flux using FLUKA and the FLUGG interface; cosmic ray generation using CRY; neutrino interaction modeling using GENIE; and a simulation of the energy deposited in the detector using GEANT4. To shorten generation time, the modeling of detector-specific aspects, such as photon transport, detector and electronics noise, and readout electronics, employs custom, parameterized simulation applications. We will describe the NOvA simulation chain, and present details on the techniques used in modeling photon transport near the ends of cells, and in developing a novel data-driven noise simulation.\r\n\r\nDue to the high intensity of the NuMI beam, the Near Detector samples a high rate of muons originating in the surrounding rock. In addition, due to its location on the surface at Ash River, MN, the Far Detector collects a large rate (~40 kHz) of cosmic muons. We will discuss the methods used in NOvA for overlaying rock muons and cosmic ray muons with simulated neutrino interactions and show how realistically the final simulation reproduces the preliminary NOvA data.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578820", "resources": [{"_type": "LocalFile", "name": "aurisano_sim_nova.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/213\/attachments\/578820\/797008\/aurisano_sim_nova.pdf", "fileName": "aurisano_sim_nova.pdf", "_fossil": "localFileMetadata", "id": "797008", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "4fbad3cbc996bc9dd0e9523be424cd7a", "affiliation": "FNAL", "_fossil": "contributionParticipationMetadata", "fullName": "HATCHER, Robert William", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "c06196a1fdeb4395a54918aa9d91ed07", "affiliation": "Tufts University", "_fossil": "contributionParticipationMetadata", "fullName": "MAYER, Nathan", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "2ec8f8d1f5ef8d84a89d5a17c088a1dd", "affiliation": "Indiana University", "_fossil": "contributionParticipationMetadata", "fullName": "MUSSER, James", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "929dc9fb8b13228148cec82f9cd2ecf1", "affiliation": "Harvard", "_fossil": "contributionParticipationMetadata", "fullName": "SCHROETER, Raphael", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "9b53face299d25dc2ed837bf4995f5c9", "affiliation": "University of Cincinnati", "_fossil": "contributionParticipationMetadata", "fullName": "SOUSA, Alexandre", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "6643181cb4338c0a4bedd75ccdfacd86", "affiliation": "Caltech", "_fossil": "contributionParticipationMetadata", "fullName": "PATTERSON, Ryan", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "16bdf91b327521302d812ff7c61549ed", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "BACKHOUSE, Christopher", "id": "7"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/213", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "210", "speakers": [{"_type": "ContributionParticipation", "emailHash": "0924dfb6cfaf429459832827db5a0041", "affiliation": "CEA\/IRFU,Centre d'etude de Saclay Gif-sur-Yvette (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "FORMICA, Andrea", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "0924dfb6cfaf429459832827db5a0041", "affiliation": "CEA\/IRFU,Centre d'etude de Saclay Gif-sur-Yvette (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "FORMICA, Andrea", "id": "0"}], "title": "A JEE Restful service to access Conditions Data in ATLAS", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-07T12:44:03.631660+00:00", "description": "", "title": "JBCOOL-POSTER-CHEP-v4.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/210\/attachments\/578821\/797009\/JBCOOL-POSTER-CHEP-v4.pdf", "filename": "JBCOOL-POSTER-CHEP-v4.pdf", "content_type": "application\/pdf", "type": "file", "id": 797009, "size": 1179027}], "title": "Slides", "default_folder": false, "id": 578821, "description": ""}], "_type": "Contribution", "description": "Usage of Conditions Data in ATLAS is extensive for offline reconstruction and analysis (for example: alignment, calibration, data quality).  The system is based on the LCG Conditions Database infrastructure, with read and write access via an ad hoc C++ API (COOL), a system which was developed before Run 1 data taking began.  The infrastructure dictates that the data is organized into separate schemas (assigned to subsystems\/groups storing distinct and independent sets of conditions), making it difficult to access information from several schemas at the same time.\r\n\r\nWe have thus created PL\/SQL functions containing queries to provide content extraction at multi-schema level. The PL\/SQL API has been exposed to external clients by means of an intermediate java application server (JBoss), where an application delivering access to the DB via RESTful services has been deployed.  The services allow navigation over multiple schema content, via simple URLs. The queried data can be retrieved either in XML or JSON formats, via simple clients (curl, wget or Web browsers).", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578821", "resources": [{"_type": "LocalFile", "name": "JBCOOL-POSTER-CHEP-v4.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/210\/attachments\/578821\/797009\/JBCOOL-POSTER-CHEP-v4.pdf", "fileName": "JBCOOL-POSTER-CHEP-v4.pdf", "_fossil": "localFileMetadata", "id": "797009", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "c195ef3b576ede75b0ccfd47c33fba3a", "affiliation": "University of Oxford (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "GALLAS, Elizabeth", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/210", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "211", "speakers": [{"_type": "ContributionParticipation", "emailHash": "176c296782ce29da0f210b740b97eeb0", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "WANG, Lu", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "176c296782ce29da0f210b740b97eeb0", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "WANG, Lu", "id": "0"}], "title": "Applying deep neural networks to HEP job statistics", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T09:20:58.880756+00:00", "description": "", "title": "wanglu_toprint.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/211\/attachments\/578822\/797010\/wanglu_toprint.pdf", "filename": "wanglu_toprint.pdf", "content_type": "application\/pdf", "type": "file", "id": 797010, "size": 267530}], "title": "Poster", "default_folder": false, "id": 578822, "description": ""}], "_type": "Contribution", "description": "The cluster of CC-IHEP is a middle sized computing system providing 10 thousands CPU cores, 3 PB disk storage, and 40 GB \/s IO throughput. Its 1000+ users come from serials of HEP experiments including ATLAS, BESIII, CMS, DYB, JUNO, YBJ etc. In such a system, job statistics is necessary to find performance bottlenecks, locate software pitfalls, identify suspicious behaviors and make resource provisions, especially for new experiments whose computing modeling are still developing and refining. \r\nTo fulfill this requirement, we have developed and deployed a job statistics system which consists of an instrumenting agent, a central database, a data summarizer and a visualizer on the IHEP cluster. In the first half of 2014, the system has collected 1 million valid job records from BESIII experiment. Each job record includes static information from batch system, average efficiency from process manager and detailed IO parameters from VFS interfaces.To analyze this dataset we find that DNNs (Deep Neural Networks) is a useful technique for data classification and abnormity detection. \r\nThis paper demonstrates how we train a job classifier with DNNs. It firstly describes how we label the dataset semi-automatically from about 20% jobs samples which have hints of job type in their job-option-file names. Then some adapted  data pre-processing steps will be presented. After that, it will describe the DNNs model which has achieved a precision of 96.6% with 240\u2019000 labeled job samples (Ratio of training set and testing set is 7:3), 6 classes. It will also compare the results with those from a linear model and a MLPs (Multi-Layer Perceptrons) model. Impacts of meta-parameters including learning rate, batch size will be discussed. Examples of how we leverage the classification results to find software problems and detect abnormal job behaviors will be given at the last part of this paper.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578822", "resources": [{"_type": "LocalFile", "name": "wanglu_toprint.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/211\/attachments\/578822\/797010\/wanglu_toprint.pdf", "fileName": "wanglu_toprint.pdf", "_fossil": "localFileMetadata", "id": "797010", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/211", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:30:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "218", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2d10c567422b53eb7fccd0134c728788", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MESCHI, Emilio", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2d10c567422b53eb7fccd0134c728788", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MESCHI, Emilio", "id": "0"}], "title": "File-based data flow in the CMS Filter Farm", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T05:42:01.221287+00:00", "description": "", "title": "CHEP_F3_EM.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/218\/attachments\/578823\/797011\/CHEP_F3_EM.pdf", "filename": "CHEP_F3_EM.pdf", "content_type": "application\/pdf", "type": "file", "id": 797011, "size": 3012313}, {"_type": "attachment", "modified_dt": "2015-04-13T05:42:01.221287+00:00", "description": "", "title": "CHEP_F3_EM.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/218\/attachments\/578823\/797012\/CHEP_F3_EM.pptx", "filename": "CHEP_F3_EM.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797012, "size": 14488062}], "title": "Slides", "default_folder": false, "id": 578823, "description": ""}], "_type": "Contribution", "description": "During the LHC Long Shutdown 1, the CMS DAQ system underwent a partial redesign to replace obsolete network equipment, use more homogeneous switching technologies, and prepare the ground for future upgrades of the detector front-ends. The software and hardware infrastructure to provide input, execute the High Level Trigger (HLT) algorithms and deal with output data transport and storage has also been redesigned to be completely file-based. This approach provides a complete decoupling between the HLT algorithms and the input and output data flow. All the metadata needed for bookkeeping of the data flow and the HLT process lifetimes are also generated in the form of small \u201cdocuments\u201d using the JSON encoding, by either services in the flow of the HLT execution (for rates etc.) or watchdog processes. These \u201cfiles\" can remain memory-resident or be written to disk if they are to be used in another part of the system (e.g. for aggregation of output data). We discuss how this redesign improves to the robustness and flexibility of the CMS DAQ and the performance of the system currently being commissioned for the LHC Run 2.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578823", "resources": [{"_type": "LocalFile", "name": "CHEP_F3_EM.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/218\/attachments\/578823\/797011\/CHEP_F3_EM.pdf", "fileName": "CHEP_F3_EM.pdf", "_fossil": "localFileMetadata", "id": "797011", "_deprecated": true}, {"_type": "LocalFile", "name": "CHEP_F3_EM.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/218\/attachments\/578823\/797012\/CHEP_F3_EM.pptx", "fileName": "CHEP_F3_EM.pptx", "_fossil": "localFileMetadata", "id": "797012", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "321d291a26425f7ba000c1227f961f93", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "ANDRE, Jean-Marc Olivier", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "ddaeb584ae450aae5064a948e08ead3c", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. DOBSON, Marc", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "8b7c922625aaf91ee706a1518921a755", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DUPONT, Aymeric Arnaud", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "b03db255046e47e29d52f585a312fecf", "affiliation": "Univ. of California Los Angeles (US)", "_fossil": "contributionParticipationMetadata", "fullName": "ERHAN, Samim", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "4694ee2e44416e0163f30d20408ccad0", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GIGI, Dominique", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "6fb6d2d7f55ab188239ef3702f47965a", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GLEGE, Frank", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "37e8bb7f1971a51948ad4fce29334bf7", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GOMEZ CEBALLOS RETUERTO, Guillelmo", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "0686dcacfe13161c58c9e8a9fd9d65d2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HEGEMAN, Jeroen", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "98493644fe4b7f32b27b9e10cf03a340", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "HOLZNER, Andre Georg", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "932403f3327b0e3b2903995e513382a2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MASETTI, Lorenzo", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "82e7a21a5011f52cb8606f0f0601f8d0", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MEIJERS, Frans", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "d3be8d0289e184c2c754180e84f1638b", "affiliation": "Aristotle Univ. of Thessaloniki (GR)", "_fossil": "contributionParticipationMetadata", "fullName": "ANDRONIDIS, Anastasios", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "060d7d9521c75cc21082d88edbe36aae", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MOMMSEN, Remi", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "5ae3a32d3ffcbc90d737c183bebe4201", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MOROVIC, Srecko", "id": "14"}, {"_type": "ContributionParticipation", "emailHash": "7e2c5e676f7ba0f9af6e617d71de2a44", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "NUNEZ BARRANCO FERNANDEZ, Carlos", "id": "15"}, {"_type": "ContributionParticipation", "emailHash": "b22d1baa246adbc92c9a6050219aeea8", "affiliation": "Fermi National Accelerator Laboratory (FNAL)", "_fossil": "contributionParticipationMetadata", "fullName": "O'DELL, Vivian", "id": "16"}, {"_type": "ContributionParticipation", "emailHash": "7cdd48f784a087c6d9b6f6003cfc03f7", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ORSINI, Luciano", "id": "17"}, {"_type": "ContributionParticipation", "emailHash": "c20242a86672d5fc162e300100556857", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "PAUS, Christoph", "id": "18"}, {"_type": "ContributionParticipation", "emailHash": "1951a3ffe6d5cf1393f669f572be4751", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "PETRUCCI, Andrea", "id": "19"}, {"_type": "ContributionParticipation", "emailHash": "88f4d725c22d118782d55d91895473a9", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "PIERI, Marco", "id": "20"}, {"_type": "ContributionParticipation", "emailHash": "dea4bd4e0305108f26ad10d80d8a923c", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. RACZ, Attila", "id": "21"}, {"_type": "ContributionParticipation", "emailHash": "4288d38b57c8078dd7a98743244aafa0", "affiliation": "Nottingham Trent University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "ROBERTS, Penelope Amelia", "id": "22"}, {"_type": "ContributionParticipation", "emailHash": "148e6617a9a734113e8ef6b1f86cb767", "affiliation": "University of Wisconsin (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BAWEJ, Tomasz Adrian", "id": "23"}, {"_type": "ContributionParticipation", "emailHash": "e61604d9f35ae72b3531378bca0e6216", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SAKULIN, Hannes", "id": "24"}, {"_type": "ContributionParticipation", "emailHash": "01ca3cdd821be71f01a7fa78f1cd3e2c", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SCHWICK, Christoph", "id": "25"}, {"_type": "ContributionParticipation", "emailHash": "f458e4333696558cf9533ea62f29fc06", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "STIEGER, Benjamin", "id": "26"}, {"_type": "ContributionParticipation", "emailHash": "7b150004ac09fb1883e427cdc460cedc", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "SUMOROK, Konstanty", "id": "27"}, {"_type": "ContributionParticipation", "emailHash": "0b3372b1a406c07547c0658d54373e3a", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. VEVERKA, Jan", "id": "28"}, {"_type": "ContributionParticipation", "emailHash": "d8f0435f2ab3190ac7faaf3007671c08", "affiliation": "Sezione di Pisa (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "ZAZA, Salvatore", "id": "29"}, {"_type": "ContributionParticipation", "emailHash": "842f03deb43d34961e29b5c1ee95392e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ZEJDL, Petr", "id": "30"}, {"_type": "ContributionParticipation", "emailHash": "34b7170966da6ae240c3a7c096c08295", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "BEHRENS, Ulf", "id": "31"}, {"_type": "ContributionParticipation", "emailHash": "3a7e6bfcd4dbb638710fce76e52d2342", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BRANSON, James Gordon", "id": "32"}, {"_type": "ContributionParticipation", "emailHash": "d33e3ae247733829a0d0c9511973c44e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHAZE, Olivier", "id": "33"}, {"_type": "ContributionParticipation", "emailHash": "779d4951aab39403b13ffbc89e9f0c90", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CITTOLIN, Sergio", "id": "34"}, {"_type": "ContributionParticipation", "emailHash": "14823d25b1099daf48e1923cddbe3811", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DARLEA, Georgiana Lavinia", "id": "35"}, {"_type": "ContributionParticipation", "emailHash": "c5db4520580bcc09fe8df060c043fc7b", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DELDICQUE, Christian", "id": "36"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/218", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:00:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:15:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "219", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2d10c567422b53eb7fccd0134c728788", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MESCHI, Emilio", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2d10c567422b53eb7fccd0134c728788", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MESCHI, Emilio", "id": "0"}], "title": "THE DAQ NEEDLE IN THE BIG-DATA HAYSTACK", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T08:47:40.451080+00:00", "description": "", "title": "CHEP_DN_EM.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/219\/attachments\/578824\/797013\/CHEP_DN_EM.pdf", "filename": "CHEP_DN_EM.pdf", "content_type": "application\/pdf", "type": "file", "id": 797013, "size": 3157743}, {"_type": "attachment", "modified_dt": "2015-04-13T08:47:40.451080+00:00", "description": "", "title": "CHEP_DN_EM.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/219\/attachments\/578824\/797014\/CHEP_DN_EM.pptx", "filename": "CHEP_DN_EM.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797014, "size": 22974582}], "title": "Slides", "default_folder": false, "id": 578824, "description": ""}], "_type": "Contribution", "description": "Technology convergences in the post-LHC era\r\n\r\n\r\nIn the course of the last three decades HEP experiments have had to face the challenge of manipulating larger and larger masses of data from increasingly complex and heterogeneous detectors with hundreds of millions of electronic channels. The traditional approach of low-level data reduction using ad-hoc electronics working on fast analog signals, followed by global readout and digitisation, and a final stage of centralised processing in a more or less monolithic system has reached its limit before the LHC era. LHC experiments have been forced to turn to a distributed approach, leveraging the appearance of high speed switched networks developed for digital telecommunication and the internet. This has led to a generation of experiments where the use of custom electronics, analysing coarser-granularity analog or digital \u201cfast\u201d data, is limited to the first phase of triggering, where predictable latency and real time processing, as well as reliable, low-jitter clock and trigger distribution, are a necessity dictated by the limits of the front end readout buffers. Low speed monitoring (temperatures, pressures, etc.) and controls (thresholds, calibrations, voltages, etc.) have remained decoupled and considered an altogether separate realm in the detector design and operation.\r\n\r\nWe believe that it is now time for the HEP community to prepare for the next \u201crevolution\u201d. Already, the mass of persistent data produced by e.g. the LHC experiments means that multiple pass end-to-end offline processing is becoming increasingly burdensome. Some experiments (e.g. ALICE) are moving towards a single-pass system for data reduction, relying on fast calibration feedback loops for zero suppression and low-level pattern recognition into the online system. The pristine \u201craw\u201d channel readouts become thus volatile and no longer permanently stored. Others (e.g. LHCb) read out every channel for each beam crossing and delegate the entirety of the data reduction, reconstruction and selection to a fully software system. The latter approach is particularly attractive if low power techniques can be developed to counter the negative effects of the consequent increase in material budget for services and cooling in the active areas of the detector.\r\nFurther developments can be envisaged.\r\nOn the one hand very large scale integration paired with progress in radiation hard technologies, as well as the appearance of high-bandwidth bidirectional optical links, both on and off-silicon, could make intelligent very-front-end electronics and high-speed low-power readout a possibility already in the next decade, thus lifting strict latency limitations. At the same time, the traditional distinction between readout, trigger and control data channels will become increasingly artificial, paving the way to the possibility of running fully programmable algorithms at on- or near-detector electronics.\u00a0\r\nOn the other hand, boosted by the \u201cbig data\u201d industry, massively parallel and distributed analysis of unstructured data has become ubiquitous in commercial applications. Apart for their attractiveness for use in monitoring of both detector parameters and data flow, as well as data analysis, these technologies indicate in our opinion a possible evolutionary path for future DAQ and trigger architectures. In particular, a new trend is emerging from the data mining and analytics world which consists in \u201cbringing the algorithm to the data\u201d. For HEP experiments, this might mean to abandon the consolidated paradigm represented by the triad low-level trigger - event building - high level trigger.\r\nHow close can we bring our algorithms to the detector ? Can we take advantage of the ideas, software (and hardware) technologies developed for data mining and search engines ? Can we imagine a future detector with extremely deep multi-stage, asynchronous or even virtual pipelines, where data streams from the various detector channels are analysed and indexed in quasi-real-time, and the final selection is operated as a distributed \u201csearch for interesting event parts\u201d ?\r\nCan we push these ideas even further, removing the inflexible notion of pre-processed datasets and paving the way to completely new forms of selection and analysis, that can be developed, tested and implemented \u201conline\u201d as aggregation and reduction algorithms making use of the full, unstructured information from the experiment and directly returning the high-level physics quantities of interest ?\r\nWe investigate the potential impact of these different developments in the design of detector readout, trigger and data acquisition systems in the next decades.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578824", "resources": [{"_type": "LocalFile", "name": "CHEP_DN_EM.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/219\/attachments\/578824\/797013\/CHEP_DN_EM.pdf", "fileName": "CHEP_DN_EM.pdf", "_fossil": "localFileMetadata", "id": "797013", "_deprecated": true}, {"_type": "LocalFile", "name": "CHEP_DN_EM.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/219\/attachments\/578824\/797014\/CHEP_DN_EM.pptx", "fileName": "CHEP_DN_EM.pptx", "_fossil": "localFileMetadata", "id": "797014", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/219", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:15:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "289", "speakers": [{"_type": "ContributionParticipation", "emailHash": "37a9d67c5498998436724740f824542b", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LETTS, James", "id": "9"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c64735b9540bc92c4d230a2ca252b7cf", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GUTSCHE, Oliver", "id": "0"}], "title": "Using the glideinWMS System as a Common Resource Provisioning Layer in CMS", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T04:43:25.329974+00:00", "description": "", "title": "CHEP15-TALK-FINAL.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/289\/attachments\/578825\/797015\/CHEP15-TALK-FINAL.pdf", "filename": "CHEP15-TALK-FINAL.pdf", "content_type": "application\/pdf", "type": "file", "id": 797015, "size": 1166560}], "title": "Slides", "default_folder": false, "id": 578825, "description": ""}], "_type": "Contribution", "description": "CMS will require access to more than 125k processor cores for the beginning of Run2 in 2015 to carry out its ambitious physics program with more and higher complexity events. During Run1 these resources were predominantly provided by a mix of grid sites and local batch resources. During the long shut down cloud infrastructures, diverse opportunistic resources and HPC supercomputing centers were made available to CMS, which further complicated the operations of the submission infrastructure. In this presentation we will discuss the CMS effort to adopt and deploy the glideinWMS system as a common resource provisioning layer to grid, cloud, local batch, and opportunistic resources and sites. We will address the challenges associated with integrating the various types of resources, the efficiency gains and simplifications associated with using a common resource provisioning layer, and discuss the solutions found. We will finish with an outlook of future plans for how CMS is moving forward on resource provisioning for more heterogenous architectures and services.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578825", "resources": [{"_type": "LocalFile", "name": "CHEP15-TALK-FINAL.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/289\/attachments\/578825\/797015\/CHEP15-TALK-FINAL.pdf", "fileName": "CHEP15-TALK-FINAL.pdf", "_fossil": "localFileMetadata", "id": "797015", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "0e16821a7ef57745e150ed35cc005ab4", "affiliation": "Vilnius University (LT)", "_fossil": "contributionParticipationMetadata", "fullName": "BALCAS, Justas", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "d854ed989b482b2f04dd5615b5a9f643", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "BELFORTE, Stefano", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "e82c46a3a0d49dd555c888d113eace3a", "affiliation": "University of Nebraska (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BOCKELMAN, Brian Paul", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "14c1288bca407f501ee1fa70d5f30369", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "WISSING, Christoph", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "35f4f869ebf48e550006a86e20965547", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LARSON, Krista", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "91b6e0458cfc38c6e2b8701f5a7b9a24", "affiliation": "Universita & INFN, Milano-Bicocca (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "MASCHERONI, Marco", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "8ce8233ffe359f39d3fdd125371fe45b", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MASON, David Alexander", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "c1a42d65e29a742bc3b359fe0eb2186c", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MC CREA, Alison", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "37a9d67c5498998436724740f824542b", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LETTS, James", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "a85f2ccb15edffa1d06959fbd105561b", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "SAIZ SANTOS, Maria Dolores", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "ef50b0669d021bfbb6de721581c091c8", "affiliation": "University of California San Diego", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. SFILIGOI, Igor", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "068b9018b69ea40467e02d92d6a35844", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "HUFNAGEL, Dirk", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "573555c7eea26b662289bd639076bd77", "affiliation": "Brown University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "PIPEROV, Stefan", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "cd17961312c904e3bb4b71723c8e87ac", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. COLLING, David", "id": "14"}, {"_type": "ContributionParticipation", "emailHash": "19fbe91d19a3691e61eb5e432e34734a", "affiliation": "National Centre for Physics (PK)", "_fossil": "contributionParticipationMetadata", "fullName": "KHAN, Farrukh Aftab", "id": "15"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/289", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "288", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ecef85c95b0dddfc15be379651cd7344", "affiliation": "GSI DARMSTADT", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LINEV, Sergey", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ecef85c95b0dddfc15be379651cd7344", "affiliation": "GSI DARMSTADT", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LINEV, Sergey", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "c07a62d98c1de4bc48f01d73891b07d5", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BELLENOT, Bertrand", "id": "1"}], "title": "JSROOT version 3 \u2013 JavaScript library for ROOT", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-07T14:33:49.982163+00:00", "description": "", "title": "JSROOT.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/288\/attachments\/578826\/797016\/JSROOT.pdf", "filename": "JSROOT.pdf", "content_type": "application\/pdf", "type": "file", "id": 797016, "size": 1481365}], "title": "Slides", "default_folder": false, "id": 578826, "description": ""}], "_type": "Contribution", "description": "This is further development of JSRootIO project. Code was mostly rewritten to make it modular; I\/O part was clearly separated from the graphics. \r\n\r\nMany new interactive features were implemented: \r\n\r\n - loading of required functionality on the fly;\r\n - possibility of dynamic update of objects drawings;\r\n - automatic resize of drawings when browser window is resized;\r\n - move\/resize of elements like statbox and title;\r\n - context menu for different elements.\r\n\r\nOn the [central JSROOT page][1] one could open ROOT files, browse their content and display different ROOT objects like histograms, graphs or canvases. Produced drawings can be easily integrated into other HTML pages via 'iframe' tag. Simple and flexible JSROOT API provided; it is used to implement graphical interface for THttpServer class. \r\n\r\nJSROOT is now part of actual ROOT distribution. On [http:\/\/web-docs.gsi.de\/~linev\/js\/][2] one could found latest version together with documentation and examples. \r\n\r\n[1]: http:\/\/web-docs.gsi.de\/~linev\/js\/3.0\/\r\n[2]: http:\/\/web-docs.gsi.de\/~linev\/js\/", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578826", "resources": [{"_type": "LocalFile", "name": "JSROOT.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/288\/attachments\/578826\/797016\/JSROOT.pdf", "fileName": "JSROOT.pdf", "_fossil": "localFileMetadata", "id": "797016", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/288", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:15:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "4", "speakers": [{"_type": "ContributionParticipation", "emailHash": "c88a53152f14b88576d3914278e24322", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BENSON, Sean", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ebca60259ea463194fa833498a22a363", "affiliation": "Ruprecht-Karls-Universitaet Heidelberg (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "VESTERINEN, Mika Anton", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "c88a53152f14b88576d3914278e24322", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BENSON, Sean", "id": "4"}], "title": "The LHCb turbo stream", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T02:46:01.356678+00:00", "description": "", "title": "CHEP2015-SB-LHCbTurboStream.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/4\/attachments\/578827\/797017\/CHEP2015-SB-LHCbTurboStream.pdf", "filename": "CHEP2015-SB-LHCbTurboStream.pdf", "content_type": "application\/pdf", "type": "file", "id": 797017, "size": 2730233}], "title": "Slides", "default_folder": false, "id": 578827, "description": ""}], "_type": "Contribution", "description": "The LHCb experiment will record an unprecedented dataset of beauty and charm hadron decays during Run II of the LHC, set to take place between 2015 and 2018. A key computing challenge is to store and process this data, which limits the maximum output rate of the LHCb trigger. So far, LHCb has written out a few kHz of events containing the full raw sub-detector data, which are passed through a full offline event reconstruction before being considered for physics analysis. Charm physics in particular is limited by trigger output rate constraints. A new streaming strategy includes the possibility to perform the physics analysis with candidates reconstructed in the trigger, thus bypassing the offline reconstruction. In the \"turbo stream\" the trigger will write out a compact summary of \"physics\" objects containing all information necessary for analyses, and this will allow an increased output rate and thus higher average efficiencies and smaller selection biases. This idea will be commissioned and developed during 2015 with a selection of physics analyses. It is anticipated that the turbo stream will be adopted by an increasing number of analyses during the remainder of LHC Run-II (2015-2018) and ultimately in Run-III (starting in 2020) with the upgraded LHCb detector.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578827", "resources": [{"_type": "LocalFile", "name": "CHEP2015-SB-LHCbTurboStream.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/4\/attachments\/578827\/797017\/CHEP2015-SB-LHCbTurboStream.pdf", "fileName": "CHEP2015-SB-LHCbTurboStream.pdf", "_fossil": "localFileMetadata", "id": "797017", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "0c16591cb302e19fdb5628b9015f5313", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GLIGOROV, Vladimir", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "10fc0c32be829bc87d01d57897b28fe7", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "WILLIAMS, J Michael", "id": "2"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/4", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:45:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "281", "speakers": [{"_type": "ContributionParticipation", "emailHash": "30a73745afbaea1197a46a3cad7c58b9", "affiliation": "NIKHEF (NL)", "_fossil": "contributionParticipationMetadata", "fullName": "TEMPLON, Jeff", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "30a73745afbaea1197a46a3cad7c58b9", "affiliation": "NIKHEF (NL)", "_fossil": "contributionParticipationMetadata", "fullName": "TEMPLON, Jeff", "id": "0"}], "title": "Scheduling multicore workload on shared multipurpose clusters", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T23:56:33.396146+00:00", "description": "", "title": "CHEP.2015.templon.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/281\/attachments\/578828\/797018\/CHEP.2015.templon.pdf", "filename": "CHEP.2015.templon.pdf", "content_type": "application\/pdf", "type": "file", "id": 797018, "size": 1564912}, {"_type": "attachment", "modified_dt": "2015-04-13T23:56:33.396146+00:00", "description": "", "title": "CHEP.2015.templon.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/281\/attachments\/578828\/797019\/CHEP.2015.templon.pptx", "filename": "CHEP.2015.templon.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797019, "size": 2348474}], "title": "Slides", "default_folder": false, "id": 578828, "description": ""}], "_type": "Contribution", "description": "With the advent of workloads containing explicit requests for multiple\r\ncores in a single grid job, grid sites faced a new set of challenges\r\nin workload scheduling.  The most common batch schedulers deployed at\r\nHEP computing sites do a poor job at multicore scheduling when using\r\nonly the native capabilities of those schedulers.  This talk describes\r\nhow efficient multicore scheduling was achieved at the three sites\r\nrepresented in the author list, by implementing dynamically-sized\r\nmulticore partitions via a minimalistic addition to the Torque\/Maui\r\nbatch system already in use at those sites.\r\n\r\nThe first part of the talk covers the theory related to this\r\nparticular problem, which is also applicable to e.g. the scheduling of\r\nlarge-memory jobs or data-aware jobs similarly comprising part of a\r\nhighly heterogenous workload. The system design is also presented,\r\nlinking it to previous work at Nikhef on grid-cluster\r\nscheduling.  The second part of the talk presents an evaluation of\r\nseveral months of production operation at the three sites.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578828", "resources": [{"_type": "LocalFile", "name": "CHEP.2015.templon.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/281\/attachments\/578828\/797018\/CHEP.2015.templon.pdf", "fileName": "CHEP.2015.templon.pdf", "_fossil": "localFileMetadata", "id": "797018", "_deprecated": true}, {"_type": "LocalFile", "name": "CHEP.2015.templon.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/281\/attachments\/578828\/797019\/CHEP.2015.templon.pptx", "fileName": "CHEP.2015.templon.pptx", "_fossil": "localFileMetadata", "id": "797019", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "3104caab90081a7fa54ff8c20595e0c0", "affiliation": "Centro de Investigaciones Energ. Medioambientales y Tecn. - (ES", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PEREZ-CALERO YZQUIERDO, Antonio", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "cbb0d28d39b1ca6e5d447a3e19e0e3c7", "affiliation": "Centro de Investigaciones Energ. Medioambientales y Tecn. - (ES", "_fossil": "contributionParticipationMetadata", "fullName": "FLIX MOLINA, Jose", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "197a9f75f811ff1663ab61d92a2b9561", "affiliation": "Nikhef", "_fossil": "contributionParticipationMetadata", "fullName": "STARINK, Ronald", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "3c540104c238880e46ed73772d460797", "affiliation": "Universitat Aut\u00f2noma de Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "ACOSTA SILVA, Carlos", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "a75649461e554c9fe19175f4fc21a0ed", "affiliation": "University of Manchester", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. FORTI, Alessandra", "id": "5"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/281", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "280", "speakers": [{"_type": "ContributionParticipation", "emailHash": "da4cac912389521f0838486a2a6ebd61", "affiliation": "STFC", "_fossil": "contributionParticipationMetadata", "fullName": "DE WITT, Shaun", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "da4cac912389521f0838486a2a6ebd61", "affiliation": "STFC", "_fossil": "contributionParticipationMetadata", "fullName": "DE WITT, Shaun", "id": "0"}], "title": "A Comparison of the Overheads Associated with WLCG Federation Technologies", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "For many years the Storage Resource Manager (SRM) has been the de-facto federation technology used by WLCG.  This technology has, along with the rest of the middleware stack, mediated the transfer of many Petabytes of data since the start of data taking.  In recent years, other technologies have become more popular as federation technologies because they offer additional functionalities that are not provided by SRM or they are based on more widely adopted standards.  The main technologies currently used or being evaluated with WLCG are xrootd and WebDav.\r\nIn this work we investigate the overhead associated with each technology for different commonly used storage systems within WLCG.  For jobs running at a local site, delays in file access can make a significant contribution to job efficiency and it is this we report on results from both controlled tests and in 'real life' operations.  While it is difficult to compare the overheads across different installations and hardware configurations, a comparison within an installation can help inform experiments to make the best choice of technology at any site and for any use case.", "track": "Track3: Data store and access ", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "572f6f06cb2be973d28b9d129a0a91fd", "affiliation": "University of Edinburgh (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "BHIMJI, Wahid", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "9004fd367d357871828de260587c178b", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "MILLAR, Paul", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/280", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "283", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5ae4855aa9d3c6685bd9f450814f9af6", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "STAGNI, Federico", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e716418209886a2c6221ebbe3510d139", "affiliation": "LUPM-IN2P3 (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "ARRABITO, Luisa", "id": "2"}], "title": "Prototype of a production system for CTA with DIRAC", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-07T08:29:46.625893+00:00", "description": "", "title": "CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/283\/attachments\/578829\/797020\/CHEP2015.pdf", "filename": "CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 797020, "size": 635223}], "title": "Poster", "default_folder": false, "id": 578829, "description": ""}], "_type": "Contribution", "description": "The Cherenkov Telescope Array (CTA) \u2013 an array of many tens of Imaging Atmospheric Cherenkov Telescopes deployed on an unprecedented scale \u2013 is the next generation instrument in the field of very high energy gamma-ray astronomy. CTA will operate as an open observatory providing data products to the scientific community. An average data stream of about 1 GB\/s for about 1000 hours of observation per year, thus producing several PB\/year, is expected. Large CPU time is required for data processing as well for massive Monte Carlo simulations needed for detector calibration purposes.\r\n The current CTA computing model is based on a distributed infrastructure for the archive and the data off-line processing. In order to manage the off-line data processing in a distributed environment, CTA has evaluated the DIRAC system, as base framework for the CTA production system. In particular, a production system prototype has been developed, based on the two main DIRAC components, i.e. the Workload Management and Data Management Systems. After two years of successful exploitation of this prototype, for simulations and analysis, we proved that DIRAC provides suitable functionalities needed for the CTA data processing. Based on these results, the CTA development plan aims to achieve an operational production system, based on the DIRAC Workload Management System, to be ready for the start of CTA operation phase in 2017-2018. One more important challenge consists of the development of a fully automatized execution of the CTA workflows. For this purpose, we have identified a third DIRAC component, the so-called Transformation System, which offers very interesting functionalities to achieve this automatisation. The Transformation System is a 'data-driven' system, allowing to automatically trigger data processing and data management operations according to pre-defined scenarios. In this paper, we present a brief summary of the DIRAC evaluation done so far, as well as the future developments planned for the CTA production system. In particular, we will focus on the developments of CTA automatic workflows, based on the Transformation System. As a result, we also propose some design optimizations of the Transformation System, in order to fully support the most complex workflows, envisaged in the CTA processing.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578829", "resources": [{"_type": "LocalFile", "name": "CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/283\/attachments\/578829\/797020\/CHEP2015.pdf", "fileName": "CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "797020", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "5ae4855aa9d3c6685bd9f450814f9af6", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "STAGNI, Federico", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "81335542c0329c563da4e90a0f81c4e0", "affiliation": "LUPM-IN2P3 (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "BREGEON, Johan", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "e1c4bec2dc1201b4c43be7b7a792e4eb", "affiliation": "University of Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "GRACIANI DIAZ, Ricardo", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "abfcc508622af21078d4ac47c92ceff6", "affiliation": "CPPM, Aix-Marseille Universit\u00e9, CNRS\/IN2P3, Marseille, France", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. TSAREGORODTSEV, Andrei", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "0db5da73e39293b6e051f37c0a6c35be", "affiliation": "Deutsches Elektronen-Synchrotron (DE) and Linnaeus University (SE)", "_fossil": "contributionParticipationMetadata", "fullName": "HAUPT, Andreas", "id": "6"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/283", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "282", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5ebf5a6c82b9693677a3a1fee00ddd42", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "CONDURACHE, Catalin", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "5ebf5a6c82b9693677a3a1fee00ddd42", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "CONDURACHE, Catalin", "id": "0"}], "title": "RAL Tier-1 evolution as global CernVM-FS service provider", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The CernVM-FS is firmly established as a method of software distribution for the LHC experiments at the WLCG sites. Use of CernVM-FS outside WLCG has been growing steadily, with increasing number of Virtual Organizations (VOs) both within High Energy Physics (HEP) communities and in other disciplines (i.e. Space, Natural and Life Sciences) having identified this technology as a more efficient way of maintaining and accessing their software across the Grid.\r\n\r\nWe give an overview on the work carried out at RAL to establish an infrastructure able to offer the CernVM-FS service to a broad range of HEP and non-HEP organizations and on the role played by the Tier-1 to support the CernVM-FS developers group. \r\n\r\nWe discuss the CernVM-FS Replica service delivered as part of the WLCG Stratum-1 network and we then focus on the facility provided to setup a complete service for the non-LHC communities. Master Repository (Stratum-0) and Replica\/Mirror (Stratum-1) facilities are presented, together with a customized mechanism to upload and maintain the master repositories by VO Software Grid Managers.\r\n\r\nThe RAL Tier-1 began offering CernVM-FS to small VOs in the UK in 2012. With support from the EGI the RAL Tier-1 has, since September 2013, been leading the CernVM-FS Task Force which has successfully developed an infrastructure, modeled on that deployed by WLCG, providing a resilient, distributed CernVM-FS service to non-LHC VOs across Europe and indeed replicated around the world. The evolution and current status of the worldwide non-LHC Stratum-0 and Stratum-1 network topology where RAL Tier-1 is a key player is discussed as part of the sustained effort made by the EGI CernVM-FS Task Force to promote the use of CernVM-FS and cooperation on cross-replicating repositories for VOs supported by multiple collaborations.\r\n\r\nThe pros and cons for a possible CernVM-FS service consolidation are explored. Using a single High Available Stratum-1 system for both WLCG and other HEP and non-HEP communities is cost-effective and will optimise the resources, as long as it is kept as a critical service without rising the maintenance costs.", "track": "Track3: Data store and access ", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "3b8d65ff191a56dbc6a18714dbf816f0", "affiliation": "STFC Rutherford Appleton Laboratory (UK)", "_fossil": "contributionParticipationMetadata", "fullName": "KELLY, John", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "84c6ace0bbe762b8ebd2bc3870870a69", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "COLLIER, Ian Peter", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/282", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:45:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:00:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "285", "speakers": [{"_type": "ContributionParticipation", "emailHash": "9004fd367d357871828de260587c178b", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MILLAR, Paul", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "a198240a6b21354217ead633867b3e31", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. FUHRMANN, Patrick", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "9004fd367d357871828de260587c178b", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MILLAR, Paul", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "b25e13a15662342acbe71a242e379e59", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. BUCHHOLZ, Quirin", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "d96310dde0f53841456b312025ab1877", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. MKRTCHYAN, Tigran", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "8d26ed6854ded78641202361191acea7", "affiliation": "NDGF", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BEHRMANN, Gerd", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "2f89590a83d4f4b4a573f2d8d9fb76eb", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "BERNARDT, Christian", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "ede687cc1fd66698a4d3625c52758c3d", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. SCHWANK, Karsten", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "d01d0e9eec1542b90bc8b2f80a6fb543", "affiliation": "FNAL", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ROSSI, Albert", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "6400c2cc73bf4ffb1d6d4985b52d6786", "affiliation": "FNAL", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LITVINTSEV, Dmitry", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "658d546d54ac53a37b0ada80f4ba522f", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. VAN DER REEST, Peter", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "bf63b5305594c47600ad190114b08d07", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GUELZOW, Volker", "id": "10"}], "title": "dCache, Sync-and-Share for Big Data", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T22:49:35.271745+00:00", "description": "", "title": "millar-dcache-sync-n-share.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/285\/attachments\/578830\/797021\/millar-dcache-sync-n-share.pdf", "filename": "millar-dcache-sync-n-share.pdf", "content_type": "application\/pdf", "type": "file", "id": 797021, "size": 4486034}], "title": "Slides", "default_folder": false, "id": 578830, "description": ""}], "_type": "Contribution", "description": "The availability of cheap, easy-to-use sync-and-share cloud services has split the scientific storage world into the traditional big data management systems and  the very attractive sync-and-share services. With the former, the location of data is well understood while the latter is mostly operated in the Cloud, resulting in a rather complex legal situation. \r\n\r\nBeside legal issues, those two worlds have little overlap in user authentication and access protocols. While traditional storage technologies, popular in HEP, are based on X509, cloud services and sync-n-share software technologies are generally based on user\/password authentication or mechanisms like SAML or Open ID Connect. Similarly, data access models offered by both are somewhat different, with sync-n-share services often using proprietary protocols.\r\n\r\nAs both approaches are very attractive, dCache.org developed a hybrid system, providing the best of both worlds. To avoid reinvent the wheel, dCache.org decided to embed another Open Source project: OwnCloud.  This offers the required modern access capabilities but does not support the managed data functionality needed for large capacity data storage.\r\n\r\nWith this hybrid system, scientist can share files and synchronize their data with laptops or mobile devices as easy as with any other cloud storage service. On top of this, the same data can be accessed via established mechanisms, like GridFTP to serve the Globus Transfer Service or the WLCG FTS3 tool, or the data can be made available to worker nodes or HPC applications via a mounted filesystem. As dCache provides a flexible authentication module, the same user can access its storage via different authentication mechanisms; e.g., X.509 and SAML. Additionally, users can specify the desired quality of service or trigger media transitions as necessary, so tuning data access latency to the planned access profile. Such features are a natural consequence of using dCache.\r\n\r\nWe will describe the design of the hybrid dCache\/OwnCloud system, report on several months of operations experience running it at DESY, and elucidate on the future road-map.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578830", "resources": [{"_type": "LocalFile", "name": "millar-dcache-sync-n-share.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/285\/attachments\/578830\/797021\/millar-dcache-sync-n-share.pdf", "fileName": "millar-dcache-sync-n-share.pdf", "_fossil": "localFileMetadata", "id": "797021", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/285", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "284", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ede687cc1fd66698a4d3625c52758c3d", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. SCHWANK, Karsten", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ede687cc1fd66698a4d3625c52758c3d", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. SCHWANK, Karsten", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "d96310dde0f53841456b312025ab1877", "affiliation": "Deutsches Elektronen-Synchrotron DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. MKRTCHYAN, Tigran", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "9004fd367d357871828de260587c178b", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MILLAR, Paul", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "2f89590a83d4f4b4a573f2d8d9fb76eb", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "BERNARDT, Christian", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "a198240a6b21354217ead633867b3e31", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. FUHRMANN, Patrick", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "6400c2cc73bf4ffb1d6d4985b52d6786", "affiliation": "FNAL", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LITVINTSEV, Dmitry", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "8d26ed6854ded78641202361191acea7", "affiliation": "NDGF", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BEHRMANN, Gerd", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "d01d0e9eec1542b90bc8b2f80a6fb543", "affiliation": "FNAL", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ROSSI, Albert", "id": "7"}], "title": "dCache, enabling tape systems to handle small files efficiently.", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "Increasingly, sites are using dCache to support communities that have different requirements from WLCG; as an example, DESY facilities and services now support photon sciences and biology groups. This presents new use-cases for dCache. Of particular interest is the chaotic file size distribution with a peak towards small files. This is problematic because tertiary storage systems, and tape in particular, are optimized for storing large files. Direct storing of the users files results in unacceptably poor performance.\r\n\r\nAs dCache bridges the filesystem view with the underlying storage and manages transitions between media, it is the natural place to solve the poor performance from storing small files on tape. We achieved this by introducing a new service that reconciles user demand against tape behavior.\r\n\r\nThe service is transparent to the users and packs files into containers (currently zip files) based on configurable policies. These containers are written directly into the same dCache, which then stores them on tape.  Both the small files and the containers file then benefit from dCache features, such as caching and load-balancing. No additional storage is necessary and the service itself scales by running multiple instances within the same dCache, sharing the load.\r\n\r\nWe describe the design, report on DESY's experience of running the service over the past six months, and detail the future plans for making it generally available.", "track": "Track3: Data store and access ", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/284", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:45:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:00:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "287", "speakers": [{"_type": "ContributionParticipation", "emailHash": "999374fec7057ef5475cac0d50d640f0", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. PETERS, Andreas Joachim", "id": "4"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "526ebd66173cf4a22d191ac12c5edab8", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "VAN DER STER, Dan", "id": "0"}], "title": "Ceph-based storage services for Run2 and beyond", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T00:28:48.641276+00:00", "description": "", "title": "Ceph_CHEP_2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/287\/attachments\/578831\/797022\/Ceph_CHEP_2015.pdf", "filename": "Ceph_CHEP_2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 797022, "size": 1971490}], "title": "Slides", "default_folder": false, "id": 578831, "description": ""}], "_type": "Contribution", "description": "In 2013, CERN IT evaluated then deployed a petabyte-scale Ceph cluster to support OpenStack use-cases in production. As of fall 2014, this cluster stores around 300 TB of data comprising more than a thousand VM images and a similar number of block device volumes. With more than a year of smooth operations, we will present our experience and tuning best-practices.\r\n\r\nBeyond the cloud storage use-cases, we have been exploring Ceph-based services to satisfy the growing storage requirements during and after Run2. First, we have developed a Ceph back-end for CASTOR, allowing this service to deploy thin disk server nodes which act as gateways to Ceph; this feature marries the strong data archival and cataloging features of CASTOR with the resilient and performant Ceph subsystem for disk. Second, we have developed RADOSFS, a lightweight storage API which builds a POSIX-like filesystem on top of the Ceph object layer. When combined with Xrootd, RADOSFS can offer a scalable object interface compatible with our HEP data processing applications. Lastly the same object layer is being used to build a scalable and inexpensive NFS service for several user communities.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578831", "resources": [{"_type": "LocalFile", "name": "Ceph_CHEP_2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/287\/attachments\/578831\/797022\/Ceph_CHEP_2015.pdf", "fileName": "Ceph_CHEP_2015.pdf", "_fossil": "localFileMetadata", "id": "797022", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "bc9744b244becb77d8e097418d081341", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MASCETTI, Luca", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "925eddbe32f7924a4ac5bbfb3d696f74", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ROUSSEAU, Herve", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "999374fec7057ef5475cac0d50d640f0", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. PETERS, Andreas Joachim", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "5931284fe6587485302a0da1f01ecbab", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "LAMANNA, Massimo", "id": "5"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/287", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "286", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ecef85c95b0dddfc15be379651cd7344", "affiliation": "GSI DARMSTADT", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LINEV, Sergey", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ecef85c95b0dddfc15be379651cd7344", "affiliation": "GSI DARMSTADT", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LINEV, Sergey", "id": "0"}], "title": "THttpServer class in ROOT", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-07T14:33:05.507568+00:00", "description": "", "title": "httpserver.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/286\/attachments\/578832\/797023\/httpserver.pdf", "filename": "httpserver.pdf", "content_type": "application\/pdf", "type": "file", "id": 797023, "size": 1109682}], "title": "Slides", "default_folder": false, "id": 578832, "description": ""}], "_type": "Contribution", "description": "New THttpServer class in ROOT implements http server for arbitrary ROOT-based application. It is based on embeddable Civetweb server and provides direct access to all registered for the server objects. THttpServer also supports FastCGI interface and therefore can be integrated with many standard web servers like Apache.\r\n\r\nMain advantage of http server usage in ROOT \u2013 one could access objects data from running applications without need to create temporary files. Together with standard binary representation objects can be converted into JSON (JavaScript Object Notation), using new TBufferJSON class. Server also provides access to data members and collections in registered objects. \r\n\r\nGeneric user interface for THttpServer implemented with HTML\/JavaScript and based on newest JSROOT development. With any modern web browser one could list, display and monitor objects available on http server. Different possibilities are provided to integrate dynamic graphical elements into other HTML pages.\r\n\r\nTHttpServer available in both 5-34 and 6-02 versions of ROOT. Online documentation and examples can be found on ROOT web site and on [http:\/\/web-docs.gsi.de\/~linev\/js\/][1]\r\n\r\n\r\n  [1]: http:\/\/web-docs.gsi.de\/~linev\/js\/", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578832", "resources": [{"_type": "LocalFile", "name": "httpserver.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/286\/attachments\/578832\/797023\/httpserver.pdf", "fileName": "httpserver.pdf", "_fossil": "localFileMetadata", "id": "797023", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/286", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:45:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "263", "speakers": [{"_type": "ContributionParticipation", "emailHash": "3ff2d337ba260b7e730554f80f69db7c", "affiliation": "University of Oslo (NO)", "_fossil": "contributionParticipationMetadata", "fullName": "NILSEN, Jon Kerr", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "3ff2d337ba260b7e730554f80f69db7c", "affiliation": "University of Oslo (NO)", "_fossil": "contributionParticipationMetadata", "fullName": "NILSEN, Jon Kerr", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "269913119a8e37ae971b93b8559c6092", "affiliation": "University of Oslo (NO)", "_fossil": "contributionParticipationMetadata", "fullName": "CAMERON, David", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "2bfc3dcae6061d54150b9904432bf7ee", "affiliation": "Jozef Stefan Institute (SI)", "_fossil": "contributionParticipationMetadata", "fullName": "FILIPCIC, Andrej", "id": "2"}], "title": "ARC Control Tower: A flexible generic distributed job management framework", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T09:39:16.956146+00:00", "description": "", "title": "ARC_Control_Tower_-_CHEP2015v5.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/263\/attachments\/578833\/797024\/ARC_Control_Tower_-_CHEP2015v5.pdf", "filename": "ARC_Control_Tower_-_CHEP2015v5.pdf", "content_type": "application\/pdf", "type": "file", "id": 797024, "size": 2956722}], "title": "Slides", "default_folder": false, "id": 578833, "description": ""}], "_type": "Contribution", "description": "While current grid middlewares are quite advanced in terms of connecting jobs to resources, their client tools are generally quite minimal and features for managing large sets of jobs are left to the user to implement. The ARC Control Tower (aCT) is a very flexible job management framework that can be run on anything from a single user\u2019s laptop to a multi-server distributed setup. aCT was originally designed to enable ATLAS jobs to be submitted to the ARC CE. However, with the recent redesign of aCT where the ATLAS specific elements are clearly separated from the ARC job management parts, the control tower can now easily be reused as a flexible generic distributed job manager for other communities. This paper will give a detailed explanation how aCT works as a job management framework and go through the steps needed to create a simple job manager using aCT and show that it can easily manage thousands of jobs.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578833", "resources": [{"_type": "LocalFile", "name": "ARC_Control_Tower_-_CHEP2015v5.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/263\/attachments\/578833\/797024\/ARC_Control_Tower_-_CHEP2015v5.pdf", "fileName": "ARC_Control_Tower_-_CHEP2015v5.pdf", "_fossil": "localFileMetadata", "id": "797024", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/263", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:00:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "262", "speakers": [{"_type": "ContributionParticipation", "emailHash": "b212876691c36e979802f2affedc273c", "affiliation": "J. Stefan Institute, Ljubljana, Slovenia", "_fossil": "contributionParticipationMetadata", "fullName": "STARIC, marko", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "b612c8383dc70d60c7c7cc3dffa3d55d", "affiliation": "Jozef Stefan Institute", "_fossil": "contributionParticipationMetadata", "fullName": "ZUPANC, Anze", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "b212876691c36e979802f2affedc273c", "affiliation": "J. Stefan Institute, Ljubljana, Slovenia", "_fossil": "contributionParticipationMetadata", "fullName": "STARIC, marko", "id": "1"}], "title": "Physics Analysis Software Framework for Belle II", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T03:24:40.454743+00:00", "description": "", "title": "staric_BelleIIana.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/262\/attachments\/578834\/797025\/staric_BelleIIana.pdf", "filename": "staric_BelleIIana.pdf", "content_type": "application\/pdf", "type": "file", "id": 797025, "size": 1144115}], "title": "Slides", "default_folder": false, "id": 578834, "description": ""}], "_type": "Contribution", "description": "We present software framework being developed for physics analyses using\r\nthe data collected by the Belle II experiment. The analysis workflow is\r\norganized in a modular way integrated within the Belle II software framework\r\n(BASF2). A set\r\nof physics analysis modules that perform simple and well defined tasks\r\nand are common to almost all physics analyses are provided. The physics\r\nmodules do not communicate with each other directly but only through the\r\ndata access protocols that are part of the BASF2. The physics modules are written in C++, Python or combination\r\nof both. Typically, a user performing a physics analysis only needs to\r\nprovide a job configuration file with analysis\u2019 specific sequence of\r\nphysics modules that can be then executed on the Grid.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578834", "resources": [{"_type": "LocalFile", "name": "staric_BelleIIana.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/262\/attachments\/578834\/797025\/staric_BelleIIana.pdf", "fileName": "staric_BelleIIana.pdf", "_fossil": "localFileMetadata", "id": "797025", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/262", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "261", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5152afe0ee224171ff18a8a6a0abb342", "affiliation": "National Research Centre Kurchatov Institute (RU), Moscow Institute for Physics and Technology (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "RYABINKIN, Eygene", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "5152afe0ee224171ff18a8a6a0abb342", "affiliation": "National Research Centre Kurchatov Institute (RU), Moscow Institute for Physics and Technology (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "RYABINKIN, Eygene", "id": "2"}], "title": "Tier-1 in Kurchatov Institute: status before Run-2 and HPC integration", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T13:08:24.431423+00:00", "description": "", "title": "rrc-ki-tier-1-chep-2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/261\/attachments\/578835\/797026\/rrc-ki-tier-1-chep-2015.pdf", "filename": "rrc-ki-tier-1-chep-2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 797026, "size": 962694}], "title": "Slides", "default_folder": false, "id": 578835, "description": ""}], "_type": "Contribution", "description": "We present the status of RRC-KI-T1, new Russian Tier-1 that supports ALICE, ATLAS and LHCb.  Our aim is to enter the full production mode just before the beginning of Run-2 and we will talk about our current setup, deployed services and middleware, workflow, achievements and problems on the route of bringing yet another Tier-1 for WLCG.\r\n\r\nAnother facet of our current activity is making the parts of the processing resources at our HPC facilities to be available for Grid on-demand usage.  This requires us to adopt our HPC environment to be able to process Grid jobs, to use the storage at our Tier-1 as the native storage element and to adopt site-level services for interoperability with HPC parts.  We also collaborate with Kurchatov Institute BigData laboratory that adopts PanDA workload management system for HPC environments in the context of ATLAS and ALICE workloads, the task that also requires major tweaks of HPC worker nodes to incorporate them into PanDA and WLCG environments.  We will present our hardware and software architecture that powers such integration and results of test for its real-world usage.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578835", "resources": [{"_type": "LocalFile", "name": "rrc-ki-tier-1-chep-2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/261\/attachments\/578835\/797026\/rrc-ki-tier-1-chep-2015.pdf", "fileName": "rrc-ki-tier-1-chep-2015.pdf", "_fossil": "localFileMetadata", "id": "797026", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "5a6b72abaf2d59578213bddab34923a9", "affiliation": "National Research Centre Kurchatov Institute (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "BEREZHNAYA, Alexandra", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "38e03b9d1a3550c40ec192816ef393b5", "affiliation": "National Research Centre Kurchatov Institute (RU), Moscow Institute for Physics and Technology (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "VELIKHOV, Vasily", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "3761127c9886deed4c88811f05eec0ea", "affiliation": "National Research Centre Kurchatov Institute (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "LAZIN, Yury", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "449e1fcad5dba78cefdf62cd93dcd691", "affiliation": "National Research Centre Kurchatov Institute (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "LYALIN, Ilya", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "d0b2967bf7a189ae700a3a4e8388de54", "affiliation": "National Research Centre Kurchatov Institute (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "TKACHENKO, Igor", "id": "5"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/261", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:00:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "260", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a77587f3ef2eb484c886fe5002a774a1", "affiliation": "Research Center for Nuclear Physics (RCNP), Osaka University", "_fossil": "contributionParticipationMetadata", "fullName": "TAKAHASHI, Tomonori", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "a77587f3ef2eb484c886fe5002a774a1", "affiliation": "Research Center for Nuclear Physics (RCNP), Osaka University", "_fossil": "contributionParticipationMetadata", "fullName": "TAKAHASHI, Tomonori", "id": "0"}], "title": "The Electronics, Online Trigger System and Data Acquisition System of the J-PARC E16 Experiment", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T08:43:31.383768+00:00", "description": "", "title": "tntakahashi-chep2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/260\/attachments\/578836\/797027\/tntakahashi-chep2015.pdf", "filename": "tntakahashi-chep2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 797027, "size": 6973036}], "title": "Slides", "default_folder": false, "id": 578836, "description": ""}], "_type": "Contribution", "description": "## 1. Introduction ##\r\nThe J-PARC E16 experiment aims to investigate the chiral symmetry restoration in cold nuclear matter and the origin of the hadron mass through the systematic study of the mass modification of vector mesons.\r\nIn the experiment, \r\n$e^{+}e^{-}$ decay of slowly-moving $\\phi$ mesons in the normal nuclear matter density are intensively studied using several nuclear targets (H, C, Cu and Pb).\r\nThe dependence of the modification on the nuclear size and the meson momentum will be  measured for the first time.\r\n## 2. Experiment ##\r\nThe experiment will be performed in 2016 at the high-momentum beam line of the J-PARC hadron experimental facility, \r\nwhere a 30-GeV proton beam with a high intensity of $1\\times10^{10}$ per pulse (2-second spill per 6-second cycle) is delivered to experimental targets. \r\nSince the material budget around the targets is sensitive to the $e^{+}e^{-}$ measurement, \r\nthin detector systems are under construction.\r\nThe targets are surrounded by GEM Trackers (GTR) with three tracking planes to achieve the good resolution of 100 $\\mu$m in the high rate environment of 5 kHz\/mm$^{2}$.\r\nThe electrons (positrons) are identified by two types of counters. \r\nOne is the Hadron Blind Detector (HBD), which is a threshold type gas Cherenkov detector using GEM, \r\nand the other is the Lead-glass EM calorimeter (LG).\r\n## 3. Trigger electronics ##\r\nThe first level trigger is decided by the three fold coincidence of $\\sim$620-ch from the GTR, $\\sim$940-ch from the HBD and $\\sim$1000-ch from the LG. \r\nCathode foils which face to the read out strips of the most outside GTR and pads of the HBD are divided into trigger segments. \r\nA pulse fired on the GEM cathode foils are fed into an amplifier-shaper-discriminator (ASD) ASIC, which has been developed by our group in cooperation with \r\n[Open-It](http:\/\/openit.kek.jp\/)[1].  \r\nThe LG signals are discriminated by a commercial fast comparator.\r\nIn order to gather the trigger primitives, which are sent from the GTR, HBD and LG in parallel LVDS signals,\r\na trigger merger board (TRG-MRG) has been developed. \r\nThe TRG-MRG produces time stamps of the trigger primitives with a resolution of less than 4 nsec  by using a Xilinx Kintex-7 FPGA. \r\nThe time stamps are serialized by the FPGA and transmitted to a global trigger decision module via optical fibers at each link rate of 5 Gbps or more. \r\nThe global trigger module utilizes a Belle-II Universal Trigger Board 3.\r\nThe first level trigger as well as a global clock of $\\sim$125 MHz is distributed by Belle-II FTSW boards via Category-7 LAN cables to the front-end-modules described bellow. \r\n## 4. Readout electronics ##\r\nThe numbers of readout channels amount to $\\sim$56k, $\\sim$36k and $\\sim$1k for the GTR, HBD and LG, respectively.\r\nIn the current design, waveforms from all of the readout channels will be recorded by using analog memory ASICs to obtain timing and charge deposit information and to distinguish pulse pile-up in the high rate environment for the offline analysis. \r\nThe waveform from the GTR and HBD are stored with a 25 nsec cycle in APV25s1[2] chips and then transferred to the Scalable Readout System, which has been developed by the CERN RD51 Collaboration[3] (an R&D collaboration for MGPDs). \r\nThe LGs are read out by custom made boards, which employ DRS4[4] chips to record the pulses at 1 GHz. \r\nThose modules digitize the waveforms and perform the zero suppression at online. \r\nThe data are collected by the [DAQ-Middleware](http:\/\/daqmw.kek.jp)[5] using gigabit Ethernet and 10G Ethernet links.\r\nThe expected data rate is 660 MB\/spill with the event rate of 2k\/spill after zero suppression.\r\n## 5. Summary ##\r\nThis is an overview talk on the electronics and trigger system\r\nfor the J-PARC E16 experiment.\r\nOther contributions for the detail of the DAQ software, trigger ASIC,\r\nand so on are also prepared and submitted by coauthors.\r\n\r\n## References ##\r\n[1] [http:\/\/openit.kek.jp\/](http:\/\/openit.kek.jp\/) (in Japanese)\r\n\r\n[2] [M. Raymond _et al_., IEEE NSS Conf. Rec. 2 (2000) 9\/113.](http:\/\/ieeexplore.ieee.org\/xpl\/articleDetails.jsp?arnumber=949881)\r\n\r\n[3] [http:\/\/rd51-public.web.cern.ch\/RD51-Public\/](http:\/\/rd51-public.web.cern.ch\/RD51-Public\/)\r\n\r\n[4] [http:\/\/www.psi.ch\/drs\/](http:\/\/www.psi.ch\/drs\/)\r\n\r\n[5] [http:\/\/daqmw.kek.jp\/](http:\/\/daqmw.kek.jp\/) (in Japanese)", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578836", "resources": [{"_type": "LocalFile", "name": "tntakahashi-chep2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/260\/attachments\/578836\/797027\/tntakahashi-chep2015.pdf", "fileName": "tntakahashi-chep2015.pdf", "_fossil": "localFileMetadata", "id": "797027", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "bce47fa0aea8f065a048be8ff811db8f", "affiliation": "High Energy Accelerator Research Organization (KEK)", "_fossil": "contributionParticipationMetadata", "fullName": "HAMADA, Eitaro", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "80b639329ca995258f43de70a074a64a", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "IKENO, Masahiro", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "1606e50bea6b72d12b7ee332cb84d0e0", "affiliation": "RIKEN, Nishina Center", "_fossil": "contributionParticipationMetadata", "fullName": "KAWAMA, Daisuke", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "ebb136639dc1b94ba02b90fc5db93499", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "MORINO, Yuhei", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "17488d1ac2de0b46336dbfeca29abe85", "affiliation": "Department of Physics, University of Tokyo \/ RIKEN, Nishina Center", "_fossil": "contributionParticipationMetadata", "fullName": "NAKAI, Wataru", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "cc7c3f3a894576c3678e52e4632de850", "affiliation": "Department of Physics, University of Tokyo", "_fossil": "contributionParticipationMetadata", "fullName": "OBARA, Yuki", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "9fef0eb8eef4e2453f1c3f7ab2638a6d", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "OZAWA, Kyoichiro", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "10c6782b9f4aa418aff83d40e88c2d30", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "SENDAI, Hiroshi", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "460c2e7145b6be90b93a59055bd272d4", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "TANAKA, M. M.", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "dcc164fbdea33315cfc22e49cb7a8b6f", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "UCHIDA, Tomohisa", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "14fe8cb28e5f268fe0430955a2bb7980", "affiliation": "RIKEN, Nishina Center", "_fossil": "contributionParticipationMetadata", "fullName": "YOKKAICHI, Satoshi", "id": "11"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/260", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:00:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "267", "speakers": [{"_type": "ContributionParticipation", "emailHash": "b4169d214f99022fe255ea33a34c4cda", "affiliation": "Universitaet Zuerich (CH)", "_fossil": "contributionParticipationMetadata", "fullName": "STORACI, Barbara", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "b4169d214f99022fe255ea33a34c4cda", "affiliation": "Universitaet Zuerich (CH)", "_fossil": "contributionParticipationMetadata", "fullName": "STORACI, Barbara", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "e37007829748689849399b63d38d181d", "affiliation": "Ruprecht-Karls-Universitaet Heidelberg (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "DE CIAN, Michel", "id": "0"}], "title": "Optimization of the LHCb track reconstruction", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-07T18:56:06.571113+00:00", "description": "", "title": "Storaci_CHEP.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/267\/attachments\/578837\/797028\/Storaci_CHEP.pdf", "filename": "Storaci_CHEP.pdf", "content_type": "application\/pdf", "type": "file", "id": 797028, "size": 1825734}], "title": "Slides", "default_folder": false, "id": 578837, "description": ""}], "_type": "Contribution", "description": "The LHCb track reconstruction uses sophisticated pattern recognition algorithms to reconstruct trajectories of charged particles. Their main feature is the use of a Hough-transform like approach to connect track segments from different subdetectors, allowing for having no tracking stations in the magnet of LHCb. While yielding a high efficiency, the track reconstruction is a major contributor to the overall timing budget of the software trigger of LHCb, and will continue to be so in the light of the higher track multiplicity expected from Run II of the LHC.\r\nIn view of this fact, key parts of the pattern recognition have been revised and redesigned. We will present the main features which were studied. A staged approach strategy for the track reconstruction in the software trigger was investigated: it allows unifying complementary sets of tracks coming from the different stages of the high level trigger, resulting in a more flexible trigger strategy and a better overlap between online and offline reconstructed tracks. Furthermore the use of parallelism was investigated, using SIMD instructions for time-critical parts of the software or - in a later stage - using GPU-driven track reconstruction.\r\nIn addition a new approach to monitoring was implemented, where quantities important for track reconstruction are monitored on a regular basis, using an automated framework for comparing different figures of merit.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578837", "resources": [{"_type": "LocalFile", "name": "Storaci_CHEP.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/267\/attachments\/578837\/797028\/Storaci_CHEP.pdf", "fileName": "Storaci_CHEP.pdf", "_fossil": "localFileMetadata", "id": "797028", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/267", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "266", "speakers": [{"_type": "ContributionParticipation", "emailHash": "cc7c3f3a894576c3678e52e4632de850", "affiliation": "University of Tokyo", "_fossil": "contributionParticipationMetadata", "fullName": "OBARA, Yuki", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "cc7c3f3a894576c3678e52e4632de850", "affiliation": "University of Tokyo", "_fossil": "contributionParticipationMetadata", "fullName": "OBARA, Yuki", "id": "0"}], "title": "Development of GEM trigger electronics for the J-PARC E16 experiment", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T00:39:48.258163+00:00", "description": "", "title": "CHEP2015_Obara2.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/266\/attachments\/578838\/797029\/CHEP2015_Obara2.pdf", "filename": "CHEP2015_Obara2.pdf", "content_type": "application\/pdf", "type": "file", "id": 797029, "size": 9528873}], "title": "Poster", "default_folder": false, "id": 578838, "description": ""}], "_type": "Contribution", "description": "The purpose of the J-PARC E16 experiment is to investigate the origin of\r\nhadron mass through the chiral symmetry restoration in nuclear matter.\r\nIn the experiment, we measure mass spectra of vector mesons in nuclei\r\nin the $e^{+}e^{-}$ decay channel \r\nwith high precision and high statistics.\r\n\r\n We use a 30 GeV proton beam with high intensity of $10^{10}$ per spill \r\nto achieve high statistics\r\nand targets of 0.1% interaction length to suppress an electron \r\nbackground caused by $\\gamma$ - conversion in the targets.\r\nIn the spectrometer of the experiment, GEM Trackers (GTR), which are\r\ncomposed of three layers of tracking planes in a magnetic field, are used to \r\nmeasure momenta of the decay electrons in the high rate environment.\r\nHadron Blind Detectors (HBD), which are gas\r\nCerenkov counters using GEM, and Lead Glass Calorimeters (LG) are\r\nplaced outside the GTR to identify electrons.\r\nThe $e^{+}e^{-}$ event trigger consists of three-fold coincidence of signals \r\nfrom the most outside GTR, HBD and LG.\r\nThe number of channels for GTR, HBD and LG is about 620, 940, and 1000,\r\nrespectively.\r\nEfficient trigger system selecting events of $e^{+}e^{-}$ decays from\r\nhuge background events must be constructed in order to reduce the trigger rate\r\nto 1-2 kHz that our DAQ can cope with.\r\n\r\n We developed new Amplifier-Shaper-Discriminator (ASD) ASICs\r\nwhich can deal with large detector capacitance originating from\r\nGEM and have a short shaping time to handle the high rate counting.\r\nThe naive idea to extract the trigger signal from these GEM detectors,\r\nsuch as GTR and HBD, is utilizing signals of strips or pads on \r\nthe anode readout plane which are used for tracking or electron\r\nidentification.\r\nThis idea requires, however, R&D of complex frontend circuits and \r\na large number of channels for the fast signal outputs.\r\nIn order to avoid these problems, the trigger signals are\r\npicked up from the cathode plane of the induction gap of these GEM\r\nchambers, namely, the last GEM foil in the stack.\r\nConsidering to use the signals from the GEM foil, it is difficult to\r\ncope with large capacitance of the order of nF by using normal preamps.\r\n\r\n Requirements of the trigger electronics for GTR are to cope with the large\r\ndetector capacitance of about 2 nF, a fast shaping time and a good\r\nsignal-to-noise ratio for the minimum input charge of 10 fC.\r\nThe cathode plane of a GEM foil of the most outside GTR, whose size is\r\n$300 \\times 300 \\ \\mathrm{mm}^2$, has detector capacitance of about 50\r\nnF.\r\nThe GEM foil is divided into 24 segments in order to reduce the\r\ncapacitance to about 2 nF and a counting rate in each segment, and to\r\nroughly track charged particles.\r\nThe rough tracking has an important role of decreasing a background of\r\nelectrons which do not come from targets.  \r\nIn the forward region of the spectrometer, the maximum hit rate of each\r\nsegment is expected to be 1-2 MHz.\r\nThus, we set the shaping time to 25 ns corresponding to a pulse width of\r\nabout 200 ns.\r\nThe electric circuit of the ASIC was designed to suppress Equivalent\r\nNoise Charge (ENC) under\r\n$2\\times 10^4$ for the input detector capacitance of 2 nF.\r\nThe ASD ASIC chip satisfying the above requirements has been\r\ndeveloped by our group in cooperation with Open-It[1].\r\nA modified version of the ASIC for GTR is used as the electronics of \r\nthe GEM trigger for HBD.\r\n\r\n A prototype of a preamp board with the ASIC chips was produced.\r\nSize of the board should be enough small to be installed in narrow\r\nspaces of GTR modules.\r\nOne of functions of the preamp board is to convert discriminated digital\r\noutputs of the ASIC to parallel LVDS signals which are sent to a trigger merger\r\nboard (TRG-MRG).\r\nAlso, the board needs to receive slow control signals from the TRG-MRG to\r\nactivate and control digital functions of the ASIC.\r\n\r\n This contribution will report the development status of the GEM trigger \r\nsystem and result of performance test of the ASIC chip.\r\n\r\nReference: 1. http:\/\/openit.kek.jp\/", "track": "Track1: Online computing ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578838", "resources": [{"_type": "LocalFile", "name": "CHEP2015_Obara2.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/266\/attachments\/578838\/797029\/CHEP2015_Obara2.pdf", "fileName": "CHEP2015_Obara2.pdf", "_fossil": "localFileMetadata", "id": "797029", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "46a2e15bbb40b8331bf1528b0975db45", "affiliation": "High Energy Accelerator Research Organization (KEK)", "_fossil": "contributionParticipationMetadata", "fullName": "IKENO, Masahiro", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "dcc164fbdea33315cfc22e49cb7a8b6f", "affiliation": "High Energy Accelerator Research Organization (KEK)", "_fossil": "contributionParticipationMetadata", "fullName": "UCHIDA, Tomohisa", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "9fef0eb8eef4e2453f1c3f7ab2638a6d", "affiliation": "High Energy Accelerator Research Organization (KEK)", "_fossil": "contributionParticipationMetadata", "fullName": "OZAWA, Kyoichiro", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "1606e50bea6b72d12b7ee332cb84d0e0", "affiliation": "RIKEN Nishina Center", "_fossil": "contributionParticipationMetadata", "fullName": "KAWAMA, Daisuke", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "10c6782b9f4aa418aff83d40e88c2d30", "affiliation": "High Energy Accelerator Research Organization (KEK)", "_fossil": "contributionParticipationMetadata", "fullName": "SENDAI, Hiroshi", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "ad7cd8169726e930e5ac6d6ec11409ce", "affiliation": "Research Center for Nuclear Physics (RCNP)", "_fossil": "contributionParticipationMetadata", "fullName": "TAKAHASHI, Tomonori", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "460c2e7145b6be90b93a59055bd272d4", "affiliation": "High Energy Accelerator Research Organization (KEK)", "_fossil": "contributionParticipationMetadata", "fullName": "TANAKA, M. M.", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "17488d1ac2de0b46336dbfeca29abe85", "affiliation": "University of Tokyo, RIKEN Nishina Center", "_fossil": "contributionParticipationMetadata", "fullName": "NAKAI, Wataru", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "bce47fa0aea8f065a048be8ff811db8f", "affiliation": "High Energy Accelerator Research Organization (KEK)", "_fossil": "contributionParticipationMetadata", "fullName": "HAMADA, Eitaro", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "ebb136639dc1b94ba02b90fc5db93499", "affiliation": "High Energy Accelerator Research Organization (KEK)", "_fossil": "contributionParticipationMetadata", "fullName": "MORINO, Yuhei", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "14fe8cb28e5f268fe0430955a2bb7980", "affiliation": "RIKEN Nishina Center", "_fossil": "contributionParticipationMetadata", "fullName": "YOKKAICHI, Satoshi", "id": "11"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/266", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "265", "speakers": [{"_type": "ContributionParticipation", "emailHash": "7a6c8f49981d093b41b365a4b522378d", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. EULISSE, Giulio", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "7a6c8f49981d093b41b365a4b522378d", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. EULISSE, Giulio", "id": "0"}], "title": "Optimizing CMS build infrastructure via Apache Mesos", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:08:00.781469+00:00", "description": "", "title": "poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/265\/attachments\/578839\/797030\/poster.pdf", "filename": "poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 797030, "size": 1225062}], "title": "Slides", "default_folder": false, "id": 578839, "description": ""}], "_type": "Contribution", "description": "The Offline Software of the CMS Experiment at the Large Hadron Collider (LHC) at CERN consists of 6M lines of in-house code, developed over a decade by nearly 1000 physicists, as well as a comparable amount of general use open-source code. A critical ingredient to the success of the construction and early operation of the WLCG was the convergence, around the year 2000, on the use of a homogeneous environment of commodity x86-64 processors and Linux.\r\n\r\nApache Mesos is a cluster manager that provides efficient resource isolation and sharing across distributed applications, or frameworks. It can run Hadoop, Jenkins, Spark, Aurora, and other applications on a dynamically shared pool of nodes.\r\n\r\nWe present how we migrated our continuos integration system to schedule jobs on an relatively small Apache Mesos enabled cluster and how this resulted in better resource usage, higher peak performance and lower latency thanks to the dynamic scheduling capabilities of Mesos.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578839", "resources": [{"_type": "LocalFile", "name": "poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/265\/attachments\/578839\/797030\/poster.pdf", "fileName": "poster.pdf", "_fossil": "localFileMetadata", "id": "797030", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "9a8ce9aebaabdde7956acf3f73f3b7a5", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ELMER, Peter", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "4e0d9311bdd9ddd970e7cf2f86e61010", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MUZAFFAR, Shahzad Malik", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "5ed09727f43058232b24b8ad13f529a8", "affiliation": "Vilnius University (LT)", "_fossil": "contributionParticipationMetadata", "fullName": "ABDURACHMANOV, David", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "e5f00aa7a85f6e7fcba1b687c488430c", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "DEGANO, Alessandro", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "f08b489b181d542f07a9c64be879b2b6", "affiliation": "Universidad de los Andes (CO)", "_fossil": "contributionParticipationMetadata", "fullName": "MENDEZ LOPEZ, David Gonzalo", "id": "5"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/265", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:30:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "264", "speakers": [{"_type": "ContributionParticipation", "emailHash": "12f22af47940e879cc2380585a26078b", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "FISK, Ian", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "059c67f423c94e588bce78fb5a617463", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GIRONE, Maria", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "12f22af47940e879cc2380585a26078b", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "FISK, Ian", "id": "1"}], "title": "Improvements in the CMS Computing System for Run2", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T03:49:55.701648+00:00", "description": "", "title": "Computing_Evolution_CHEP.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/264\/attachments\/578840\/797031\/Computing_Evolution_CHEP.pdf", "filename": "Computing_Evolution_CHEP.pdf", "content_type": "application\/pdf", "type": "file", "id": 797031, "size": 841865}, {"_type": "attachment", "modified_dt": "2015-04-13T03:49:55.701648+00:00", "description": "", "title": "Computing_Evolution_CHEP.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/264\/attachments\/578840\/797032\/Computing_Evolution_CHEP.pptx", "filename": "Computing_Evolution_CHEP.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797032, "size": 352618}], "title": "Slides", "default_folder": false, "id": 578840, "description": ""}], "_type": "Contribution", "description": "Beginning in 2015 CMS will collected and produce data and simulation adding to 10B new events a year. In order to realize the physics potential of the experiment these events need to be stored, processed, and delivered to analysis users on a global scale. CMS has 150k processor cores and 80PB of disk storage and there is constant pressure to reduce the resources needed and increase the efficiency of usage. In this presentation we will comprehensively overview the improvements made in the computing system for Run2 by CMS in the areas of data and simulation processing, data distribution, data management and data access.   The system has been examined and we will discuss the improvements in the entire data and workflow systems: CMS processing processing and analysis workflow tools, the development and deployment of dynamic data placement infrastructure, and progress toward operating a global data federation. We will describe the concepts and approaches to utilize the variety of CMS CPU resources, ranging from established Grid sites to HPC centers, Cloud resources and CMS' own High Level Trigger farm.    We will explain the strategy for improving how effectively the storage is used and the commissioning, validation and challenge activities will be presented.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578840", "resources": [{"_type": "LocalFile", "name": "Computing_Evolution_CHEP.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/264\/attachments\/578840\/797031\/Computing_Evolution_CHEP.pdf", "fileName": "Computing_Evolution_CHEP.pdf", "_fossil": "localFileMetadata", "id": "797031", "_deprecated": true}, {"_type": "LocalFile", "name": "Computing_Evolution_CHEP.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/264\/attachments\/578840\/797032\/Computing_Evolution_CHEP.pptx", "fileName": "Computing_Evolution_CHEP.pptx", "_fossil": "localFileMetadata", "id": "797032", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/264", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:45:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "269", "speakers": [{"_type": "ContributionParticipation", "emailHash": "9e7dd8f84b1524d686815744ec524fea", "affiliation": "University of Manchester (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "MCNAB, Andrew", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "9e7dd8f84b1524d686815744ec524fea", "affiliation": "University of Manchester (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "MCNAB, Andrew", "id": "0"}], "title": "LHCb experience with running jobs in virtual machines", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:25:25.097958+00:00", "description": "", "title": "mcnab-lhcb-vm-14apr15.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/269\/attachments\/578841\/797033\/mcnab-lhcb-vm-14apr15.pdf", "filename": "mcnab-lhcb-vm-14apr15.pdf", "content_type": "application\/pdf", "type": "file", "id": 797033, "size": 686591}], "title": "Slides", "default_folder": false, "id": 578841, "description": ""}], "_type": "Contribution", "description": "The LHCb experiment has been running production jobs in virtual machines since 2013 as part of its DIRAC-based infrastructure. We describe the architecture of these virtual machines and the steps taken to replicate the WLCG worker node environment expected by user and production jobs. This relies on the CernVM 3 system for providing root images for virtual machines. We use the cvmfs distributed filesystem to supply the root partition files, the LHCb software stack, and the bootstrapping scripts necessary to configure the virtual machines for us. Using this approach, we have been able to minimise the amount of contextualisation which must be provided by the virtual machine managers. We explain the process by which the virtual machine is able to receive payload jobs submitted to DIRAC by users and production managers, and how this differs from payloads executed within conventional DIRAC pilot jobs on batch queue based sites. We compare our operational experiences of running production on VM based sites managed using OpenStack, Vac, BOINC, and Condor. Finally we describe our requirements for monitoring which are specific to the additional responsibilities for experiments when operating virtual machines which were previously undertaken by the system managers of worker nodes, and how this is facilitated by the new DIRAC Pilot 2.0 architecture.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578841", "resources": [{"_type": "LocalFile", "name": "mcnab-lhcb-vm-14apr15.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/269\/attachments\/578841\/797033\/mcnab-lhcb-vm-14apr15.pdf", "fileName": "mcnab-lhcb-vm-14apr15.pdf", "_fossil": "localFileMetadata", "id": "797033", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "5ae4855aa9d3c6685bd9f450814f9af6", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "STAGNI, Federico", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "2186fba70f2f17dd5000102c658bbb8a", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "LUZZI, Cinzia", "id": "2"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/269", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:00:00"}, "duration": 15, "session": "Track 8 Session", "keywords": [], "id": "268", "speakers": [{"_type": "ContributionParticipation", "emailHash": "4b2dee02cc9c077ec5c48080350a3b29", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CAMPORA PEREZ, Daniel Hugo", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "4b2dee02cc9c077ec5c48080350a3b29", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CAMPORA PEREZ, Daniel Hugo", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "244c8becc3b603c6e5722f14640d8e65", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "COUTURIER, Ben", "id": "1"}], "title": "SIMD studies in the LHCb reconstruction software", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T01:17:26.328208+00:00", "description": "", "title": "SIMD_studies_in_the_LHCb_reconstruction_software.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/268\/attachments\/578842\/797034\/SIMD_studies_in_the_LHCb_reconstruction_software.pdf", "filename": "SIMD_studies_in_the_LHCb_reconstruction_software.pdf", "content_type": "application\/pdf", "type": "file", "id": 797034, "size": 1367033}, {"_type": "attachment", "modified_dt": "2015-04-14T01:17:26.328208+00:00", "description": "", "title": "SIMD_studies_in_the_LHCb_reconstruction_software.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/268\/attachments\/578842\/797035\/SIMD_studies_in_the_LHCb_reconstruction_software.pptx", "filename": "SIMD_studies_in_the_LHCb_reconstruction_software.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797035, "size": 1893060}], "title": "Slides", "default_folder": false, "id": 578842, "description": ""}], "_type": "Contribution", "description": "During the data taking process in the LHC at CERN, millions of collisions are recorded every second by the LHCb Detector. The LHCb \"Online\" computing farm, counting around 15000 cores, is dedicated to the recontruction of the events in real-time, in order to filter those with interesting Physics. The ones kept are later analysed \"Offline\" in a more precise fashion on the Grid. This imposes very stringent requirements on the Reconstruction Software, which has to be as efficient as possible.\r\n\r\nModern CPUs support so-called \"vector-extensions\", which extend their Instruction Sets, allowing for concurrent execution across functional units. Several libraries expose the Single Instruction Multiple Data programming paradigm to issue these instructions. The use of vectorisation in our codebase can provide performance boosts, leading ultimately to Physics reconstruction enhancements.\r\n\r\nIn this paper, we present vectorisation studies of significant reconstruction algorithms. A variety of vectorisation libraries are analysed and compared in terms of design, maintainability and performance. We also present the steps taken to systematically measure the performance of the released software, to ensure the consistency of the run-time of the vectorized software.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578842", "resources": [{"_type": "LocalFile", "name": "SIMD_studies_in_the_LHCb_reconstruction_software.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/268\/attachments\/578842\/797034\/SIMD_studies_in_the_LHCb_reconstruction_software.pdf", "fileName": "SIMD_studies_in_the_LHCb_reconstruction_software.pdf", "_fossil": "localFileMetadata", "id": "797034", "_deprecated": true}, {"_type": "LocalFile", "name": "SIMD_studies_in_the_LHCb_reconstruction_software.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/268\/attachments\/578842\/797035\/SIMD_studies_in_the_LHCb_reconstruction_software.pptx", "fileName": "SIMD_studies_in_the_LHCb_reconstruction_software.pptx", "_fossil": "localFileMetadata", "id": "797035", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/268", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:00:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "59", "speakers": [{"_type": "ContributionParticipation", "emailHash": "96cb4a6a9a1a85aa56b1d32a3426a372", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CANO, Eric", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "40e0b4b7026ba0265906491d2541a393", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CANCIO MELIA, German", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "bfe6e11293d9673643962538bbac55bc", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BAHYL, Vlado", "id": "1"}], "title": "Experiences and challenges running CERN's high-capacity tape archive", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T13:53:08.935713+00:00", "description": "", "title": "CHEP-2015-CERN-Tape-Archive.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/59\/attachments\/578843\/797036\/CHEP-2015-CERN-Tape-Archive.pdf", "filename": "CHEP-2015-CERN-Tape-Archive.pdf", "content_type": "application\/pdf", "type": "file", "id": 797036, "size": 1796796}, {"_type": "attachment", "modified_dt": "2015-04-09T13:53:08.935713+00:00", "description": "", "title": "CHEP-2015-CERN-Tape-Archive.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/59\/attachments\/578843\/797037\/CHEP-2015-CERN-Tape-Archive.pptx", "filename": "CHEP-2015-CERN-Tape-Archive.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797037, "size": 9104917}], "title": "Slides", "default_folder": false, "id": 578843, "description": ""}], "_type": "Contribution", "description": "CERN\u2019s tape-based archive system has collected over 70 Petabytes of data during the first run of the LHC. The Long Shutdown is being used for migrating the complete 100 Petabytes data archive to higher-density tape media. During LHC Run 2, the archive will have to cope with yearly growth rates of  up to 40-50 Petabytes. In this contribution, we will describe the scalable architecture for coping with the storage and long-term archival of such massive data amounts, as well as the procedures and tools developed for the proactive and efficient operation of the tape infrastructure. This will include also the features developed for automated problem detection, identification and notification. We will also review the challenges resulting and mechanisms devised for measuring and enhancing availability and reliability, as well as ensuring the long-term integrity and bit-level preservation of the complete data repository. Finally, we will present an outlook in terms of the future performance and capacity requirements growth and how they match the expected tape technology evolution.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578843", "resources": [{"_type": "LocalFile", "name": "CHEP-2015-CERN-Tape-Archive.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/59\/attachments\/578843\/797036\/CHEP-2015-CERN-Tape-Archive.pdf", "fileName": "CHEP-2015-CERN-Tape-Archive.pdf", "_fossil": "localFileMetadata", "id": "797036", "_deprecated": true}, {"_type": "LocalFile", "name": "CHEP-2015-CERN-Tape-Archive.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/59\/attachments\/578843\/797037\/CHEP-2015-CERN-Tape-Archive.pptx", "fileName": "CHEP-2015-CERN-Tape-Archive.pptx", "_fossil": "localFileMetadata", "id": "797037", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "96cb4a6a9a1a85aa56b1d32a3426a372", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CANO, Eric", "id": "2"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/59", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:45:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "58", "speakers": [{"_type": "ContributionParticipation", "emailHash": "14823d25b1099daf48e1923cddbe3811", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DARLEA, Georgiana Lavinia", "id": "5"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "148e6617a9a734113e8ef6b1f86cb767", "affiliation": "University of Wisconsin (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BAWEJ, Tomasz Adrian", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "34b7170966da6ae240c3a7c096c08295", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "BEHRENS, Ulf", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "3a7e6bfcd4dbb638710fce76e52d2342", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BRANSON, James Gordon", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "d33e3ae247733829a0d0c9511973c44e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHAZE, Olivier", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "779d4951aab39403b13ffbc89e9f0c90", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CITTOLIN, Sergio", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "14823d25b1099daf48e1923cddbe3811", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DARLEA, Georgiana Lavinia", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "c5db4520580bcc09fe8df060c043fc7b", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DELDICQUE, Christian", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "ddaeb584ae450aae5064a948e08ead3c", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. DOBSON, Marc", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "8b7c922625aaf91ee706a1518921a755", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DUPONT, Aymeric Arnaud", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "b03db255046e47e29d52f585a312fecf", "affiliation": "Univ. of California Los Angeles (US)", "_fossil": "contributionParticipationMetadata", "fullName": "ERHAN, Samim", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "b4aa2021387f14d802bddeb2eb454884", "affiliation": "University of Kent (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "FORREST, Andrew Kevin", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "4694ee2e44416e0163f30d20408ccad0", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GIGI, Dominique", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "6fb6d2d7f55ab188239ef3702f47965a", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GLEGE, Frank", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "37e8bb7f1971a51948ad4fce29334bf7", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GOMEZ CEBALLOS RETUERTO, Guillelmo", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "0686dcacfe13161c58c9e8a9fd9d65d2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HEGEMAN, Jeroen", "id": "14"}, {"_type": "ContributionParticipation", "emailHash": "516193e369370693f2991ccec37b4dfc", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "HOLZNER, Andre Georg", "id": "15"}, {"_type": "ContributionParticipation", "emailHash": "932403f3327b0e3b2903995e513382a2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MASETTI, Lorenzo", "id": "16"}, {"_type": "ContributionParticipation", "emailHash": "160a9938f201efdf2e473626b9d9ddda", "affiliation": "C", "_fossil": "contributionParticipationMetadata", "fullName": "MEIJERS, Frans", "id": "17"}, {"_type": "ContributionParticipation", "emailHash": "2d10c567422b53eb7fccd0134c728788", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MESCHI, Emilio", "id": "18"}, {"_type": "ContributionParticipation", "emailHash": "060d7d9521c75cc21082d88edbe36aae", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MOMMSEN, Remi", "id": "19"}, {"_type": "ContributionParticipation", "emailHash": "7e2c5e676f7ba0f9af6e617d71de2a44", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "NUNEZ BARRANCO FERNANDEZ, Carlos", "id": "20"}, {"_type": "ContributionParticipation", "emailHash": "b22d1baa246adbc92c9a6050219aeea8", "affiliation": "Fermi National Accelerator Laboratory (FNAL)", "_fossil": "contributionParticipationMetadata", "fullName": "O'DELL, Vivian", "id": "21"}, {"_type": "ContributionParticipation", "emailHash": "7cdd48f784a087c6d9b6f6003cfc03f7", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ORSINI, Luciano", "id": "22"}, {"_type": "ContributionParticipation", "emailHash": "c20242a86672d5fc162e300100556857", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "PAUS, Christoph", "id": "23"}, {"_type": "ContributionParticipation", "emailHash": "1951a3ffe6d5cf1393f669f572be4751", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "PETRUCCI, Andrea", "id": "24"}, {"_type": "ContributionParticipation", "emailHash": "88f4d725c22d118782d55d91895473a9", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "PIERI, Marco", "id": "25"}, {"_type": "ContributionParticipation", "emailHash": "dea4bd4e0305108f26ad10d80d8a923c", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. RACZ, Attila", "id": "26"}, {"_type": "ContributionParticipation", "emailHash": "e61604d9f35ae72b3531378bca0e6216", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SAKULIN, Hannes", "id": "27"}, {"_type": "ContributionParticipation", "emailHash": "01ca3cdd821be71f01a7fa78f1cd3e2c", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SCHWICK, Christoph", "id": "28"}, {"_type": "ContributionParticipation", "emailHash": "f458e4333696558cf9533ea62f29fc06", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "STIEGER, Benjamin", "id": "29"}, {"_type": "ContributionParticipation", "emailHash": "7b150004ac09fb1883e427cdc460cedc", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "SUMOROK, Konstanty", "id": "30"}, {"_type": "ContributionParticipation", "emailHash": "0b3372b1a406c07547c0658d54373e3a", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. VEVERKA, Jan", "id": "31"}, {"_type": "ContributionParticipation", "emailHash": "ab329ca1ce2b90cbb96f4c581619f0d7", "affiliation": "Staffordshire University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "WAKEFIELD, Christopher Colin", "id": "32"}, {"_type": "ContributionParticipation", "emailHash": "842f03deb43d34961e29b5c1ee95392e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ZEJDL, Petr", "id": "33"}, {"_type": "ContributionParticipation", "emailHash": "d3be8d0289e184c2c754180e84f1638b", "affiliation": "Aristotle Univ. of Thessaloniki (GR)", "_fossil": "contributionParticipationMetadata", "fullName": "ANDRONIDIS, Anastasios", "id": "34"}, {"_type": "ContributionParticipation", "emailHash": "321d291a26425f7ba000c1227f961f93", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "ANDRE, Jean-Marc Olivier", "id": "35"}, {"_type": "ContributionParticipation", "emailHash": "5ae3a32d3ffcbc90d737c183bebe4201", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MOROVIC, Srecko", "id": "36"}], "title": "Online data handling and storage at the CMS experiment", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T03:21:03.990808+00:00", "description": "", "title": "CHEP2015_slides_SM.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/58\/attachments\/578844\/797038\/CHEP2015_slides_SM.pdf", "filename": "CHEP2015_slides_SM.pdf", "content_type": "application\/pdf", "type": "file", "id": 797038, "size": 2590165}], "title": "Slides", "default_folder": false, "id": 578844, "description": ""}], "_type": "Contribution", "description": "The CMS experiment at CERN is one of the two general-purpose detectors on the Large Hadron Collider (LHC) in the Geneva area, Switzerland. Its infrastructure has undergone massive upgrades during 2013 and 2014, which lead to major changes in the philosophy of its DAQ (Data AcQuisition) system. One of the major components of this system is the Storage Manager, which is responsible for buffering the online data generated at the level of the readout units (RUs), which crosses the computing farm undergoing different processing and filtering stages all the way to the builder units (BUs). The Storage Manager at CMS is made up of three components: the distributed file system, the Merger service and the Transfer System. In the production DAQ system there will be around 50 BUs which will concurrently write their output data at an expected aggregated rate of 2 GB\/s. A merger service has been put in place to aggregate this data. Counting the merger reading of the data provided by the BUs, its writing back of the merged data and the final reading for transfer to Tier0, an estimated bandwidth of 7GB\/s in concurrent r\/w mode is needed. Moreover, the Storage Manager has to be able to cope with being disconnected from Tier0 for 1 week, so an estimated of 250TB of total usable disk space is required. A unified name space file system (Lustre) has been chosen in order to cope with these requirements. Three different implementations of the merger service are proposed, each one providing different advantages: robustness, easy debugging, bandwidth requirements reduction. The merger is also providing the bookkeeping needed for establishing the handshake protocol between the Transfer System and the Tier0 facility at CERN, which ensures data consistency and integrity throughout the transfer process. Even if the handshake protocol itself is identical to the one from the Run1, some adjustments needed to be performed in the Transfer System in order to accommodate the new structures providing the required meta-information related to the acquired data. In addition to the nominal transfer of data to Tier0, the Transfer System needs to intelligently distribute the data, for a number of data streams need to be (also) stored locally in the CMS network for various consumers to process on site. In this article we present the various technological and implementation choices of the three components of the Storage Manager.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578844", "resources": [{"_type": "LocalFile", "name": "CHEP2015_slides_SM.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/58\/attachments\/578844\/797038\/CHEP2015_slides_SM.pdf", "fileName": "CHEP2015_slides_SM.pdf", "_fossil": "localFileMetadata", "id": "797038", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/58", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "55", "speakers": [{"_type": "ContributionParticipation", "emailHash": "105ddff0ca7c9692f4c25ca934c68b5d", "affiliation": "Tokyo Institute of Technology (JP)", "_fossil": "contributionParticipationMetadata", "fullName": "KOBAYASHI, Dai", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "105ddff0ca7c9692f4c25ca934c68b5d", "affiliation": "Tokyo Institute of Technology (JP)", "_fossil": "contributionParticipationMetadata", "fullName": "KOBAYASHI, Dai", "id": "1"}], "title": "Performance of the ATLAS Muon Trigger in Run I and Upgrades for Run II", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T09:24:01.328795+00:00", "description": "", "title": "CHEP20150413.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/55\/attachments\/578846\/797040\/CHEP20150413.pdf", "filename": "CHEP20150413.pdf", "content_type": "application\/pdf", "type": "file", "id": 797040, "size": 3393743}], "title": "Poster", "default_folder": false, "id": 578846, "description": ""}, {"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-17T00:53:27.450366+00:00", "description": "", "title": "CHEP20150417_2.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/55\/attachments\/578845\/797039\/CHEP20150417_2.pdf", "filename": "CHEP20150417_2.pdf", "content_type": "application\/pdf", "type": "file", "id": 797039, "size": 5168593}], "title": "Slides", "default_folder": false, "id": 578845, "description": ""}], "_type": "Contribution", "description": "The ATLAS experiment at the Large Hadron Collider (LHC) has taken data at a centre-of-mass energy between 900 GeV and 8 TeV during Run I (2009-2013). The LHC delivered an integrated luminosity of about 20 fb$^{\u22121}$ in 2012, which required dedicated strategies to guard the highest possible physics output while reducing effectively the event rate. The Muon High Level Trigger has successfully adapted to the changing environment of a low luminosity in 2010 to the luminosities encountered in 2012. The selection strategy has been optimized for the various physics analyses involving muons in the final state. We will present the excellent performance achieved during Run I.\r\n\r\nIn preparation for the next data taking period (Run II) several hardware and software upgrades to the ATLAS Muon Trigger have been performed to deal with the increased trigger rate expected at higher center of mass energy and increased instantaneous luminosity. We will highlight the development of novel algorithms that have been developed to maintain a highly efficient event selection while reducing the processing time by a factor of three. In addition, the two stages of the high level trigger that was deployed in Run I will be merged for Run II. We will discuss novel approaches that are being developed to further improve the trigger algorithms for Run II and beyond.", "track": "Track1: Online computing ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578846", "resources": [{"_type": "LocalFile", "name": "CHEP20150413.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/55\/attachments\/578846\/797040\/CHEP20150413.pdf", "fileName": "CHEP20150413.pdf", "_fossil": "localFileMetadata", "id": "797040", "_deprecated": true}], "_deprecated": true}, {"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578845", "resources": [{"_type": "LocalFile", "name": "CHEP20150417_2.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/55\/attachments\/578845\/797039\/CHEP20150417_2.pdf", "fileName": "CHEP20150417_2.pdf", "_fossil": "localFileMetadata", "id": "797039", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/55", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "54", "speakers": [{"_type": "ContributionParticipation", "emailHash": "818c88f50c6ae2b9ebe7de416071983a", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FRANZONI, Giovanni", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "818c88f50c6ae2b9ebe7de416071983a", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FRANZONI, Giovanni", "id": "0"}], "title": "Monte Carlo Production Management at CMS", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The analysis of the LHC data at the Compact Muon Solenoid (CMS) experiment requires the production of a large number of simulated events. During the runI of LHC (2010-2012), CMS has produced over 12 Billion simulated events, organized in approximately sixty different campaigns each emulating specific detector conditions and LHC running conditions (pile up).\r\n\r\nIn order to aggregate the information needed for the configuration and prioritization of the events production, assure the book-keeping and of all the processing requests placed by the physics analysis groups, and to interface with the CMS production infrastructure, the web-based service 'Monte Carlo Management' (McM) has been developed and put in production in 2012.\r\n\r\nMcM is based on recent server infrastructure technology (CherryPy + java) and relies on a CouchDB database back-end.\r\n\r\nThis contribution will cover the one and half year of operational experience managing samples of simulated events for CMS, the evolution of its functionalities and the extension of its capability to monitor the status and advancement of the events production.", "track": "Track2: Offline software ", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "7cf14c4152dc02469644163a6b167fb6", "affiliation": "Vilnius University (LT)", "_fossil": "contributionParticipationMetadata", "fullName": "NORKUS, Antanas", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "cec75b69988e0271d1fc765c105b07ef", "affiliation": "Chulalongkorn University (TH)", "_fossil": "contributionParticipationMetadata", "fullName": "SRIMANOBHAS, Phat", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "6f31143d6e40d8f7574d6928ac862d64", "affiliation": "Universite Claude Bernard-Lyon I (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "BOUDOUL, Gaelle", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "27e4c37d4ec2d563e81e48a6cc40d017", "affiliation": "California Institute of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. VLIMANT, Jean-Roch", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "d822d887b66fc730b4a205a15c3bc139", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "POL, Adrian", "id": "5"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/54", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "57", "speakers": [{"_type": "ContributionParticipation", "emailHash": "c5fff1e296ec574187817b412c5fd077", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PFEIFFER, Andreas", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c5fff1e296ec574187817b412c5fd077", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PFEIFFER, Andreas", "id": "0"}], "title": "Multi-threaded Object Streaming", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T13:00:18.106299+00:00", "description": "", "title": "MultiThreadedObjectStreaming-CHEP-2015-poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/57\/attachments\/578847\/797041\/MultiThreadedObjectStreaming-CHEP-2015-poster.pdf", "filename": "MultiThreadedObjectStreaming-CHEP-2015-poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 797041, "size": 763162}], "title": "Slides", "default_folder": false, "id": 578847, "description": ""}], "_type": "Contribution", "description": "The CMS experiment at CERN's Large Hadron Collider in Geneva redesigned the code handling the conditions data during the last years, aiming to increase performance and enhance maintainability. The new design includes a move to serialise all payloads before storing them into the database, allowing the handling of the payloads in external tools independent of a given software release. In this talk we present the results of performance studies done using the serialisation package from the Boost suite as well as serialisation done with the ROOT (v5) tools. Furthermore, as the Boost tools allow parallel (de-)serialisation, we show the performance gains achieved with parallel threads when de-serialising a realistic set of conditions in CMS. Without specific optimisations an overall speed up of a factor of 3-4 was achieved using multi-threaded loading and de-serialisation of our conditions.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578847", "resources": [{"_type": "LocalFile", "name": "MultiThreadedObjectStreaming-CHEP-2015-poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/57\/attachments\/578847\/797041\/MultiThreadedObjectStreaming-CHEP-2015-poster.pdf", "fileName": "MultiThreadedObjectStreaming-CHEP-2015-poster.pdf", "_fossil": "localFileMetadata", "id": "797041", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "a7a7869681df44c70a20722d524703a9", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GOVI, Giacomo", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "2e87625d2035544993c5da10819245b8", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "OJEDA SANDONIS, Miguel", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/57", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "56", "speakers": [{"_type": "ContributionParticipation", "emailHash": "c86566d804b0db7f7f91222b9bbe8165", "affiliation": "Alpes Lasers SA", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. POSS, Stephane Guillaume", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c86566d804b0db7f7f91222b9bbe8165", "affiliation": "Alpes Lasers SA", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. POSS, Stephane Guillaume", "id": "0"}], "title": "ALDIRAC, a commercial extension to DIRAC", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "We provide a report on ALDIRAC, the first DIRAC extension for a commercial application. DIRAC is a complete distributed computing solution, initially implemented for the LHCb experiment but now used by a wider community. The ALDIRAC extension is designed for the Alpes Lasers SA company in Neuchatel, Switzerland, to perform the simulation of the properties of Quantum Cascade Lasers on a Cloud system, namely Amazon EC2.\r\n\r\nIn this report, we will demonstrate that DIRAC is well suited to be used as a commercial solution. We will put an emphasis on the software developments required to use it in such a context. In particular, the resources used will be detailed. Moreover, as the Intellectual Property is an essential aspect of the business, a special treatment of the simulation software installation was implemented. Additional developments were necessary: due to the limited in-house computing resources, in particular network bandwidth, a system was designed to automatically deploy a complete DIRAC server on Amazon EC2 based on external signals. The machines provided by Amazon EC2 give the ability to quickly scale up and down the capabilities of the service. We will show the challenges faced and the solutions provided to allow agility with controlled costs. Finally the chosen data model will be presented: it is based on a postgresql database, where the every simulation result is stored individually. A set of meta data is used to select interesting simulation results.", "track": "Track7: Clouds and virtualization", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "e1c4bec2dc1201b4c43be7b7a792e4eb", "affiliation": "University of Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "GRACIANI DIAZ, Ricardo", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "abfcc508622af21078d4ac47c92ceff6", "affiliation": "CPPM, Aix-Marseille Universit\u00e9, CNRS\/IN2P3, Marseille, France", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. TSAREGORODTSEV, Andrei", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/56", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "51", "speakers": [{"_type": "ContributionParticipation", "emailHash": "18f12d579cee5d8345191b00dae91d02", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "JUNG, Christopher", "id": "5"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e8f5f6811898914eda13848d629d48b0", "affiliation": "Kalrsruhe Institute of Technology", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HARDT, Marcus", "id": "0"}], "title": "Distributed Root analysis on an ARM cluster", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-07T09:37:44.806094+00:00", "description": "", "title": "cubies-poster_matthias_4.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/51\/attachments\/578848\/797042\/cubies-poster_matthias_4.pdf", "filename": "cubies-poster_matthias_4.pdf", "content_type": "application\/pdf", "type": "file", "id": 797042, "size": 16828392}], "title": "Poster", "default_folder": false, "id": 578848, "description": ""}], "_type": "Contribution", "description": "An ARM cluster, CEPH, ROOT and the energy balance\r\n\r\nThe total cost of ownershipt (TCO) of todays computer centres are\r\nincreasingly driven the power consumption of computing equipment. The\r\nquestion arises if Intel based CPUs are still the best choice for analysis\r\ntasks.  Furthermore, data-driven computing models are emerging.\r\n\r\nThis contribution compares performance, TCO, power and energy consumption\r\nof an ARM based CEPH cluster with off the shelf computers and an average\r\nGridKa compute node. The ARM cluster was constructed from 16 CubieBoards\r\nthat comprise a dual Core ARM-A7 CPU at 1GHz as well as a SATA controller.\r\n\r\nAs analysis use case of this comparison we chose a\r\nsimple high-energy di-muon events simulated for LHC.\r\n\r\nThe goal is to assess which configuration is better suited for future\r\nanalysis hardware.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578848", "resources": [{"_type": "LocalFile", "name": "cubies-poster_matthias_4.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/51\/attachments\/578848\/797042\/cubies-poster_matthias_4.pdf", "fileName": "cubies-poster_matthias_4.pdf", "_fossil": "localFileMetadata", "id": "797042", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "4f4f83da347625e52dcf7e8434337537", "affiliation": "KIT", "_fossil": "contributionParticipationMetadata", "fullName": "Mrs. GUDU, Diana", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "39c79ef9bd1cdca2f8551a2c1b0a73ac", "affiliation": "KIT", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. SCHNEPF, Matthias", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "7e12b3a76ee1451a015ca673d81c8ab3", "affiliation": "KIT", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. RISCHE, Bernd", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "18f12d579cee5d8345191b00dae91d02", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "JUNG, Christopher", "id": "5"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/51", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:30:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "258", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6959ac7e775d5ad7e2efb9311e584779", "affiliation": "GSI Darmstadt", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. UHLIG, Florian", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6959ac7e775d5ad7e2efb9311e584779", "affiliation": "GSI Darmstadt", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. UHLIG, Florian", "id": "0"}], "title": "New developments in the FairRoot framework", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T03:37:34.092134+00:00", "description": "", "title": "CHEP_FU_3.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/258\/attachments\/578849\/797043\/CHEP_FU_3.pdf", "filename": "CHEP_FU_3.pdf", "content_type": "application\/pdf", "type": "file", "id": 797043, "size": 1199870}], "title": "Slides", "default_folder": false, "id": 578849, "description": ""}], "_type": "Contribution", "description": "The FairRoot framework is the standard framework for simulation, reconstruction and data analysis developed at GSI for the future experiments at the FAIR facility.\r\n\r\nThe framework delivers base functionality for simulation, i.e.: Infrastructure to easily implement a set of detectors, fields, and event generators. Moreover, the framework decouples the user code (e.g.: Geometry description, detector response, etc.) completely from the used MC engine.  The framework also handles the Input\/Output (IO). The output of single detectors (tasks) can be switched on (made persistence) or off (transient) in a simple and flexible way.\r\n\r\nFor reconstruction and\/or data analysis the user code is organized in modular tasks that implement the different states of a state machine. The order in which these tasks are executed is defined via a so-called steering macro. This scheme allows a very flexible handling of the reconstruction and data analysis configurations, it also allow for mixing of simulation and data reconstruction. Reconstruction tasks can run separately after simulation or directly on the fly within the simulation.  \r\n\r\nThe modular design of the framework has allowed a smooth transition to a message queue based system, which makes it possible to parallelize the execution of the tasks without re-designing or re-writing the existing user code. The new design also allows implementing the processes in different programming languages or on different hardware platforms.  For the communication between the different processes modern technologies like protocol buffers and Boost serialization are also used.  \r\n\r\nThe framework with a focus on the basic building blocks and the transition to the message queue based system will be presented.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578849", "resources": [{"_type": "LocalFile", "name": "CHEP_FU_3.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/258\/attachments\/578849\/797043\/CHEP_FU_3.pdf", "fileName": "CHEP_FU_3.pdf", "_fossil": "localFileMetadata", "id": "797043", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "76c9359faca4e532f72185ed5bd6a4c1", "affiliation": "GSI Darmstadt", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. AL-TURANY, Mohammad", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "396aaf2b0d0420e93a557a9d05f69f62", "affiliation": "GSI Darmstadt", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KARABOWICZ, Radoslaw", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "32469ad582447e084625f1c255d9f6df", "affiliation": "GSI Darmstadt", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. RYBALCHENKO, Alexey", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "0f459986c243c405e578186528431c78", "affiliation": "GSI Darmstadt", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LEBEDEV, Andrey", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "fed442c4fd59b2d8ee00a5b8ca6f2b56", "affiliation": "GSI Darmstadt", "_fossil": "contributionParticipationMetadata", "fullName": "MANAFOV, Anar", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "300bdfcba5517e107932a61321652a75", "affiliation": "GSI Darmstadt", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KRESAN, Dmytro", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "6b9225b85cccf61b94e240dcd41c75e2", "affiliation": "GSI Darmstadt", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WINKLER, Nicolas", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "e796c948ea7dc1856af5fd64e6105231", "affiliation": "GSI Darmstadt", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BERTINI, Denis", "id": "8"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/258", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "53", "speakers": [{"_type": "ContributionParticipation", "emailHash": "04c9ec740725688fbf7150ddaddb0317", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "MARTIN-HAUGH, Stewart", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "04c9ec740725688fbf7150ddaddb0317", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "MARTIN-HAUGH, Stewart", "id": "1"}], "title": "The performance and development of the Inner Detector Trigger at ATLAS for LHC Run 2", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T23:23:12.746540+00:00", "description": "", "title": "ATL-DAQ-SLIDE-2015-149.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/53\/attachments\/578850\/797044\/ATL-DAQ-SLIDE-2015-149.pdf", "filename": "ATL-DAQ-SLIDE-2015-149.pdf", "content_type": "application\/pdf", "type": "file", "id": 797044, "size": 2451432}], "title": "Slides", "default_folder": false, "id": 578850, "description": ""}], "_type": "Contribution", "description": "A description of the design and performance of the newly reimplemented \r\ntracking algorithms for the ATLAS trigger for LHC Run 2, to commence in \r\nspring 2015, is provided. The ATLAS High Level Trigger (HLT) has been \r\nrestructured to run as a more flexible single stage process, rather than \r\nthe two separate Level 2 and Event Filter stages used during Run 1. To \r\nmake optimal use of this new scenario, a new tracking strategy has been \r\nimplemented for Run 2 for the HLT. This new strategy will use a Fast \r\nTrack Finder (FTF) algorithm to directly seed the subsequent Precision \r\nTracking, and will result in improved track parameter resolution and \r\nsignificantly faster execution times than achieved during Run 1 but \r\nwith no significant reduction in efficiency. The performance and timing \r\nof the algorithms for numerous physics signatures in the trigger are \r\npresented. The profiling infrastructure, constructed to provide prompt \r\nfeedback from the optimisation, is described, including the methods used \r\nto monitor the relative perfor- mance improvements as the code evolves. \r\nThe online deployment and commissioning, together with the first \r\nmeasurements with the Run 2 data are also discussed.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578850", "resources": [{"_type": "LocalFile", "name": "ATL-DAQ-SLIDE-2015-149.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/53\/attachments\/578850\/797044\/ATL-DAQ-SLIDE-2015-149.pdf", "fileName": "ATL-DAQ-SLIDE-2015-149.pdf", "_fossil": "localFileMetadata", "id": "797044", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/53", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:15:00"}, "duration": 15, "session": "Track 8 Session", "keywords": [], "id": "52", "speakers": [{"_type": "ContributionParticipation", "emailHash": "4f621a41727f4c88a4563a9ea1fb0151", "affiliation": "Karlsruhe Institute of Technology", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. BAEHR, Steffen", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "4f621a41727f4c88a4563a9ea1fb0151", "affiliation": "Karlsruhe Institute of Technology", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. BAEHR, Steffen", "id": "0"}], "title": "Online-Analysis of Hits in the Belle-II Pixeldetector for Separation of Slow Pions from Background", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T08:57:00.069476+00:00", "description": "", "title": "Slow_Pion_Recovery_CHEP_15.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/52\/attachments\/578851\/797045\/Slow_Pion_Recovery_CHEP_15.pdf", "filename": "Slow_Pion_Recovery_CHEP_15.pdf", "content_type": "application\/pdf", "type": "file", "id": 797045, "size": 732458}, {"_type": "attachment", "modified_dt": "2015-04-12T08:57:00.069476+00:00", "description": "", "title": "Slow_Pion_Recovery_CHEP_15.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/52\/attachments\/578851\/797046\/Slow_Pion_Recovery_CHEP_15.pptx", "filename": "Slow_Pion_Recovery_CHEP_15.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797046, "size": 3095436}], "title": "Slides", "default_folder": false, "id": 578851, "description": ""}], "_type": "Contribution", "description": "The impending Upgrade of the Belle experiment is expected to increase the generated data set by a factor of 50.\r\nThis means that for the planned pixeldetector, which is the closest to the interaction point, the data rates are going to increase to over 20 GB\/s.\r\nCombined with data generated by the other detectors, this rate is too big to be efficiently send out to offline processing.\r\nThis is makes the employment of online data reduction schemes necessary, in which background is detected and rejected in order to reduce the data rates.\r\nIn this paper an approach for efficient online data reduction for the planned pixeldetector of Belle-II is presented.\r\nThe approach centers around the usage of an algorithm, the NeuroBayes, that is based on multivariate analysis, allowing the identification of signal and background by analysing clusters of hits in the pixeldetector on FPGAs.\r\nThe algorithm is leveraging the fact that hits of signal particles can have very different characteristics, compared to background, when passing through the pixeldetector.\r\nThe applicability and advantages in performance are shown through the D* decay.\r\nIn Belle-II these decays produce pions with such a small transversal momentum, that they barley escape the pixel detector itself.\r\nIn a common approach like extrapolation of tracks from outer detectors to RoIs, these pions are simply lost, since they do not reach all necessary layers of a detector. \r\nMeanwhile usage of the cluster analysis succeeds in separating those pions from background, allowing to retain the data.\r\nFor that characteristics of corresponding hits, like the total amount of charge deposited in the pixels, are used for separation.\r\nThe capability for effective data reduction is underlined by a background reduction of at least 90% and signal efficiency of 95 %, for slow pions.\r\nAn implementation of the algorithm for usage on Virtex-6 FPGAs, that are used at the pixeldetector, was performed.\r\nIt is shown that the resulting implementation succeeds in replicating the efficiency of the algorithm, implemented in software, while throughputs that suffice hard realtime constraints, set by the read out system of Belle-II, are achieved and efficient use of the resources present on the FPGA is made.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578851", "resources": [{"_type": "LocalFile", "name": "Slow_Pion_Recovery_CHEP_15.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/52\/attachments\/578851\/797045\/Slow_Pion_Recovery_CHEP_15.pdf", "fileName": "Slow_Pion_Recovery_CHEP_15.pdf", "_fossil": "localFileMetadata", "id": "797045", "_deprecated": true}, {"_type": "LocalFile", "name": "Slow_Pion_Recovery_CHEP_15.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/52\/attachments\/578851\/797046\/Slow_Pion_Recovery_CHEP_15.pptx", "fileName": "Slow_Pion_Recovery_CHEP_15.pptx", "_fossil": "localFileMetadata", "id": "797046", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "c8e45a279f663c2d25c9e9f6f609181c", "affiliation": "Karlsruhe Institute of Technology", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SANDER, Oliver", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "440170f1a68c8f4922678b224b5309da", "affiliation": "Karlsruhe Institute of Technology", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HECK, Martin", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "57d05c334cb867790851ffd8e8469854", "affiliation": "Karlsruhe Institute of Technology", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. PULVERMACHER, Christian", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "d1f351caf3d60b941ca3d31e3c9ba670", "affiliation": "Karlsruhe Institute of Technology", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. FEINDT, Michael", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "35594d61dff339989e87bf8f284033b1", "affiliation": "Karlsruhe Institute of Technology", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. BECKER, Juergen", "id": "5"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/52", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:45:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:00:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "537", "speakers": [{"_type": "ContributionParticipation", "emailHash": "724eaee9049030292ba246ebb388a9f0", "affiliation": "urn:Google", "_fossil": "contributionParticipationMetadata", "fullName": "URAM, Tom", "id": "6"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "f3174c496400e0050cf9775ffa9c2796", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CHILDERS, Taylor", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "6c25ba6db79eb1bb5da942978077e136", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LE COMPTE, Thomas", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "724eaee9049030292ba246ebb388a9f0", "affiliation": "urn:Google", "_fossil": "contributionParticipationMetadata", "fullName": "URAM, Tom", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "2751d00d233e32081392e273d3b83521", "affiliation": "Duke University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BENJAMIN, Doug", "id": "3"}], "title": "Achieving production-level use of HEP software at the Argonne Leadership Computing Facility", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T08:59:43.589122+00:00", "description": "", "title": "uram_argonne_chep2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/537\/attachments\/578852\/797047\/uram_argonne_chep2015.pdf", "filename": "uram_argonne_chep2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 797047, "size": 2159836}], "title": "Slides", "default_folder": false, "id": 578852, "description": ""}], "_type": "Contribution", "description": "HEP\u2019s demand for computing resources has grown beyond the capacity of the Grid, and these demands will accelerate with the higher energy and luminosity planned for Run II. Mira, the ten petaflops supercomputer at the Argonne Leadership Computing Facility, is a potentially significant compute resource for HEP research. Through an award of fifty million hours on Mira, we have delivered millions of events to LHC Experiments by establishing the means of marshaling jobs through serial stages on local clusters, and parallel stages on Mira. We are running several HEP applications, including Alpgen, Pythia, Sherpa, and Geant4. Event generators, such as Sherpa, typically have a split workload: a small scale integration phase, and a second, more scalable, event-generation phase. To accommodate this workload on Mira we have developed two Python-based Django applications, Balsam and ARGO. Balsam is a generalized scheduler interface which uses a plugin system for interacting with scheduler software such as Condor, Cobalt, and Torque. ARGO is a workflow manager that submits jobs to instances of Balsam. Through these mechanisms, the serial and parallel tasks within jobs are executed on the appropriate resources. This approach and its integration with the PanDA production system will be discussed.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578852", "resources": [{"_type": "LocalFile", "name": "uram_argonne_chep2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/537\/attachments\/578852\/797047\/uram_argonne_chep2015.pdf", "fileName": "uram_argonne_chep2015.pdf", "_fossil": "localFileMetadata", "id": "797047", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/537", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:45:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:00:00"}, "duration": 15, "session": "Track 8 Session", "keywords": [], "id": "536", "speakers": [{"_type": "ContributionParticipation", "emailHash": "f3174c496400e0050cf9775ffa9c2796", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CHILDERS, Taylor", "id": "4"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "f3174c496400e0050cf9775ffa9c2796", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CHILDERS, Taylor", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "6c25ba6db79eb1bb5da942978077e136", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LE COMPTE, Thomas", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "724eaee9049030292ba246ebb388a9f0", "affiliation": "urn:Google", "_fossil": "contributionParticipationMetadata", "fullName": "URAM, Tom", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "2751d00d233e32081392e273d3b83521", "affiliation": "Duke University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BENJAMIN, Doug", "id": "3"}], "title": "Simulation of LHC events on a million threads", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T09:07:15.325044+00:00", "description": "", "title": "TaylorChilders.MillionThreads.v3.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/536\/attachments\/578853\/797048\/TaylorChilders.MillionThreads.v3.pdf", "filename": "TaylorChilders.MillionThreads.v3.pdf", "content_type": "application\/pdf", "type": "file", "id": 797048, "size": 14736744}], "title": "Slides", "default_folder": false, "id": 578853, "description": ""}], "_type": "Contribution", "description": "Demand for Grid resources is expected to double during LHC Run II as compared to Run I; the capacity of the grid, however, will not double. The HEP community must consider how to bridge this computing gap. Two approaches to meeting this demand include targeting larger compute resources, and using the available compute resources as efficiently as possible. Argonne\u2019s Mira, the fifth fastest supercomputer in the world, can run roughly five times the number of parallel processes that the ATLAS experiment typically uses on the Grid. We have ported Alpgen, a serial x86 code, to run as a parallel application under MPI on the Blue Gene\/Q architecture. By analysis of the Alpgen code, we reduced the memory footprint to allow running 64 threads per node, utilizing the four hardware threads available per core on the PowerPC A2 processor. Event generation and unweighting, typically run as independent serial phases, are coupled together in a single job in this scenario, reducing intermediate writes to the filesystem. By these optimizations, we have successfully run LHC proton-proton physics event generation at the scale of a million threads, filling two-thirds of Mira.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578853", "resources": [{"_type": "LocalFile", "name": "TaylorChilders.MillionThreads.v3.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/536\/attachments\/578853\/797048\/TaylorChilders.MillionThreads.v3.pdf", "fileName": "TaylorChilders.MillionThreads.v3.pdf", "_fossil": "localFileMetadata", "id": "797048", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/536", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "535", "speakers": [{"_type": "ContributionParticipation", "emailHash": "7af24c6a2607a4e6e9e2ae714ee6a2da", "affiliation": "University of Tsukuba", "_fossil": "contributionParticipationMetadata", "fullName": "TATEBE, Osamu", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "7af24c6a2607a4e6e9e2ae714ee6a2da", "affiliation": "University of Tsukuba", "_fossil": "contributionParticipationMetadata", "fullName": "TATEBE, Osamu", "id": "2"}], "title": "Data Integrity for Silent Data Corruption in Gfarm File System", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T01:54:23.232850+00:00", "description": "", "title": "CHEP2015-poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/535\/attachments\/578854\/797049\/CHEP2015-poster.pdf", "filename": "CHEP2015-poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 797049, "size": 905896}], "title": "Poster", "default_folder": false, "id": 578854, "description": ""}], "_type": "Contribution", "description": "Files in storage are often corrupted silently without any explicit\r\nerror.  This is typically due to file system software bug, RAID\r\ncontroller firmware bug, and some other reasons.  Most critical issue\r\nis damaged data is read without any error.  Although there are several\r\nmechanisms to detect data corruption in different layers such as ECC\r\nin disk and memory and TCP checksum, the data may be damaged.  To cope\r\nwith the silent data corruption, the file system level detection is\r\neffective.  Btrfs and ZFS have a mechanism to detect it by adding\r\nchecksum in each block.  However, data replication is often required\r\nto correct the damaged data, which may waste storage capacity in local\r\nfile system since it is required only for data integrity.\r\n\r\nGfarm file system is a distributed file system that federates storages\r\namong several institutions in wide area.  Large installations include\r\nJapan Lattice Data Grid (JLDG) with 4PB storage capacity in 9 storage\r\nsites, and HPCI shared storage with 20PB storage capacity in 3 storage\r\nsites.  It has file replicas to improve access performance from\r\ndistant clients, and also to improve fault tolerance.  The number and\r\nthe locations of file replicas are specified by an extended attribute\r\nof a directory or a file.\r\n\r\nWe design the data integrity feature in Gfarm file system by\r\nautomatically calculating digest like md5 or sha256 when accessing\r\nfiles.  The file digest is calculatd at a storage node before writing\r\nto a storage when a file is created, and managed in file system\r\nmetadata.  It can detect data corruption even when writing to storage.\r\nThe digest is also calculated when reading a file.  After reading\r\nentire data of file, the read system call returns input\/output error\r\nwhen the digest mismatches.  This ensures corrupted data cannot be\r\nread.  When creating a file replica, this digest check is also\r\nperformed.  To cope with data corruption during the network transfer\r\nfrom a client, it also supports digest calculation at client side to\r\nensure end-to-end data integrity.\r\n\r\nThis paper describes a design and implementation of data integrity\r\nfeature in Gfarm file system.  Due to native and required feature of\r\nfile replicas in Gfarm file system, the data integrity can be\r\nsupported without any waste storage capacity.  To reduce additional\r\noverhead for calculating digest, the digest is only calculated when\r\naccessing files sequentially.  This access pattern is the most typical\r\ncase in Gfarm file system.  For files that is created by random access\r\nwrite, the digest is calculated when creating the file replica.  This\r\ndesign enables a minimum additional overhead and enough data integrity\r\nsupport.", "track": "Track3: Data store and access ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578854", "resources": [{"_type": "LocalFile", "name": "CHEP2015-poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/535\/attachments\/578854\/797049\/CHEP2015-poster.pdf", "fileName": "CHEP2015-poster.pdf", "_fossil": "localFileMetadata", "id": "797049", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "cbfea1a9fb316e8f29b8b7a1430813a3", "affiliation": "U", "_fossil": "contributionParticipationMetadata", "fullName": "YOSHIE, Tomoteru", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/535", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:15:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:30:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "63", "speakers": [{"_type": "ContributionParticipation", "emailHash": "7b0a240f7a59d03e34101d2917c82149", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. CORREIA FERNANDES, Joao", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "71b1051564773805611f55454d63bc23", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BARON, Thomas", "id": "0"}], "title": "Offering Global Collaboration Services beyond CERN and HEP", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T05:14:31.570828+00:00", "description": "", "title": "Vidyo_Indico_Global_CHEP_2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/63\/attachments\/578855\/797050\/Vidyo_Indico_Global_CHEP_2015.pdf", "filename": "Vidyo_Indico_Global_CHEP_2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 797050, "size": 1541258}, {"_type": "attachment", "modified_dt": "2015-04-15T05:14:31.570828+00:00", "description": "", "title": "Vidyo_Indico_Global_CHEP_2015.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/63\/attachments\/578855\/797051\/Vidyo_Indico_Global_CHEP_2015.pptx", "filename": "Vidyo_Indico_Global_CHEP_2015.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797051, "size": 4335275}], "title": "Slides", "default_folder": false, "id": 578855, "description": ""}], "_type": "Contribution", "description": "The CERN IT department has built over the years a performant and integrated ecosystem of collaboration tools, from videoconference and webcast services to event management software. These services have been designed and evolved in very close collaboration with the various communities surrounding the laboratory and have been massively adopted by CERN users. To cope with this very heavy usage, global infrastructures have been deployed which take full advantage of CERN's international and global nature. \r\nIf these services and tools are instrumental in enabling the worldwide collaboration which generates major HEP breakthroughs, they would certainly also benefit other sectors of science in which globalisation has already taken place. Some of these services are driven by commercial software (Vidyo or Wowza for example), some others have been developed internally and have already been made available to the world as Open Source Software in line with CERN's spirit and mission. Indico for example is now installed in 100+ institutes worldwide. \r\nBut providing the software is often not enough and institutes, collaborations and project teams do not always possess the expertise, or human or material resources that are needed to set up and maintain such services. Regional and national institutions have to answer needs which are growingly global and often contradict their operational capabilities or organisational mandate and so are looking at existing worldwide service offers such as CERN's. \r\nWe believe that the accumulated experience obtained through the operation of a large scale worldwide collaboration service combined with CERN's global network and its recently-deployed Agile Infrastructure would allow the Organisation to set up and operate collaborative services, such as Indico and Vidyo, at a much larger scale and on behalf of worldwide research and education institutions and thus answer these pressing demands while optimizing resources at a global level. Such services would be built over a robust and massively scalable Indico server to which the concept of communities would be added, and which would then serve as a hub for accessing other collaboration services such as Vidyo, on the same simple and successful model currently in place for CERN users.\r\nThis talk will describe this vision, its benefits and the steps which have already been taken to make it come to life.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578855", "resources": [{"_type": "LocalFile", "name": "Vidyo_Indico_Global_CHEP_2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/63\/attachments\/578855\/797050\/Vidyo_Indico_Global_CHEP_2015.pdf", "fileName": "Vidyo_Indico_Global_CHEP_2015.pdf", "_fossil": "localFileMetadata", "id": "797050", "_deprecated": true}, {"_type": "LocalFile", "name": "Vidyo_Indico_Global_CHEP_2015.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/63\/attachments\/578855\/797051\/Vidyo_Indico_Global_CHEP_2015.pptx", "fileName": "Vidyo_Indico_Global_CHEP_2015.pptx", "_fossil": "localFileMetadata", "id": "797051", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "0ccb5305e98ea446dc37370c1c25d9db", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FERREIRA, Pedro", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "7b0a240f7a59d03e34101d2917c82149", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. CORREIA FERNANDES, Joao", "id": "2"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/63", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "532", "speakers": [{"_type": "ContributionParticipation", "emailHash": "fdf72c50739c48378b9f205d84cef90d", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MAGNONI, Luca", "id": "18"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c8d25c6f0ff0c41bc3d4fe69044b2bd6", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SAIZ, Pablo", "id": "0"}], "title": "WLCG Monitoring, consolidation and further evolution", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T10:21:40.913914+00:00", "description": "", "title": "CHEP2015Poster_MonCons.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/532\/attachments\/578856\/797052\/CHEP2015Poster_MonCons.pdf", "filename": "CHEP2015Poster_MonCons.pdf", "content_type": "application\/pdf", "type": "file", "id": 797052, "size": 1780600}, {"_type": "attachment", "modified_dt": "2015-04-13T10:21:40.913914+00:00", "description": "", "title": "CHEP2015Poster_MonCons.ppt", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/532\/attachments\/578856\/797053\/CHEP2015Poster_MonCons.ppt", "filename": "CHEP2015Poster_MonCons.ppt", "content_type": "application\/vnd.ms-powerpoint", "type": "file", "id": 797053, "size": 10354176}], "title": "Slides", "default_folder": false, "id": 578856, "description": ""}], "_type": "Contribution", "description": "The WLCG monitoring system solves a challenging task of keeping track of the LHC computing activities on the WLCG infrastructure, ensuring health and performance of the distributed services at more than 160 sites. The current challenge consists of decreasing the effort needed to operate the monitoring service and to satisfy the constantly growing requirements for its scalability and performance. This contribution describes the recent consolidation work aimed to reduce the complexity of the system, and to ensure more effective operations, support and service management. This was done by unifying where possible the implementation of the monitoring components. The contribution also covers further steps like the evaluation of the new technologies for data storage, processing and visualization and migration to a new technology stack", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578856", "resources": [{"_type": "LocalFile", "name": "CHEP2015Poster_MonCons.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/532\/attachments\/578856\/797052\/CHEP2015Poster_MonCons.pdf", "fileName": "CHEP2015Poster_MonCons.pdf", "_fossil": "localFileMetadata", "id": "797052", "_deprecated": true}, {"_type": "LocalFile", "name": "CHEP2015Poster_MonCons.ppt", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/532\/attachments\/578856\/797053\/CHEP2015Poster_MonCons.ppt", "fileName": "CHEP2015Poster_MonCons.ppt", "_fossil": "localFileMetadata", "id": "797053", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "dd2d05df8869eb91f302ce4046407f24", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ANDREEVA, Julia", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "efd528c4fecb23d4785370a2ba2989b2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MAGINI, Nicolo", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "e0c62f86305aa41c91d857b662068a19", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ROISER, Stefan", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "e603395dfb4d403f0f08762f7fa6e118", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "TARRAGON CROS, Jacobo", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "06b2aec3ceab21254ba9e6fd9454a255", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DZHUNOV, Ivan Antoniev", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "2ea6e988cc7e5089e06b01ced71ed0cc", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "TUCKETT, David", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "77c17826be119bea60c91127d897cd14", "affiliation": "Universidad Complutense (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "MARTIN DE LOS RIOS SAIZ, Hector", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "68c97bd2a016fa65faad2c6e3e91bb2a", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SCHULZ, Markus", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "a5843b16355c349d023bc2a924360f64", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "AIMAR, Alberto", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "4a66eba8bd02dcd2372a3cd6255f9fc5", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KARAVAKIS, Edward", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "aa717f53388810bc2bae04d4c563508e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CONS, Lionel", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "fdf72c50739c48378b9f205d84cef90d", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MAGNONI, Luca", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "dc823b395ebcf8e16699e15ab5d301cd", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BABIK, Marian", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "f8a1c3051ab62071163450a232a4e82f", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "LITMAATH, Maarten", "id": "14"}, {"_type": "ContributionParticipation", "emailHash": "bd221996d4a256a8003a73d408902cf5", "affiliation": "University of Manchester (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "FORTI, Alessandra", "id": "15"}, {"_type": "ContributionParticipation", "emailHash": "a0dd1a3a7589f1e2a04cc967f2ae6c39", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DI GIROLAMO, Alessandro", "id": "16"}, {"_type": "ContributionParticipation", "emailHash": "cbc73225e092c0fa43663ce9496fd7cc", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SCIABA, Andrea", "id": "17"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/532", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:30:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "531", "speakers": [{"_type": "ContributionParticipation", "emailHash": "94c96cb9abacdef3f192e4dff0f13905", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. CARMINATI, Federico", "id": "10"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "94c96cb9abacdef3f192e4dff0f13905", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. CARMINATI, Federico", "id": "10"}], "title": "The GeantV project: preparing the future of simulation", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T02:02:00.056018+00:00", "description": "", "title": "GeantVChep15.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/531\/attachments\/578857\/797054\/GeantVChep15.pdf", "filename": "GeantVChep15.pdf", "content_type": "application\/pdf", "type": "file", "id": 797054, "size": 3656750}], "title": "Slides", "default_folder": false, "id": 578857, "description": ""}], "_type": "Contribution", "description": "Detector simulation is consuming at least half of the HEP computing cycles, and even so, experiments have to take hard decisions on what to simulate, as their needs greatly surpass the availability of computing resources. New experiments still in the design phase such as FCC, CLIC and ILC as well as upgraded versions of the existing LHC detectors will push further the simulation requirements. Since computing resources will not increase at best, it is therefore necessary to sustain the progress of High Energy Physics and to explore innovative ways of speeding up simulation. The GeantV project aims at developing a high performance detector simulation system integrating fast and full simulation that can be ported on different computing architectures, including accelerators. After more than two years of R&D the project has produced a prototype capable of transporting particles in complex geometries exploiting micro-parallelism, SIMD and multithreading. Portability is obtained via C++ template techniques that allow the development of machine-independent computational kernels. A set of tables derived from Geant4 for cross sections and final states provides a realistic shower development and, having been ported into a Geant4 physics list, is also a basis for a performance comparison.\r\nThe talk will describe the development of the project and the main R&D results motivating the technical choices of the project. It will review the current results and the major challenges facing the project. We will conclude with an outline of the future roadmaps and major milestones for the project.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578857", "resources": [{"_type": "LocalFile", "name": "GeantVChep15.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/531\/attachments\/578857\/797054\/GeantVChep15.pdf", "fileName": "GeantVChep15.pdf", "_fossil": "localFileMetadata", "id": "797054", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "5e451b3b7969b96f79563ef9500e6f3e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GHEATA, Andrei", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "7e8bcc2855b51c4e93b19e095a8b2ec9", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "WENZEL, Sandro Christian", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "8c01aed7220e755911911d2e59aa3676", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "NOVAK, Mihaly", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "565043fdb94911d4ac35d97cfc629533", "affiliation": "University of Copenhagen (DK)", "_fossil": "contributionParticipationMetadata", "fullName": "DE FINE LICHT, Johannes Christof", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "1116d4041f556f2f1783abc7e8ad2e51", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "APOSTOLAKIS, John", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "73c47f432ac574bc76aba1a48761b1ff", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "JUN, Soon Yung", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "5b0402a9cd32d8915ceeb56718af64b6", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CANAL, Philippe", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "5a02abf7bf489401510b64b45e39a213", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "ELVIRA, Victor Daniel", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "e7e207b4c5969bac3a9c7e859482b70a", "affiliation": "FermiLab (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LIMA, Guilherme", "id": "9"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/531", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "530", "speakers": [{"_type": "ContributionParticipation", "emailHash": "8e0b4b70e0c63fe9bd8bfd953277a521", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "SPINOSO, Vincenzo", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "c3a9d60000380ace3b532f9f854d7620", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "DONVITO, Giacinto", "id": "4"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "8e0b4b70e0c63fe9bd8bfd953277a521", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "SPINOSO, Vincenzo", "id": "0"}], "title": "Monitoring cloud-based infrastructures", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "INFN-Bari is involved in PRISMA and RECAS, two national projects aiming respectively at setting up an OpenStack-based cloud infrastructure for the public administration and the scientific data analysis, and upgrading the computing resources to a new T1-sized infrastructure. \r\nAs Bari is also a T2 for the CMS and Alice experiments, setting up the cloud resources so that they can be used for high energy physics is one of the main goals: the PaaS+IaaS platform provided by PRISMA will be installed on the RECAS resources at the beginning of 2015, providing about 2000 cores of additional resources to play as regular worker nodes. \r\nIt is fundamental to rethink all the monitoring infrastructure, to get a new elastic, scalable and automatic setup. \r\nIn this work a new setup for monitoring the cloud resources will be shown: in particular, it allows to know the availability of the underlying IaaS infrastructure and the status of all the IaaS\/PaaS services running on the OpenStack tesbed during the whole life of each virtual machine. Also, it is possible to get the history of all the sensors related to the infrastructure, together with graphs. Finally, users are provided with \"monitoring as a Service\" features: they can instanciate a service they wish together with a machine monitoring the service itself and showing its status to the user. The monitoring infrastructure is based on Zabbix, a powerful and flexible monitoring tool, together with OpenStack itself providing Ceilometer and APIs.\r\nIt will be shown also how Zabbix serves the monitoring purposes of the whole remaining farm, basically set up with a classic batch system, cluster file system and grid middleware. All the nodes have sensors monitoring the services they run; also the network infrastructure is monitored against topological loops, high rates of packet collisions, generic unavailability. As the network topology is known in advance, we cross the information coming from the monitoring tools to build a dynamic map of the whole farm: if a machine is moved, the dynamic map moves the server as well after some time. \r\nThanks to this new design, monitoring definitely helps the system administrator in providing stable services even when dealing with a new big infrastructure and new services.", "track": "Track7: Clouds and virtualization", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "0c0c1a1c4a6cbd1d20b3ca67975d7afc", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "BRUNO, Alessandro", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "193fcfd0e0a5b4019b7ee6ac34951820", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "ANTONACCI, Marica", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "3f09d2d07f5da6ad4292b3f8a0733054", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "NICOTRI, Stefano", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "c3a9d60000380ace3b532f9f854d7620", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "DONVITO, Giacinto", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/530", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:30:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "539", "speakers": [{"_type": "ContributionParticipation", "emailHash": "bb189345b5b466feb3ad6a3afb118c23", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ANDRADE, Pedro", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "bb189345b5b466feb3ad6a3afb118c23", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ANDRADE, Pedro", "id": "0"}], "title": "Monitoring Evolution at CERN", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T16:11:16.373511+00:00", "description": "", "title": "CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/539\/attachments\/578858\/797055\/CHEP2015.pdf", "filename": "CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 797055, "size": 6347987}], "title": "Slides", "default_folder": false, "id": 578858, "description": ""}], "_type": "Contribution", "description": "Over the past two years, the operation of the CERN Data Centres went through significant changes with the introduction of new mechanisms for hardware procurement, new services for cloud infrastructure and configuration management, among other improvements. These changes resulted in an increase of resources being operated in a more dynamic environment. Today, the CERN Data Centres provide over 11000 multi-core processor servers, 130 PB disk servers, 100 PB tape robots, and 150 high performance tape drives.\r\n\r\nTo cope with these developments, an evolution of the data centre monitoring tools was also required. This modernisation was based on a number of guiding rules: sustain the increase of resources, adapt to the new dynamic nature of the data centres, make monitoring data easier to share, give more flexibility to Service Managers on how they publish and consume monitoring metrics and logs, establish a common repository of monitoring data, optimize the handling of monitoring notifications, and replace the previous toolset by new open source technologies with large adoption and community support.\r\n\r\nThis talk will explain how these improvements were delivered, present the architecture and technologies of the new monitoring tools, and review the experience of its production deployment.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578858", "resources": [{"_type": "LocalFile", "name": "CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/539\/attachments\/578858\/797055\/CHEP2015.pdf", "fileName": "CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "797055", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "87366904b6eed619221ed639a2e9ba91", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FIORINI, Benjamin", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "ac1d6c410fa1b7358cb9b84511899262", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MURPHY, Susie", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "e35646dfb91df4d866d574e404b65a93", "affiliation": "Universidad de Oviedo (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "PIGUEIRAS, Luis", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "536c6fd1e28acd23c41394f923ad9038", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "COELHO DOS SANTOS, Miguel", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "1620d842f2c484dc6d75c0711f2299a2", "affiliation": "Technische Universitaet Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "STARAKIEWICZ, Lukasz", "id": "5"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/539", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:00:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "538", "speakers": [{"_type": "ContributionParticipation", "emailHash": "4476ecb76f14659d9de3008f3371c157", "affiliation": "Johann-Wolfgang-Goethe Univ. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KISEL, Ivan", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "4476ecb76f14659d9de3008f3371c157", "affiliation": "Johann-Wolfgang-Goethe Univ. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KISEL, Ivan", "id": "0"}], "title": "4-Dimensional Event Building in the First-Level Event Selection of the CBM Experiment.", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T08:49:47.647823+00:00", "description": "", "title": "Kisel_CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/538\/attachments\/578859\/797056\/Kisel_CHEP2015.pdf", "filename": "Kisel_CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 797056, "size": 14600420}], "title": "Slides", "default_folder": false, "id": 578859, "description": ""}], "_type": "Contribution", "description": "The future heavy-ion experiment CBM (FAIR\/GSI, Darmstadt, Germany) will focus on the measurement of very rare probes at interaction rates up to 10 MHz with data flow of up to 1 TB\/s. The beam will provide free stream of beam particles without bunch structure. That requires full online event reconstruction and selection not only in space, but also in time, so-called 4D event building and selection. This is a task of the First-Level Event Selection (FLES).\r\n\r\nThe FLES reconstruction and selection package consists of several modules: track finding, track fitting, short-lived particles finding, event building and event selection. Since all detector measurements contain also time information, the event building is done at all stages of the reconstruction process. The input data are distributed within the FLES farm in a form of so-called time-slices, which time length is proportional to a compute power of a processing node. A time-slice is reconstructed in parallel between cores within a CPU, thus minimizing communication between CPUs. After all tracks of the whole time-slice are found and fitted in 4D, they are collected into clusters of tracks originated from common primary vertices, which then are fitted, thus identifying 4D interaction points registered within the time-slice. Secondary tracks are associated with primary vertices according to their estimated production time. After that short-lived particles are found and the full e\r\nvent building process is finished. The last stage of the FLES package is a selection of events according to the requested trigger signatures.\r\n\r\nWe describe in details all stages of the FLES package and present results of tests on many-core computer farms with up to 3000 cores, focusing mainly on parallel implementations of the track finding and the event building stages as the most complicated and time consuming parts of the package. The track finding efficiency remains stable and the processing time grows as a polynomial of second order with respect to the number of events in the time-slice. The first results of J\/psi selection are presented and discussed.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578859", "resources": [{"_type": "LocalFile", "name": "Kisel_CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/538\/attachments\/578859\/797056\/Kisel_CHEP2015.pdf", "fileName": "Kisel_CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "797056", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/538", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "50", "speakers": [{"_type": "ContributionParticipation", "emailHash": "53281eed1430ea0a1c827b38f6891b94", "affiliation": "NIKHEF", "_fossil": "contributionParticipationMetadata", "fullName": "KEIJSER, Jan Justinus", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "53281eed1430ea0a1c827b38f6891b94", "affiliation": "NIKHEF", "_fossil": "contributionParticipationMetadata", "fullName": "KEIJSER, Jan Justinus", "id": "0"}], "title": "Using an Intel Galileo board as a Hardware Token Server", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T22:31:29.543634+00:00", "description": "", "title": "poster-chep2015-galileo.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/50\/attachments\/578860\/797057\/poster-chep2015-galileo.pdf", "filename": "poster-chep2015-galileo.pdf", "content_type": "application\/pdf", "type": "file", "id": 797057, "size": 1698530}], "title": "Poster", "default_folder": false, "id": 578860, "description": ""}], "_type": "Contribution", "description": "The Intel Galileo Arduino board is a low cost, low power 32bit Pentium-class computer. It is normally used for embedded devices but it can also run a full-blown version of Linux. \r\n\r\nGrid security can be greatly enhanced using hardware token for two-factor authentication. Two-factor autentication is based on the idea that in order to obtain access you need both something you know (i.e. a password) and something you possess (i.e. a hardware token). Pilot job security is improved by storing the pilot job grid certificate\/private key pair on such a hardware token. In the Netherlands we use SafeNet eToken PRO's for generating and storing private keys for robot services. Typically, robot hardware tokens are inserted into server-class systems inside a data center for maximum security. \r\n\r\nThis research was done to address the power consumption issue of such server-class systems. An Intel Galileo board typically draws 5 W of power, which is less than even the most energy efficient Atom-based server. As part of this research we built a 32bit version of CentOS 7, tuned specifically to run on the Galileo board. On top of this we installed and configured the hardware token drivers and the 'myproxy' software needed to upload robot grid proxies to a Globus MyProxy or Dirac Proxy store.\r\n\r\nNext to the poster a live demonstration of the Galileo+eToken setup is shown.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578860", "resources": [{"_type": "LocalFile", "name": "poster-chep2015-galileo.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/50\/attachments\/578860\/797057\/poster-chep2015-galileo.pdf", "fileName": "poster-chep2015-galileo.pdf", "_fossil": "localFileMetadata", "id": "797057", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/50", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:15:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "115", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e242703b591969d6f6a3b167c9db8cbb", "affiliation": "National Research Centre \"Kurchatov Institute\"", "_fossil": "contributionParticipationMetadata", "fullName": "Ms. GOLOSOVA, Marina", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "017a16ae1e2a9695af7cb13df4915de1", "affiliation": "National Research Centre \"Kurchatov Institute\"", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GRIGORIEVA, Maria", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "e242703b591969d6f6a3b167c9db8cbb", "affiliation": "National Research Centre \"Kurchatov Institute\"", "_fossil": "contributionParticipationMetadata", "fullName": "Ms. GOLOSOVA, Marina", "id": "1"}], "title": "Studies of Big Data meta-data segmentation between relational and non-relational databases", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T10:52:45.815144+00:00", "description": "", "title": "CHEP2015.Golosova.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/115\/attachments\/578861\/797059\/CHEP2015.Golosova.pdf", "filename": "CHEP2015.Golosova.pdf", "content_type": "application\/pdf", "type": "file", "id": 797059, "size": 1659893}, {"_type": "attachment", "modified_dt": "2015-04-10T10:52:45.815144+00:00", "description": "", "title": "CHEP2015.Golosova.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/115\/attachments\/578861\/797058\/CHEP2015.Golosova.pptx", "filename": "CHEP2015.Golosova.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797058, "size": 743426}], "title": "Slides", "default_folder": false, "id": 578861, "description": ""}], "_type": "Contribution", "description": "In recent years the concepts of Big Data became well established in IT-technologies. Most systems (for example Distributed Data Management or Workload Management systems) produce metadata that describes actions performed on jobs, stored data or other entities and its volume takes one to the realms of Big Data on many occasions. This metadata can be used to obtain information about the current system state, the aggregation of data for summary purposes and for statistical and trend analysis of the processes this system drives. The latter requires metadata to be stored for a long period of time. On the example of PanDA (Workload Management System for distributed production and analysis for the ATLAS experiment at the LHC and astro-particle experiments AMS and LSST) it can be seen that the growth rate of the volume of stored information has increased significantly over the last few years: from 500k completed jobs per day in 2011 up to 2 million nowadays. \r\nDatabase is the central component of the PanDA architecture. Currently RDBMS (Oracle or MySQL) is used as the storage backend. To provide better performance and scalability the data stored in relational storage is partitioned into actual (\u201clive\u201d) and archive (historical) parts. But even in this scheme, as the \u201carchived\u201d data volume grows, the underlying software and hardware stack encounters certain limits that negatively affect processing speed and the possibilities of metadata analysis. We had investigated a new class of database technologies commonly referred to as NoSQL databases. We suggest to use NoSQL solution for finalized, reference portion of essentially read-only data to improve performance and scalability. We had developed and implemented a heterogeneous storage which consists of both relational and non-relational databases and provides an API for unified access to stored meta-data. \r\nWe present methods of partitioning the data between two database classes, methods for efficient storage of NoSQL backend for archived data, including the analysis of different indexing schemes based on the statistics of the most frequently used queries to the historical data. We also present a comparison between different NoSQL databases to conclude their applicability to our solution. Performance of \u201carchived\u201d data storage in the analytical tasks is shown in the quantitative scalability and performance test results, including testing NoSQL storage against Oracle RDBMS. \r\nThis work is conducted as a part of the project to expand PanDA beyond HEP and LHC experiments.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578861", "resources": [{"_type": "LocalFile", "name": "CHEP2015.Golosova.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/115\/attachments\/578861\/797059\/CHEP2015.Golosova.pdf", "fileName": "CHEP2015.Golosova.pdf", "_fossil": "localFileMetadata", "id": "797059", "_deprecated": true}, {"_type": "LocalFile", "name": "CHEP2015.Golosova.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/115\/attachments\/578861\/797058\/CHEP2015.Golosova.pptx", "fileName": "CHEP2015.Golosova.pptx", "_fossil": "localFileMetadata", "id": "797058", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "6fef2a09accf1a707bc4c992c15a8438", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KLIMENTOV, Alexei", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "ae4376aedbf0e52c2f9c833172e30352", "affiliation": "Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. POTEKHIN, Maxim", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "5e093574484f75dbebdf207897fba8d6", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. DIMITROV, Gancho", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "5152afe0ee224171ff18a8a6a0abb342", "affiliation": "National Research Centre \"Kurchatov Institute\"", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. RYABINKIN, Eygene", "id": "5"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/115", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "252", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a0d9ac70c251483116a2d03a20a53e70", "affiliation": "University of Florida (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KIM, Bockjoo", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "d53b7694c18383e63cac3dde04ab3feb", "affiliation": "University of Florida (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BOURILKOV, Dimitri", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "8cd7249271606dc78d28c727b62dc434", "affiliation": "University of Florida (US)", "_fossil": "contributionParticipationMetadata", "fullName": "AVERY, Paul Ralph", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "219c4cb46472e72448f974cdb9bb3c10", "affiliation": "University of Florida (US)", "_fossil": "contributionParticipationMetadata", "fullName": "FU, Yu", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "3703a4bc181e064699a6d48c8de0e140", "affiliation": "Florida International University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "RODRIGUEZ, Jorge Luis", "id": "4"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "a0d9ac70c251483116a2d03a20a53e70", "affiliation": "University of Florida (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KIM, Bockjoo", "id": "0"}], "title": "File Access Optimization with the Lustre Filesystem at Florida CMS T2", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T01:31:41.334873+00:00", "description": "", "title": "Lustre_Optimization_CHEP2015_A0_Poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/252\/attachments\/578862\/797060\/Lustre_Optimization_CHEP2015_A0_Poster.pdf", "filename": "Lustre_Optimization_CHEP2015_A0_Poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 797060, "size": 1623766}], "title": "Poster", "default_folder": false, "id": 578862, "description": ""}], "_type": "Contribution", "description": "One of the CMS Tier2 centers, the Florida CMS Tier2 center, has been using the Lustre filesystem for its data storage backend system since 2004. Recently, the data access pattern at our site has changed greatly due to various new access methods that include file transfers through the GridFTP servers, read access from the worker nodes, and remote read access through xrootd. In order to optimize the file access performance, we have to consider all the possible access patterns and each pattern needs to be studied separately. In this presentation, we report on our work to optimize file access with the Lustre filesystem at the Florida CMS T2 using an approach based on the analyzing these access patterns.", "track": "Track3: Data store and access ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578862", "resources": [{"_type": "LocalFile", "name": "Lustre_Optimization_CHEP2015_A0_Poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/252\/attachments\/578862\/797060\/Lustre_Optimization_CHEP2015_A0_Poster.pdf", "fileName": "Lustre_Optimization_CHEP2015_A0_Poster.pdf", "_fossil": "localFileMetadata", "id": "797060", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "d53b7694c18383e63cac3dde04ab3feb", "affiliation": "University of Florida (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BOURILKOV, Dimitri", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "8cd7249271606dc78d28c727b62dc434", "affiliation": "University of Florida (US)", "_fossil": "contributionParticipationMetadata", "fullName": "AVERY, Paul Ralph", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "219c4cb46472e72448f974cdb9bb3c10", "affiliation": "University of Florida (US)", "_fossil": "contributionParticipationMetadata", "fullName": "FU, Yu", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "3703a4bc181e064699a6d48c8de0e140", "affiliation": "Florida International University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "RODRIGUEZ, Jorge Luis", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/252", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:15:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:30:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "117", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e01112e59cf6a84f3608604fa39300dc", "affiliation": "LMU Munich", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BEAUJEAN, Frederik", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e01112e59cf6a84f3608604fa39300dc", "affiliation": "LMU Munich", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BEAUJEAN, Frederik", "id": "0"}], "title": "The Bayesian analysis toolkit: version 1.0 and beyond", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T01:11:20.318136+00:00", "description": "", "title": "Beaujean CHEP2015", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/117\/attachments\/578863\/797061\/Beaujean.pdf", "filename": "Beaujean.pdf", "content_type": "application\/pdf", "type": "file", "id": 797061, "size": 1075297}], "title": "Slides", "default_folder": false, "id": 578863, "description": ""}], "_type": "Contribution", "description": "The Bayesian analysis toolkit [(BAT)](https:\/\/www.mppmu.mpg.de\/bat\/)\r\nis a C++ package centered around Markov-chain Monte Carlo sampling. It\r\nis used in analyses of various particle-physics experiments such as\r\nATLAS and Gerda. The software has matured over the last few years to a\r\nversion 1.0. We will summarize the lessons learned and report on the\r\ncurrent developments of a complete redesign targeting multicore and\r\nmultiprocessor architectures and supporting many more sampling\r\nalgorithms both built-in and user-supplied.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578863", "resources": [{"_type": "LocalFile", "name": "Beaujean CHEP2015", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/117\/attachments\/578863\/797061\/Beaujean.pdf", "fileName": "Beaujean.pdf", "_fossil": "localFileMetadata", "id": "797061", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "f6a3b3104187c0084e941e3f1943fe08", "affiliation": "Max Planck institute for physics", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. CALDWELL, Allen", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "4e147beda804e6aecb2b204b89b4534f", "affiliation": "Max Planck institute for physics", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SCHULZ, Oliver", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "3700782ac6be69cff60e7b600b681c79", "affiliation": "Technical University Munich", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GREENWALD, Daniel", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "5bf928bc2a841ec9cab4a53bb414d6b5", "affiliation": "Technical University Dortmund", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. KROENINGER, Kevin Alexander", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "85f863191188abed761d707cfc440a8a", "affiliation": "Max Planck institute for physics", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KLUTH, Stefan", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "17cf99de5529b920d08cc2d0eb362877", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KOLLAR, Daniel", "id": "6"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/117", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:30:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "116", "speakers": [{"_type": "ContributionParticipation", "emailHash": "65149bdeea9f5c348fe88b7464fc0b79", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ROVERE, Marco", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "65149bdeea9f5c348fe88b7464fc0b79", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ROVERE, Marco", "id": "0"}], "title": "The Data Quality Monitoring Software for the CMS experiment at the LHC", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T06:21:25.055539+00:00", "description": "", "title": "CHEP2015_MR_DQM.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/116\/attachments\/578864\/797062\/CHEP2015_MR_DQM.pdf", "filename": "CHEP2015_MR_DQM.pdf", "content_type": "application\/pdf", "type": "file", "id": 797062, "size": 8961945}], "title": "Slides", "default_folder": false, "id": 578864, "description": ""}], "_type": "Contribution", "description": "The Data Quality Monitoring (DQM) Software is a central tool in the CMS experiment. Its flexibility allows for integration in several key environments: Online, for real-time detector monitoring; Offline, for the final, fine-grained data analysis and certification; Release-Validation, to constantly validate the functionalities and the performance of the reconstruction software; in Monte Carlo productions. Since the end of data taking at a center of mass energy of 8 TeV, the environment in which the DQM lives has undergone fundamental changes. In turn, the DQM system has made significant upgrades in many areas to respond to not only the changes in infrastructure, but also the growing specialized needs of the collaboration with an emphasis on more sophisticated methods for evaluating data quality, as well as advancing the DQM system to provide quality assessments of various Monte Carlo simulations versus data distributions, monitoring changes in physical effects due to modifications of algorithms or framework, and enabling regression modeling for long-term effects for the CMS detector. The central tool to deliver Data Quality information is an interactive web site for browsing data quality histograms (DQMGUI), and its transition to becoming a distributed system will also be presented. In this contribution the usage of the DQM Software in the different environments and its integration in the CMS Reconstruction Software Framework (CMSSW) and in all production workflows are presented, with emphasis on recent developments and improvement in advance of the LHC restart at 13 TeV. The main technical challenges and the adopted solutions to them will be also discussed with emphasis on functionality and long-term robustness.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578864", "resources": [{"_type": "LocalFile", "name": "CHEP2015_MR_DQM.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/116\/attachments\/578864\/797062\/CHEP2015_MR_DQM.pdf", "fileName": "CHEP2015_MR_DQM.pdf", "_fossil": "localFileMetadata", "id": "797062", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "b0e06beb2db3bde774e696c80432b929", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. DE GUIO, Federico", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "911018d1fc46bb599b917e57b99f1799", "affiliation": "Rutgers, State Univ. of New Jersey (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. DUGGAN, Daniel", "id": "2"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/116", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "111", "speakers": [{"_type": "ContributionParticipation", "emailHash": "1d7caa59ddd1371760449102910a7dc0", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DOMINGUES CORDEIRO, Cristovao Jose", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "1d7caa59ddd1371760449102910a7dc0", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DOMINGUES CORDEIRO, Cristovao Jose", "id": "0"}], "title": "Monitoring the Delivery of Virtualized Resources to the LHC Experiments", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T06:54:25.580150+00:00", "description": "", "title": "CHEP2015-Monitoring_the_Delivery_of_Virtualized_Resources_to_the_LHC_Experiments.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/111\/attachments\/578865\/797063\/CHEP2015-Monitoring_the_Delivery_of_Virtualized_Resources_to_the_LHC_Experiments.pdf", "filename": "CHEP2015-Monitoring_the_Delivery_of_Virtualized_Resources_to_the_LHC_Experiments.pdf", "content_type": "application\/pdf", "type": "file", "id": 797063, "size": 6511435}], "title": "Poster", "default_folder": false, "id": 578865, "description": ""}], "_type": "Contribution", "description": "The adoption of Cloud technologies by the LHC experiments places the fabric management burden of monitoring virtualized resources upon the VO. In addition to monitoring the status of the virtual machines and triaging the results, it must be understood if the resources actually provided match with any agreements relating to the supply. Monitoring the instantiated virtual machines is therefore a fundamental activity and hence this paper describes how the Ganglia monitoring system can be used within the cloud computing scope of the LHC experiments. Extending upon this, it is then shown how the integral of the time-series monitoring data obtained can be repurposed to provide a consumer-side accounting record, which can then be compared with the concrete agreements that exist between the supplier of the resources and the consumer. From this, it is not clear though how the performance of the resources differ both within and between providers. Hence, the case is made for a benchmarking metric to normalize the results along with some preliminary investigation on obtaining such a metric.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578865", "resources": [{"_type": "LocalFile", "name": "CHEP2015-Monitoring_the_Delivery_of_Virtualized_Resources_to_the_LHC_Experiments.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/111\/attachments\/578865\/797063\/CHEP2015-Monitoring_the_Delivery_of_Virtualized_Resources_to_the_LHC_Experiments.pdf", "fileName": "CHEP2015-Monitoring_the_Delivery_of_Virtualized_Resources_to_the_LHC_Experiments.pdf", "_fossil": "localFileMetadata", "id": "797063", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "a0dd1a3a7589f1e2a04cc967f2ae6c39", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DI GIROLAMO, Alessandro", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "0cb6ba9ae68e30d8bf3ef4a3e0f294c2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GIORDANO, Domenico", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "6049ebfaa694e419b61cfd107f553289", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FIELD, Laurence", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "7b97af96ccc24f7684d7b1be0f2f605c", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SPIGA, Daniele", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "8d35204702739f7932f2dbd878e75259", "affiliation": "Universidad de Oviedo (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "VILLAZON ESTEBAN, Luis", "id": "5"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/111", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "110", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e79fbfa7642755008e862ffde72f0431", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MEINHARD, Helge", "id": "4"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c727556c54276ef10216447932a29942", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "LOSSENT, Alexandre", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "90a44128c8a73a712af44c2bc1647406", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GONZALEZ ALVAREZ, Alvaro", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "b991855361bef16c6677d882a85a15bc", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "APARICIO COTARELO, Borja", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "8f9407fc19f5dc18d6c799e98062131a", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ANDERSEN, Terje", "id": "7"}], "title": "Extending software repository hosting to code review and testing", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "We will present how CERN's services around Issue Tracking and Version Control have evolved, and what the plans for the future are. We will describe the services' main design, integration and structure, giving special attention to the new requirements from the community of users in terms of collaboration and integration tools and how we address this challenge when defining new services based on GitLab for collaboration and Code Review and Jenkins for Continuous Integration. These new services complement the existing ones to create a new global \"development service\".", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "332594c45718a46f96157256a16f5a04", "affiliation": "Warsaw University of Technology (PL)", "_fossil": "contributionParticipationMetadata", "fullName": "TRZCINSKA, Anna", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "85af41d4db34744c700d1b3312c0c6e0", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ASBURY, David", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "8eaa52c2690c8404ba595dd969b19de2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "HOIMYR, Nils", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "e79fbfa7642755008e862ffde72f0431", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MEINHARD, Helge", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/110", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:15:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "113", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5ae4855aa9d3c6685bd9f450814f9af6", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "STAGNI, Federico", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "5ae4855aa9d3c6685bd9f450814f9af6", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "STAGNI, Federico", "id": "2"}], "title": "Pilots 2.0: DIRAC pilots for all the skies", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T01:57:13.826650+00:00", "description": "", "title": "PilotsForAllSkies.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/113\/attachments\/578866\/797064\/PilotsForAllSkies.pdf", "filename": "PilotsForAllSkies.pdf", "content_type": "application\/pdf", "type": "file", "id": 797064, "size": 872205}, {"_type": "attachment", "modified_dt": "2015-04-12T01:57:13.826650+00:00", "description": "", "title": "PilotsForAllSkies.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/113\/attachments\/578866\/797065\/PilotsForAllSkies.pptx", "filename": "PilotsForAllSkies.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797065, "size": 363015}], "title": "Slides", "default_folder": false, "id": 578866, "description": ""}], "_type": "Contribution", "description": "In the last few years, new types of computing infrastructures, such as IAAS (Infrastructure as a Service) and IAAC (Infrastructure as a Client), gained popularity. New resource may come as part of pledged resources, while others are in the form of opportunistic ones. Most of these new infrastructures are based on virtualization techniques, others don't. Meanwhile, some concepts, such as distributed queues, lost appeal, while still supporting a vast amount of resources. Virtual Organizations are therefore facing heterogeneity of the available resources and the use of an Interware software like DIRAC to hide the diversity of underlying resources has become essential.\r\nThe DIRAC WMS is based on the concept of pilot jobs that was introduced back in 2004. A pilot is what creates the possibility to run jobs on a worker node. The advantages of the pilot job concept are now well established. The pilots are not only increasing the visible efficiency of the user jobs but also help to manage the heterogeneous computing resources presenting them to the central services in a uniform coherent way. Within DIRAC, we developed a new generation of pilot jobs, that we dubbed Pilots 2.0. Pilots 2.0 are not tied to a specific infrastructure; rather they are generic, fully configurable and extendible pilots. A Pilot 2.0 can be sent, as a script to be run, or it can be fetched from a remote location. A pilot 2.0 can run on every computing resource, e.g.: on CREAM Computing elements, on DIRAC Computing elements, on Virtual Machines as part of the contextualization script, or IAAC resources, provided that these machines are properly configured, hiding all the details of the WN's infrastructure. Pilots 2.0 can be generated server and client side. Pilots 2.0 are the \"pilots to fly in all the skies\", aiming at easy use of computing power, in whatever form it is presented. Another aim is the unification and simplification of the monitoring infrastructure for all kind of computing resources by using pilots as a network of distributed sensors coordinated by a central resource monitoring system.\r\nPilots 2.0 have been developed using the command pattern: each command is realizing an atomic function, and can be easily activated and de-activated based on the WN type. VOs using DIRAC can tune pilots 2.0 as they need, and extend or replace each and every pilot command in an easy way. In this paper we describe how Pilots 2.0 work with distributed and heterogeneous resources providing the abstraction necessary to deal with different kind of computing resources.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578866", "resources": [{"_type": "LocalFile", "name": "PilotsForAllSkies.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/113\/attachments\/578866\/797064\/PilotsForAllSkies.pdf", "fileName": "PilotsForAllSkies.pdf", "_fossil": "localFileMetadata", "id": "797064", "_deprecated": true}, {"_type": "LocalFile", "name": "PilotsForAllSkies.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/113\/attachments\/578866\/797065\/PilotsForAllSkies.pptx", "fileName": "PilotsForAllSkies.pptx", "_fossil": "localFileMetadata", "id": "797065", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "2186fba70f2f17dd5000102c658bbb8a", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "LUZZI, Cinzia", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "9e7dd8f84b1524d686815744ec524fea", "affiliation": "University of Manchester (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "MCNAB, Andrew", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "abfcc508622af21078d4ac47c92ceff6", "affiliation": "CPPM, Aix-Marseille Universit\u00e9, CNRS\/IN2P3, Marseille, France", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. TSAREGORODTSEV, Andrei", "id": "3"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/113", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "253", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6f6a0a09bedfe285f5c3b56598a01274", "affiliation": "Universitat Aut\u00f2noma de Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "DELFINO REZNICEK, Manuel", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "b9484a9341914ec69fc4a3f599f16826", "affiliation": "Institut de F\u00edsica d'Altes Energies (IFAE)", "_fossil": "contributionParticipationMetadata", "fullName": "ACIN PORTELLA, Vanessa", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "6f6a0a09bedfe285f5c3b56598a01274", "affiliation": "Universitat Aut\u00f2noma de Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "DELFINO REZNICEK, Manuel", "id": "1"}], "title": "Free cooling on the Mediterranean shore: Energy efficiency upgrades at PIC", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:30:06.342402+00:00", "description": "", "title": "PIC_Free_Cooling_CHEP_A0.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/253\/attachments\/578867\/797066\/PIC_Free_Cooling_CHEP_A0.pdf", "filename": "PIC_Free_Cooling_CHEP_A0.pdf", "content_type": "application\/pdf", "type": "file", "id": 797066, "size": 10981101}], "title": "Poster", "default_folder": false, "id": 578867, "description": ""}], "_type": "Contribution", "description": "Energy consumption is an increasing concern for data centers. This paper summarizes recent energy efficiency upgrades at the Port d\u2019Informaci\u00f3 Cient\u00edfica (PIC) in Barcelona, Spain which have considerably lowered energy consumption. The upgrades were particularly challenging, as they involved modifying the already existing machine room, which is shared by PIC with the general IT services of the Universitat Aut\u00f2noma de Barcelona (UAB), with all the services in full operation, as well as the introduction of \u201cfree cooling\u201d techniques in a location 20 Km from the Mediterranean sea. The upgrades targeted three distinct areas: First, the segregation of hot and cold air zones using and innovative horizontal layout, where hot air is channelled through openings in a false ceiling to a second story hot air plenum. This segregation allows increasing the cold air inlet temperature according to the latest ASHRAE recommendations. Second, the introduction of an outside air economizer which replaces obsolete CRAH systems with air-to-air heat exchangers. This system, built entirely from industrial components, also incorporates an adiabatic cooling module and enables the \u201cfree\u201d removal of over 300 kW of IT heat load during 6000 hours a year. Third, the introduction of UPS systems based on IGBT technology, in order to better match the impedance characteristics of the IT load. In addition, a transversal activity has been done to fully integrate cooling and UPS infrastructure monitoring into PIC\u2019s overall IT monitoring framework based on Nagios. This required development of a ModBus\/TCPIP gateway server.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578867", "resources": [{"_type": "LocalFile", "name": "PIC_Free_Cooling_CHEP_A0.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/253\/attachments\/578867\/797066\/PIC_Free_Cooling_CHEP_A0.pdf", "fileName": "PIC_Free_Cooling_CHEP_A0.pdf", "_fossil": "localFileMetadata", "id": "797066", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "6f6a87ba290ed8810c81f7053f3e9cb3", "affiliation": "Universitat Aut\u00f2noma de Barcelona", "_fossil": "contributionParticipationMetadata", "fullName": "HERBERA LUNA, Adri\u00e0", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "492546c783f2089a1e53f3d61983a1f6", "affiliation": "Universitat Aut\u00f2noma de Barcelona", "_fossil": "contributionParticipationMetadata", "fullName": "HERN\u00c1NDEZ S\u00c1NCHEZ, Jordi", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/253", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:15:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "119", "speakers": [{"_type": "ContributionParticipation", "emailHash": "068b9018b69ea40467e02d92d6a35844", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "HUFNAGEL, Dirk", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "068b9018b69ea40467e02d92d6a35844", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "HUFNAGEL, Dirk", "id": "0"}], "title": "The CMS Tier-0 goes Cloud and Grid for LHC Run 2", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T08:36:59.530486+00:00", "description": "", "title": "dirk.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/119\/attachments\/578868\/797067\/dirk.pdf", "filename": "dirk.pdf", "content_type": "application\/pdf", "type": "file", "id": 797067, "size": 488696}], "title": "Slides", "default_folder": false, "id": 578868, "description": ""}], "_type": "Contribution", "description": "In 2015, CMS will embark on a new era of collecting LHC collisions at unprecedented rates and complexity. This will put a tremendous stress on our computing systems. Prompt Processing of the raw data by the Tier-0 infrastructure will no longer be constrained to CERN alone due to the significantly increased resource requirements. In LHC Run 2, we will need to operate it as a distributed system utilizing both the CERN Cloud-based Agile Infrastructure and a significant fraction of the CMS Tier-1 Grid resources. In another big change for LHC Run 2, we will process all data using the multi-threaded framework to deal with the increased event complexity and to ensure efficient use of the resources. This contribution will cover the evolution of the Tier-0 infrastructure and present scale testing results and experiences from the first data taking in 2015.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578868", "resources": [{"_type": "LocalFile", "name": "dirk.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/119\/attachments\/578868\/797067\/dirk.pdf", "fileName": "dirk.pdf", "_fossil": "localFileMetadata", "id": "797067", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "c64735b9540bc92c4d230a2ca252b7cf", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GUTSCHE, Oliver", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "fff8dfb14c994e489cb16579b5ad1791", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CONTRERAS, Luis", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "1ab30f4066c48fc202816d553d46d1fe", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "TIRADANI, Anthony", "id": "3"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/119", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "118", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e8fb00f4f09841337efaeff677169cd2", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "SEXTON-KENNEDY, Elizabeth", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "bfd891314296e5e4c00c4d56555d463a", "affiliation": "University of Notre Dame (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MC CAULEY, Thomas", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "e8fb00f4f09841337efaeff677169cd2", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "SEXTON-KENNEDY, Elizabeth", "id": "1"}], "title": "Open access to high-level data and analysis tools in the CMS experiment at the LHC", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The CMS experiment, in recognition of its commitment to data preservation and open access as well as to education and outreach, has made its first public release of high-level data: up to half of the proton-proton collision data at 7 TeV from 2010 in CMS Analysis Object Data format. CMS has prepared, in collaboration with CERN and the other LHC experiments, an open data web portal based on Invenio. The portal provides access to CMS public data as well as to analysis tools and documentation for the public. The tools include an event display and histogram application that run in the browser. In addition a virtual machine is available which contains a CMS software environment along with XRootD access to the data. Within the virtual machine the public can analyse CMS data; example code is provided. We describe the accompanying tools and documentation and discuss the first experience of data use.", "track": "Track5: Computing activities and Computing models", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/118", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:30:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "429", "speakers": [{"_type": "ContributionParticipation", "emailHash": "b3766e1bb47ff6f9cdf65d395d9c0ad8", "affiliation": "Lawrence Livermore Nat. Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LANGE, David", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "b3766e1bb47ff6f9cdf65d395d9c0ad8", "affiliation": "Lawrence Livermore Nat. Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LANGE, David", "id": "0"}], "title": "Simulation and Reconstruction Upgrades for the CMS experiment", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T10:05:24.155096+00:00", "description": "", "title": "chep2015_recoSimUpgrades.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/429\/attachments\/578869\/797068\/chep2015_recoSimUpgrades.pdf", "filename": "chep2015_recoSimUpgrades.pdf", "content_type": "application\/pdf", "type": "file", "id": 797068, "size": 2568070}, {"_type": "attachment", "modified_dt": "2015-04-12T10:05:24.155096+00:00", "description": "", "title": "chep2015_recoSimUpgrades.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/429\/attachments\/578869\/797069\/chep2015_recoSimUpgrades.pptx", "filename": "chep2015_recoSimUpgrades.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797069, "size": 5084256}], "title": "Slides", "default_folder": false, "id": 578869, "description": ""}], "_type": "Contribution", "description": "Over the past several years, the CMS experiment has made significant changes to its detector simulation and reconstruction applications motivated by the planned program of detector upgrades over the next decade. These upgrades include both completely new tracker and calorimetry systems and changes to essentially all major detector components to meet the requirements of very high pileup operations. We have untaken an ambitious program to implement these changes into the Geant4-based simulation and reconstruction applications of CMS in order to perform physics analyses to both demonstrate the improvements from the detector upgrades and to influence the detector design itself. In this presentation, we will discuss our approach to generalizing much of the CMS codebase to efficiently and effectively work for multiple detector designs. We will describe our computational solutions for very high pileup reconstruction and simulation algorithms, and show results based on our work.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578869", "resources": [{"_type": "LocalFile", "name": "chep2015_recoSimUpgrades.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/429\/attachments\/578869\/797068\/chep2015_recoSimUpgrades.pdf", "fileName": "chep2015_recoSimUpgrades.pdf", "_fossil": "localFileMetadata", "id": "797068", "_deprecated": true}, {"_type": "LocalFile", "name": "chep2015_recoSimUpgrades.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/429\/attachments\/578869\/797069\/chep2015_recoSimUpgrades.pptx", "fileName": "chep2015_recoSimUpgrades.pptx", "_fossil": "localFileMetadata", "id": "797069", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/429", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:45:00"}, "duration": 15, "session": "Track 8 Session", "keywords": [], "id": "428", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5b0402a9cd32d8915ceeb56718af64b6", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CANAL, Philippe", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "5b0402a9cd32d8915ceeb56718af64b6", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CANAL, Philippe", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "73c47f432ac574bc76aba1a48761b1ff", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "JUN, Soon Yung", "id": "1"}], "title": "Detector Simulation On Modern Coprocessors", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T07:40:08.845927+00:00", "description": "", "title": "chep2015-vecphys.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/428\/attachments\/578870\/797070\/chep2015-vecphys.pdf", "filename": "chep2015-vecphys.pdf", "content_type": "application\/pdf", "type": "file", "id": 797070, "size": 1106523}, {"_type": "attachment", "modified_dt": "2015-04-13T07:40:08.845927+00:00", "description": "", "title": "chep2015-vecphys.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/428\/attachments\/578870\/797071\/chep2015-vecphys.pptx", "filename": "chep2015-vecphys.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797071, "size": 1455693}], "title": "Slides", "default_folder": false, "id": 578870, "description": ""}], "_type": "Contribution", "description": "The recent prevalence of hardware architectures of many-core or accelerated\r\nprocessors opens opportunities for concurrent programming models taking\r\nadvantages of both SIMD and SIMT architectures. The Geant Vector Prototype\r\nhas been designed both to exploit the vector capability of main stream\r\nCPUs and to take advantage of Coprocessors including NVidia\u2019s GPU and Intel\r\nXeon Phi. The characteristics of each of those architectures are very\r\ndifferent in term of the vectorization depth, parallelization needed to achieve\r\noptimal performance or memory access latency and speed. Between each\r\nplatforms the number of individual tasks to be processed \u2018at once\u2019 for\r\nefficient use of the hardware varies sometimes by an order of magnitude.\r\nThe granularity of the code executed may also need to be dynamically adjusted.\r\nAn additional challenge is to avoid the code duplication often inherent to\r\nsupporting heterogeneous platforms. We will present the challenges, solutions\r\nand resulting performance of running an end to end detector simulation\r\nconcurrently on a main stream CPU and a coprocessor and detail the broker\r\nimplementation bridging the disparity between the two architectures. The\r\nimpacts of task decomposition, vectorization, efficient sampling techniques\r\nand data look-up using track level parallelism will be also evaluated on\r\nvector and massively parallel architectures.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578870", "resources": [{"_type": "LocalFile", "name": "chep2015-vecphys.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/428\/attachments\/578870\/797070\/chep2015-vecphys.pdf", "fileName": "chep2015-vecphys.pdf", "_fossil": "localFileMetadata", "id": "797070", "_deprecated": true}, {"_type": "LocalFile", "name": "chep2015-vecphys.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/428\/attachments\/578870\/797071\/chep2015-vecphys.pptx", "fileName": "chep2015-vecphys.pptx", "_fossil": "localFileMetadata", "id": "797071", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "5e451b3b7969b96f79563ef9500e6f3e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GHEATA, Andrei", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "e7e207b4c5969bac3a9c7e859482b70a", "affiliation": "FermiLab (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LIMA, Guilherme", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "8c01aed7220e755911911d2e59aa3676", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "NOVAK, Mihaly", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "362d1634ae1bc01cde64fb0881d042d4", "affiliation": "Bhabha Atomic Research Centre (IN)", "_fossil": "contributionParticipationMetadata", "fullName": "SEHGAL, Raman", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "f5e7378e16e9fee96ad835deb6454086", "affiliation": "National Technical Univ. of Ukraine \"Kyiv Polytechnic Institute", "_fossil": "contributionParticipationMetadata", "fullName": "SHADURA, Oksana", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "7e8bcc2855b51c4e93b19e095a8b2ec9", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "WENZEL, Sandro Christian", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "ddb4e4ec90b85b573b5dd09e3f0e05c2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "APOSTOLAKIS, John", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "f4b45dbfcfef5c90c1b484b28e777c71", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "BANDIERAMONTE, Marilena", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "fc171e67d3d8e09687c21b9a42fdc39f", "affiliation": "National and Kapodistrian University of Athens (GR)", "_fossil": "contributionParticipationMetadata", "fullName": "BITZES, Georgios", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "6a2d92d7b45e7af553c5547d2f944431", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BRUN, Rene", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "94c96cb9abacdef3f192e4dff0f13905", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. CARMINATI, Federico", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "565043fdb94911d4ac35d97cfc629533", "affiliation": "University of Copenhagen (DK)", "_fossil": "contributionParticipationMetadata", "fullName": "DE FINE LICHT, Johannes Christof", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "50198189b9d7bc94a02bd81c85665ef9", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "DUHEM, Laurent", "id": "14"}, {"_type": "ContributionParticipation", "emailHash": "5a02abf7bf489401510b64b45e39a213", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "ELVIRA, Victor Daniel", "id": "15"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/428", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "534", "speakers": [{"_type": "ContributionParticipation", "emailHash": "cbfea1a9fb316e8f29b8b7a1430813a3", "affiliation": "University of Tsukuba", "_fossil": "contributionParticipationMetadata", "fullName": "YOSHIE, Tomoteru", "id": "26"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c1b7cb9e71f353d0ceda1edf38422f90", "affiliation": "University of Tsukuba", "_fossil": "contributionParticipationMetadata", "fullName": "AMAGASA, Toshiyuki", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "7af24c6a2607a4e6e9e2ae714ee6a2da", "affiliation": "University of Tsukuba", "_fossil": "contributionParticipationMetadata", "fullName": "TATEBE, Osamu", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "cbfea1a9fb316e8f29b8b7a1430813a3", "affiliation": "University of Tsukuba", "_fossil": "contributionParticipationMetadata", "fullName": "YOSHIE, Tomoteru", "id": "0"}], "title": "Sharing lattice QCD data over a widely distributed file system", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T21:01:28.826627+00:00", "description": "", "title": "CHEP2015-Yoshie-534.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/534\/attachments\/578871\/797072\/CHEP2015-Yoshie-534.pdf", "filename": "CHEP2015-Yoshie-534.pdf", "content_type": "application\/pdf", "type": "file", "id": 797072, "size": 9902028}], "title": "Poster", "default_folder": false, "id": 578871, "description": ""}], "_type": "Contribution", "description": "JLDG is a data-grid for the lattice QCD (LQCD) community in\r\nJapan. Several large research groups in Japan have been working on\r\nlattice QCD simulations using supercomputers distributed over distant\r\nsites. The JLDG provides such collaborations with an efficient method of\r\ndata management and sharing. \r\n\r\nFile servers installed on 9 sites are connected to the NII SINET VPN\r\ncalled HEPnet-J\/sc and are bound into a single file system with the\r\nGFarm, a grid-base file system software. Because the file system looks\r\nthe same from any sites, users can do analyses (measurement of physical\r\nquantities) on a supercomputer on a site, using data generated and\r\nstored in the JLDG at a different site. \r\n\r\nSince the official start of operation in 2008, the JLDG has been\r\nimproved in various ways. Among others, the following two have\r\ndrastically improved usability of the JLDG. 1) Implementation of FUSE\r\nmount which enables users to mount the JLDG file system as a unix file\r\nsystem. 2) Development of fast data copy sub-system between the JLDG and\r\nthe HPCI Shared Storage, where the HPCI is a Japanese national project\r\nstarted in 2011 for constructing High Performance Computing\r\nInfrastructure.  \r\n\r\nWe present a brief description of hardware and software of the JLDG,\r\nfocusing mainly on the above two improvements and report performance and\r\nstatistics of the JLDG. As of October 2014, 11 research groups \r\n(66 users) store their daily research data of 4.0PB including replica\r\nand 63 million files in total. Number of publications for works used the\r\nJLDG is 105. The large number of publications and recent rapid increase\r\nof disk usage convince us that the JLDG has grown up into a useful\r\ninfrastructure for LQCD community in Japan.", "track": "Track3: Data store and access ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578871", "resources": [{"_type": "LocalFile", "name": "CHEP2015-Yoshie-534.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/534\/attachments\/578871\/797072\/CHEP2015-Yoshie-534.pdf", "fileName": "CHEP2015-Yoshie-534.pdf", "_fossil": "localFileMetadata", "id": "797072", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "e5c54e6e296c08d305a286a97736f58d", "affiliation": "University of Tsukuba", "_fossil": "contributionParticipationMetadata", "fullName": "AOKI, Sinya", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "3a1795095fd40b723402be3aab5f5c03", "affiliation": "Nagoya University", "_fossil": "contributionParticipationMetadata", "fullName": "AOKI, Yasumichi", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "be96372946bf7b1dc73b39533f24c110", "affiliation": "Nagoya University", "_fossil": "contributionParticipationMetadata", "fullName": "AOYAMA, Tatsumi", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "8b795ddc942c66e89401b92cc7717e09", "affiliation": "RIKEN", "_fossil": "contributionParticipationMetadata", "fullName": "DOI, Takumi", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "88128bbee1c093adbbdb40bd52136464", "affiliation": "Kyoto University", "_fossil": "contributionParticipationMetadata", "fullName": "FUKUMURA, Kazumi", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "729616a016cbf5995023c09e83702135", "affiliation": "Osaka University", "_fossil": "contributionParticipationMetadata", "fullName": "ISHII, Noriyoshi", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "3d2132879f6d0de87af7e61812c073d5", "affiliation": "Hiroshima University", "_fossil": "contributionParticipationMetadata", "fullName": "ISHIKAWA, Ken-Ichi", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "ea7da09ba8971799f109b9920e257272", "affiliation": "Tokyo Institute of Technology", "_fossil": "contributionParticipationMetadata", "fullName": "JITSUMOTO, Hideyuki", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "a609b78333a9a46d2e9388acef68844a", "affiliation": "Osaka University", "_fossil": "contributionParticipationMetadata", "fullName": "KAMANO, Hiroyuki", "id": "14"}, {"_type": "ContributionParticipation", "emailHash": "92c95decf5f4aed671b79473d19dfdde", "affiliation": "Hitachi Solutions East Japan,Ltd.", "_fossil": "contributionParticipationMetadata", "fullName": "KONNO, Yukiko", "id": "15"}, {"_type": "ContributionParticipation", "emailHash": "e19e172c669f300de25bd9ec05dc9a56", "affiliation": "High Energy Accelerator Research Organization (KEK)", "_fossil": "contributionParticipationMetadata", "fullName": "MATSUFURU, Hideo", "id": "16"}, {"_type": "ContributionParticipation", "emailHash": "9cc7f12ab874d6385d79c6a88eef1937", "affiliation": "Hitachi Solutions East Japan, Ltd.", "_fossil": "contributionParticipationMetadata", "fullName": "MIKAMI, Yoshiaki", "id": "17"}, {"_type": "ContributionParticipation", "emailHash": "398bf9830072548384bc377ce3dcb09e", "affiliation": "Nagoya University", "_fossil": "contributionParticipationMetadata", "fullName": "MIURA, Kohtaroh", "id": "25"}, {"_type": "ContributionParticipation", "emailHash": "34776ae4aa505fc97632a359db8f28f1", "affiliation": "University of Tsukuba", "_fossil": "contributionParticipationMetadata", "fullName": "SATO, Mitsuhisa", "id": "24"}, {"_type": "ContributionParticipation", "emailHash": "3b4b49b953a3a2353c983d5a1d1e449c", "affiliation": "Kanazawa university", "_fossil": "contributionParticipationMetadata", "fullName": "TAKEDA, Shinji", "id": "18"}, {"_type": "ContributionParticipation", "emailHash": "045a910205c882b55f4df5cd588e8a3c", "affiliation": "Osaka University", "_fossil": "contributionParticipationMetadata", "fullName": "TOGAWA, Hiroaki", "id": "19"}, {"_type": "ContributionParticipation", "emailHash": "10e215fabb896e2d3abf9a66b0679333", "affiliation": "RIKEN Advanced Institute for Computational Science", "_fossil": "contributionParticipationMetadata", "fullName": "UKAWA, Akira", "id": "20"}, {"_type": "ContributionParticipation", "emailHash": "f41186fefe6ea87ea7e32cb8b43d3a2b", "affiliation": "University of Tsukuba", "_fossil": "contributionParticipationMetadata", "fullName": "UKITA, Naoya", "id": "21"}, {"_type": "ContributionParticipation", "emailHash": "f62554df9665bad5c77a9945c11804e4", "affiliation": "RIKEN", "_fossil": "contributionParticipationMetadata", "fullName": "WATANABE, Yasushi", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "93b9dc935f42187b211bb6db3053b729", "affiliation": "University of Tsukuba", "_fossil": "contributionParticipationMetadata", "fullName": "YAMAZAKI, Takeshi", "id": "13"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/534", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:15:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:30:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "421", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a361249913aed7f4211e409f162867df", "affiliation": "Femilab", "_fossil": "contributionParticipationMetadata", "fullName": "KUTSCHKE, Robert", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "a361249913aed7f4211e409f162867df", "affiliation": "Femilab", "_fossil": "contributionParticipationMetadata", "fullName": "KUTSCHKE, Robert", "id": "0"}], "title": "Breaking the Silos: The art Documentation Suite", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T02:44:47.824272+00:00", "description": "", "title": "artdoc.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/421\/attachments\/578872\/797073\/artdoc.pdf", "filename": "artdoc.pdf", "content_type": "application\/pdf", "type": "file", "id": 797073, "size": 626895}], "title": "Slides", "default_folder": false, "id": 578872, "description": ""}], "_type": "Contribution", "description": "The art event processing framework is used by almost all new experiments at Fermilab, and by several outside of Fermilab. All use art as an external product in the same sense that the compiler, ROOT, Geant4, CLHEP and boost are external products. The art team has embarked on a campaign to document art and develop training materials for new users. Many new users of art have little or no knowledge of C++, software engineering, build systems or the many external packages used by art or their experiments, such as ROOT, CLHEP, HEPPDT, and boost.  To effectively teach art requires that the training materials include appropriate introductions to these topics as they are encountered.\r\nExperience has shown that simply refering readers to the existing native documentation does not work; too often a simple idea that they need to understand is described in a context that presumes prerequisites that are unimportant for a beginning user of art.\r\nThere is the additional complication that the training materials must be presented in a way that does not presume knowledge of any of the experiments using art.  Finally, new users of art arrive at random times throughout the year and the training materials must allow them to start to learn art at any time. This presentation will explain the strategies adopted by the art team to develop a documentation suite that complies with these boundary conditions. It will also show the present status of the documentation suite, including feedback we have received from pilot users.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578872", "resources": [{"_type": "LocalFile", "name": "artdoc.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/421\/attachments\/578872\/797073\/artdoc.pdf", "fileName": "artdoc.pdf", "_fossil": "localFileMetadata", "id": "797073", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "fe605a2b3b5c9c50970007d7ad02a6dd", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PATERNO, Marc", "id": "1"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/421", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "420", "speakers": [{"_type": "ContributionParticipation", "emailHash": "07fcf6b59f67c4b6bea1d11ef506274d", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LAVRIJSEN, Wim", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "07fcf6b59f67c4b6bea1d11ef506274d", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LAVRIJSEN, Wim", "id": "0"}], "title": "Deep Integration: Python in the Cling World", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-08T15:58:24.879231+00:00", "description": "", "title": "WimLavrijsen_CHEP2015_PyROOT_Status_Poster_FINAL.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/420\/attachments\/578873\/797074\/WimLavrijsen_CHEP2015_PyROOT_Status_Poster_FINAL.pdf", "filename": "WimLavrijsen_CHEP2015_PyROOT_Status_Poster_FINAL.pdf", "content_type": "application\/pdf", "type": "file", "id": 797074, "size": 3462248}], "title": "Poster", "default_folder": false, "id": 578873, "description": ""}], "_type": "Contribution", "description": "The language improvements in C++11\/14 greatly reduce the amount of boilerplate code required and allow resource ownership to be clarified in interfaces. On top, the Cling C++ interpreter brings a truly interactive experience and real dynamic behavior to the language. Taken together, these developments bring C++ much closer to Python in ability, allowing the combination of PyROOT\/cppyy and Cling to integrate the two languages on a new level. This paper describes the current state of the art, including cross-language callbacks, automatic resource management, software transactional memory, automatic template instantiations, and the ability to use Python from Cling.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578873", "resources": [{"_type": "LocalFile", "name": "WimLavrijsen_CHEP2015_PyROOT_Status_Poster_FINAL.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/420\/attachments\/578873\/797074\/WimLavrijsen_CHEP2015_PyROOT_Status_Poster_FINAL.pdf", "fileName": "WimLavrijsen_CHEP2015_PyROOT_Status_Poster_FINAL.pdf", "_fossil": "localFileMetadata", "id": "797074", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/420", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:30:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "423", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e8fb00f4f09841337efaeff677169cd2", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "SEXTON-KENNEDY, Elizabeth", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e8fb00f4f09841337efaeff677169cd2", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "SEXTON-KENNEDY, Elizabeth", "id": "0"}], "title": "A Comparative Analysis of Event Processing Frameworks used in HEP", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:01:24.073478+00:00", "description": "", "title": "FW2015CHEPtalk.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/423\/attachments\/578874\/797075\/FW2015CHEPtalk.pdf", "filename": "FW2015CHEPtalk.pdf", "content_type": "application\/pdf", "type": "file", "id": 797075, "size": 148803}], "title": "Slides", "default_folder": false, "id": 578874, "description": ""}], "_type": "Contribution", "description": "Today there are many different experimental event processing frameworks in use by running or about to be running experiments. This talk will compare and contrast the different components of these frameworks and highlight the different solutions chosen by different groups.\u00a0 In the past there have been attempts at shared framework projects for example the collaborations on the BaBar framework (between BaBar, CDF, and CLEO), on the Gaudi framework (between LHCb and ATLAS), on AliROOT\/FairROOT (between Alice and GSI\/Fair),\u00a0and in some ways on art (Fermilab based experiments) and CMS\u2019 framework. \u00a0However, for reasons that will be discussed, these collaborations did not result in common frameworks shared among the intended experiments. Though importantly, two of the resulting projects have succeeded in providing frameworks that are shared among many customer experiments: Fermilab's art framework and\u00a0GSI\/Fair's FairROOT.\u00a0Interestingly, several projects are considering remerging their frameworks after many years apart. I'll report on an investigation\u00a0and analysis of these realities.\u00a0 With the advent of the need for multi-threaded frameworks and the scarce available manpower, it is important to collaborate in the future; however it is also important to understand why previous attempts at multi-experiment frameworks either worked or didn\u2019t work.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578874", "resources": [{"_type": "LocalFile", "name": "FW2015CHEPtalk.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/423\/attachments\/578874\/797075\/FW2015CHEPtalk.pdf", "fileName": "FW2015CHEPtalk.pdf", "_fossil": "localFileMetadata", "id": "797075", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/423", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:45:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "422", "speakers": [{"_type": "ContributionParticipation", "emailHash": "7553d4db8d9901e91d5bbb73ec902007", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. AL-TURANY, Mohammad", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "7553d4db8d9901e91d5bbb73ec902007", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. AL-TURANY, Mohammad", "id": "0"}], "title": "ALFA: The new ALICE-FAIR software framework", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T00:05:31.489216+00:00", "description": "", "title": "ALFA_Chep_2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/422\/attachments\/578875\/797076\/ALFA_Chep_2015.pdf", "filename": "ALFA_Chep_2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 797076, "size": 1668216}, {"_type": "attachment", "modified_dt": "2015-04-14T00:05:31.489216+00:00", "description": "", "title": "ALFA_Chep_2015.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/422\/attachments\/578875\/797077\/ALFA_Chep_2015.pptx", "filename": "ALFA_Chep_2015.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797077, "size": 1468252}], "title": "Slides", "default_folder": false, "id": 578875, "description": ""}], "_type": "Contribution", "description": "The commonalities between the ALICE and FAIR experiments and their computing requirements lead to the development of large parts of a common software framework in an experiment independent way.  The FairRoot project has already shown the feasibility of such an approach for the FAIR experiments and extending it beyond FAIR to experiments at other facilities. The ALFA framework is a joint development between ALICE Online-Offline (O2) and FairRoot teams.\r\n\r\nALFA is designed as a flexible, elastic system, which balances reliability and ease of development with performance using multi-processing and multi-threading. A message-based approach has been adopted; such an approach will support the use of the software on different hardware platforms, including heterogeneous systems. Each process in ALFA assumes limited communication and reliance on other processes. Such a design will add horizontal scaling (multiple processes) to \u201cvertical scaling\u201d provided by multiple threads to meet computing and throughput demands.\r\nALFA does not dictate any application protocols. Potentially, any content-based processor or any source can change the application protocol. The framework supports different serialization standards for data exchange between different hardware and software languages. The concept and design of this new framework as well as the already implemented set of utilities and interfaces will be presented.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578875", "resources": [{"_type": "LocalFile", "name": "ALFA_Chep_2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/422\/attachments\/578875\/797076\/ALFA_Chep_2015.pdf", "fileName": "ALFA_Chep_2015.pdf", "_fossil": "localFileMetadata", "id": "797076", "_deprecated": true}, {"_type": "LocalFile", "name": "ALFA_Chep_2015.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/422\/attachments\/578875\/797077\/ALFA_Chep_2015.pptx", "fileName": "ALFA_Chep_2015.pptx", "_fossil": "localFileMetadata", "id": "797077", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "3f41c6112486480c3ebd506d39003e9e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BUNCIC, Predrag", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "0bb08d2c00ac538180032342288ed2b3", "affiliation": "GSI - Helmholtzzentrum fur Schwerionenforschung GmbH (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "KOLLEGGER, Thorsten Sven", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "c41f10a395fb3c00d6b4ce69a53a3e17", "affiliation": "Johann-Wolfgang-Goethe Univ. (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "LINDENSTRUTH, Volker", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "adad1c096b61f79e930480c8d88632ea", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "VANDE VYVRE, Pierre", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "b20a73292573f0a7430f708c8a1903e6", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "HRISTOV, Peter", "id": "5"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/422", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:15:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:30:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "425", "speakers": [{"_type": "ContributionParticipation", "emailHash": "300bdfcba5517e107932a61321652a75", "affiliation": "GSI - Helmholtzzentrum fur Schwerionenforschung GmbH (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "KRESAN, Dmytro", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "300bdfcba5517e107932a61321652a75", "affiliation": "GSI - Helmholtzzentrum fur Schwerionenforschung GmbH (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "KRESAN, Dmytro", "id": "0"}], "title": "Online\/Offline reconstruction of trigger-less readout in the R3B experiment at FAIR", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T02:42:37.843498+00:00", "description": "", "title": "CHEP2015_kresan.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/425\/attachments\/578876\/797078\/CHEP2015_kresan.pdf", "filename": "CHEP2015_kresan.pdf", "content_type": "application\/pdf", "type": "file", "id": 797078, "size": 2351450}, {"_type": "attachment", "modified_dt": "2015-04-13T02:42:37.843498+00:00", "description": "", "title": "CHEP2015_kresan.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/425\/attachments\/578876\/797079\/CHEP2015_kresan.pptx", "filename": "CHEP2015_kresan.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797079, "size": 1986174}], "title": "Slides", "default_folder": false, "id": 578876, "description": ""}], "_type": "Contribution", "description": "The R3B (Reactions with Rare Radioactive Beams) experiment is one of the planned experiments at the future FAIR facility at GSI Darmstadt. R3B will cover experimental reaction studies with exotic nuclei far off stability, thus enabling a broad physics programs with rare-isotope beams with emphasis on nuclear structure and dynamics. Several different detection subsystems as well as sophisticated DAQ system and data-analysis software are being developed for this purpose.\r\nThe data analysis software for R3B is based on FairRoot framework and called R3BRoot.   R3BRoot is being used for simulation and detector design studies for the last few years. Recently, it was successfully used directly with the data acquisition and for the analysis of the R3B test beam-time in April 2014. For the future beam times the framework has to deal with the free streaming readout of the detectors.  The implementation within R3BRoot to fulfill this trigger-less run mode will be presented as well as the set of tools developed for the online reconstruction and quality assurance of the data during the run.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578876", "resources": [{"_type": "LocalFile", "name": "CHEP2015_kresan.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/425\/attachments\/578876\/797078\/CHEP2015_kresan.pdf", "fileName": "CHEP2015_kresan.pdf", "_fossil": "localFileMetadata", "id": "797078", "_deprecated": true}, {"_type": "LocalFile", "name": "CHEP2015_kresan.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/425\/attachments\/578876\/797079\/CHEP2015_kresan.pptx", "fileName": "CHEP2015_kresan.pptx", "_fossil": "localFileMetadata", "id": "797079", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "7553d4db8d9901e91d5bbb73ec902007", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. AL-TURANY, Mohammad", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "6959ac7e775d5ad7e2efb9311e584779", "affiliation": "GSI - Helmholtzzentrum fur Schwerionenforschung GmbH (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "UHLIG, Florian", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "e796c948ea7dc1856af5fd64e6105231", "affiliation": "GSI Darmstadt", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BERTINI, Denis", "id": "3"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/425", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": null, "keywords": [], "id": "424", "speakers": [{"_type": "ContributionParticipation", "emailHash": "401695ca248365aedad9213722d76b55", "affiliation": "Department of Physics", "_fossil": "contributionParticipationMetadata", "fullName": "GREEN, Christopher", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "401695ca248365aedad9213722d76b55", "affiliation": "Department of Physics", "_fossil": "contributionParticipationMetadata", "fullName": "GREEN, Christopher", "id": "0"}], "title": "A flexible, extensible software development ecosystem for HEP collaborations", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "Over the recent two decades the explosion in the number of people in\r\neach of the largest HEP experiment collaborations has allowed those\r\ncollaborations to develop experiment-specific solutions to many of the\r\nproblems of software development: the management of source code for\r\ncontrolled collaborative development, the building and testing of\r\ncomplex and interdependent software artifacts, and the delivery to both\r\ndevelopers and users the products of building the software. However, it\r\nis not necessarily in the interest of each new experiment to re-invent\r\nthese tools, nor is it the best use of the limited resources of the HEP\r\ncommunity to build their own full-featured solutions. To solve these\r\nproblems, the Scientific Software Infrastructure group in the Fermilab\r\nScientific Computing Division have developed an \"ecosystem\" of\r\ncooperating practices and tools to satisfy the needs of a typical\r\ncollaboration. A powerful synergy of the sharing of practices and tools\r\nis that members of the increasingly mobile community can use the\r\nknowledge they acquire on one experiment when contributing to another\r\nexperiment.\r\n\r\nThese practices and tools are designed to work in concert, but the full\r\nsuite is not required. The system is intentionally weakly-coupled, so\r\nthat an experiment may replace any element with its own preferred\r\nsolution, if some external constraint requires. Some or all of these\r\ntools are being used by most new Fermilab experiments, and several\r\noutside Fermilab. The practices and tools include:\r\n\r\n * good use of modern source code control systems,\r\n * a software product management system,\r\n * a build-and-test system supporting parallel execution,\r\n * an event-processing framework product used by multiple projects,\r\n * delivery of a set of commonly-used HEP libraries,\r\n * delivery of the compiler suite used to build all the software,\r\n * a recommended development and testing strategy, and\r\n * a requirements-driven project management methodology.\r\n\r\nIn this paper, we report on the methodologies, practices and tools, and\r\non their use by current and future experiments both at Fermilab and\r\noutside of Fermilab. We also discuss how they are used in the\r\ndevelopment of the art event-processing framework.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "6fac3d6f6a7e5fdfeca402e31f4c1868", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "KOWALKOWSKI, Jim", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "fe605a2b3b5c9c50970007d7ad02a6dd", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PATERNO, Marc", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "d8ca9d551e825229c15135d60907da60", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GARREN, Lynn", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/contribution\/424", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "427", "speakers": [{"_type": "ContributionParticipation", "emailHash": "98d2fcdd64af6e627d3a9172bbe5253e", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CABALLERO BEJAR, Jose", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "e82c46a3a0d49dd555c888d113eace3a", "affiliation": "University of Nebraska (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BOCKELMAN, Brian Paul", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "98d2fcdd64af6e627d3a9172bbe5253e", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CABALLERO BEJAR, Jose", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "e82c46a3a0d49dd555c888d113eace3a", "affiliation": "University of Nebraska (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BOCKELMAN, Brian Paul", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "2963fcad5cb2518db7dcafef841c97a5", "affiliation": "Brookhaven National Laboratory (BNL)-Unknown-Unknown", "_fossil": "contributionParticipationMetadata", "fullName": "HOVER, John", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "6a8aba43c37af88cebb6347ae8173fb5", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DE STEFANO JR, John Steven", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "191c900816db7d7f6af67c519eb6ffbc", "affiliation": "Indiana University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "TEIGE, Scott Werner", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "5afca37087cebfaeb2fdc78c241d0516", "affiliation": "University of Indiana", "_fossil": "contributionParticipationMetadata", "fullName": "NEAL, Vince", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "b9c28f3cc7a3834138497dded410dfb8", "affiliation": "Indiana University", "_fossil": "contributionParticipationMetadata", "fullName": "QUICK, Robert", "id": "6"}], "title": "Evolution of the Open Science Grid Application Software Installation Service (OASIS)", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The Open Science Grid Application Software Installation Service (OASIS) \r\nprovides an application installation service for Open Science Grid (OSG) virtual organizations (VOs) built on top of the CERN Virtual Machine File System (CVMFS).  \r\nThis paper provides an overview and progress report of the OASIS service, which has been in production for over 18 months.  \r\nOASIS can be used either directly, as a service run by OSG Operations, or as a standalone software product.  \r\nIn either use case, the VOs' files feed into a network of Stratum One replication servers, along with other partners within a federation.  \r\nThe OASIS network has the flexibility of providing pure CVMFS repositories, the OSG-run central OASIS repository, or VO-run OASIS repositories.  \r\nDepending on their needs, VOs can integrate into OASIS at several different points, leading to a more complex trust model than that of the standard CVMFS service.\r\n\r\nThis paper describes the evolution of usage and implementation of OASIS over the past 18 months, as well as its operational successes, evolutions and issues.  \r\nTopics such as monitoring and auditing of the distributed content, handling of cryptographic keys, \r\nand establishing technical and organizational trust relationships are discussed.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/427", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:45:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "426", "speakers": [{"_type": "ContributionParticipation", "emailHash": "062d1eaeaadf683ccde1fb8cb2daf5e0", "affiliation": "University of Wisconsin (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. VUOSALO, Carl", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "b3766e1bb47ff6f9cdf65d395d9c0ad8", "affiliation": "Lawrence Livermore Nat. Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LANGE, David", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "062d1eaeaadf683ccde1fb8cb2daf5e0", "affiliation": "University of Wisconsin (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. VUOSALO, Carl", "id": "1"}], "title": "MiniAOD: A new analysis data format for CMS", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T22:26:14.660853+00:00", "description": "", "title": "miniaod_main.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/426\/attachments\/578877\/797080\/miniaod_main.pdf", "filename": "miniaod_main.pdf", "content_type": "application\/pdf", "type": "file", "id": 797080, "size": 407052}], "title": "Slides", "default_folder": false, "id": 578877, "description": ""}], "_type": "Contribution", "description": "The CMS experiment has developed a new analysis object format (the \"mini-AOD\") targeted to be less than 10% of the size of the Run 1 AOD format. The motivation for the Mini-AOD format is to have a small and quickly derived data format from which the majority of CMS analysis users can perform their analysis work. This format is targeted at having sufficient information to serve about 80% of CMS analysis, while dramatically simplifying the disk and I\/O resources needed for analysis. This improvement should bring substantial improvements in resource needs and turn-around time for analysis applications. Such large reductions were achieved using a number of techniques, including defining light-weight physics object candidate representations, increasing transverse momentum thresholds for storing physics-object candidates, and reduced numerical precision when it is not required at the analysis level. In this contribution we discuss the critical components of the mini-AOD format, our experience with its deployment and the planned physics analysis model for Run 2 based on the mini-AOD.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578877", "resources": [{"_type": "LocalFile", "name": "miniaod_main.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/426\/attachments\/578877\/797080\/miniaod_main.pdf", "fileName": "miniaod_main.pdf", "_fossil": "localFileMetadata", "id": "797080", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/426", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:15:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "308", "speakers": [{"_type": "ContributionParticipation", "emailHash": "8f42bcf5a598f1e145ad66dda0e23c3f", "affiliation": "INSTITUE OF HIGH ENERGY PHYSICS, University of Chinese Academy of Sciences", "_fossil": "contributionParticipationMetadata", "fullName": "Ms. LEI, Xiaofeng", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "8f42bcf5a598f1e145ad66dda0e23c3f", "affiliation": "INSTITUE OF HIGH ENERGY PHYSICS, University of Chinese Academy of Sciences", "_fossil": "contributionParticipationMetadata", "fullName": "Ms. LEI, Xiaofeng", "id": "0"}], "title": "BESIII Physics Data Storing and Processing on HBase and MapReduce", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T07:49:42.280466+00:00", "description": "", "title": "chep-leixf.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/308\/attachments\/578878\/797081\/chep-leixf.pdf", "filename": "chep-leixf.pdf", "content_type": "application\/pdf", "type": "file", "id": 797081, "size": 1325862}], "title": "Slides", "default_folder": false, "id": 578878, "description": ""}], "_type": "Contribution", "description": "In the past years, we have successfully applied Hadoop to high-energy physics analysis. Although, we have not only improved the efficiency of data analysis, but also reduced the cost of cluster building so far, there are still some spaces to be optimized, like static pre-selection, low-efficient random data reading and I\/O bottleneck caused by Fuse which is used to access HDFS. In order to change this situation, this paper presents a new analysis platform for high-energy physics data storing and analyzing. \r\nThe data structure is changed from DST tree-like files to HBase according to the features of the data itself and analysis processes, since HBase is more suitable for processing random data reading than DST files and enable HDFS to be accessed directly. A few of optimization measures are taken for the purpose of getting a good performance and as well as a customized protocol is defined for data serializing and desterilizing for the sake of decreasing the storage space in HBase. In order to make full use of locality of data storing in HBase, utilizing a new MapReduce model and a new split policy for HBase regions are proposed in the paper. In addition, we establish a dynamic pluggable easy-to-use tag (event metadata) based pre-selection subsystem. It can assist physicists even to filter out 999\u2030 uninterested data, if the conditions are set properly. This means that a lot of I\/O resources can be saved, the CPU usage can be improved and consuming time for data analysis can be reduced. Finally, several use cases are designed, the test results show that the new platform has an excellent performance with 3.5 times faster with pre-selection and 20% faster without pre-selection, and the new platform is stable and scalable as well.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578878", "resources": [{"_type": "LocalFile", "name": "chep-leixf.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/308\/attachments\/578878\/797081\/chep-leixf.pdf", "fileName": "chep-leixf.pdf", "_fossil": "localFileMetadata", "id": "797081", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "61da47719bda539bba38913ee7ec3d8c", "affiliation": "INSTITUE OF HIGH ENERGY PHYSICS", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. LI, Qiang", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "4327760c58f7f774a5044983564125a2", "affiliation": "INSTITUE OF HIGH ENERGY PHYSICS", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SUN, Gongxing", "id": "2"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/308", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "309", "speakers": [{"_type": "ContributionParticipation", "emailHash": "c33bc7413c790c4057b46cd10a60e86f", "affiliation": "IHEP", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ZOU, Jiaheng", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "33bda03bc5a535d48779811313257d8b", "affiliation": "IHEP", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. LI, Weidong", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "e1e2a24dff8c78d2c078091f397fc033", "affiliation": "SDU", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. HUANG, Xingtao", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c33bc7413c790c4057b46cd10a60e86f", "affiliation": "IHEP", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ZOU, Jiaheng", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "33bda03bc5a535d48779811313257d8b", "affiliation": "IHEP", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. LI, Weidong", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "e1e2a24dff8c78d2c078091f397fc033", "affiliation": "SDU", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. HUANG, Xingtao", "id": "2"}], "title": "The SNiPER offline software framework for non-collider physics experiments", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T07:22:49.531537+00:00", "description": "", "title": "SNiPER_CHEP2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/309\/attachments\/578879\/797082\/SNiPER_CHEP2015.pdf", "filename": "SNiPER_CHEP2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 797082, "size": 735225}], "title": "Poster", "default_folder": false, "id": 578879, "description": ""}], "_type": "Contribution", "description": "SNiPER (the abbreviation of Software for Non-collider Physics ExpeRiments) has been developed based on common requirements from both cosmic ray and nuclear reactor neutrino experiments. This contribution will introduce the detailed design and implementation of the SNiPER software. Compared to the existing offline software frameworks in the high energy physics domain, the design of SNiPER is more focused on efficiency and flexibility. It contains a compact kernel for software components management, job configuration, event execution control, input\/output data flow, etc. Both event data model and in-memory data management can be customized by different applications. In SNiPER, a job is composed of one or more low coupling tasks, and various services and algorithms can be specified in each task. By using this kind of structure, both multiple I\/O streams and conditional execution of algorithm subsets can be easily realized. In order to facilitate event correlation analysis (a necessity for non-collider physics experiments), the FIFO buffer is used to store adjacent events and a sophisticated method of memory updating is applied, so that accessing to successive events within the user-defined window according to event timestamp becomes possible. The SNiPER also supports concurrent computing by running multi-threads concurrently with a single job. Currently a nascent product of SNiPER has been released and is being used by the JUNO and LHAASO experiments. The practices show that the software architecture is universal and expandable, and SNiPER can be used by non-collider physics experiments to build their offline data analysis and processing systems.", "track": "Track2: Offline software ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578879", "resources": [{"_type": "LocalFile", "name": "SNiPER_CHEP2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/309\/attachments\/578879\/797082\/SNiPER_CHEP2015.pdf", "fileName": "SNiPER_CHEP2015.pdf", "_fossil": "localFileMetadata", "id": "797082", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "a696972a0dc29e70843f619695acb1e9", "affiliation": "IHEP", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. LIN, Tao", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "f4e24875ff678ae645f7167061ae49a8", "affiliation": "SDU", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. LI, Teng", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "d57a941b534787d4d1685695d0d4abd4", "affiliation": "IHEP", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. ZHANG, Kun", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "032ea5ebcc72600d7d55e58348b47732", "affiliation": "IHEP", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. DENG, Ziyan", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "3965600a40921a34d8ad1c22a3a778da", "affiliation": "IHEP", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CAO, Guofu", "id": "7"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/309", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:45:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "05:00:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "300", "speakers": [{"_type": "ContributionParticipation", "emailHash": "f93592852bac53b82654044c3bcd6fe9", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "KELSEY, Dave", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "f93592852bac53b82654044c3bcd6fe9", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "KELSEY, Dave", "id": "0"}], "title": "The production deployment of IPv6 on WLCG", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T02:22:19.533481+00:00", "description": "", "title": "Kelsey16apr15.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/300\/attachments\/578880\/797083\/Kelsey16apr15.pdf", "filename": "Kelsey16apr15.pdf", "content_type": "application\/pdf", "type": "file", "id": 797083, "size": 723668}, {"_type": "attachment", "modified_dt": "2015-04-15T02:22:19.533481+00:00", "description": "", "title": "Kelsey16apr15.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/300\/attachments\/578880\/797084\/Kelsey16apr15.pptx", "filename": "Kelsey16apr15.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797084, "size": 331753}], "title": "Slides", "default_folder": false, "id": 578880, "description": ""}], "_type": "Contribution", "description": "The world is rapidly running out of IPv4 addresses; the number of IPv6 end systems connected to the internet is increasing; WLCG and the LHC experiments may soon have access to worker nodes and\/or virtual machines (VMs) possessing only an IPv6 routable address.  The HEPiX IPv6 Working Group (http:\/\/hepix-ipv6.web.cern.ch\/) has been investigating, testing and planning for dual-stack services on WLCG for several years. Following feedback from our working group, many of the storage technologies in use on WLCG have recently been made IPv6-capable. The worldwide HEP computing community now needs to deploy dual-stack IPv6\/IPv4 services on WLCG to allow such use of IPv6-only resources.\r\n\r\nThis paper will present the IPv6 requirements, tests and plans of each of the four LHC experiments together with the tests performed both on the IPv6 test-bed and in targeted use of WLCG production services. This is primarily aimed at IPv6-only worker nodes or VMs accessing several different implementations of a global dual-stack federated storage service. The changes required to the operational infrastructure, including monitoring and security, will be addressed as will the implications for site management. The working group will present its deployment plan for dual-stack storage services, together with other essential central and monitoring services, to start during 2015.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578880", "resources": [{"_type": "LocalFile", "name": "Kelsey16apr15.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/300\/attachments\/578880\/797083\/Kelsey16apr15.pdf", "fileName": "Kelsey16apr15.pdf", "_fossil": "localFileMetadata", "id": "797083", "_deprecated": true}, {"_type": "LocalFile", "name": "Kelsey16apr15.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/300\/attachments\/578880\/797084\/Kelsey16apr15.pptx", "fileName": "Kelsey16apr15.pptx", "_fossil": "localFileMetadata", "id": "797084", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "2986285e931ab50b0c2dac8e08a713d5", "affiliation": "IN2P3 Computing Center", "_fossil": "contributionParticipationMetadata", "fullName": "BERNIER, Jerome", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "a2dbd3d2df26e3fbc0360f6b3594fd21", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CAMPANA, Simone", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "16faa2bd30d57a625b0b00b8ac571384", "affiliation": "Fermilab", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CHADWICK, Keith", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "543ad8ae46ecd1be94327a51c4495526", "affiliation": "Acad. of Sciences of the Czech Rep. (CZ)", "_fossil": "contributionParticipationMetadata", "fullName": "CHUDOBA, Jiri", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "fecbc4752eaee2cab98bdd811db0c590", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "DEWHURST, Alastair", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "1d190d5de5fd2ef90f5259b7711bd4f6", "affiliation": "FZU ASCR", "_fossil": "contributionParticipationMetadata", "fullName": "ELIAS, Marek", "id": "24"}, {"_type": "ContributionParticipation", "emailHash": "ab582d4be1a2ef22000359db44b34578", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "FINNERN, Thomas", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "a03bb0b3b47a554f9eb69c2fca89dcb7", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GRIGORAS, Costin", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "35721fed37de20dfa93d4c5ffdac389c", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "HOEFT, Bruno Heinrich", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "e27baf4efd9e198268df8a6d99d570f4", "affiliation": "STFC - Rutherford Appleton Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. IDICULLA, Tiju", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "073b7fe5499f0ba3d6f1de2429069b2d", "affiliation": "Universitat Aut\u00f2noma de Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "LOPEZ MUNOZ, Fernando", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "0912ed3f7fc8599ad4d34add50eb5564", "affiliation": "University of Oxford", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. MACMAHON, Ewan", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "c99110761f958279ccba0d51deb0af5e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MARTELLI, Edoardo", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "f50800924fa96c0b39ce79d14f8dd3f0", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "NANDAKUMAR, Raja", "id": "14"}, {"_type": "ContributionParticipation", "emailHash": "27bb5c3239953b62388cb363633d1334", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "OHRENBERG, Kars", "id": "15"}, {"_type": "ContributionParticipation", "emailHash": "c6f8dc98cff3ded41cb19185a6526e1f", "affiliation": "Universit\u00e0 degli Studi  e INFN Milano (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "PRELZ, Francesco", "id": "16"}, {"_type": "ContributionParticipation", "emailHash": "6794b20ff1a79867a414c13ad66a52f2", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "RAND, Duncan", "id": "17"}, {"_type": "ContributionParticipation", "emailHash": "cbc73225e092c0fa43663ce9496fd7cc", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SCIABA, Andrea", "id": "18"}, {"_type": "ContributionParticipation", "emailHash": "d6555edde14f7ee8e12da269bce3d50a", "affiliation": "CSC Oy", "_fossil": "contributionParticipationMetadata", "fullName": "TIGERSTEDT, Ulf", "id": "19"}, {"_type": "ContributionParticipation", "emailHash": "98c30cd056776b299f3f8acc278de1b1", "affiliation": "California Institute of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "VOICU, Ramiro", "id": "20"}, {"_type": "ContributionParticipation", "emailHash": "fc2d531a1d59c0d27cf4550e45442ae6", "affiliation": "University of London (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "WALKER, Christopher John", "id": "21"}, {"_type": "ContributionParticipation", "emailHash": "e86323288b18387d48dc1af9718c14d6", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WILDISH, Tony", "id": "22"}, {"_type": "ContributionParticipation", "emailHash": "b4e7a4ea1d679a57a1467fc9ae74c443", "affiliation": "Imperial College Sci., Tech. & Med. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "FAYER, Simon", "id": "23"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/300", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "301", "speakers": [{"_type": "ContributionParticipation", "emailHash": "dd2a5fc1efe1424f52d88f62ddfd3bce", "affiliation": "Moscow Institute of Physics and Technology, Moscow", "_fossil": "contributionParticipationMetadata", "fullName": "KAZEEV, Nikita", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "c10fdf0d2ddefd23d451c9de740427fe", "affiliation": "Moscow Institute of Physics and Technology, Moscow", "_fossil": "contributionParticipationMetadata", "fullName": "USTYUZHANIN, Andrey", "id": "5"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2ae03a3bd281e7b4dbb4fd56ebc47fa5", "affiliation": "Yandex", "_fossil": "contributionParticipationMetadata", "fullName": "ARTEMOV, Alexey", "id": "4"}], "title": "LHCb EventIndex", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T21:37:38.184442+00:00", "description": "", "title": "CHEP_poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/301\/attachments\/578881\/797085\/CHEP_poster.pdf", "filename": "CHEP_poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 797085, "size": 4272655}], "title": "Poster", "default_folder": false, "id": 578881, "description": ""}], "_type": "Contribution", "description": "The LHCb experiment routinely generates up to 10^10 events per year. Organizing such an amount of data in a convenient manner for interactive analysis is non-trivial. It becomes even more complicated as every event undergoes several versions of reconstructions, and users have to be able to navigate through many different versions of the same event. This paper presents the LHCb EventIndex: an event search system designed for organizing LHCb events. Its primary goal is to allow a fast selection of subsets of events fulfilling a combination of high-level conditions, such as which trigger has fired, or how many muons were found in the event. \r\n\r\nThe system is designed with scalability and extensibility in mind. We describe the part of the system that deals with Grid scanning, event storage and user interface. Events are stored by modern open-source NoSQL solutions, with row keys being organized so that certain searches can be run with only a partial table scan. Such embarrassingly parallel organization makes search time upper bound equal to a full scan of a regional server, a constant independent of the total amount of data stored. User interaction is handled by a master-slave python middleware. \r\n\r\nWe demonstrate the scalability of the system by performance evaluation on the real LHCb datasets. This system is also being evaluated by the LHC ATLAS experiment.", "track": "Track3: Data store and access ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578881", "resources": [{"_type": "LocalFile", "name": "CHEP_poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/301\/attachments\/578881\/797085\/CHEP_poster.pdf", "fileName": "CHEP_poster.pdf", "_fossil": "localFileMetadata", "id": "797085", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "1361efd0daf15bbf4cc3141a47fc97af", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHARPENTIER, Philippe", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "2d3197cea2de7705cbaf2df989cdf1da", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CLEMENCIC, Marco", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "5ae4855aa9d3c6685bd9f450814f9af6", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "STAGNI, Federico", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "dd2a5fc1efe1424f52d88f62ddfd3bce", "affiliation": "Moscow Institute of Physics and Technology, Moscow", "_fossil": "contributionParticipationMetadata", "fullName": "KAZEEV, Nikita", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "c10fdf0d2ddefd23d451c9de740427fe", "affiliation": "Moscow Institute of Physics and Technology, Moscow", "_fossil": "contributionParticipationMetadata", "fullName": "USTYUZHANIN, Andrey", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "50069652b1a5b3249c617f604919c0d6", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CATTANEO, Marco", "id": "6"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/301", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": null, "keywords": [], "id": "302", "speakers": [{"_type": "ContributionParticipation", "emailHash": "01d8b4651ebc30fa461a08ef3a9a6d53", "affiliation": "LAWRENCE BERKELEY NATIONAL LAB", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BROWN, David", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "01d8b4651ebc30fa461a08ef3a9a6d53", "affiliation": "LAWRENCE BERKELEY NATIONAL LAB", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BROWN, David", "id": "0"}], "title": "Wire Chamber Hit Simulation for Mu2e", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [], "_type": "Contribution", "description": "This presentation will describe the design, implementation and performance of classes developed for translating G4 energy deposits recorded in the active volume of a wire chamber into digital detector signals, targeted for the Mu2e experiment.  One class models the physics processes whereby ionization energy deposits become a current pulse on the wire, including the effects of cluster formation, cluster drift, diffusion, gas gain, and current attenuation and dispersion in the wire.  Another class models the electronics used to shape, amplify, discriminate and digitize currents at the wire ends, combining effects from overlapping signals, thermal noise and channel-to-channel cross-talk through a waveform sum. Gas cell properties and electronic circuit parameters are configurable.  Non-linear time-to-distance and non-Gaussian resolutions are emergent properties of the simulation, driven by the low-level properties.  The wire signal simulation is fully integrated with G4-based Mu2e simulation application, adding around 10% to the total simulation processing time.  This simulation was used to estimate the experimental performance in the Mu2e design report.", "track": "Track2: Offline software ", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "a361249913aed7f4211e409f162867df", "affiliation": "Femilab", "_fossil": "contributionParticipationMetadata", "fullName": "KUTSCHKE, Robert", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "481ff8e9086a7836a320fa9c69a4f837", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GAPONENKO, Andrei", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/contribution\/302", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:30:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "303", "speakers": [{"_type": "ContributionParticipation", "emailHash": "8cc835a252b47dda5903455882d0be2a", "affiliation": "Moscow Institute of Physics and Technology, Moscow", "_fossil": "contributionParticipationMetadata", "fullName": "HUSHCHYN, Mikhail", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "8cc835a252b47dda5903455882d0be2a", "affiliation": "Moscow Institute of Physics and Technology, Moscow", "_fossil": "contributionParticipationMetadata", "fullName": "HUSHCHYN, Mikhail", "id": "0"}], "title": "Disk storage management for LHCb based on Data Popularity estimator", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T02:07:44.641413+00:00", "description": "", "title": "DataPopularityPresentation.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/303\/attachments\/578882\/797086\/DataPopularityPresentation.pdf", "filename": "DataPopularityPresentation.pdf", "content_type": "application\/pdf", "type": "file", "id": 797086, "size": 654882}], "title": "Slides", "default_folder": false, "id": 578882, "description": ""}], "_type": "Contribution", "description": "The amount of data produced by the LHCb experiment every year consists of several petabytes. This data is kept on disk and tape storage systems. Disks are much faster than tapes, but are way more expensive and hence disk space is limited. It is impossible to fit the whole data taken during the experiment's lifetime on disk, but fortunately fast access to datasets are no longer needed after the analysis requiring them are over. So it is highly important to identify which datasets should be kept on disk and which ones should be kept as archives on tape. The metrics to be used for deprecating datasets\u2019 caching is based on the \u201cpopularity\u201d of this dataset, i.e. whether it is likely to be used in the future or not. We discuss here the approach and the studies carried out for optimizing such a Data Popularity estimator. \r\n\r\n\r\nInput information to the estimator are the dataset usage history and metadata (size, type, configuration etc). The system is designed to select the datasets which may be used in the future and thus should remain on disk. Studies have therefore been performed on how to optimize the usage of dataset information from the past for predicting its future popularity. In particular, we have carried out a detailed comparison of various time series analysis, machine learning classifier, clustering and regression algorithms. We demonstrate that our approach is capable of improving significantly the disk usage efficiency.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578882", "resources": [{"_type": "LocalFile", "name": "DataPopularityPresentation.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/303\/attachments\/578882\/797086\/DataPopularityPresentation.pdf", "fileName": "DataPopularityPresentation.pdf", "_fossil": "localFileMetadata", "id": "797086", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "50069652b1a5b3249c617f604919c0d6", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CATTANEO, Marco", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "1361efd0daf15bbf4cc3141a47fc97af", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHARPENTIER, Philippe", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "c10fdf0d2ddefd23d451c9de740427fe", "affiliation": "ITEP Institute for Theoretical and Experimental Physics (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "USTYUZHANIN, Andrey", "id": "3"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/303", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "304", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e106556bcded279b116df4aee2de6c31", "affiliation": "University of Victoria (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SOBIE, Randy", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "e106556bcded279b116df4aee2de6c31", "affiliation": "University of Victoria (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SOBIE, Randy", "id": "0"}], "title": "Glint: VM image distribution in a multi-cloud environment", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The HEP community is increasingly using clouds that are distributed around the world for running its applications. The stringent software criteria of HEP experiments require that we use the identical (secure) virtual machine (VM) image at all sites with a minimal set of site-specific customizations. Nearly all cloud systems (such as OpenStack) require that the VM image to be instantiated must be stored in the local repository (e.g. Glance in OpenStack). This requirement forces the user to manually distribute images to target clouds. As the number of clouds utilized grows, this procedure becomes increasingly time-consuming and error-prone. To remedy this situation, we designed and constructed an image replication system, called Glint, to mange VM images in multiple OpenStack Glance repositories.\r\n\r\nOn a single OpenStack cloud, Glance gives the user (with the appropriate credentials) the ability to use pre-configured images or upload customized images using the OpenStack Horizon web interface or command line interface.  A user with access to multiple OpenStack clouds (each with separate credentials) will likely find that the set of pre-configured images will be different at each site and the user will need to upload their customized image to each cloud.  \r\n\r\nGlint is designed to streamline the distribution of application images to multiple clouds. Glint integrates into the OpenStack framework and provides access to its functions via the OpenStack Horizon web interface. It registers with the OpenStack Keystone Identity service during installation and authorizes requests by validating the user's security token with the local Keystone service.  Glint gives the user ability to add remote clouds (with their credentials) and migrate images from one to many cloud sites.    \r\n\r\nWe describe the features of Glint and its integration in the OpenStack framework.  The initial use of Glint for HEP application is described.   Future developments and potential integration into the OpenStack code base are discussed.", "track": "Track7: Clouds and virtualization", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "f13af63ad4bbdfdce02559c629f8d6bc", "affiliation": "University of Victoria (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "GABLE, Ian", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "32e62a0d1cb79b90404b2e9b5eae5126", "affiliation": "University of Victoria (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "TAYLOR, Ryan", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "ebf920eeaeb952921229cbbafabd92d7", "affiliation": "University of Victoria (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "BERGHAUS, Frank Olaf", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "70bf976e0adc8bccf178516895a365dd", "affiliation": "University of Victoria (CA)", "_fossil": "contributionParticipationMetadata", "fullName": "LEAVETT-BROWN, Colin Roy", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "e490ca70cc14e2ca27f8071da6597bdc", "affiliation": "U", "_fossil": "contributionParticipationMetadata", "fullName": "PATERSON, Michael", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "0cb8f3c84f79ded1da04cdaf2b5f719a", "affiliation": "University of Victoria", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. DESMARAIS, Ron", "id": "6"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/304", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:15:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:30:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "305", "speakers": [{"_type": "ContributionParticipation", "emailHash": "8c3829aeb9782b9da4c2474b6257cbab", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHARALAMPIDIS, Ioannis", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "8c3829aeb9782b9da4c2474b6257cbab", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHARALAMPIDIS, Ioannis", "id": "0"}], "title": "CernVM WebAPI - Controling Virtual Machines from the Web", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T06:10:16.577950+00:00", "description": "", "title": "CernVM-WebAPI.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/305\/attachments\/578883\/797087\/CernVM-WebAPI.pdf", "filename": "CernVM-WebAPI.pdf", "content_type": "application\/pdf", "type": "file", "id": 797087, "size": 6313501}], "title": "Slides", "default_folder": false, "id": 578883, "description": ""}], "_type": "Contribution", "description": "Lately there is a trend in scientific projects to look for computing resources in the volunteering community. In addition, to reduce the development effort required to port the scientific\u00a0software stack to all the known platforms, the use of Virtual Machines (VMs) as end-projects is becoming increasingly popular. Unfortunately, the installation and the interfacing with the existing volunteering computing infrastructure (such as BOINC) is left to the end-user, therefore restricting even more the audience to only sufficiently software-capable people.\r\n\r\nCernVM WebAPI is a software solution addressing this specific case in a way that opens wide new application opportunities. It offers a very simple API for setting-up, controlling and \u00a0interfacing with a VM instance in the user\u2019s computer, while in the same time offloading the\r\nuser from all the burden of downloading, installing and configuring the hypervisor. WebAPI comes with a lightweight javascript library that guides the user through the application installation process. Malicious usage is prohibited by offering a per-domain PKI validation mechanism.\r\n\r\nIn this contribution we will overview this new technology, discuss its security features and examine some test cases where it is already in use.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578883", "resources": [{"_type": "LocalFile", "name": "CernVM-WebAPI.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/305\/attachments\/578883\/797087\/CernVM-WebAPI.pdf", "fileName": "CernVM-WebAPI.pdf", "_fossil": "localFileMetadata", "id": "797087", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "1bd1c33b04e4a9807617ed433dd84282", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SKANDS, Peter", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "5464ee52c2740264980ed36d4553e207", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GANIS, Gerardo", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "771122777d10e8f8dc1c3aa7156922e8", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BLOMER, Jakob", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "b7670e16208d58f21cb7eff8f0036404", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MEUSEL, Rene", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "3f41c6112486480c3ebd506d39003e9e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BUNCIC, Predrag", "id": "5"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/305", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:45:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "306", "speakers": [{"_type": "ContributionParticipation", "emailHash": "643fd6db08ab4293fe2cc8ccee9a7f0d", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "FROST, Oliver", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "643fd6db08ab4293fe2cc8ccee9a7f0d", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "FROST, Oliver", "id": "0"}], "title": "Cellular Automaton based Track Finding for the Central Drift Chamber of Belle II", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T19:23:40.677115+00:00", "description": "", "title": "2015_04_13_chep_oliver_frost.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/306\/attachments\/578884\/797088\/2015_04_13_chep_oliver_frost.pdf", "filename": "2015_04_13_chep_oliver_frost.pdf", "content_type": "application\/pdf", "type": "file", "id": 797088, "size": 4868821}], "title": "Slides", "default_folder": false, "id": 578884, "description": ""}], "_type": "Contribution", "description": "With the upgraded electron-positron-collider facility, SuperKEKB and Belle II, the Japanese high energy research center KEK strives to exceed its own world record luminosity by a factor of 40.\r\nTo provide a solid base for the event reconstruction within the central drift chamber in the enhanced luminosity setup, a powerful track finding algorithm coping with the higher beam induced backgrounds is developed at DESY.\r\nPursing a bottom-up approach, which is less susceptible to the increased number of background hits compared to global event reconstruction techniques such as the Hough transformation and its successors, we present a generalization of the cellular automaton.\r\nWhile maintaining the high execution speed by circumventing the combinatorial backtracking in the graph of local hit information and extrapolations naturally arising in bottom-up approaches, this so called weighted cellular automaton integrates the adaptiveness of the Hopfield network into the original algorithm.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578884", "resources": [{"_type": "LocalFile", "name": "2015_04_13_chep_oliver_frost.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/306\/attachments\/578884\/797088\/2015_04_13_chep_oliver_frost.pdf", "fileName": "2015_04_13_chep_oliver_frost.pdf", "_fossil": "localFileMetadata", "id": "797088", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "e716f0fe28b7e9b021151cddf71af18b", "affiliation": "Deutsches Elektronen Synchrotron (DESY)", "_fossil": "contributionParticipationMetadata", "fullName": "KLEINWORT, Claus", "id": "1"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/306", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "307", "speakers": [{"_type": "ContributionParticipation", "emailHash": "9be6419ac243e41cfcf9edcb3af36752", "affiliation": "IHEP", "_fossil": "contributionParticipationMetadata", "fullName": "Ms. ZENG, Shan", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "9be6419ac243e41cfcf9edcb3af36752", "affiliation": "IHEP", "_fossil": "contributionParticipationMetadata", "fullName": "Ms. ZENG, Shan", "id": "0"}], "title": "SDN implementations at IHEP", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "This Paper describes two research aspects and practices of SDN at IHEP. The first one is the SDN practice for the data transferring across the internet, in which a virtual private network based on SDN is designed and built,and an intelligent network route algorithm is developed and deployed in the SDN controller to make full use of IPv6 resources. Experimental results show that this practice can solve the network bandwidth problem between cooperation members of the high energy physics experiments around China.The second one is about the deployment of SDN in the internal data center of IHEP, which provides the flexible network management in the virtualization environment, but the latency and the throughput of the SDN controller is far from our expectation. In order to improve the performance of the SDN controller, we compare four controllers in our second test bed, including NOX, NOX-MT,Beacon and Maestro, and analyzed the performance both in latency and in throughput. At last, a new SDN controller design through the method of patching and reducing the data path request frequency,which is dedicated to high performance SDN in the internal data center of IHEP is proposed.", "track": "Track6: Facilities, Infrastructure, Network", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "c23c808c40dd4dd23cb23df39d9a3c70", "affiliation": "IHEP", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. QI, Fazhi", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "2ef249acd88e2e7df5b657364e0b8055", "affiliation": "IHEP", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. CUI, Tao", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "bc2253117d94e742e6ead0855043d8aa", "affiliation": "IHEP", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. SUN, Zhihui", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "9c007c9bbfe8989ba5753f4727cff1f4", "affiliation": "IHEP", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. CHEN, Gang", "id": "4"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/307", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "09:40:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "10:05:00"}, "duration": 25, "session": "Plenary", "keywords": [], "id": "568", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2e6780d520127502454830ec573033ae", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MAENO, Tadashi", "id": "0"}], "primaryauthors": [], "title": "Track5 Summary", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T07:03:07.053931+00:00", "description": "", "title": "T5_summary.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/568\/attachments\/578885\/797089\/T5_summary.pdf", "filename": "T5_summary.pdf", "content_type": "application\/pdf", "type": "file", "id": 797089, "size": 3181318}], "title": "Slides", "default_folder": false, "id": 578885, "description": ""}], "_type": "Contribution", "description": "", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578885", "resources": [{"_type": "LocalFile", "name": "T5_summary.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/568\/attachments\/578885\/797089\/T5_summary.pdf", "fileName": "T5_summary.pdf", "_fossil": "localFileMetadata", "id": "797089", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/568", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "10:05:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "10:30:00"}, "duration": 25, "session": "Plenary", "keywords": [], "id": "569", "speakers": [{"_type": "ContributionParticipation", "emailHash": "9e7dd8f84b1524d686815744ec524fea", "affiliation": "University of Manchester (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "MCNAB, Andrew", "id": "0"}], "primaryauthors": [], "title": "Track7 Summary", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T05:13:23.157393+00:00", "description": "", "title": "mcnab-track7-16apr15.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/569\/attachments\/578886\/797090\/mcnab-track7-16apr15.pdf", "filename": "mcnab-track7-16apr15.pdf", "content_type": "application\/pdf", "type": "file", "id": 797090, "size": 65139531}], "title": "Slides", "default_folder": false, "id": 578886, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578886", "resources": [{"_type": "LocalFile", "name": "mcnab-track7-16apr15.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/569\/attachments\/578886\/797090\/mcnab-track7-16apr15.pdf", "fileName": "mcnab-track7-16apr15.pdf", "_fossil": "localFileMetadata", "id": "797090", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/569", "roomFullname": null}, {"startDate": {"date": "2015-04-15", "tz": "Europe\/Zurich", "time": "02:45:00"}, "endDate": {"date": "2015-04-15", "tz": "Europe\/Zurich", "time": "03:30:00"}, "duration": 45, "session": "Plenary", "keywords": [], "id": "560", "speakers": [{"_type": "ContributionParticipation", "emailHash": "059c67f423c94e588bce78fb5a617463", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GIRONE, Maria", "id": "0"}], "primaryauthors": [], "title": "Distributed Data Management and Distributed File Systems", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T13:57:46.579662+00:00", "description": "", "title": "CHEP_MG5.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/560\/attachments\/578887\/797091\/CHEP_MG5.pptx", "filename": "CHEP_MG5.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797091, "size": 53186332}], "title": "Slides", "default_folder": false, "id": 578887, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578887", "resources": [{"_type": "LocalFile", "name": "CHEP_MG5.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/560\/attachments\/578887\/797091\/CHEP_MG5.pptx", "fileName": "CHEP_MG5.pptx", "_fossil": "localFileMetadata", "id": "797091", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/560", "roomFullname": null}, {"startDate": {"date": "2015-04-15", "tz": "Europe\/Zurich", "time": "04:00:00"}, "endDate": {"date": "2015-04-15", "tz": "Europe\/Zurich", "time": "04:30:00"}, "duration": 30, "session": "Plenary", "keywords": [], "id": "561", "speakers": [{"_type": "ContributionParticipation", "emailHash": "0bb08d2c00ac538180032342288ed2b3", "affiliation": "GSI - Helmholtzzentrum fur Schwerionenforschung GmbH (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "KOLLEGGER, Thorsten Sven", "id": "0"}], "primaryauthors": [], "title": "Computing at FAIR", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T08:44:38.194694+00:00", "description": "", "title": "20150415-CHEP-FAIRComputing.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/561\/attachments\/578888\/797092\/20150415-CHEP-FAIRComputing.pdf", "filename": "20150415-CHEP-FAIRComputing.pdf", "content_type": "application\/pdf", "type": "file", "id": 797092, "size": 5489825}, {"_type": "attachment", "modified_dt": "2015-04-14T08:44:38.194694+00:00", "description": "", "title": "20150415-CHEP-FAIRComputing.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/561\/attachments\/578888\/797093\/20150415-CHEP-FAIRComputing.pptx", "filename": "20150415-CHEP-FAIRComputing.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797093, "size": 15960974}], "title": "Slides", "default_folder": false, "id": 578888, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578888", "resources": [{"_type": "LocalFile", "name": "20150415-CHEP-FAIRComputing.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/561\/attachments\/578888\/797092\/20150415-CHEP-FAIRComputing.pdf", "fileName": "20150415-CHEP-FAIRComputing.pdf", "_fossil": "localFileMetadata", "id": "797092", "_deprecated": true}, {"_type": "LocalFile", "name": "20150415-CHEP-FAIRComputing.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/561\/attachments\/578888\/797093\/20150415-CHEP-FAIRComputing.pptx", "fileName": "20150415-CHEP-FAIRComputing.pptx", "_fossil": "localFileMetadata", "id": "797093", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/561", "roomFullname": null}, {"startDate": {"date": "2015-04-15", "tz": "Europe\/Zurich", "time": "04:30:00"}, "endDate": {"date": "2015-04-15", "tz": "Europe\/Zurich", "time": "05:00:00"}, "duration": 30, "session": "Plenary", "keywords": [], "id": "562", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6f6ccdba8cc58bd88426eca23d65649f", "affiliation": "OpenStack Foundation", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. FIFIELD, Tom", "id": "1"}], "primaryauthors": [], "title": "Expanding OpenStack community in academic fields", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T02:03:04.729673+00:00", "description": "", "title": "2015-04-CHEP-OpenStack.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/562\/attachments\/578889\/797094\/2015-04-CHEP-OpenStack.pdf", "filename": "2015-04-CHEP-OpenStack.pdf", "content_type": "application\/pdf", "type": "file", "id": 797094, "size": 5061717}], "title": "Slides", "default_folder": false, "id": 578889, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578889", "resources": [{"_type": "LocalFile", "name": "2015-04-CHEP-OpenStack.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/562\/attachments\/578889\/797094\/2015-04-CHEP-OpenStack.pdf", "fileName": "2015-04-CHEP-OpenStack.pdf", "_fossil": "localFileMetadata", "id": "797094", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/562", "roomFullname": null}, {"startDate": {"date": "2015-04-15", "tz": "Europe\/Zurich", "time": "05:00:00"}, "endDate": {"date": "2015-04-15", "tz": "Europe\/Zurich", "time": "05:30:00"}, "duration": 30, "session": "Plenary", "keywords": [], "id": "563", "speakers": [{"_type": "ContributionParticipation", "emailHash": "057fdd43cca900c1bd6837c1167688b3", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "LOPIENSKI, Sebastian", "id": "0"}], "primaryauthors": [], "title": "Computer Security for HEP", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T00:39:12.002913+00:00", "description": "", "title": "2015.04_Lopienski_-_CHEP_-_Computer_Security.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/563\/attachments\/578890\/797095\/2015.04_Lopienski_-_CHEP_-_Computer_Security.pdf", "filename": "2015.04_Lopienski_-_CHEP_-_Computer_Security.pdf", "content_type": "application\/pdf", "type": "file", "id": 797095, "size": 3309550}, {"_type": "attachment", "modified_dt": "2015-04-15T00:39:12.002913+00:00", "description": "", "title": "2015.04_Lopienski_-_CHEP_-_Computer_Security.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/563\/attachments\/578890\/797096\/2015.04_Lopienski_-_CHEP_-_Computer_Security.pptx", "filename": "2015.04_Lopienski_-_CHEP_-_Computer_Security.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797096, "size": 16346662}], "title": "Slides", "default_folder": false, "id": 578890, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578890", "resources": [{"_type": "LocalFile", "name": "2015.04_Lopienski_-_CHEP_-_Computer_Security.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/563\/attachments\/578890\/797095\/2015.04_Lopienski_-_CHEP_-_Computer_Security.pdf", "fileName": "2015.04_Lopienski_-_CHEP_-_Computer_Security.pdf", "_fossil": "localFileMetadata", "id": "797095", "_deprecated": true}, {"_type": "LocalFile", "name": "2015.04_Lopienski_-_CHEP_-_Computer_Security.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/563\/attachments\/578890\/797096\/2015.04_Lopienski_-_CHEP_-_Computer_Security.pptx", "fileName": "2015.04_Lopienski_-_CHEP_-_Computer_Security.pptx", "_fossil": "localFileMetadata", "id": "797096", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/563", "roomFullname": null}, {"startDate": {"date": "2015-04-15", "tz": "Europe\/Zurich", "time": "05:30:00"}, "endDate": {"date": "2015-04-15", "tz": "Europe\/Zurich", "time": "06:00:00"}, "duration": 30, "session": "Plenary", "keywords": [], "id": "564", "speakers": [], "primaryauthors": [], "title": "EMC Corporation", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-22T00:18:30.932250+00:00", "description": "", "title": "EMC.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/564\/attachments\/578891\/797097\/EMC.pdf", "filename": "EMC.pdf", "content_type": "application\/pdf", "type": "file", "id": 797097, "size": 5653526}], "title": "Slides", "default_folder": false, "id": 578891, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578891", "resources": [{"_type": "LocalFile", "name": "EMC.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/564\/attachments\/578891\/797097\/EMC.pdf", "fileName": "EMC.pdf", "_fossil": "localFileMetadata", "id": "797097", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/564", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "07:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "07:45:00"}, "duration": 45, "session": "Plenary", "keywords": [], "id": "565", "speakers": [{"_type": "ContributionParticipation", "emailHash": "9f655ee0d367b6896f4dd3a37228be6f", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "NEUFELD, Niko", "id": "0"}], "primaryauthors": [], "title": "Future of DAQ Frameworks and Approaches, and Their Evolution toward \u2018Internet of Things\u2019", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T08:51:58.423503+00:00", "description": "", "title": "daq-fw-iot-pres.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/565\/attachments\/578892\/797098\/daq-fw-iot-pres.pdf", "filename": "daq-fw-iot-pres.pdf", "content_type": "application\/pdf", "type": "file", "id": 797098, "size": 9469723}], "title": "Slides", "default_folder": false, "id": 578892, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578892", "resources": [{"_type": "LocalFile", "name": "daq-fw-iot-pres.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/565\/attachments\/578892\/797098\/daq-fw-iot-pres.pdf", "fileName": "daq-fw-iot-pres.pdf", "_fossil": "localFileMetadata", "id": "797098", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/565", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "07:45:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "08:30:00"}, "duration": 45, "session": "Plenary", "keywords": [], "id": "566", "speakers": [{"_type": "ContributionParticipation", "emailHash": "756cb760c9acd08f152e44ca48421792", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "ERNST, Michael", "id": "0"}], "primaryauthors": [], "title": "The Changing Face of Networks and Implications for Future HEP Computing Models", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T07:24:51.064754+00:00", "description": "", "title": "Networking-Ernst.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/566\/attachments\/578893\/797100\/Networking-Ernst.pdf", "filename": "Networking-Ernst.pdf", "content_type": "application\/pdf", "type": "file", "id": 797100, "size": 3716532}, {"_type": "attachment", "modified_dt": "2015-04-16T07:24:51.064754+00:00", "description": "", "title": "Networking-Ernst.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/566\/attachments\/578893\/797099\/Networking-Ernst.pptx", "filename": "Networking-Ernst.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797099, "size": 6871474}], "title": "Slides", "default_folder": false, "id": 578893, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578893", "resources": [{"_type": "LocalFile", "name": "Networking-Ernst.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/566\/attachments\/578893\/797100\/Networking-Ernst.pdf", "fileName": "Networking-Ernst.pdf", "_fossil": "localFileMetadata", "id": "797100", "_deprecated": true}, {"_type": "LocalFile", "name": "Networking-Ernst.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/566\/attachments\/578893\/797099\/Networking-Ernst.pptx", "fileName": "Networking-Ernst.pptx", "_fossil": "localFileMetadata", "id": "797099", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/566", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "09:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "09:30:00"}, "duration": 30, "session": "Plenary", "keywords": [], "id": "567", "speakers": [{"_type": "ContributionParticipation", "emailHash": "771122777d10e8f8dc1c3aa7156922e8", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BLOMER, Jakob", "id": "0"}], "primaryauthors": [], "title": "Experiences on File Systems: Which is the best FS for you?", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": null, "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T00:00:29.116536+00:00", "description": "", "title": "dfs.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/567\/attachments\/578894\/797101\/dfs.pdf", "filename": "dfs.pdf", "content_type": "application\/pdf", "type": "file", "id": 797101, "size": 2523442}], "title": "Slides", "default_folder": false, "id": 578894, "description": ""}], "_type": "Contribution", "description": "", "track": null, "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578894", "resources": [{"_type": "LocalFile", "name": "dfs.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/567\/attachments\/578894\/797101\/dfs.pdf", "fileName": "dfs.pdf", "_fossil": "localFileMetadata", "id": "797101", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/15\/contribution\/567", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:45:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "229", "speakers": [{"_type": "ContributionParticipation", "emailHash": "c20242a86672d5fc162e300100556857", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "PAUS, Christoph", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c20242a86672d5fc162e300100556857", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "PAUS, Christoph", "id": "0"}], "title": "Dynamic Data Management for the Distributed CMS Computing System", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T08:20:54.953868+00:00", "description": "", "title": "chep-dyndata-1.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/229\/attachments\/578895\/797102\/chep-dyndata-1.pdf", "filename": "chep-dyndata-1.pdf", "content_type": "application\/pdf", "type": "file", "id": 797102, "size": 2820094}], "title": "Slides", "default_folder": false, "id": 578895, "description": ""}], "_type": "Contribution", "description": "The Dynamic Data Management (DDM) framework is designed to manage the majority of the CMS data in an automated fashion. At the moment 51 CMS Tier-2 data centers have the ability to host about 20 PB of data. Tier-1 centers will also be included adding substantially more space. The goal of DDM is to facilitate the management of the data distribution and optimize the accessibility of data for the physics analyses. The basic idea is to replicate highly demanded data and remove data replicas that are not often used. There are four distinct parts that work together on achieving this goal: newly created datasets are inserted into the managed data pool, highly popular datasets are replicated across multiple data centers, data centers are cleaned of the least popular data, and temporarily unavailable sites are accounted by creating replicas of missing datasets. We are also developing various metrics that show whether the system behaves as expected. The CMS Popularity Service is the authoritative source of the popularity information -- CPU time, number of accesses, number of users\/day, etc. of datasets, blocks and files. The information is collected from various resources namely directly from the CMS Software, from the CMS Remote Analysis Builder (CRAB) and from files accessed via xrootd. It is subsequently summarized and exposed using a REST API to other CMS services for further use. This paper describes the architecture of these components and details the experience of commissioning and running the system over the last year.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578895", "resources": [{"_type": "LocalFile", "name": "chep-dyndata-1.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/229\/attachments\/578895\/797102\/chep-dyndata-1.pdf", "fileName": "chep-dyndata-1.pdf", "_fossil": "localFileMetadata", "id": "797102", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/229", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:30:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "228", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ede687cc1fd66698a4d3625c52758c3d", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "SCHWANK, Karsten", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ede687cc1fd66698a4d3625c52758c3d", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "SCHWANK, Karsten", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "5e72f36f1f5b3074ab745c4f574b789d", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "KRUECKER, Dirk", "id": "1"}], "title": "Data preservation for the HERA Experiments @ DESY using dCache technology", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T09:19:16.720228+00:00", "description": "", "title": "desy-presentation.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/228\/attachments\/578896\/797103\/desy-presentation.pdf", "filename": "desy-presentation.pdf", "content_type": "application\/pdf", "type": "file", "id": 797103, "size": 2664693}], "title": "Slides", "default_folder": false, "id": 578896, "description": ""}], "_type": "Contribution", "description": "We report on the status of the data preservation project at DESY for the\r\nHERA experiments and present the latest design of the storage which is a\r\ncentral element for bit-preservation. The HEP experiments based at the\r\nHERA acceleerator at DESY collected large and unique datasets during the\r\nperiod 1992 to 2007. In addition, corresponding Monte Carlo simulation\r\ndatasets were produced, which are significantly larger by volume and still\r\nbeing added to as the final analyses are completed. As part of the ongoing\r\nDPHEP data preservation efforts at DESY, these data sets must be\r\ntransferred into storage systems that guarantee a reliable long term\r\naccess. At the same time, given that the experiments are still active,\r\neasy access to the data must be guaranteed for the coming years.\r\n  The long term storage system is two-fold: an archive part where the\r\ndata exists on two tape copies and an online part where the full dataset\r\ncan be kept available and allows easy access to all HERA data. The archive\r\nand online parts are physically separate. The demanding aspect of this\r\ndata is not only the size of about 1PB but also the large number (about 4\r\nmillion) of files and the broad range of file sizes from a few KB to a few\r\nhundred of GB. To achieve a high level of reliability, we use the dCache\r\ndistributed storage solution and make use of its replication capabilities\r\nand tape interfaces. We describe the dCache installation with tape backend\r\nthat is used as mass storage together with an newly introduced small files\r\nservice that allows for the automatic creation of tape friendly container\r\nfiles, containing many single (small) files. From the user's point of\r\nview, this is done in a fully transparent way in terms of creation and\r\naccess to the data.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578896", "resources": [{"_type": "LocalFile", "name": "desy-presentation.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/228\/attachments\/578896\/797103\/desy-presentation.pdf", "fileName": "desy-presentation.pdf", "_fossil": "localFileMetadata", "id": "797103", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "a198240a6b21354217ead633867b3e31", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "FUHRMANN, Patrick", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "b6c0880cbbcfc431d7c87084519934e5", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "SOUTH, David", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "a2b3450748ef067b1408b085a6ce18a4", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "LEWENDEL, Birgit", "id": "4"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/228", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "227", "speakers": [{"_type": "ContributionParticipation", "emailHash": "25d9c39b3bbd63706b9069a4e9bc80d5", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PARDI, Silvio", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "74f37452da18d70a445f84d900eff716", "affiliation": "Universita e INFN, Roma I (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "DE SALVO, Alessandro", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "25d9c39b3bbd63706b9069a4e9bc80d5", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PARDI, Silvio", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "74f37452da18d70a445f84d900eff716", "affiliation": "Universita e INFN, Roma I (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "DE SALVO, Alessandro", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "9f0167501724b282e5ea3906007a186c", "affiliation": "Univ. + INFN", "_fossil": "contributionParticipationMetadata", "fullName": "DORIA, Alessandra", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "be94bcf4e42480309d7fd4e274b250ed", "affiliation": "DANTE", "_fossil": "contributionParticipationMetadata", "fullName": "CAPONE, Vincenzo", "id": "4"}], "title": "A prototype Infrastructure for Cloud-based distributed services in High Availability over WAN", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The advancements in technologies on provisioning  end-to-end network services over geographical networks, together with the consolidation of Cloud Technologies, allow the creation of innovative scenarios for data centers.\r\n\r\nIn this work, we present  the architecture and performance studies concerning  a prototype of distributed Tier2 infrastructure for HEP, instantiated between the two Italian sites of INFN-Roma1 and INFN-Napoli.\r\n\r\nThe Network Infrastructure is based on a Layer-2 geographical link, provided by the Italian NREN (GARR),  directly connecting the two remote LANs of the named sites. By exploiting the possibilities offered by the new distributed file systems, a shared storage area with synchronous copy has been set up. The Computing Infrastructure, based on a OpenStack facility, is using a set of distributed Hypervisors installed in both sites. The Cloud is then using the common distributed storage facility to allow High Availability (HA) and Crash Recovery.\r\nThe result is a strongly coupled system, enabling the creation, management and migration of Virtual Machines in both sites.\r\n\r\nThe main parameter to be taken into account when managing two remote sites with a single framework is the the effect of the latency, due to the distance and the end-to-end service overhead. In order to understand the capabilities and limits of our setup, the impact of latency has been investigated by means of a set of stress tests, including data I\/O throughput, metadata access performance evaluation and network occupancy, during the life cycle of a Virtual Machine. A set of resilience tests has been also performed, in order to verify the stability of the system on event of hardware or software fault.\r\n\r\nThe results of this work show that the reliability and robustness of the chosen architecture are effective enough to build a real system and to provide common services. This prototype can also be extended to multiple sites, by changing the network topology, thus creating a National Network of Cloud-based distributed services in HA over WAN.", "track": "Track7: Clouds and virtualization", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "0e11b602f0fb55fd859226db6501a729", "affiliation": "INFN Roma", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PASQUALUCCI, Enrico", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "59639cfc77bc8e5c12d52e4f0d7bd7a1", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "CARLINO, Giampaolo", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "d473bbcd38d8649862d2124160ad515e", "affiliation": "Dipartim.di Fisica G.Marconi Rome I-Universita di Roma I \"La Sap", "_fossil": "contributionParticipationMetadata", "fullName": "BULFON, Cristina", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "704a601ae94eca0d0060d3ce4631c533", "affiliation": "GARR - the Italian National Research and Education Network", "_fossil": "contributionParticipationMetadata", "fullName": "PUCCIO, Lorenzo", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "fd37f4fd0ab0db66bd24580c580fdd0a", "affiliation": "GARR - the Italian National Research and Education Network", "_fossil": "contributionParticipationMetadata", "fullName": "CARBONI, Massimo", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "172062e9ed22aa57975d89af45109434", "affiliation": "GARR - the Italian National Research and Education Network", "_fossil": "contributionParticipationMetadata", "fullName": "BOLLETTA, Paolo", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "92f3f5e165134e346477c14761afe805", "affiliation": "INFN Sez. Roma1", "_fossil": "contributionParticipationMetadata", "fullName": "GRAZIOSI, Carlo", "id": "10"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/227", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:45:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:00:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "226", "speakers": [{"_type": "ContributionParticipation", "emailHash": "8f14d48f0c3b00fdaea268fa6294ee99", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DENIS, Marek Kamil", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "8f14d48f0c3b00fdaea268fa6294ee99", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DENIS, Marek Kamil", "id": "0"}], "title": "Cloud Federation - the new way to build distributed clouds", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T05:22:21.420106+00:00", "description": "", "title": "CHEP2015_-_CLOUD_FEDERATION.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/226\/attachments\/578897\/797104\/CHEP2015_-_CLOUD_FEDERATION.pdf", "filename": "CHEP2015_-_CLOUD_FEDERATION.pdf", "content_type": "application\/pdf", "type": "file", "id": 797104, "size": 1053711}], "title": "Slides", "default_folder": false, "id": 578897, "description": ""}], "_type": "Contribution", "description": "Cloud federation brings an old concept into new technology, allowing for sharing resources between independent cloud installations. Cloud computing starts to play major role in HEP and e-science allowing resources to be obtained on demand. Cloud federation supports sharing between independent organizations and companies coming from the commercial world such as public clouds, bringing new ways of joint collaboration along with security and usability.\r\n\r\nFrom the OpenStack Icehouse release, cloud federation, based on the SAML2 protocol, is now integrated with the standard open source tree and becoming a reality. Examples of this configuration both command line and web based use of providers such as EduGain or connections to public clouds such as Rackspace.\r\n\r\nThis presentation will demonstrate what functionalities OpenStack brings, what are the main concepts, how it was designed, how federation with the SAML ECP extension allows use of REST and CLIs, and what are our plans to use real hybrid clouds - identity federation, optimal image sharing and networking layer which would allow for creating distributed vLANs.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578897", "resources": [{"_type": "LocalFile", "name": "CHEP2015_-_CLOUD_FEDERATION.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/226\/attachments\/578897\/797104\/CHEP2015_-_CLOUD_FEDERATION.pdf", "fileName": "CHEP2015_-_CLOUD_FEDERATION.pdf", "_fossil": "localFileMetadata", "id": "797104", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/226", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:00:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "225", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e86323288b18387d48dc1af9718c14d6", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WILDISH, Tony", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "d4502e02b88247265f7423d8fafc6ba3", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "RIAHI, Hassen", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "e86323288b18387d48dc1af9718c14d6", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WILDISH, Tony", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "bc7330245b15761981349f5fa0fed77c", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "CIANGOTTINI, Diego", "id": "2"}], "title": "AsyncStageOut: Distributed user data management for CMS Analysis", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T22:07:31.633311+00:00", "description": "", "title": "ASO_final.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/225\/attachments\/578898\/797105\/ASO_final.pdf", "filename": "ASO_final.pdf", "content_type": "application\/pdf", "type": "file", "id": 797105, "size": 1494855}, {"_type": "attachment", "modified_dt": "2015-04-12T22:07:31.633311+00:00", "description": "", "title": "ASO_final.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/225\/attachments\/578898\/797106\/ASO_final.pptx", "filename": "ASO_final.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797106, "size": 3490785}], "title": "Slides", "default_folder": false, "id": 578898, "description": ""}], "_type": "Contribution", "description": "AsyncStageOut (ASO) is a new component of the distributed data analysis system of CMS, CRAB, designed for managing users' data. It addresses a major weakness of the previous model, namely that data movement was part of the job execution resulting in inefficient use of job slots and an unacceptable failure rate at the end of the jobs.\r\n\r\nASO foresees the management of up to 400k files per day of various sizes, spread worldwide across more than 60 sites. It must handle up to 1000 individual users per month, and work with minimal delay. This creates challenging requirements for system scalability, performance and monitoring.\r\n\r\nASO uses FTS to schedule and execute the transfers between the storage elements of the source and destination sites. It has evolved from a limited prototype to a highly adaptable service, which manages and monitors the user file placement and bookkeeping. To ensure system scalability and data monitoring, it employs new technologies such as a NoSQL database and re-uses existing components of PhEDEx and the FTS Dashboard.\r\n\r\nWe present the asynchronous stage-out strategy and the architecture of the solution we implemented to deal with those issues and challenges. The deployment model for the high availability and scalability of the service is discussed. The performance of the system during the commissioning and the first phase of production are also shown, along with results from simulations designed to explore the limits of scalability.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578898", "resources": [{"_type": "LocalFile", "name": "ASO_final.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/225\/attachments\/578898\/797105\/ASO_final.pdf", "fileName": "ASO_final.pdf", "_fossil": "localFileMetadata", "id": "797105", "_deprecated": true}, {"_type": "LocalFile", "name": "ASO_final.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/225\/attachments\/578898\/797106\/ASO_final.pptx", "fileName": "ASO_final.pptx", "_fossil": "localFileMetadata", "id": "797106", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "dd2d05df8869eb91f302ce4046407f24", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ANDREEVA, Julia", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "4a66eba8bd02dcd2372a3cd6255f9fc5", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KARAVAKIS, Edward", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "91b6e0458cfc38c6e2b8701f5a7b9a24", "affiliation": "Universita & INFN, Milano-Bicocca (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "MASCHERONI, Marco", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "d566d9c238d24452797875574a57cae6", "affiliation": "Univ. of California San Diego (US)", "_fossil": "contributionParticipationMetadata", "fullName": "TANASIJCZUK, Andres Jorge", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "65fd5d0f517c5ad7a467486ead92360b", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. VAANDERING, Eric", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "d41d8cd98f00b204e9800998ecf8427e", "affiliation": "DiSCC, Vilnius University, Lithuania", "_fossil": "contributionParticipationMetadata", "fullName": "BACLAS, Justas", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "d41d8cd98f00b204e9800998ecf8427e", "affiliation": "CIEMAT, Madrid, Spain", "_fossil": "contributionParticipationMetadata", "fullName": "HERNANDEZ, Jose", "id": "9"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/225", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "224", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ef2db26b320ecf7a9b75abbaef349287", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LASSNIG, Mario", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ef2db26b320ecf7a9b75abbaef349287", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LASSNIG, Mario", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "cf29a05330d5f43955651b32543f7283", "affiliation": "University of Vienna (AT)", "_fossil": "contributionParticipationMetadata", "fullName": "VIGNE, Ralph", "id": "6"}], "title": "Scalable and fail-safe deployment of the ATLAS Distributed Data Management system Rucio", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-03-27T14:25:23.530704+00:00", "description": "", "title": "Scalable_and_fail-safe_deployment_of_Rucio.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/224\/attachments\/578899\/797107\/Scalable_and_fail-safe_deployment_of_Rucio.pdf", "filename": "Scalable_and_fail-safe_deployment_of_Rucio.pdf", "content_type": "application\/pdf", "type": "file", "id": 797107, "size": 831999}], "title": "Poster", "default_folder": false, "id": 578899, "description": ""}], "_type": "Contribution", "description": "This contribution details the deployment of Rucio, the ATLAS Distributed Data Management system. The main complication is that Rucio interacts with a wide variety of external services, and connects globally distributed data centres under different technological and administrative control, at an unprecedented data volume. It is therefore not possibly to create a duplicate instance of Rucio for testing or integration. Every software upgrade or configuration change is thus potentially disruptive and requires fail-safe software and automatic error recovery. Rucio uses a three-layer scaling and mitigation strategy based on quasi-realtime monitoring. This strategy mainly employs independent stateless services, automatic failover, and service migration. The technologies used for deployment and mitigation include OpenStack, Puppet, Graphite, HAProxy, Apache, and nginx. In this contribution, the reasons and design decisions for the deployment, the actual implementation, and an evaluation of all involved services and components are discussed.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578899", "resources": [{"_type": "LocalFile", "name": "Scalable_and_fail-safe_deployment_of_Rucio.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/224\/attachments\/578899\/797107\/Scalable_and_fail-safe_deployment_of_Rucio.pdf", "fileName": "Scalable_and_fail-safe_deployment_of_Rucio.pdf", "_fossil": "localFileMetadata", "id": "797107", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "46207806844c107a8fefe566f669b206", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BARISITS, Martin", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "c0af0a453fb7de772f32e585bcdcd2ba", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GARONNE, Vincent", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "024219837a74f1e2a8825afaccd81fbf", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SERFON, Cedric", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "6db250f3509377bad379e91097c39ae5", "affiliation": "Bergische Universitaet Wuppertal (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "BEERMANN, Thomas", "id": "7"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/224", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:00:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "223", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6db250f3509377bad379e91097c39ae5", "affiliation": "Bergische Universitaet Wuppertal (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "BEERMANN, Thomas", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6db250f3509377bad379e91097c39ae5", "affiliation": "Bergische Universitaet Wuppertal (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "BEERMANN, Thomas", "id": "0"}], "title": "A study on dynamic data placement for the ATLAS Distributed Data Management system", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T05:30:22.327611+00:00", "description": "", "title": "ATL-SOFT-SLIDE-2015-109.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/223\/attachments\/578900\/797108\/ATL-SOFT-SLIDE-2015-109.pdf", "filename": "ATL-SOFT-SLIDE-2015-109.pdf", "content_type": "application\/pdf", "type": "file", "id": 797108, "size": 372829}], "title": "Slides", "default_folder": false, "id": 578900, "description": ""}], "_type": "Contribution", "description": "This contribution presents a study on the applicability and usefulness of dynamic data placement methods for data-intensive systems, such as ATLAS distributed data management (DDM). In this system the jobs are sent to the data, therefore having a good distribution of data is significant. Ways of forecasting workload patterns are examined which then are used to redistribute data to achieve a better overall utilisation of computing resources and to reduce waiting time for jobs before they can run on the grid.  This method is based on a tracer infrastructure that is able to monitor and store historical data accesses and which is used to create popularity reports. These reports provide detailed summaries about data accesses in the past, including information about the accessed files, the involved users and the sites. From this past data it is possible to then make near-term forecasts for data popularity in the future. This study evaluates simple prediction methods as well as more complex methods like neural networks.  Based on the outcome of the predictions a redistribution algorithm deletes unused replicas and adds new replicas for potentially popular datasets. Finally, a grid simulator is used to examine the effects of the redistribution. The simulator replays workload on different data distributions while measuring the job waiting time and site usage. The study examines how the average waiting time is affected by the amount of data that is moved, how it differs for the various forecasting methods and how that compares to the optimal data distribution.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578900", "resources": [{"_type": "LocalFile", "name": "ATL-SOFT-SLIDE-2015-109.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/223\/attachments\/578900\/797108\/ATL-SOFT-SLIDE-2015-109.pdf", "fileName": "ATL-SOFT-SLIDE-2015-109.pdf", "_fossil": "localFileMetadata", "id": "797108", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "9f60b1ecea45598b615f77b2b41150f1", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "STEWART, Graeme", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "5cebfcc5291b81b71da33ace1d767fec", "affiliation": "Bergische Universitaet Wuppertal (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "MATTIG, Peter", "id": "2"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/223", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:00:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "222", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a9cb2a1f46769e9b21118763c1883604", "affiliation": "Instituto de Fisica Corpuscular (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "SANCHEZ, Javier", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "a9cb2a1f46769e9b21118763c1883604", "affiliation": "Instituto de Fisica Corpuscular (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "SANCHEZ, Javier", "id": "0"}], "title": "Distributed Data Collection for the ATLAS EventIndex.", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-07T08:01:47.810928+00:00", "description": "", "title": "ATL-SOFT-SLIDE-2015-108.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/222\/attachments\/578901\/797109\/ATL-SOFT-SLIDE-2015-108.pdf", "filename": "ATL-SOFT-SLIDE-2015-108.pdf", "content_type": "application\/pdf", "type": "file", "id": 797109, "size": 597978}], "title": "Slides", "default_folder": false, "id": 578901, "description": ""}], "_type": "Contribution", "description": "The ATLAS EventIndex contains records of all events processed by ATLAS, in all processing stages. These records include the references to the files containing each event (the GUID of the file) and the internal \u201cpointer\u201d to each event in the file. This information is collected by all jobs that run at Tier-0 or on the Grid and process ATLAS events. Each job produces a snippet of information for each permanent output file. This information is packed and transfered to a central broker at CERN using an ActiveMQ messaging system, and then is unpacked, sorted and reformatted in order to be stored and catalogued into a central Hadoop server. This talk describes in detail the Producer\/Consumer architecture to convey this information from the running jobs through the messaging system to the Hadoop server.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578901", "resources": [{"_type": "LocalFile", "name": "ATL-SOFT-SLIDE-2015-108.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/222\/attachments\/578901\/797109\/ATL-SOFT-SLIDE-2015-108.pdf", "fileName": "ATL-SOFT-SLIDE-2015-108.pdf", "_fossil": "localFileMetadata", "id": "797109", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "74cb8291d5a2396b10ef9a355f4d38da", "affiliation": "Instituto de Fisica Corpuscular (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "FERNANDEZ CASANI, Alvaro", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "1e0a696b292903384c9b5f8507834d45", "affiliation": "IFIC-Valencia", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GONZALEZ DE LA HOZ, Santiago", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "c11231d8066a85165b23821c2e018d66", "affiliation": "Universidad de Valencia (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "SANCHEZ, Javier", "id": "3"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/222", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "221", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a5efba4a70913e2df05be5f3fdfb3ee5", "affiliation": "Laboratoire de l'Accelerateur Lineaire (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HRIVNAC, Julius", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "a5efba4a70913e2df05be5f3fdfb3ee5", "affiliation": "Laboratoire de l'Accelerateur Lineaire (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. HRIVNAC, Julius", "id": "1"}], "title": "QuerySpaces on Hadoop for the ATLAS EventIndex", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T00:00:55.581805+00:00", "description": "", "title": "QuerySpaces.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/221\/attachments\/578902\/797110\/QuerySpaces.pdf", "filename": "QuerySpaces.pdf", "content_type": "application\/pdf", "type": "file", "id": 797110, "size": 585332}], "title": "Slides", "default_folder": false, "id": 578902, "description": ""}], "_type": "Contribution", "description": "The new ATLAS EventIndex catalogue uses a Hadoop cluster to store information on each event processed by ATLAS. Several tools belonging to the Hadoop eco-system are used to organise the data in HDFS, catalogue it internally, and provide the search functionality. This presentation will describe the Hadoop-based implementation of the adaptive query engine serving as the back-end for the ATLAS EventIndex. The QuerySpaces implementation handles both original data and search results providing fast and efficient mechanisms for new user queries using already accumulated knowledge for optimisation. Detailed description and statistics about user requests are collected in HBase tables and HDFS files. Requests are associated to their results and a graph of relations between them is created to be used to find the most efficient way of providing answers to new requests. The environment is completely transparent to users and is accessible over several command-line interfaces, a Web Service and a programming API.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578902", "resources": [{"_type": "LocalFile", "name": "QuerySpaces.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/221\/attachments\/578902\/797110\/QuerySpaces.pdf", "fileName": "QuerySpaces.pdf", "_fossil": "localFileMetadata", "id": "797110", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "1ac84e9dd5fc3113c1e4ee308aac1693", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "TOEBBICKE, Rainer", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "d8619a9dff304fef97736b1d40cb7569", "affiliation": "Laboratoire de l'Accelerateur Lineaire (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "YUAN, Ruijun", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "87a100f703ae951e1cd6f8f0c55b8582", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CRANSHAW, Jack", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "92cc19bc26fd5920c677f699da8c4264", "affiliation": "Universit\u00e0 degli Studi e INFN Genova", "_fossil": "contributionParticipationMetadata", "fullName": "FAVARETO, Andrea", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "7ca77722c99b3cb5ba43a17522e24d50", "affiliation": "Universidad Autonoma de Madrid", "_fossil": "contributionParticipationMetadata", "fullName": "GLASMAN, Claudia", "id": "5"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/221", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "220", "speakers": [{"_type": "ContributionParticipation", "emailHash": "35f0509459ee62f9ddcf584936cca96a", "affiliation": "Universit\u00e0 e INFN Genova (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BARBERIS, Dario", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c195ef3b576ede75b0ccfd47c33fba3a", "affiliation": "University of Oxford (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "GALLAS, Elizabeth", "id": "0"}], "title": "Integration of the EventIndex with other ATLAS systems", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T14:17:58.007811+00:00", "description": "", "title": "ATL-SOFT-SLIDE-2015-100.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/220\/attachments\/578903\/797111\/ATL-SOFT-SLIDE-2015-100.pdf", "filename": "ATL-SOFT-SLIDE-2015-100.pdf", "content_type": "application\/pdf", "type": "file", "id": 797111, "size": 2429630}], "title": "Poster", "default_folder": false, "id": 578903, "description": ""}], "_type": "Contribution", "description": "The ATLAS EventIndex System, developed for use in LHC Run 2, is designed to index every processed event in ATLAS, replacing the TAG System used in Run 1. Its storage infrastructure, based on Hadoop, necessitates revamping how information in this system relates to other ATLAS systems. In addition, the scope of this new application is different from that of the TAG System. It will store fewer derived quantities, but store more indexes since the fundamental mechanisms for retrieving these indexes will be better integrated into all stages of processing, allowing more events from later stages of processing to be indexed than was possible with the previous system.\r\nConnections with other systems are fundamentally critical to assess dataset completeness, identify data duplication, and check data integrity, but also needed to enhance user and system interfaces accessing information in EventIndex. This presentation will give an overview of the ATLAS systems involved, the relevant metadata, and describe the technologies we are deploying to complete these connections.", "track": "Track3: Data store and access ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578903", "resources": [{"_type": "LocalFile", "name": "ATL-SOFT-SLIDE-2015-100.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/220\/attachments\/578903\/797111\/ATL-SOFT-SLIDE-2015-100.pdf", "fileName": "ATL-SOFT-SLIDE-2015-100.pdf", "_fossil": "localFileMetadata", "id": "797111", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "5b3e126078a6dedb8418173ea224b6c8", "affiliation": "Federico Santa Maria Technical University (CL)", "_fossil": "contributionParticipationMetadata", "fullName": "PROKOSHIN, Fedor", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/220", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "114", "speakers": [{"_type": "ContributionParticipation", "emailHash": "9fdcc3dac42d3568bc6b9e5fb3454a3f", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ALVAREZ AYLLON, Alejandro", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "9fdcc3dac42d3568bc6b9e5fb3454a3f", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ALVAREZ AYLLON, Alejandro", "id": "0"}], "title": "FTS3 on the Web", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-01T12:18:26.203848+00:00", "description": "", "title": "fts3web.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/114\/attachments\/578904\/797112\/fts3web.pdf", "filename": "fts3web.pdf", "content_type": "application\/pdf", "type": "file", "id": 797112, "size": 3826299}], "title": "Poster", "default_folder": false, "id": 578904, "description": ""}], "_type": "Contribution", "description": "FTS3 is the service responsible for the distribution of the LHC data across the WLCG Infrastructure. To facilitate its use outside the traditional grid environment we have provided a web application - known as WebFTS - fully oriented towards final users, and easily usable within a browser.\r\n\r\nThis web application is completely decoupled from the core service, and interfaces with it via a REST API. Following the principle of maximum reusability, we have recently developed a pluggable OAuth2 provider for the FTS3 REST interface, so any third party can now use FTS3 as a framework for building their own transfer-based applications, effectively outsourcing any concern about delegation, scheduling or underlying protocol details.\r\n\r\nHere we will describe how FTS3 and WebFTS fit together using this approach, and how the same interface could be easily reused by external developers to create their own web services relying on FTS3 as a gateway between the Grid, the Web and the Cloud. We will also provide the outline for some potentially interesting uses of this idea, for example a synchronization mechanism between storage elements.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578904", "resources": [{"_type": "LocalFile", "name": "fts3web.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/114\/attachments\/578904\/797112\/fts3web.pdf", "fileName": "fts3web.pdf", "_fossil": "localFileMetadata", "id": "797112", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "b8b5d3fc8e61cdad2dd038664f65d89b", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MANZI, Andrea", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "d00e17b2efa3f0b0609e489beb0a8411", "affiliation": "B.P. Konstantinov Petersburg Nuclear Physics Institute - PNPI (", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. KIRYANOV, Andrey", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "2bc2abdabb8d1f21dae00de97010754f", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SALICHOS, Michail", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "f6899d026502f8f5a43544a4ca322c95", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SIMON, Michal Kamil", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "044a6c45e1089ebc257b67572ae11f8d", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "KEEBLE, Oliver", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "b0c6bfb81e227b0d79f905ee0b64dd8e", "affiliation": "National Technical Univ. of Athens (GR)", "_fossil": "contributionParticipationMetadata", "fullName": "SKARPATHIOTAKI, Christina", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "68c97bd2a016fa65faad2c6e3e91bb2a", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SCHULZ, Markus", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "cc47a9e2d58495274095bb64d75714f1", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ABAD RODRIGUEZ, Andres", "id": "7"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/114", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "391", "speakers": [{"_type": "ContributionParticipation", "emailHash": "0227951f2719a9a4069e664a2d4227fd", "affiliation": "IN2P3\/LAL", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BINET, Sebastien", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "0227951f2719a9a4069e664a2d4227fd", "affiliation": "IN2P3\/LAL", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BINET, Sebastien", "id": "0"}], "title": "pawgo: an interactive analysis workstation", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "pawgo: an interactive analysis workstation\r\n==========================================\r\n\r\nCurrent interactive analysis toolkits usually leverage a\r\nTuring-complete general programming language, such as `C++`\r\nor `python`, married with some kind of interpreter (_e.g.:_ `CINT` or\r\n`CLing`) and a graphical user interface to present results (`ROOT`,\r\n`matplotlib` or `Chaco`.)\r\nAn obvious advantage of using a general programming language is that\r\none can tap the full power of that language to build any analysis.\r\nBut conversely, users have to comply with the sometimes verbose ways\r\nof that language to describe \"graphics\" entities or express\r\nmanipulations to be applied to these entities, letting much to be\r\nimproved on the interactivity and reactivity fronts.\r\n\r\n`pawgo` is a prototype re-investigating the use of a Domain Specific\r\nLanguage (DSL) - whith a nod to the old `PAW` and `PAW++`\r\napplications. \r\n\r\nThis paper will first introduce the overall architecture of `pawgo`, a\r\nset of reusable `Go` libraries packaged as a portable application, and\r\nthe tools it leverages (`d3.js`, `WebSockets` and `RPC` to name a few)\r\nto enable both remote and local work as well as batch execution mode.\r\nMost importantly, the paper will describe the use of the grammar of\r\ngraphics to overcome the impedance mismatch between general\r\nprogramming languages and graphics manipulations, thus easily allowing\r\nusers to create new graphic objects, animations or styles.\r\nIndeed, the grammar of graphics lets users and developers focus on the\r\n\"what\" to display, letting the underlying library deal with the minute\r\ndetails and ceremony of the \"how\" best to display.\r\n\r\nFinally, the paper will present prospects and work ahead for `pawgo`\r\nto graduate from a prototype to a full-fledged analysis workstation.", "track": "Track2: Offline software ", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/391", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "390", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e43cbedc9022e3c3d7335122469626db", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "D'URSO, Domenico", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2199f3493ec3861f745d89d8c0705c05", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "DURANTI, Matteo", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "e43cbedc9022e3c3d7335122469626db", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "D'URSO, Domenico", "id": "1"}], "title": "A flexible and modular data format ROOT-based implementation for HEP", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T15:41:45.058095+00:00", "description": "", "title": "miniDST_100_70.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/390\/attachments\/578905\/797113\/miniDST_100_70.pdf", "filename": "miniDST_100_70.pdf", "content_type": "application\/pdf", "type": "file", "id": 797113, "size": 974430}], "title": "Poster", "default_folder": false, "id": 578905, "description": ""}], "_type": "Contribution", "description": "A flexible and modular data format implementation for HEP applications is presented. \r\nDesigned to face HEP data issues, the implementation is based on the CERN ROOT toolkit. \r\nThe design is aimed to create a data format as much as possible modular and easily upgradable and extendable.\r\nEvent informations are split into different files, that may contain different parts of the event (i.e. different sub-detectors) or different levels of abstraction (i.e. from raw information to high level quantities), in a fully transparent way for the final user.\r\nFully exploiting the ROOT \"Friend\" concept and the C++ inheritance, it has been possible to achieve the desired modularity and flexibility: the possibility to upgrade and extend without a full reprocessing, but also the simplicity of a standard ROOT-ple functionality (i.e. TTree::Draw()).\r\nThe file splitting in self-consistent TFile friends allows also a much more efficient distribution and upgrade of the data to the regional centers of a typical HEP experiment. It also gives the possibility to download and process only a small section of the event information, for example for subdetector studies or specific analyses, even on the user laptop.", "track": "Track2: Offline software ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578905", "resources": [{"_type": "LocalFile", "name": "miniDST_100_70.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/390\/attachments\/578905\/797113\/miniDST_100_70.pdf", "fileName": "miniDST_100_70.pdf", "_fossil": "localFileMetadata", "id": "797113", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "d22ac7922513e639e150d8c42e32190b", "affiliation": "Massachusetts Inst. of Technology (US)", "_fossil": "contributionParticipationMetadata", "fullName": "ZUCCON, Paolo", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/390", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:30:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:45:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "151", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2368fbf6eeafdccf7f1cf1924e2bde61", "affiliation": "Southern Methodist University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KAMA, Sami", "id": "4"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "9f60b1ecea45598b615f77b2b41150f1", "affiliation": "University of Glasgow (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "STEWART, Graeme", "id": "0"}], "title": "Requirements for a Next Generation Framework: ATLAS Experience", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T05:18:54.938431+00:00", "description": "", "title": "Requirements_for_a_Next_Generation_Framework.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/151\/attachments\/578906\/797114\/Requirements_for_a_Next_Generation_Framework.pdf", "filename": "Requirements_for_a_Next_Generation_Framework.pdf", "content_type": "application\/pdf", "type": "file", "id": 797114, "size": 2461863}, {"_type": "attachment", "modified_dt": "2015-04-13T05:18:54.938431+00:00", "description": "", "title": "Requirements_for_a_Next_Generation_Framework.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/151\/attachments\/578906\/797115\/Requirements_for_a_Next_Generation_Framework.pptx", "filename": "Requirements_for_a_Next_Generation_Framework.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797115, "size": 2218010}], "title": "Slides", "default_folder": false, "id": 578906, "description": ""}], "_type": "Contribution", "description": "The challenge faced by HEP experiments from the current and expected architectural evolution of CPUs and co-processors is how to successfully exploit concurrency and keep memory consumption within reasonable limits. This is a major change from frameworks which were designed for serial event processing on single core processors in the 2000s. ATLAS has recently considered this problem in some detail through its Future Frameworks Requirements group.\r\n\r\nHere we report on the major considerations of the group, which was charged with considering the best strategies to exploit current and anticipated CPU technologies. The group has re-examined the basic architecture of event processing and considered how the building blocks of a framework (algorithms, services, tools and incidents) should evolve. The group has also had to take special care to ensure that the use cases of the ATLAS high level trigger are encompassed, which differ in important ways from offline event processing (for example, 99% of events are rejected, which must be done with minimum resource investment). Finally, the group has considered how best to\r\nuse the wide variety of concurrency techniques, such as multi-processing, multi-threading, offloaded accelerator technology, parallel I\/O, and how to exploit resources such as high performance computing sites.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578906", "resources": [{"_type": "LocalFile", "name": "Requirements_for_a_Next_Generation_Framework.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/151\/attachments\/578906\/797114\/Requirements_for_a_Next_Generation_Framework.pdf", "fileName": "Requirements_for_a_Next_Generation_Framework.pdf", "_fossil": "localFileMetadata", "id": "797114", "_deprecated": true}, {"_type": "LocalFile", "name": "Requirements_for_a_Next_Generation_Framework.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/151\/attachments\/578906\/797115\/Requirements_for_a_Next_Generation_Framework.pptx", "fileName": "Requirements_for_a_Next_Generation_Framework.pptx", "_fossil": "localFileMetadata", "id": "797115", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "ee92c40a897713aa60e98f857dc82844", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "BAINES, John", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c86400decf25d81fa6a9e609c133e417", "affiliation": "AGH Univ. of Science and Technology, Krakow", "_fossil": "contributionParticipationMetadata", "fullName": "BOLD, Tomasz", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "c3b302a4fc2fd1ca19138cb3a3e5479a", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CALAFIURA, Paolo", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "2368fbf6eeafdccf7f1cf1924e2bde61", "affiliation": "Southern Methodist University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KAMA, Sami", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "b0300a2ba72bacc349647c1313f4ba21", "affiliation": "University of Arizona (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LAMPL, Walter", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "287eff907e8b88ac28442324d4258129", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LEGGETT, Charles", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "9308ae80f0f8aaf14aef2fa1243f3fec", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MALON, David", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "e3a9c215d966fc5e240cdb5f16cc8116", "affiliation": "University of Edinburgh (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "WYNNE, Benjamin Michael", "id": "8"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/151", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:15:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "150", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ab58d4e264ffe77c17ef591cb9a7c76a", "affiliation": "Royal Holloway, University of London", "_fossil": "contributionParticipationMetadata", "fullName": "COWAN, Glen", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "9344b8dfa3c6cea5bcfa52aa56f897b9", "affiliation": "LAL-Orsay, FR", "_fossil": "contributionParticipationMetadata", "fullName": "ROUSSEAU, David", "id": "0"}], "title": "The ATLAS Higgs Machine Learning Challenge", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T17:17:06.739498+00:00", "description": "", "title": "cowan_chep_2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/150\/attachments\/578907\/797116\/cowan_chep_2015.pdf", "filename": "cowan_chep_2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 797116, "size": 2195674}], "title": "Slides", "default_folder": false, "id": 578907, "description": ""}], "_type": "Contribution", "description": "High Energy Physics has been using Machine Learning techniques (commonly known as Multivariate Analysis) since the 1990s with Artificial Neural Net and more recently with Boosted Decision Trees, Random Forest etc. Meanwhile, Machine Learning has become a full blown field of computer science. With the emergence of Big Data, data scientists are developing new Machine Learning algorithms to extract meaning from large heterogeneous data.\r\n\r\nHEP has exciting and difficult problems like the extraction of the Higgs boson signal, and at the same time data scientists have advanced algorithms: the goal of the HiggsML project was to bring the two together by a \u201cchallenge\u201d: participants from all over the world and any scientific background could compete online to obtain the best Higgs to tau tau signal significance on a set of ATLAS fully simulated Monte Carlo signal and background.  \r\n\r\nInstead of HEP physicists browsing through machine learning papers and trying to infer which new algorithms might be useful for HEP, then coding and tuning them, the challenge has brought realistic HEP data to the data scientists on the Kaggle platform, which is well known in the Machine Learning community.\r\nThe challenge has been organized by the ATLAS collaboration associated to data scientists, in partnership with the Paris Saclay Center for Data Science, CERN and Google.\r\nThe challenge ran from May to September 2014, drawing considerable attention. 1785 teams participated, making it the most popular challenge ever on the Kaggle platform. New Machine Learning techniques have been used by the participants with significantly better results than usual HEP tools. \r\nThis presentation has two parts: the first one describes how a HEP problem was simplified (not too much!) and wrapped up into an online challenge, the second what was learned from the challenge, in terms of new Machine Learning algorithms and techniques which could have an impact on future HEP analysis.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578907", "resources": [{"_type": "LocalFile", "name": "cowan_chep_2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/150\/attachments\/578907\/797116\/cowan_chep_2015.pdf", "fileName": "cowan_chep_2015.pdf", "_fossil": "localFileMetadata", "id": "797116", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "ab58d4e264ffe77c17ef591cb9a7c76a", "affiliation": "Royal Holloway, University of London", "_fossil": "contributionParticipationMetadata", "fullName": "COWAN, Glen", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "b5d397d364f729abc4025f6fc8fa6aa8", "affiliation": "Laboratoire de l'Accelerateur Lineaire (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "ADAM BOURDARIOS, Claire", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "37afc1506b15a2f9c37e3e8769142580", "affiliation": "LAL-Orsay, FR", "_fossil": "contributionParticipationMetadata", "fullName": "K\u00c9GL, Bal\u00e1zs", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "023559fcb995ce32f10454960a13a306", "affiliation": "Laboratoire de Recherche en Informatique", "_fossil": "contributionParticipationMetadata", "fullName": "GERMAIN-RENAUD, Cecile", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "638aad170aa2c73ac1edfca42827d4d1", "affiliation": "Chalearn", "_fossil": "contributionParticipationMetadata", "fullName": "GUYON, Isabelle", "id": "5"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/150", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "153", "speakers": [{"_type": "ContributionParticipation", "emailHash": "324a6154e0c7d98ce1a072b8f2327c65", "affiliation": "Universita e INFN Roma Tor Vergata (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "MAZZAFERRO, Luca", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "324a6154e0c7d98ce1a072b8f2327c65", "affiliation": "Universita e INFN Roma Tor Vergata (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "MAZZAFERRO, Luca", "id": "0"}], "title": "Bringing ATLAS production to HPC resources - A use case with the Hydra supercomputer of the Max Planck Society", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-08T08:40:15.867345+00:00", "description": "", "title": "HYDRA-ATLAS-Integration.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/153\/attachments\/578908\/797117\/ATL-SOFT-SLIDE-2015-107-printed.pdf", "filename": "ATL-SOFT-SLIDE-2015-107-printed.pdf", "content_type": "application\/pdf", "type": "file", "id": 797117, "size": 2071141}], "title": "Poster", "default_folder": false, "id": 578908, "description": ""}], "_type": "Contribution", "description": "The possible usage of HPC resources by ATLAS is now becoming viable due to the changing nature of these systems and it is also very attractive due to the need for increasing amounts of simulated data.\r\n\r\nIn recent years the architecture of HPC systems has evolved, moving away  from specialized monolithic systems, to a more generic linux type platform. This change means that the deployment of non HPC specific  codes has become much easier. The timing of this evolution perfectly  suits the needs of ATLAS and opens a new window of opportunity.\r\n\r\nThe ATLAS experiment at CERN will begin a period of high luminosity data  taking in 2015. This high luminosity phase will be accompanied by a need  for increasing amounts of simulated data which is expected to exceed the  capabilities of the current Grid infrastructure.\r\n\r\nATLAS aims to address this need by opportunistically accessing resources  such as cloud and HPC systems. This paper presents the results of a  pilot project undertaken by ATLAS and the MPP\/RZG to provide access to  the HYDRA supercomputer facility. Hydra is the supercomputer of the Max \r\nPlanck Society, it is a linux based supercomputer with over 80000 cores and 4000 physical nodes located at the RZG near Munich.\r\n\r\nThis paper describes the work undertaken to integrate Hydra into the ATLAS production system by using the Nordugrid ARC-CE and other standard Grid components. The customization of these components and the strategies for HPC usage are discussed as well as possibilities for future directions.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578908", "resources": [{"_type": "LocalFile", "name": "HYDRA-ATLAS-Integration.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/153\/attachments\/578908\/797117\/ATL-SOFT-SLIDE-2015-107-printed.pdf", "fileName": "ATL-SOFT-SLIDE-2015-107-printed.pdf", "_fossil": "localFileMetadata", "id": "797117", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "8e71d7cd7b1ecaa6d25ed31477bdfdbc", "affiliation": "LMU Munich", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KENNEDY, John", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "85f863191188abed761d707cfc440a8a", "affiliation": "Max-Planck-Institut fuer Physik (Werner-Heisenberg-Institut) (D", "_fossil": "contributionParticipationMetadata", "fullName": "KLUTH, Stefan", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "73f9d874a0a0bd47c5e380afc1d800bf", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WALKER, Rodney", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/153", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:15:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:30:00"}, "duration": 15, "session": "Track 8 Session", "keywords": [], "id": "152", "speakers": [{"_type": "ContributionParticipation", "emailHash": "c961d8268c95605b7c0fe0d2cdbd4603", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "PANITKIN, Sergey", "id": "5"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c961d8268c95605b7c0fe0d2cdbd4603", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "PANITKIN, Sergey", "id": "5"}], "title": "Integration of PanDA workload management system with Titan supercomputer at OLCF.", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T07:52:40.382234+00:00", "description": "", "title": "Integration_of_Titan_with_PanDA_CHEP.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/152\/attachments\/578909\/797118\/Integration_of_Titan_with_PanDA_CHEP.pdf", "filename": "Integration_of_Titan_with_PanDA_CHEP.pdf", "content_type": "application\/pdf", "type": "file", "id": 797118, "size": 1928528}], "title": "Slides", "default_folder": false, "id": 578909, "description": ""}], "_type": "Contribution", "description": "The PanDA (Production and Distributed Analysis) workload management system (WMS) was developed to meet the scale and complexity of LHC distributed computing for the ATLAS experiment.\u00a0\r\n\u00a0While PanDA currently uses more than 100,000 cores at well over 100 Grid sites with a peak performance of 0.3 petaFLOPS, next LHC data taking run will require more resources than Grid computing can possibly provide.\r\nTo alleviate these challenges, ATLAS is engaged in an ambitious program to expand the current computing model to include additional resources such as the opportunistic use of supercomputers.\r\n\r\nWe will describe a project aimed at integration of PanDA WMS with\u00a0Titan supercomputer at Oak Ridge Leadership Computing Facility (OLCF).\r\nCurrent approach utilizes modified PanDA pilot framework for job submission to\u00a0Titan's batch queues and local data management, with light-weight MPI wrappers to run single threaded workloads in parallel on Titan's multi-core worker nodes.\u00a0It also gives PanDA new capability to collect, in real time, information about unused worker nodes on Titan, which allows precisely define the size and duration of jobs submitted to Titan according to available free resources.\r\nThis capability significantly reduces PanDA job wait time while improving Titan\u2019s utilization efficiency.\r\nThis implementation was tested with a variety of Monte-Carlo workloads on Titan and is being tested on several other supercomputing platforms.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578909", "resources": [{"_type": "LocalFile", "name": "Integration_of_Titan_with_PanDA_CHEP.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/152\/attachments\/578909\/797118\/Integration_of_Titan_with_PanDA_CHEP.pdf", "fileName": "Integration_of_Titan_with_PanDA_CHEP.pdf", "_fossil": "localFileMetadata", "id": "797118", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "ae65bffa7bc1bd6543d1748f5c46b7c6", "affiliation": "Joint Inst. for Nuclear Research (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "OLEYNIK, Danila", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "275c5b84f25777c7a0398e21b4b855bc", "affiliation": "Joint Inst. for Nuclear Research (RU)", "_fossil": "contributionParticipationMetadata", "fullName": "PETROSYAN, Artem", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c8b8647f9e6a89a40a3aae5e517a0c30", "affiliation": "ATLAS", "_fossil": "contributionParticipationMetadata", "fullName": "VANIACHINE, Alexandre", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "849637192af92a0f322682b2abc1e859", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WENAUS, Torre", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "a5e2efd46353da79fec29fc6172e4194", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SCHOVANCOVA, Jaroslava", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "e6819ee253837bc1a19f5e93c8ef001e", "affiliation": "University of Texas at Arlington (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DE, Kaushik", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "6fef2a09accf1a707bc4c992c15a8438", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KLIMENTOV, Alexei", "id": "7"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/152", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "155", "speakers": [{"_type": "ContributionParticipation", "emailHash": "a2dbd3d2df26e3fbc0360f6b3594fd21", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CAMPANA, Simone", "id": "9"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "4a66eba8bd02dcd2372a3cd6255f9fc5", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KARAVAKIS, Edward", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "e6819ee253837bc1a19f5e93c8ef001e", "affiliation": "University of Texas at Arlington (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DE, Kaushik", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "1982509370626e760ae06e6f8f80e2a5", "affiliation": "C", "_fossil": "contributionParticipationMetadata", "fullName": "DI GIROLAMO, Alessandro", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "f8a1c3051ab62071163450a232a4e82f", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "LITMAATH, Maarten", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "2e6780d520127502454830ec573033ae", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "MAENO, Tadashi", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "103bfaf03f6ad0cb7ea04144cd5424af", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MEDRANO LLAMAS, Ramon", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "de99e5c8c7b2c0edb4ab024a79d9a913", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "NILSSON, Paul", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "849637192af92a0f322682b2abc1e859", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WENAUS, Torre", "id": "7"}], "title": "gLExec integration with the ATLAS PanDA workload management system", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-07T19:18:40.611313+00:00", "description": "", "title": "CHEP2015Poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/155\/attachments\/578910\/797119\/CHEP2015Poster.pdf", "filename": "CHEP2015Poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 797119, "size": 1191172}], "title": "Slides", "default_folder": false, "id": 578910, "description": ""}], "_type": "Contribution", "description": "The ATLAS Experiment at the Large Hadron Collider has collected data during Run 1 and is ready to collect data in Run 2. The ATLAS data are distributed, processed and analysed at more than 130 grid and cloud sites across the world. At any given time, there are more than 150,000 concurrent jobs running and about a million jobs are submitted on a daily basis on behalf of thousands of physicists within the ATLAS collaboration. The Production and Distributed Analysis (PanDA) workload management system has proved to be a key component of ATLAS and plays a crucial role in the success of the large-scale distributed computing as it is the sole system for distributed processing of Grid jobs across the collaboration since October 2007.\r\n\r\nATLAS user jobs are executed on worker nodes by pilots sent to the sites by pilot factories. This pilot architecture has greatly improved job reliability and although it has clear advantages, such as making the working environment homogeneous by hiding any potential heterogeneities, the approach presents security and traceability issues distinct from standard batch jobs for which the submitter is also the payload owner. Jobs initially inherit the identity of the pilot submitter, typically a robot certificate with very limited rights. By default the payload jobs then execute directly under that same identity on a Worker Node. This exposes the pilot environment to the payload, requiring any pilot 'secrets' such as the proxy to be hidden; it constrains the rights and identity of the user job to be identical to the pilot; and it requires sites to take extra measures to achieve user traceability and user job isolation. \r\n\r\nTo address these security risks, the gLExec tool and framework can be used to let the payloads for each user be executed under a different UNIX user identity that uniquely identifies the ATLAS user.\r\n\r\nThis presentation describes the recent improvements and evolution of the security model within the ATLAS PanDA system, including improvements in the PanDA pilot, in the PanDA server and their integration with MyProxy, a credential caching system that entitles a person or a service to act in the name of the issuer of the credential. Finally, we will present results from ATLAS user jobs running with gLExec and give an insight into future deployment plans.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578910", "resources": [{"_type": "LocalFile", "name": "CHEP2015Poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/155\/attachments\/578910\/797119\/CHEP2015Poster.pdf", "fileName": "CHEP2015Poster.pdf", "_fossil": "localFileMetadata", "id": "797119", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "f04de6eee7b8dcc20dedefcb9fe659da", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BARREIRO MEGINO, Fernando Harald", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "a2dbd3d2df26e3fbc0360f6b3594fd21", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. CAMPANA, Simone", "id": "9"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/155", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "154", "speakers": [{"_type": "ContributionParticipation", "emailHash": "23800c33a82cb6a00e0559fdef216ca3", "affiliation": "Instituto de Fisica Corpuscular (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GONZALEZ DE LA HOZ, Santiago", "id": "13"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "1e0a696b292903384c9b5f8507834d45", "affiliation": "IFIC-Valencia", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. GONZALEZ DE LA HOZ, Santiago", "id": "0"}], "title": "Spanish ATLAS Tier-2 facing up to Run-2 period of LHC", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-03-16T13:14:31.031911+00:00", "description": "", "title": "ATL-SOFT-SLIDE-2015-071-posterTier2-aprobado.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/154\/attachments\/578911\/797120\/ATL-SOFT-SLIDE-2015-071-posterTier2-aprobado.pdf", "filename": "ATL-SOFT-SLIDE-2015-071-posterTier2-aprobado.pdf", "content_type": "application\/pdf", "type": "file", "id": 797120, "size": 8304937}], "title": "Poster", "default_folder": false, "id": 578911, "description": ""}], "_type": "Contribution", "description": "The goal of this work is to describe the way of addressing the main challenges of Run-2 by the Spanish ATLAS Tier-2. The considerable increase of energy and luminosity for the upcoming Run-2 w.r.t. Run-1 has led to a revision of the ATLAS computing model as well as some of the main ATLAS computing tools. The adaptation to these changes will be shown, with the peculiarities that it is a distributed Tier-2 composed of three sites and its members are involved on ATLAS computing tasks with a hub of research, innovation and education.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578911", "resources": [{"_type": "LocalFile", "name": "ATL-SOFT-SLIDE-2015-071-posterTier2-aprobado.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/154\/attachments\/578911\/797120\/ATL-SOFT-SLIDE-2015-071-posterTier2-aprobado.pdf", "fileName": "ATL-SOFT-SLIDE-2015-071-posterTier2-aprobado.pdf", "_fossil": "localFileMetadata", "id": "797120", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "db09f775118f67f75a97f106292d6a9f", "affiliation": "Universidad Autonoma de Madrid (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "DEL PESO, Jose", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c11231d8066a85165b23821c2e018d66", "affiliation": "Universidad de Valencia (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "SANCHEZ, Javier", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "84499ee9181678cc5d6b02788cef4aa0", "affiliation": "Instituto de Fisica Corpuscular (IFIC) UV-CSIC (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "SANCHEZ MARTINEZ, Victoria", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "86e7090326921ebffd27a5b092e236a1", "affiliation": "IFIC", "_fossil": "contributionParticipationMetadata", "fullName": "VILLAPLANA PEREZ, Miguel", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "9ec797595c3aba68f4e69a7fc412829a", "affiliation": "Universite Mohammed V (MA)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. FASSI, Farida", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "19701f2de82fdbcb183a5117cb576c42", "affiliation": "Instituto de Fisica Corpuscular (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "FERNANDEZ CASANI, Alvaro", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "6e0a678c021cd907e7bad504537fb6ff", "affiliation": "IFIC - Valencia - Spain", "_fossil": "contributionParticipationMetadata", "fullName": "KACI, Mohammed", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "0fe4833ab7060f302eda2b0b24518bee", "affiliation": "Universidad de Valencia (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "LACORT PELLICER, Victor Ruben", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "e4d2d55f7a625d91f14ce5dc7ff704eb", "affiliation": "Universidad Autonoma de Madrid (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "MONTIEL GONZALEZ, Almudena Del Rocio", "id": "14"}, {"_type": "ContributionParticipation", "emailHash": "ec47a2b692ab66a29a2b09df6ac7574c", "affiliation": "Universidad de Valencia (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "OLIVER GARCIA, Elena", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "28fec050a7a0cddcbe83319b039e0c92", "affiliation": "Institut de F\u00edsica d'Altes Energies - Barcelona (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "PACHECO PAGES, Andreu", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "89cb567593acb181ba8c9eadd88f6900", "affiliation": "Instituto de Fisica Corpuscular (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "SALT, Jose", "id": "12"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/154", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:00:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "157", "speakers": [{"_type": "ContributionParticipation", "emailHash": "1abe5b1516eb954743a7e0c64e7555cc", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "ELMSHEUSER, Johannes", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "1abe5b1516eb954743a7e0c64e7555cc", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "ELMSHEUSER, Johannes", "id": "0"}], "title": "New data access with HTTP\/WebDAV in the ATLAS experiment", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T08:59:02.337764+00:00", "description": "", "title": "o_130415.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/157\/attachments\/578912\/797121\/o_130415.pdf", "filename": "o_130415.pdf", "content_type": "application\/pdf", "type": "file", "id": 797121, "size": 632265}], "title": "Slides", "default_folder": false, "id": 578912, "description": ""}], "_type": "Contribution", "description": "With the exponential growth of LHC (Large Hadron Collider) data in the years 2010-2012, distributed computing has become the established way to analyze collider data. The ATLAS experiment Grid infrastructure includes more than 130 sites worldwide, ranging from large national computing centres to smaller university clusters. So far the storage technologies and access protocols to the clusters that host this tremendous amount of data vary from site to site. HTTP\/WebDAV offers the possibility to use a unified industry standard to access the storage. We present the deployment and testing of HTTP\/WebDAV for local and remote data access in the ATLAS experiment for the new data management system Rucio and the PanDA workload management system. Deployment and large scale tests have been performed using the Grid testing system HammerCloud and the ROOT HTTP plugin Davix.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578912", "resources": [{"_type": "LocalFile", "name": "o_130415.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/157\/attachments\/578912\/797121\/o_130415.pdf", "fileName": "o_130415.pdf", "_fossil": "localFileMetadata", "id": "797121", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "73f9d874a0a0bd47c5e380afc1d800bf", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WALKER, Rodney", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "9730fb357615aee6267d33cedeeabf96", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "SERFON, Cedric", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "ef31f0c4449bc0ee443d3a9f4c7ed04c", "affiliation": "Pontificia Univ. Catolica de Chile (CL)", "_fossil": "contributionParticipationMetadata", "fullName": "BLUNIER, Sylvain", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "f495da663d47a0ddc70458203fe2ac6d", "affiliation": "Universita della Calabria (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "LAVORINI, Vincenzo", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "de99e5c8c7b2c0edb4ab024a79d9a913", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "NILSSON, Paul", "id": "5"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/157", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "156", "speakers": [{"_type": "ContributionParticipation", "emailHash": "35f0509459ee62f9ddcf584936cca96a", "affiliation": "Universit\u00e0 e INFN Genova (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BARBERIS, Dario", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "5e093574484f75dbebdf207897fba8d6", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DIMITROV, Gancho", "id": "0"}], "title": "The importance of having an appropriate relational data segmentation in ATLAS", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-06T11:19:24.501671+00:00", "description": "", "title": "ATL-SOFT-SLIDE-2015-068.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/156\/attachments\/578913\/797122\/ATL-SOFT-SLIDE-2015-068.pdf", "filename": "ATL-SOFT-SLIDE-2015-068.pdf", "content_type": "application\/pdf", "type": "file", "id": 797122, "size": 1015325}], "title": "Poster", "default_folder": false, "id": 578913, "description": ""}], "_type": "Contribution", "description": "In this paper we describe specific technical solutions put in place in various database applications of the ATLAS experiment at LHC where we make use of several partitioning techniques available in Oracle 11g. With the broadly used range partitioning and its option of automatic interval partitioning we add our own logic in PLSQL procedures and scheduler jobs to sustain data sliding windows in order to enforce various data retention policies. We also make use of the new Oracle 11g reference partitioning in the Nightly Build System to achieve uniform data segmentation. However the most challenging issue was to segment the data of the new ATLAS Distributed Data Management system (Rucio), which resulted in tens of thousands list type partitions and sub-partitions. Partition and sub-partition management, index strategy, statistics gathering and queries execution plan stability are important factors when choosing an appropriate physical model for the application data management. The so-far accumulated knowledge and analysis on the new Oracle 12c version features that could be beneficial will be shared with the audience.", "track": "Track3: Data store and access ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578913", "resources": [{"_type": "LocalFile", "name": "ATL-SOFT-SLIDE-2015-068.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/156\/attachments\/578913\/797122\/ATL-SOFT-SLIDE-2015-068.pdf", "fileName": "ATL-SOFT-SLIDE-2015-068.pdf", "_fossil": "localFileMetadata", "id": "797122", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/156", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "159", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ce168f7b391d7e4113d58dc9aae233f7", "affiliation": "Albert-Ludwigs-Universitaet Freiburg (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "BOEHLER, Michael", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ce168f7b391d7e4113d58dc9aae233f7", "affiliation": "Albert-Ludwigs-Universitaet Freiburg (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "BOEHLER, Michael", "id": "2"}], "title": "Improved ATLAS HammerCloud Monitoring for local Site Administration", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-06T15:50:47.787363+00:00", "description": "", "title": "ATL-SOFT-SLIDE-2015-069.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/159\/attachments\/578914\/797123\/ATL-SOFT-SLIDE-2015-069.pdf", "filename": "ATL-SOFT-SLIDE-2015-069.pdf", "content_type": "application\/pdf", "type": "file", "id": 797123, "size": 3436447}], "title": "Slides", "default_folder": false, "id": 578914, "description": ""}], "_type": "Contribution", "description": "Every day hundreds of tests are run on the Worldwide LHC Computing Grid for the ATLAS, CMS, and LHCb experiments in order to evaluate the performance and reliability of the different computing sites. All this activity is steered, controlled, and monitored by the HammerCloud testing infrastructure. Sites with failing functionality tests are auto-excluded from the ATLAS computing grid, therefore it is essential to provide a detailed and well organized web interface for the local site administrators such that they can easily spot and promptly solve site issues. Additional functionalities have been developed to extract and visualize the most relevant information. The site administrators can now be pointed easily to major site issues which lead to site blacklisting as well as possible minor issues that are usually not conspicuous enough to warrant the blacklisting of a specific site, but can still cause undesired effects such as a non-negligible job failure rate. This contribution summarizes the different developments and optimizations of the HammerCloud web interface and gives an overview of typical use cases.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578914", "resources": [{"_type": "LocalFile", "name": "ATL-SOFT-SLIDE-2015-069.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/159\/attachments\/578914\/797123\/ATL-SOFT-SLIDE-2015-069.pdf", "fileName": "ATL-SOFT-SLIDE-2015-069.pdf", "_fossil": "localFileMetadata", "id": "797123", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "529bfef0e03e6c587d4fc87551bcb08c", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "MANCINELLI, Valentina", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "040e4690e862ef70aead477af54c707c", "affiliation": "Universitaet Bern (CH)", "_fossil": "contributionParticipationMetadata", "fullName": "SCIACCA, Francesco Giovanni", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "1abe5b1516eb954743a7e0c64e7555cc", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "ELMSHEUSER, Johannes", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "2886073831b4a98413ec3f6f2d71b490", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "LEGGER, Federica", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "048737cb7938b52b0d9d7201e4e2403c", "affiliation": "Ludwig-Maximilians-Univ. Muenchen (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "HOENIG, Friedrich", "id": "5"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/159", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": null, "keywords": [], "id": "158", "speakers": [{"_type": "ContributionParticipation", "emailHash": "b97ba385788d95ebdb7f8a19036c8839", "affiliation": "University of Chicago (US)", "_fossil": "contributionParticipationMetadata", "fullName": "VUKOTIC, Ilija", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "b97ba385788d95ebdb7f8a19036c8839", "affiliation": "University of Chicago (US)", "_fossil": "contributionParticipationMetadata", "fullName": "VUKOTIC, Ilija", "id": "0"}], "title": "Understanding and optimizing ATLAS WAN data flows", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "As new ways of accessing and transferring data distributed over the grid emerge (XRootD-based storage networks, dynamic http federations, etc.) and as the quantity of data grows, a number of challenges are presented to both the application layer and the supporting compute, network and storage infrastructures. Clearly network links between compute and storage systems remain the leading discriminant for determining accessibility and performance between client and server. A number of systems to probe the network and aggregate information about transfers have been deployed to characterize bandwidth, latency and overall throughput between sets of endpoints. We describe combining these and other information sources into a time-dependent cost-of-data-access matrix. We will show how this matrix, in combination with other performance indicators, is used to directly support job brokering and data distribution decisions. Performance of the jobs accessing remote data is compared to jobs locally accessing data. We characterize the trafic remote data access generated, review models of the transfer prioritization, and their effects on resource utilization.", "track": "Track5: Computing activities and Computing models", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "5b3e54efbd9206c289ae5b94c5cc4bdb", "affiliation": "University of Chicago (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GARDNER JR, Robert William", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "6e2ea49be71d71a8ba1739920edf11f1", "affiliation": "Budker Institute of Nuclear Physics (BINP)-Unknown-Unknown", "_fossil": "contributionParticipationMetadata", "fullName": "MAKEEV, Alexander", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/contribution\/158", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:15:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "112", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5ae4855aa9d3c6685bd9f450814f9af6", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "STAGNI, Federico", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "5ae4855aa9d3c6685bd9f450814f9af6", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "STAGNI, Federico", "id": "0"}], "title": "Jobs masonry with elastic Grid Jobs", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T01:56:35.279170+00:00", "description": "", "title": "JobMasonry.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/112\/attachments\/578915\/797124\/JobMasonry.pdf", "filename": "JobMasonry.pdf", "content_type": "application\/pdf", "type": "file", "id": 797124, "size": 990829}, {"_type": "attachment", "modified_dt": "2015-04-12T01:56:35.279170+00:00", "description": "", "title": "JobMasonry.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/112\/attachments\/578915\/797125\/JobMasonry.pptx", "filename": "JobMasonry.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797125, "size": 1031046}], "title": "Slides", "default_folder": false, "id": 578915, "description": ""}], "_type": "Contribution", "description": "The DIRAC workload management system used by LHCb Distributed Computing is based on Computing Resource reservation and late binding (also known as pilot job in the case of batch resources) that allows the serial execution of several jobs obtained from a central task queue. CPU resources can usually be reserved for limited duration only (e.g. batch queue time limit) and in order to optimize their usage, it is important to be able to use them for the whole available time. However traditionally the tasks to be performed by jobs are defined at submission time and therefore it may happen that no job fits in the available time. \r\nIn LHCb, this so-called job masonry is optimized by the usage of elastic simulation jobs: unlike data processing jobs that must process all events in the input dataset, simulation jobs offer an interesting degree of freedom as one can define the number of events to be simulated even after submission. This requires however knowing three information: the time available in the reserved resource, its CPU power and the average CPU work required for simulating one event. The decision on the number of events can then be made at the very last moment, just before starting the simulation application. This is what we call elastic jobs.\r\nLHCb simulation jobs are now all elastic, with an upper limit on the number of events per job. When several jobs are needed to complete a simulation request, enough jobs are submitted for simulating the total number of events required, assuming this upper limit for each job. New jobs are then submitted depending on the actual number of events simulated by this first batch of jobs.\r\nWe will show in this contribution how elastic jobs allow a better backfilling of computing resources as well as using resources with limited work capacity, such as short batch queues or volunteer computing resources. They also allow easily to shutdown virtual machines on cloud resources when sites require them to shutdown within a grace period.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578915", "resources": [{"_type": "LocalFile", "name": "JobMasonry.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/112\/attachments\/578915\/797124\/JobMasonry.pdf", "fileName": "JobMasonry.pdf", "_fossil": "localFileMetadata", "id": "797124", "_deprecated": true}, {"_type": "LocalFile", "name": "JobMasonry.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/112\/attachments\/578915\/797125\/JobMasonry.pptx", "fileName": "JobMasonry.pptx", "_fossil": "localFileMetadata", "id": "797125", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "1361efd0daf15bbf4cc3141a47fc97af", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CHARPENTIER, Philippe", "id": "1"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/112", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:30:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "83", "speakers": [{"_type": "ContributionParticipation", "emailHash": "16c6bfa595704a831f9cafd19c105e86", "affiliation": "Universite Catholique de Louvain (UCL) (BE)", "_fossil": "contributionParticipationMetadata", "fullName": "MANNAI, Sameh", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "16c6bfa595704a831f9cafd19c105e86", "affiliation": "Universite Catholique de Louvain (UCL) (BE)", "_fossil": "contributionParticipationMetadata", "fullName": "MANNAI, Sameh", "id": "0"}], "title": "Energy Reconstruction using Artificial Neural Networks and different analytic methods  in a Highly Granularity Semi-Digital Hadronic Calorimeter.", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T08:52:36.339330+00:00", "description": "", "title": "chep2015-mannai.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/83\/attachments\/578916\/797126\/chep2015-mannai.pdf", "filename": "chep2015-mannai.pdf", "content_type": "application\/pdf", "type": "file", "id": 797126, "size": 2491082}], "title": "Slides", "default_folder": false, "id": 578916, "description": ""}], "_type": "Contribution", "description": "The Semi-Digital Hadronic CALorimeter(SDHCAL) using Glass Resistive Plate Chambers (GRPCs) is\r\none of the two  hadronic calorimeter options proposed by the ILD (International Large Detector) project for the future (ILC) International Linear Collider experiments.\r\n\r\nIt is a sampling calorimeter with 48 layers. Each layer has a size of 1 m\u00b2 and\r\nfinely segmented into cells of 1 cm\u00b2 ensuring a high granularity which is required for \r\nthe application of the Particle Flow Algorithm (PFA) in order to improve the jet energy resolution which is the corner stone of ILC experiments.\r\n \r\nThe electronic of SDHCAL provide 2-bit readout. It is equiped with power pulisng mode reducing the power consumption and thus heating related problems.   \r\n\r\nThe performance of the SDHCAL technological prototype was tested successfully in beam tests at CERN during 2012. The next beam test will take place\r\nat CERN in December 2014 with new improvements in hardware developments. Results of this test beam will be shown.\r\n\r\nOne of the main points to be discussed concerns the energy reconstruction in SDHCAL. \r\n\r\nBased on Monte Carlo Simulation of the SDHCAL prototype with Geant4, we will show different analytic energy reconstruction methods. We will present the single particle energy resolution and the linearity of the detecor response to hadrons obtained with these methods.\r\nIn particular, we will highlight a new approch based on the Artificial Neural Networks used in \r\nthe energy reconstruction and giving promising results compared to the classical analytic methods.\r\nResults will be presented for both simulation and real data in the aim to compare them.\r\nIn the same context, we will discuss the application of The Artifical Neural Network to purify the beam test data from contamination.\r\nResults of particle separation obtained with the Artificial Neural Network will be shown and compared to classical event selection methods.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578916", "resources": [{"_type": "LocalFile", "name": "chep2015-mannai.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/83\/attachments\/578916\/797126\/chep2015-mannai.pdf", "fileName": "chep2015-mannai.pdf", "_fossil": "localFileMetadata", "id": "797126", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "22b615849a3d3d06de785268f4cff2d1", "affiliation": "Universite Claude Bernard-Lyon I (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "LAKTINEH, Imad", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "2276d51797a841be972cc65ef3b0f789", "affiliation": "Universite catholique de Louvain", "_fossil": "contributionParticipationMetadata", "fullName": "CORTINA GIL, Eduardo", "id": "2"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/83", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:30:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "09:45:00"}, "duration": 15, "session": "Track 7 Session", "keywords": [], "id": "80", "speakers": [{"_type": "ContributionParticipation", "emailHash": "20532d33773143bab9420cb697af73ad", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ZILLI, Stefano", "id": "4"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "3d116cd41660755949bc4ba15ac2a131", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BELL, Tim", "id": "3"}], "title": "Scaling the CERN OpenStack cloud", "note": {}, "location": "C210", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-12T16:11:15.410152+00:00", "description": "", "title": "ScalingOpenStackAtCern.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/80\/attachments\/578917\/797127\/ScalingOpenStackAtCern.pdf", "filename": "ScalingOpenStackAtCern.pdf", "content_type": "application\/pdf", "type": "file", "id": 797127, "size": 665332}, {"_type": "attachment", "modified_dt": "2015-04-12T16:11:15.410152+00:00", "description": "", "title": "ScalingOpenStackAtCern.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/80\/attachments\/578917\/797128\/ScalingOpenStackAtCern.pptx", "filename": "ScalingOpenStackAtCern.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797128, "size": 3122738}], "title": "Slides", "default_folder": false, "id": 578917, "description": ""}], "_type": "Contribution", "description": "CERN has been running a production OpenStack cloud since July 2013 to support physics computing and infrastructure services for the site. This is expected to reach over 100,000 cores by the end of 2015.\r\n\r\nThis talk will cover the different use cases for this service and experiences with this deployment in areas such as user management, deployment, metering and configuration of thousands of servers across two data centres.\r\n\r\nOngoing research, such as bare metal management, containers, software defined networking and orchestration will also be covered.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578917", "resources": [{"_type": "LocalFile", "name": "ScalingOpenStackAtCern.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/80\/attachments\/578917\/797127\/ScalingOpenStackAtCern.pdf", "fileName": "ScalingOpenStackAtCern.pdf", "_fossil": "localFileMetadata", "id": "797127", "_deprecated": true}, {"_type": "LocalFile", "name": "ScalingOpenStackAtCern.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/80\/attachments\/578917\/797128\/ScalingOpenStackAtCern.pptx", "fileName": "ScalingOpenStackAtCern.pptx", "_fossil": "localFileMetadata", "id": "797128", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "6c0c8194cd7e907af2da41282c25e6d0", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CASTRO LEON, Jose", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "ed30324c9ee1fdda5b1325a6ca369e29", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FERNANDEZ ALVAREZ, Luis", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "a7f74ac17e363dd0ab1ffe2342ba5c5f", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WIEBALCK, Arne", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "20532d33773143bab9420cb697af73ad", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ZILLI, Stefano", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "4d221f26e454e4ec1b4dbeaa593df38d", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MOREIRA, Belmiro", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "3946ec1dee8f2e582c4a71747872f842", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "VAN ELDIK, Jan", "id": "6"}], "subContributions": [], "room": "C210", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/7\/contribution\/80", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:45:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:00:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "81", "speakers": [{"_type": "ContributionParticipation", "emailHash": "85b48fcbbd8c3a9293a147a633af1bd3", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SILVA DE SOUSA, Bruno", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "85b48fcbbd8c3a9293a147a633af1bd3", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SILVA DE SOUSA, Bruno", "id": "0"}], "title": "Enterprise Social Networking Systems for HEP community", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T08:20:34.941843+00:00", "description": "", "title": "Social_CHEP_2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/81\/attachments\/578918\/797129\/Social_CHEP_2015.pdf", "filename": "Social_CHEP_2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 797129, "size": 3055168}, {"_type": "attachment", "modified_dt": "2015-04-14T08:20:34.941843+00:00", "description": "", "title": "Social_CHEP_2015.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/81\/attachments\/578918\/797130\/Social_CHEP_2015.pptx", "filename": "Social_CHEP_2015.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797130, "size": 7477559}], "title": "Slides", "default_folder": false, "id": 578918, "description": ""}], "_type": "Contribution", "description": "The emergence of social media platforms in the consumer space unlocked new ways of interaction between individuals on the web. People develop now their social networks and relations based on common interests and activities with the choice to opt-in or opt-out on content of their interest. This kind of platforms have also an important place to fill inside large organizations and enterprises where communication and collaborators interaction are keys for development. They are enablers to unlock hidden opportunities.\r\n\r\nEnterprise Social Networking Systems (ESN) add value to an organization by encouraging information sharing, capturing knowledge, enabling action and empowering people. For such they propose microblogging, profiles, social networking, suggestion systems and discussion forums. Microblogging introduces a lightweight and informal communication channel that primes information sharing, promotes discussion and gives an equal opportunity to everyone to reach a broad audience. Profiles and social networking are the perfect tool to seek for skills and can act as a catalyzer for new projects and opportunities. Suggestion systems and discussion forums are the perfect tools to identify evolving trends and allow new communities to born.\r\n\r\nCERN is currently rolling out an ESN which aims to unify and provide a single point of access to the multitude of information sources in the organization. It also implements social features that can be added on top of existing communication channels. This talk will review the experiences with this deployment, the risks and benefits and outlook for the future as a social community tool for HEP.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578918", "resources": [{"_type": "LocalFile", "name": "Social_CHEP_2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/81\/attachments\/578918\/797129\/Social_CHEP_2015.pdf", "fileName": "Social_CHEP_2015.pdf", "_fossil": "localFileMetadata", "id": "797129", "_deprecated": true}, {"_type": "LocalFile", "name": "Social_CHEP_2015.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/81\/attachments\/578918\/797130\/Social_CHEP_2015.pptx", "fileName": "Social_CHEP_2015.pptx", "_fossil": "localFileMetadata", "id": "797130", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "4c578dd9ff1c6f9776e432151a644d07", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GRZYWACZEWSKI, Pawel", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c317dd83fc51a33a9760490e2af9eb5a", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "ORMANCEY, Emmanuel", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "27f9819c90973eada9686cd7372f46a0", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. WAGNER, Andreas", "id": "3"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/81", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "48", "speakers": [{"_type": "ContributionParticipation", "emailHash": "82d7c36b6575474fb5d7ba2ca294dfc5", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. WARTEL, Romain", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "82d7c36b6575474fb5d7ba2ca294dfc5", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. WARTEL, Romain", "id": "0"}], "title": "Enabling identity federation for the research and academic communities", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "Federated identity management (FIM) is an arrangement made among multiple organisations that lets subscribers use the same identification data, e.g. account names & credentials, to obtain access to the secured resources and computing services of all other organisations in the group. Specifically in the various research communities there is an increased interest in a common approach as there is obviously a large potential for synergies.\r\nSeveral research communities, infrastructures and as well as federations have worked together in this area. This presentation will give an overview of this ongoing effort, including a common vision for FIM, a set of requirements and a number of recommendations for ensuring secure and interoperable operations across multiple administrative domains. In particular, the current SCI \/ Sir-T-Fi work will be presented, providing details on a common trust framework proposing minimal requirements for participants, as well as a common security incident response policy for federations.", "track": "Track5: Computing activities and Computing models", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/48", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "49", "speakers": [{"_type": "ContributionParticipation", "emailHash": "9ac43fe47c762ad8c388e1031b3b8ea6", "affiliation": "Johannes-Gutenberg-Universitaet  Mainz (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BERTELLA, Claudia", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "9ac43fe47c762ad8c388e1031b3b8ea6", "affiliation": "Johannes-Gutenberg-Universitaet  Mainz (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BERTELLA, Claudia", "id": "1"}], "title": "Real-time flavour tagging selection in ATLAS", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T12:55:15.464527+00:00", "description": "", "title": "BjetTrigger_CHEP2015_NEW.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/49\/attachments\/578919\/797131\/BjetTrigger_CHEP2015_NEW.pdf", "filename": "BjetTrigger_CHEP2015_NEW.pdf", "content_type": "application\/pdf", "type": "file", "id": 797131, "size": 4046207}], "title": "Diapositives", "default_folder": false, "id": 578919, "description": ""}], "_type": "Contribution", "description": "In high-energy physics experiments, online selection is crucial to identify the few interesting collisions from the large data volume processed. In the overall ATLAS trigger strategy, b-jet triggers are designed to identify heavy-flavor content in real-time and, in particular, provide the only option to efficiently record events with fully hadronic final states containing b-jets.  In doing so, two different, but related, challenges are faced. The physics goal is to optimise as far as possible the rejection of light jets from QCD processes, while retaining a high efficiency on selecting jets from beauty, while maintaining affordable trigger rates without raising jet energy thresholds. This maps into a challenging computing task, as charged tracks and their corresponding vertexes must be reconstructed and analysed for each jet above the desired threshold, regardless of the increasingly harsh pile-up conditions.  The performance of b-jet triggers during the LHC Run 1 data-taking campaigns is presented, together with an overview of the new online b-tagging strategy and algorithms, designed to face the above mentioned challenges, which will be adopted during Run 2.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Diapositives", "_fossil": "materialMetadata", "id": "578919", "resources": [{"_type": "LocalFile", "name": "BjetTrigger_CHEP2015_NEW.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/49\/attachments\/578919\/797131\/BjetTrigger_CHEP2015_NEW.pdf", "fileName": "BjetTrigger_CHEP2015_NEW.pdf", "_fossil": "localFileMetadata", "id": "797131", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/49", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "46", "speakers": [{"_type": "ContributionParticipation", "emailHash": "4ae2a83dcc6fe1777c1caa07ade3502b", "affiliation": "Deutsches Elektronen-Synchrotron DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. MKRTCHYAN, Tigran Mkrtchyan", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "4ae2a83dcc6fe1777c1caa07ade3502b", "affiliation": "Deutsches Elektronen-Synchrotron DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. MKRTCHYAN, Tigran Mkrtchyan", "id": "0"}], "title": "dCache: Moving to open source development model and tools.", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T02:35:45.896026+00:00", "description": "", "title": "poster-dcache-chep2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/46\/attachments\/578920\/797132\/poster-dcache-chep2015.pdf", "filename": "poster-dcache-chep2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 797132, "size": 1749155}], "title": "Poster", "default_folder": false, "id": 578920, "description": ""}], "_type": "Contribution", "description": "For over a decade, dCache.ORG has provided software which is used at more than 80 sites around the world, providing reliable services for WLCG experiments and others. This can be achieved only with a well established process from white board, where ideas are created, through packages, installed on the production systems. Since early 2013 we have moved to git as out source code management system as well as  to GitHub for our source code hosting service.\r\n\r\nThis presentation will show how techniques and tools are used by dCache developers to manage code and releases, how to handle external contributions and still keep the quality and reliability of the provided packages.", "track": "Track3: Data store and access ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578920", "resources": [{"_type": "LocalFile", "name": "poster-dcache-chep2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/46\/attachments\/578920\/797132\/poster-dcache-chep2015.pdf", "fileName": "poster-dcache-chep2015.pdf", "_fossil": "localFileMetadata", "id": "797132", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "a198240a6b21354217ead633867b3e31", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. FUHRMANN, Patrick", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "8d26ed6854ded78641202361191acea7", "affiliation": "NDGF", "_fossil": "contributionParticipationMetadata", "fullName": "BEHRMANN, Gerd", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "9004fd367d357871828de260587c178b", "affiliation": "Deutsches Elektronen-Synchrotron (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "MILLAR, Paul", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/46", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "09:00:00"}, "duration": 15, "session": "Track 5 Session", "keywords": [], "id": "47", "speakers": [{"_type": "ContributionParticipation", "emailHash": "82d7c36b6575474fb5d7ba2ca294dfc5", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. WARTEL, Romain", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "82d7c36b6575474fb5d7ba2ca294dfc5", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. WARTEL, Romain", "id": "0"}], "title": "Computer security: surviving and operating services despite highly skilled and well-funded organised crime groups", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T00:28:05.480284+00:00", "description": "", "title": "20150415_Wartel_CHEP.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/47\/attachments\/578921\/797133\/20150415_Wartel_CHEP.pdf", "filename": "20150415_Wartel_CHEP.pdf", "content_type": "application\/pdf", "type": "file", "id": 797133, "size": 995253}], "title": "Slides", "default_folder": false, "id": 578921, "description": ""}], "_type": "Contribution", "description": "This presentation gives an overview of the current computer security landscape. It describes the main vectors of compromises in the academic community including lessons learnt, reveals inner mechanisms of the underground economy to expose how our computing resources are exploited by organised crime groups, and gives recommendations how to better protect our computing infrastructures. By showing how these attacks are both sophisticated and profitable, the presentation concludes that an important mean to adopt an appropriate response is to build a tight international collaboration and trusted information sharing mechanisms within the community.", "track": "Track5: Computing activities and Computing models", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578921", "resources": [{"_type": "LocalFile", "name": "20150415_Wartel_CHEP.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/47\/attachments\/578921\/797133\/20150415_Wartel_CHEP.pdf", "fileName": "20150415_Wartel_CHEP.pdf", "_fossil": "localFileMetadata", "id": "797133", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/5\/contribution\/47", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": null, "keywords": [], "id": "44", "speakers": [{"_type": "ContributionParticipation", "emailHash": "454586d94ec6a7ebad3c68ceb1774c96", "affiliation": "SLAC National Accelerator  Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "YANG, Wei", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "dd7018927651faf265e88113c88714a2", "affiliation": "STANFORD LINEAR ACCELERATOR CENTER", "_fossil": "contributionParticipationMetadata", "fullName": "HANUSHEVSKY, Andrew", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "454586d94ec6a7ebad3c68ceb1774c96", "affiliation": "SLAC National Accelerator  Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "YANG, Wei", "id": "0"}], "title": "XrootDfs: a Posix Filesytem for XrootD", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [], "_type": "Contribution", "description": "When we first introduced XRootD storage system to the LHC, we needed a filesystem interface so that XRootD system could function as a Grid Storage Element. The result was XRootDfs, a FUSE based mountable posix filesystem. It glues all the data servers in a XRootD storage\r\nsystem together and presents it as a single, posix compliant, multi-user networked filesystem. XRootD's unique redirection mechanism\r\nrequires special handling of IO operations and metadata operations in the XRootDfs. This includes a throttling mechanism to gracefully handle extreme metadata operations; handling of returned results from all data servers in a consistent way; hiding delays of metadata operations, inluding storage media latency; enhancing the performance of concurrent IO by multiple applications; and using an advanced security plugin to\r\nensure secure data access in a multi-user environment. Over the last several years XRootDfs have been adopted by many XRootD sites for data management as well as data access by applications that were not specifically designed to use the native XRootD interface. Many of the technical methods mentioned above can also be used to glue together other types (i.e. non-XRootD) data servers to provide seamless data access.", "track": "Track3: Data store and access ", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "dd7018927651faf265e88113c88714a2", "affiliation": "STANFORD LINEAR ACCELERATOR CENTER", "_fossil": "contributionParticipationMetadata", "fullName": "HANUSHEVSKY, Andrew", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/contribution\/44", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:15:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:30:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "45", "speakers": [{"_type": "ContributionParticipation", "emailHash": "d96310dde0f53841456b312025ab1877", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. MKRTCHYAN, Tigran", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "d96310dde0f53841456b312025ab1877", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. MKRTCHYAN, Tigran", "id": "0"}], "title": "Visualization of dCache accounting information with state-of-the-art Data Analysis Tools.", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T02:30:43.746145+00:00", "description": "", "title": "elk-chep2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/45\/attachments\/578922\/797134\/elk-chep2015.pdf", "filename": "elk-chep2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 797134, "size": 1610317}], "title": "Slides", "default_folder": false, "id": 578922, "description": ""}], "_type": "Contribution", "description": "Over the previous years, storage providers in scientific infrastructures were facing a significant change in the usage profile of their resources. While in the past, a small number of experiment frameworks were accessing those resources in a coherent manner, now, a large amount of small groups or even individuals request access in a completely chaotic way. Moreover, scientific laboratories have been recently forced to provide detailed accounting information for their communities and individuals. Another consequence of the chaotic access profiles is the difficulty, for often rather small operating teams, to detect malfunctions in extremely complex storage systems, composed of a large variety of different hardware components. Although information about usage and possible malfunction is available in the corresponding log and billing files, the sheer amount of collected meta data makes it extremely difficult to be handled or interpreted. Simply the dCache production instances at DESY are producing Gigabytes of meta data per day. To cope with those pressing issues, DESY has been evaluating and put into production a Big Data processing tool, enabling our operation team to analyze log and billing information by providing a configurable and easy to interpret visualization of that data.\r\nThis presentation will demonstrate how DESY built a real-time monitoring system, visualizing dCache billing files and providing an intuitive and simple to operate Web interface, using ElasticSearch, Logstash and Kibana.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578922", "resources": [{"_type": "LocalFile", "name": "elk-chep2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/45\/attachments\/578922\/797134\/elk-chep2015.pdf", "fileName": "elk-chep2015.pdf", "_fossil": "localFileMetadata", "id": "797134", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "a198240a6b21354217ead633867b3e31", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. FUHRMANN, Patrick", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "a2b3450748ef067b1408b085a6ce18a4", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LEWENDEL, Birgit", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "20e5c2238d2a1939aa1cfc27a2770cf6", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Ms. YAKOVLEVA, Lusine", "id": "3"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/45", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:30:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "42", "speakers": [{"_type": "ContributionParticipation", "emailHash": "9dcc34863a5b84b5753c2dfd2f11901b", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BARANOWSKI, Zbigniew", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "9dcc34863a5b84b5753c2dfd2f11901b", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BARANOWSKI, Zbigniew", "id": "0"}], "title": "Evolution of Database Replication Technologies for WLCG", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:18:45.399848+00:00", "description": "", "title": "Evolution_of_data_replication_-_CHEP2015.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/42\/attachments\/578923\/797135\/Evolution_of_data_replication_-_CHEP2015.pptx", "filename": "Evolution_of_data_replication_-_CHEP2015.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797135, "size": 1264857}], "title": "Slides", "default_folder": false, "id": 578923, "description": ""}], "_type": "Contribution", "description": "During LHC run 1 ATLAS and LHCb databases have been using Oracle Streams replication technology for their use cases of data movement between online and offline Oracle databases. Moreover ATLAS has been using Streams to replicate conditions data from CERN to selected Tier 1s. GoldenGate is a new technology introduced by Oracle to replace and improve on Streams, by providing better performance, robustness, manageability and support. At the same time Oracle Active Data Guard is an another technology that fulfils several use cases in this area, by allowing the creation of read-only copies of production databases, currently in use by CMS, ALICE and more recently by ATLAS for controls data. In this paper we will provide a short review of when GoldenGate is preferable to Streams\/Active Data Guard. Further we will report the details of the migration of CERN and Tier 1s database replication setups\/configurations from Streams to GoldenGate. In particular we will describe the architecture of GoldenGate replication services, our performance tests for remote replication done in collaboration with ATLAS and Tier 1 database administrators and the return of experience from running GoldenGate in production. We will also summarize our tests for future work on how GoldenGate can be used to reduce the downtime for Oracle version upgrades and on using GoldenGate for heterogeneous database replication.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578923", "resources": [{"_type": "LocalFile", "name": "Evolution_of_data_replication_-_CHEP2015.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/42\/attachments\/578923\/797135\/Evolution_of_data_replication_-_CHEP2015.pptx", "fileName": "Evolution_of_data_replication_-_CHEP2015.pptx", "_fossil": "localFileMetadata", "id": "797135", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "3e7b2b85666dd660b798ebc095c46ba1", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "LOBATO PARDAVILA, Lorena", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "d70edd5c19de84041c4f23aad0e96511", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BLASZCZYK, Marcin", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "5e093574484f75dbebdf207897fba8d6", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "DIMITROV, Gancho", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "dd077faeac1f58ef59416503f500d418", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CANALI, Luca", "id": "4"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/42", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "43", "speakers": [{"_type": "ContributionParticipation", "emailHash": "9dcc34863a5b84b5753c2dfd2f11901b", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BARANOWSKI, Zbigniew", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "9dcc34863a5b84b5753c2dfd2f11901b", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "BARANOWSKI, Zbigniew", "id": "0"}], "title": "Scale Out Databases for CERN Use Cases", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T03:05:04.020234+00:00", "description": "", "title": "chep2015-scaleout-databases.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/43\/attachments\/578924\/797136\/chep2015-scaleout-databases.pdf", "filename": "chep2015-scaleout-databases.pdf", "content_type": "application\/pdf", "type": "file", "id": 797136, "size": 4437754}], "title": "Slides", "default_folder": false, "id": 578924, "description": ""}], "_type": "Contribution", "description": "Data generation rates are expected to grow very fast for some database workloads going into LHC run 2 and beyond. In particular this is expected for data coming from controls, logging and monitoring systems. Storing, administering and accessing big data sets in a relational database system is in certain cases very demanding on the technology and therefore on costs. Notably one of the critical parts in the architecture of Oracle database clusters is the use of shared storage. Therefore there is a high interest in the CERN database community to look for alternative solutions for storing and querying big data volumes with fast and scalable data access time. Scale out database engines are an emerging and rapidly developing area. Recently a technical solution that has attracted attention is Cloudera Impala with columnar storage provided by Parquet on top of Hadoop Distributed File System. This solution has the additional benefit of offering SQL as the main data access interface which makes it easy to integrate with existing client application. In this paper we will describe the architecture of database systems based on Impala Hadoop clusters and we will discuss the results of our tests, including tests of data loading and integration with existing data sources, notably Oracle databases. We will report on query performance tests done with various data sets of interest at CERN, notably the accelerator log database.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578924", "resources": [{"_type": "LocalFile", "name": "chep2015-scaleout-databases.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/43\/attachments\/578924\/797136\/chep2015-scaleout-databases.pdf", "fileName": "chep2015-scaleout-databases.pdf", "_fossil": "localFileMetadata", "id": "797136", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "28ad91118607ed52056bb01dfdd055c6", "affiliation": "Warsaw University of Technology (PL)", "_fossil": "contributionParticipationMetadata", "fullName": "GRZYBEK, Maciej", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "1306e370311618a67b3ad8bcc859404e", "affiliation": "Univ. Extremadura, Cen. Uni. Merida (ES)", "_fossil": "contributionParticipationMetadata", "fullName": "LANZA GARCIA, Daniel", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "dd077faeac1f58ef59416503f500d418", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "CANALI, Luca", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/43", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "08:15:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "40", "speakers": [{"_type": "ContributionParticipation", "emailHash": "9cc09ed179b9e135c83fd4de1db03537", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "TAKAI, Helio", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "9cc09ed179b9e135c83fd4de1db03537", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "TAKAI, Helio", "id": "2"}], "title": "gFEX, the ATLAS Calorimeter Level 1 Real Time Processor", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:24:00.753966+00:00", "description": "", "title": "CHEP2015-Takai-gFEX.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/40\/attachments\/578925\/797137\/CHEP2015-Takai-gFEX.pdf", "filename": "CHEP2015-Takai-gFEX.pdf", "content_type": "application\/pdf", "type": "file", "id": 797137, "size": 3692426}], "title": "Slides", "default_folder": false, "id": 578925, "description": ""}], "_type": "Contribution", "description": "The global feature extractor (gFEX) is a component of the Level-1\r\nCalorimeter trigger Phase-I upgrade for the ATLAS experiment. It is\r\nintended to identify patterns of energy associated with the hadronic\r\ndecays of high momentum Higgs, W, & Z bosons, topquarks, and exotic\r\nparticles in real time at the LHC crossing rate. The single processor\r\nboard will be implemented as a fast reconfigurable processor based on\r\nfour largeFPGAs. The board will receive coarse-granularity information\r\nfrom all the ATLAS calorimeters on 264 optical fibers with the data\r\ntransferred at the 40 MHz LHC clock frequency. The gFEX will be\r\ncontrolled by a single system-on-chip processor, ZYNQ, that will be\r\nused to configure FPGAs, monitor board health, and interface to\r\nexternalsignals. Although the board is being designed specifically for\r\nthe ATLAS experiment, itis sufficiently generic that it could be used\r\nfor fast data processing at other HEP or NP experiments. We will\r\npresent the design of the gFEX board and discuss how it is\r\nbeingimplemented.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578925", "resources": [{"_type": "LocalFile", "name": "CHEP2015-Takai-gFEX.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/40\/attachments\/578925\/797137\/CHEP2015-Takai-gFEX.pdf", "fileName": "CHEP2015-Takai-gFEX.pdf", "_fossil": "localFileMetadata", "id": "797137", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "934a16d8ce2ac85421ba0ad9ecf2007a", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "BEGEL, Michael", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c034fec7f085601d5ec11c66a807130c", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CHEN, Hucheng", "id": "3"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/40", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "07:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "08:00:00"}, "duration": 15, "session": "Track 1 Session", "keywords": [], "id": "41", "speakers": [{"_type": "ContributionParticipation", "emailHash": "60f7b38a9515c03d21a43b8bbc9de9f8", "affiliation": "University of Paderborn (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "SCHUMACHER, Jorn", "id": "8"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "60f7b38a9515c03d21a43b8bbc9de9f8", "affiliation": "University of Paderborn (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "SCHUMACHER, Jorn", "id": "8"}], "title": "FELIX: a High-throughput network approach for interfacing to front end electronics for ATLAS upgrades", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T14:10:40.576789+00:00", "description": "", "title": "CHEP2015 - FELIX.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/41\/attachments\/578926\/797139\/CHEP2015_-_FELIX.pptx", "filename": "CHEP2015_-_FELIX.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797139, "size": 1458733}, {"_type": "attachment", "modified_dt": "2015-04-09T14:10:40.576789+00:00", "description": "", "title": "PDF (no animations)", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/41\/attachments\/578926\/797138\/CHEP2015_-_FELIX.pdf", "filename": "CHEP2015_-_FELIX.pdf", "content_type": "application\/pdf", "type": "file", "id": 797138, "size": 1562786}, {"_type": "attachment", "modified_dt": "2015-04-09T14:10:40.576789+00:00", "description": "", "title": "PDF (with animations)", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/41\/attachments\/578926\/797140\/CHEP2015_-_FELIX_-_split_slides.pdf", "filename": "CHEP2015_-_FELIX_-_split_slides.pdf", "content_type": "application\/pdf", "type": "file", "id": 797140, "size": 1777780}], "title": "Slides", "default_folder": false, "id": 578926, "description": ""}], "_type": "Contribution", "description": "The ATLAS experiment at CERN is planning the full deployment of a new, unified link technology for connecting detector front-end electronics on the timescale of the LHC Run 4 (2025). It is estimated that roughly 8000 Gigabit Transceiver links (GBT), with transfer rates probably up to 9.6 Gbps, will replace existing links used for readout, detector control and distribution of timing and trigger information. In particular the links used for readout are often detector-specific. Already in Run 3 this technology will be deployed in conjunction with new muon detectors, additional muon first-level triggering electronics and new on-detector and off-detector liquid argon calorimeter electronics to be used for first level triggering. A total of roughly 2000 GBT or GBT-like links (for connecting to off-detector trigger electronics) will be needed. A new class of devices will need to be developed to interface many GBT links to the rest of the trigger, data-acquisition and detector control systems. In this paper we present FELIX, the Front End LInk eXchange. The purpose of FELIX is to route data from and to multiple GBT links via a high-performance general purpose network capable of a total throughput that could be of O(20 Tbps).\r\n\r\nFELIX is designed to be used as a multi-purpose routing device; data can be forwarded to or from multiple destinations connected to the network, e.g. data-acquisition, monitoring, detector control, calibration, or configuration systems. The system will be capable of handling special bi-directional low-latency channels, such as needed for transferring Timing, Trigger and Control, TTC, signals via GBT links or for GBT-to-GBT Direct-Output-Low-Latency connections to first level trigger electronics. The software layer will integrate support for data sampling, quality-of-service prioritization, load balancing based on a round-robin scheme and automatic failover.\r\n\r\nFELIX is the central element of a new readout architecture that replaces the legacy point-to-point links with a switched network. The new architecture will be more dynamic, flexible and adaptable. By replacing point-to-point links with a switched network, the number of single points-of-failure can be reduced and a more robust and fault-tolerant system can be built, also implementing traffic equalization schemes.\r\n\r\nIn preparation for the deployment in Run 3 and to support the ongoing detector and trigger electronics development, a FELIX technology demonstrator is planned to be available early 2015, using commercial-off-the-shelf server PC technology in combination with a commercial FPGA-based PCIe Gen3 I\/O card interfacing to up to 24 GBT links and with TTC connectivity provided by an FMC-based mezzanine card. Dedicated firmware for the Xilinx Virtex-7 FPGA installed on the I\/O card alongside an interrupt-driven Linux kernel driver and user-space software will provide the required functionality. On the network side, the FELIX demonstrator connects to an Ethernet-based network. Extensions to other high-performance network technologies, such as Infiniband are possible and will be discussed in the paper.\r\n\r\nIn this paper we introduce a new approach to interfacing to on-detector and trigger electronics on the basis of GBT link or GBT-like link technology for ATLAS in Run 3 and Run 4, highlighting the innovative elements and advantages. We will then derive the functional and performance requirements on FELIX and present the design and implementation of the FELIX demonstrator. Furthermore, we will show throughput performance results as well as networking and data processing benchmarks. We intend to show that our design is a viable solution for a multi-purpose routing device in the anticipated ATLAS architecture of LHC Run 4.", "track": "Track1: Online computing ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578926", "resources": [{"_type": "LocalFile", "name": "CHEP2015 - FELIX.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/41\/attachments\/578926\/797139\/CHEP2015_-_FELIX.pptx", "fileName": "CHEP2015_-_FELIX.pptx", "_fossil": "localFileMetadata", "id": "797139", "_deprecated": true}, {"_type": "LocalFile", "name": "PDF (no animations)", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/41\/attachments\/578926\/797138\/CHEP2015_-_FELIX.pdf", "fileName": "CHEP2015_-_FELIX.pdf", "_fossil": "localFileMetadata", "id": "797138", "_deprecated": true}, {"_type": "LocalFile", "name": "PDF (with animations)", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/41\/attachments\/578926\/797140\/CHEP2015_-_FELIX_-_split_slides.pdf", "fileName": "CHEP2015_-_FELIX_-_split_slides.pdf", "_fossil": "localFileMetadata", "id": "797140", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "0d8eb6286586f3b381d330130eb854aa", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GORINI, Benedetto", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "d4f082cec0add7e53dabba23a19e2855", "affiliation": "Brookhaven National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LANNI, Francesco", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "7e3f1857bfe32cbf246a8d1df6a0b8b4", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "LEHMANN MIOTTO, Giovanna", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "1dd42736818c3922db61236c8c0309f6", "affiliation": "Weizmann Institute of Science (IL)", "_fossil": "contributionParticipationMetadata", "fullName": "LEVINSON, Lorne", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "3aba46e1c4cb6f01bce142c0571274dd", "affiliation": "Weizmann Institute of Science (IL)", "_fossil": "contributionParticipationMetadata", "fullName": "NAREVICIUS, Julia", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "52ea21b11e9751d6b2f544572c9e0800", "affiliation": "Weizmann Institute of Science (IL)", "_fossil": "contributionParticipationMetadata", "fullName": "ROICH, Alexander", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "de198eacb83e13d3a4eee1987521a13e", "affiliation": "NIKHEF (NL)", "_fossil": "contributionParticipationMetadata", "fullName": "SCHREUDER, Frans Philip", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "ff8f272608b130d14391cbd89435c9ec", "affiliation": "NIKHEF (NL)", "_fossil": "contributionParticipationMetadata", "fullName": "VERMEULEN, Joseph", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "eb71155cdc978b2e1f16e7dfa3777651", "affiliation": "Shandong University\/Brookhaven National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "ZHANG, Jinlong", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "0ef4863a55bde82366e4deed47bfafbf", "affiliation": "A", "_fossil": "contributionParticipationMetadata", "fullName": "ANDERSON, John Thomas", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "a45b41584471e2bf35d7524e6f7d5167", "affiliation": "NIKHEF (NL)", "_fossil": "contributionParticipationMetadata", "fullName": "BORGA, Andrea", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "e04e0ac2db6aac4c862fc7025d37009e", "affiliation": "NIKHEF (NL)", "_fossil": "contributionParticipationMetadata", "fullName": "BOTERENBROOD, Henk", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "976b7485330b82874a6e30b3b5bc7e22", "affiliation": "BNL", "_fossil": "contributionParticipationMetadata", "fullName": "CHEN, Kai", "id": "14"}, {"_type": "ContributionParticipation", "emailHash": "4585120ca2ef6543ecfb39f6a7147c36", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "CHEN, Hucheng", "id": "15"}, {"_type": "ContributionParticipation", "emailHash": "0b2e87224ebb4bfca9c2d48905593958", "affiliation": "Argonne National Laboratory (US)", "_fossil": "contributionParticipationMetadata", "fullName": "DRAKE, Gary", "id": "16"}, {"_type": "ContributionParticipation", "emailHash": "93153a4ba5a1f81290fcfd8a7cade073", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "FRANCIS, David", "id": "17"}, {"_type": "ContributionParticipation", "emailHash": "065452385d8cc33a22362c8fa8fea0f4", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "VANDELLI, Wainer", "id": "18"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/1\/contribution\/41", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "07:15:00"}, "duration": 15, "session": "Track 3 Session", "keywords": [], "id": "5", "speakers": [{"_type": "ContributionParticipation", "emailHash": "0924dfb6cfaf429459832827db5a0041", "affiliation": "CEA\/IRFU,Centre d'etude de Saclay Gif-sur-Yvette (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "FORMICA, Andrea", "id": "2"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c195ef3b576ede75b0ccfd47c33fba3a", "affiliation": "University of Oxford (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "GALLAS, Elizabeth", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "0924dfb6cfaf429459832827db5a0041", "affiliation": "CEA\/IRFU,Centre d'etude de Saclay Gif-sur-Yvette (FR)", "_fossil": "contributionParticipationMetadata", "fullName": "FORMICA, Andrea", "id": "2"}], "title": "Designing a future Conditions Database based on LHC experience", "note": {}, "location": "C209", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-03-26T15:08:36.651857+00:00", "description": "", "title": "CondDB_RnD_AtlasCms.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/5\/attachments\/578927\/797141\/CondDB_RnD_AtlasCms.pdf", "filename": "CondDB_RnD_AtlasCms.pdf", "content_type": "application\/pdf", "type": "file", "id": 797141, "size": 1319527}], "title": "Slides", "default_folder": false, "id": 578927, "description": ""}], "_type": "Contribution", "description": "The ATLAS and CMS Conditions Database infrastructures have served each of the respective experiments well through LHC Run 1, providing efficient access to a wide variety of conditions information needed in online data taking and offline processing and analysis.  During the long shutdown between Run 1 and Run 2, we have taken various measures to improve our systems for Run 2.  In some cases, a drastic change was not possible because of the relatively short time scale to prepare for Run 2.  In this process, and in the process of comparing to the systems used by other experiments, we realized that for Run 3, we should consider more fundamental changes and possibilities.\r\n \r\nWe seek changes which would streamline conditions data management, improve monitoring tools, better integrate the use of metadata, incorporate analytics to better understand conditions usage, as well as investigate fundamental changes in the storage technology, which might be more efficient while minimizing maintenance of the data as well as simplify the access to it. This contribution will describe architectures we are evaluating and testing which we think will address the problematic areas while providing improved services.", "track": "Track3: Data store and access ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578927", "resources": [{"_type": "LocalFile", "name": "CondDB_RnD_AtlasCms.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/5\/attachments\/578927\/797141\/CondDB_RnD_AtlasCms.pdf", "fileName": "CondDB_RnD_AtlasCms.pdf", "_fossil": "localFileMetadata", "id": "797141", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "35f0509459ee62f9ddcf584936cca96a", "affiliation": "Universit\u00e0 e INFN Genova (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BARBERIS, Dario", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "7e3f1857bfe32cbf246a8d1df6a0b8b4", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "LEHMANN MIOTTO, Giovanna", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "a7a7869681df44c70a20722d524703a9", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "GOVI, Giacomo", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "c5fff1e296ec574187817b412c5fd077", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PFEIFFER, Andreas", "id": "5"}], "subContributions": [], "room": "C209", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/3\/contribution\/5", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "489", "speakers": [{"_type": "ContributionParticipation", "emailHash": "389b9dedaf5def8b4cca0e06753d5ba6", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. IWAI, Go", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "389b9dedaf5def8b4cca0e06753d5ba6", "affiliation": "KEK", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. IWAI, Go", "id": "0"}], "title": "A Virtual Geant4 Environment", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T07:21:43.224320+00:00", "description": "", "title": "A_Virtual_Geant4_Environment_posterA_track2_489.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/489\/attachments\/578928\/797142\/A_Virtual_Geant4_Environment_posterA_track2_489.pdf", "filename": "A_Virtual_Geant4_Environment_posterA_track2_489.pdf", "content_type": "application\/pdf", "type": "file", "id": 797142, "size": 920664}], "title": "Poster", "default_folder": false, "id": 578928, "description": ""}], "_type": "Contribution", "description": "We describe the development of an environment for Geant4 consisting of the application and data that enables users a faster and easier way to access the Geant4 applications without having to download and build the software locally. The environment is platform neutral and offers the users near-real time performance. The environment consists of data and Geant4 libraries built using the LLVM tools which can then result in bitcode that can be embedded in HTML and accessed via the browser. The bitcode is downloaded to the local machine via the browser and can then be configured by the user.\r\n\r\nThis approach provides a way of minimising the risk of leaking potentially sensitive data used to construct the Geant4 model and has application in the medical domain for treatment planning.\r\n\r\nWe describe several applications that have used this approach and describe the performance of applications built with this approach compared to native applications. We also describe potential user communities that could benefit from this approach.", "track": "Track2: Offline software ", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578928", "resources": [{"_type": "LocalFile", "name": "A_Virtual_Geant4_Environment_posterA_track2_489.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/489\/attachments\/578928\/797142\/A_Virtual_Geant4_Environment_posterA_track2_489.pdf", "fileName": "A_Virtual_Geant4_Environment_posterA_track2_489.pdf", "_fossil": "localFileMetadata", "id": "797142", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/489", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:15:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:30:00"}, "duration": 15, "session": "Track 8 Session", "keywords": [], "id": "488", "speakers": [{"_type": "ContributionParticipation", "emailHash": "5214946a04027422a4ef19ee19a0fbe1", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SRIDHARAN, Srikanth", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "5214946a04027422a4ef19ee19a0fbe1", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "SRIDHARAN, Srikanth", "id": "0"}], "title": "Evaluation of 'OpenCL for FPGA' for Data Acquisition and Acceleration in High Energy Physics applications", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-14T00:44:35.393958+00:00", "description": "", "title": "CHEP2015_Presentation_-_Evaluation_of_'OpenCL_for_FPGA'_for_DAQ_and_Acceleration_in_HEP_applications-rev6_erratum.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/488\/attachments\/578929\/797143\/CHEP2015_Presentation_-_Evaluation_of_OpenCL_for_FPGA_for_DAQ_and_Acceleration_in_HEP_applications-rev6_erratum.pdf", "filename": "CHEP2015_Presentation_-_Evaluation_of_OpenCL_for_FPGA_for_DAQ_and_Acceleration_in_HEP_applications-rev6_erratum.pdf", "content_type": "application\/pdf", "type": "file", "id": 797143, "size": 610533}, {"_type": "attachment", "modified_dt": "2015-04-14T00:44:35.393958+00:00", "description": "", "title": "CHEP2015_Presentation_-_Evaluation_of_'OpenCL_for_FPGA'_for_DAQ_and_Acceleration_in_HEP_applications-rev6_erratum.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/488\/attachments\/578929\/797144\/CHEP2015_Presentation_-_Evaluation_of_OpenCL_for_FPGA_for_DAQ_and_Acceleration_in_HEP_applications-rev6_erratum.pptx", "filename": "CHEP2015_Presentation_-_Evaluation_of_OpenCL_for_FPGA_for_DAQ_and_Acceleration_in_HEP_applications-rev6_erratum.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797144, "size": 1456915}], "title": "Slides", "default_folder": false, "id": 578929, "description": ""}], "_type": "Contribution", "description": "The proposed upgrade for the Large Hadron Collider LHCb experiment at CERN envisages a system of 500 Data sources each generating data at 100 Gbps, the acquisition and processing of which is a challenge even for state of the art FPGAs. This challenge splits into two, the Data Acquisition (DAQ) part and the Algorithm acceleration part, the later not necessarily immediately following the former. \r\n\r\nLooking first at the DAQ part, a Header Generator module was needed to packetize the streaming data coming in from the front-end electronics of the detectors, for easy access and processing by the servers. This necessitates FPGA architectures that not only handle the data generated by the experiment in real-time but also dynamically adapt to potential inadequacies of other components, such as the network and PCs, while ensuring system stability and overall data integrity. Since the data source has no flow control, this module needs to modify the stream data by dropping datasets in a controlled fashion in the event of receiving a back pressure signal from the downstream modules. Also needed was a front-end source emulator capable of generating the various data patterns, that can act as a test bed to validate the functionality and performance of the Header Generator. Such a system was earlier designed and realized in VHDL. The results from this were presented as a paper, \u2018Dynamically Adaptive Header Generator and Front-End Source Emulator for a 100 Gbps FPGA based DAQ\u2019 presented at the IEEE Real-Time Conference earlier in 2014 (RT2014). \r\n\r\nWhile this process has been traditionally carried out using hardware description languages (HDLs), the possibility exists of using OpenCL to design a DAQ system. This has the potential to simplify development for physicists using the tools, who are more familiar with traditional software as opposed to HDLs, so they can understand the system and make modifications in the  future. This is challenging due to fact that the OpenCL language is designed for Parallel Processing and not really targeted at real-time DAQ and there are major challenges in representing the cycle-accurate data acquisition and processing system in OpenCL. However, OpenCL for FPGAs may be applicable from a high level synthesis perspective. Achieving this will enable the movement of the entire FPGA design flow for High Energy Physics applications to OpenCL, rather than just the algorithm acceleration portion that involves parallel processing. \r\n\r\nFor the algorithm acceleration part, the Hough transformation will be implemented in OpenCL. This is a method to reconstruct lines from points in 2D\/3D space and can be used to identify particle tracks from hits in the VELO detector elements. Variations of this algorithm are also used for feature identification on the data from other detectors too. \r\n\r\nThis work explores the feasibility of implementing Data Acquisition and Processing system on OpenCL and evaluates the performance of this OpenCL implementation with the HDL based implementation. Development is using the Altera OpenCL compiler for FPGA.\r\n\r\nThis work was is funded under ICE-DIP, a European Industrial Doctorate project in the European Community\u2019s 7th Framework programme Marie Curie Actions under grant PITN-GA-2012-316596.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578929", "resources": [{"_type": "LocalFile", "name": "CHEP2015_Presentation_-_Evaluation_of_'OpenCL_for_FPGA'_for_DAQ_and_Acceleration_in_HEP_applications-rev6_erratum.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/488\/attachments\/578929\/797143\/CHEP2015_Presentation_-_Evaluation_of_OpenCL_for_FPGA_for_DAQ_and_Acceleration_in_HEP_applications-rev6_erratum.pdf", "fileName": "CHEP2015_Presentation_-_Evaluation_of_OpenCL_for_FPGA_for_DAQ_and_Acceleration_in_HEP_applications-rev6_erratum.pdf", "_fossil": "localFileMetadata", "id": "797143", "_deprecated": true}, {"_type": "LocalFile", "name": "CHEP2015_Presentation_-_Evaluation_of_'OpenCL_for_FPGA'_for_DAQ_and_Acceleration_in_HEP_applications-rev6_erratum.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/488\/attachments\/578929\/797144\/CHEP2015_Presentation_-_Evaluation_of_OpenCL_for_FPGA_for_DAQ_and_Acceleration_in_HEP_applications-rev6_erratum.pptx", "fileName": "CHEP2015_Presentation_-_Evaluation_of_OpenCL_for_FPGA_for_DAQ_and_Acceleration_in_HEP_applications-rev6_erratum.pptx", "_fossil": "localFileMetadata", "id": "797144", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "9f655ee0d367b6896f4dd3a37228be6f", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "NEUFELD, Niko", "id": "1"}], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/488", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "487", "speakers": [{"_type": "ContributionParticipation", "emailHash": "0f060f824b4deaa6971988d7e54161a9", "affiliation": "MPI for Physics, Munich", "_fossil": "contributionParticipationMetadata", "fullName": "SCHULZ, Oliver", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "0f060f824b4deaa6971988d7e54161a9", "affiliation": "MPI for Physics, Munich", "_fossil": "contributionParticipationMetadata", "fullName": "SCHULZ, Oliver", "id": "0"}], "title": "Background decomposition of the GERDA data with BAT", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T06:30:29.666647+00:00", "description": "", "title": "chep2015-oschulz-poster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/487\/attachments\/578930\/797145\/chep2015-oschulz-poster.pdf", "filename": "chep2015-oschulz-poster.pdf", "content_type": "application\/pdf", "type": "file", "id": 797145, "size": 429776}], "title": "Slides", "default_folder": false, "id": 578930, "description": ""}], "_type": "Contribution", "description": "GERDA is an ultra-low background experiment, designed to search for the\r\nneutrinoless double beta decay of Ge-76. The main background sources of\r\nsuch an experiment are minute radioactive ontaminations of the experimental\r\nsetup itself. Gaining a good understanding of the individual contributions\r\nto this radioactive background is vital not only for data analysis, but also\r\nguides the design of the next stage of the experiment.\r\n\r\nThe Bayesian Analysis Toolkit (BAT) was used to perform a full background\r\ndecomposition of the GERDA Phase-I data. The Bayesian approach allowed the\r\nimplementation of prior knowledge and the ability to handle competing models\r\nin a consistent way. It also yields a straightforward uncertainty propagation,\r\ntaking the correlations between model components into account.\r\n\r\nWe describe the formulation of the analysis problem and the technical\r\nrealization.  The techniques described here are of general validity and interest,\r\nand have proven to be very successful.", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578930", "resources": [{"_type": "LocalFile", "name": "chep2015-oschulz-poster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/487\/attachments\/578930\/797145\/chep2015-oschulz-poster.pdf", "fileName": "chep2015-oschulz-poster.pdf", "_fossil": "localFileMetadata", "id": "797145", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/487", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "486", "speakers": [{"_type": "ContributionParticipation", "emailHash": "e86323288b18387d48dc1af9718c14d6", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WILDISH, Tony", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "484ded748b29edd07e06a7c22a0d717d", "affiliation": "Brunel University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "CARDOSO LOPES, Raul", "id": "0"}], "title": "A survey and practical evaluation of the problem and algorithms for CMS job scheduling", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [], "_type": "Contribution", "description": "Starting from March 2012, Amazon made available ``Multi-Region Latency Based Routing.``\r\nThis is a set of policies from which users can choose a policy to serve data and pages from the\r\nAmazon cloud. By offering different options and letting their users choose from them, Amazon is acknowledging\r\nthe difference and importance of access-time to different data centres.\r\n\r\nCompared to Amazon, CMS has the advantage that its applications and,\r\nconsequently, storage demands are much more uniform\r\nthan what can possibly be found in a public cloud. However, while Amazon\r\nhas to serve data from only four different data\r\ncentres, CMS has to cope with around 70 PB of data distributed over 50 T2\r\ndata centres. Scheduling processors and jobs in\r\nan arbitrary environment like the CMS network of processors and storage\r\nunits can be modelled by reduction to problems\r\nlike the dynamic k-servers, the multiple knapsack or even the k TSP.\r\nWhatever the case, such modeling will eventually\r\ncome to the fact that these problems fall into the realm of NP-hardness,\r\nwhere approximation algorithms are the best that\r\ncan be achieved when looking for exact solutions. Nonetheless, some\r\nproblems of this nature can be solved adequately in\r\npractice, such as paging in the Linux kernel (another k-server problem).\r\n\r\nIn this paper we discuss alternatives for scheduling CMS jobs in the\r\ngrid taking into account distribution of\r\nprocessors, storage and network. We discuss the needs related to parsing\r\nand processing historical data as a basis to\r\nimplement simulations that would guide the adoption of new policies for\r\na CMS scheduler. We present initial data for\r\nmodeling based on approximation algorithms.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "e86323288b18387d48dc1af9718c14d6", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. WILDISH, Tony", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "d41d8cd98f00b204e9800998ecf8427e", "affiliation": "Brunel University (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "HOBSON, Peter", "id": "2"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/486", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:15:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:30:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "485", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ec154520e09c584120f42b3ea16f8de3", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "Miss. RONCHIERI, Elisabetta", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "ec154520e09c584120f42b3ea16f8de3", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "Miss. RONCHIERI, Elisabetta", "id": "0"}], "title": "First statistical analysis of Geant4 quality software metrics", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T23:31:38.161701+00:00", "description": "", "title": "RonchieriCHEP2015vf.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/485\/attachments\/578931\/797146\/RonchieriCHEP2015vf.pdf", "filename": "RonchieriCHEP2015vf.pdf", "content_type": "application\/pdf", "type": "file", "id": 797146, "size": 314665}], "title": "Slides", "default_folder": false, "id": 578931, "description": ""}], "_type": "Contribution", "description": "Geant4 is a widespread simulation system of \"particles through matter\" used in several experimental areas from high energy physics and nuclear experiments to medical studies. Some of its applications may involve critical use cases; therefore they would benefit from an objective assessment of the software quality of Geant4. The issue of  maintainability is especially relevant for such a widely used, mature software system, which at the present time is the result of 20 years of development.\r\n\r\nWe performed a quantitative analysis of Geant4 software quality with emphasis on maintainability.\r\n\r\nTo evaluate the maintainability of Geant4 software, we used existing standards, such as ISO\/IEC 9126, that identifies the software characteristics. Furthermore, we exploited a set of product metrics - aggregated in the program size, code distribution, control flow complexity and  object-orientation metrics categories - that allows to understand the code state. By using various software metrics tools, we were able to collect a large amount of measurements of software characteristics.\r\n\r\nIn this paper, we provide a first statistical evaluation of software metrics data related to a set of Geant4 physics packages. The analysis determined what metrics are most effective at identifying risks for the considered Geant4 packages and their correlations. We also evaluated the applicability of existing quality standards, which may derive from different application environments, to the Geant4 context.\r\n\r\nThe findings of this pilot study set the ground for further extensions to the whole of Geant4 and to other HEP software systems.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578931", "resources": [{"_type": "LocalFile", "name": "RonchieriCHEP2015vf.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/485\/attachments\/578931\/797146\/RonchieriCHEP2015vf.pdf", "fileName": "RonchieriCHEP2015vf.pdf", "_fossil": "localFileMetadata", "id": "797146", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "2ddf3172fe1ae61724cf45ac1a9b52b0", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PIA, Maria Grazia", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "2c4d3df7a0ce70fa1d12cf2b886abbde", "affiliation": "INFN CNAF", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. GIACOMINI, Francesco", "id": "2"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/485", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "484", "speakers": [{"_type": "ContributionParticipation", "emailHash": "769117bacc977b39854c27c60df0c5cc", "affiliation": "International Cloud Cooperation", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KAWAMURA, Gen", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "769117bacc977b39854c27c60df0c5cc", "affiliation": "International Cloud Cooperation", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. KAWAMURA, Gen", "id": "0"}], "title": "Effective administration through gaining the portability of the LHC Computing Grid sites", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-09T08:08:53.689968+00:00", "description": "", "title": "CHEP_PosterPresentation-70CMx100CM.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/484\/attachments\/578932\/797147\/CHEP_PosterPresentation-70CMx100CM.pdf", "filename": "CHEP_PosterPresentation-70CMx100CM.pdf", "content_type": "application\/pdf", "type": "file", "id": 797147, "size": 982284}], "title": "Slides", "default_folder": false, "id": 578932, "description": ""}], "_type": "Contribution", "description": "Grid computing enables deployments of large scale distributed computational infrastructures among different research facilities. It has been recently proposed that the Grid infrastructure be based on cloud computing. Provisioning systems and automated management frameworks using Cobbler, Rocks, Cfengine and Puppet are being successfully applied to many systems. Having implemented these new concepts in Grid sites within the Worldwide LHC Computing Grid (WLCG), they are solving many problems with huge world-wide collaborations and experiments, such as ATLAS and CMS. \r\n\r\nSoftware provisioning like Cobbler and Rocks involves the process of selecting a target machine, a server, loading the different systems (bare operating systems, middlewares, and applications), and customizing individual components and configuring the cluster to make them ready for production. The provisioning system needs a variety of actions: for example, creating or changing boot images; specifying particular hardware and network parameters; and lastly, starting the machine, its softwares, Grid middlewares and high energy physics applications. Typically, because of the complexity of the systems, system administrators will perform these tasks using various provisioning and management tools. However, there is a lack of generic control tools for their huge collaborations that can adequately integrate and automate these tasks for the large scale Grid sites and local experimental systems in the WLCG environments. Therefore, even though fundamental components between the large Grid production site and the small experimental system are similar, every administrator needs to customize and optimize these different tools and administrative codes when the site needs to be built and merged with WLCG collaborations. \r\n\r\nIn terms of resource provisioning, system management, meta-scheduling and portability of the Grid clusters, the combination of Grid and cloud resources for deploying the WLCG computing infrastructure introduces new challenges and opportunities. We demonstrate a way of effective administration and collaboration by using typical Grid middlewares and various services: globus, TorquePBS, CREAMCE, dCache, StoRM and gLite middlewares. Multiple virtual Grid clusters have been prepared and the deployments show the effectiveness of a generalized approach gaining the portability of the grid clusters. \r\n\r\nIn the present paper, we argue for integrating the concept of a localized LHC Computing Grid in a testing and deployment framework with that of the so\u00ad-called Portable-\u00adGrid system. Using virtualization and revision control technologies to share generic system management codes, procedures installing grid systems, required libraries, experimental codes and bare operating systems, the remotely prepared Grid systems can be immediately deployed into virtual images and executed under suitable hypervisors installed on target host platforms and grid clusters. This approach is allowing the instant construction of localized Grid sites in a scenario based on OpenStack cloud computing. Further, it enables a new paradigm applicable also to general experiments in physics and computer science.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578932", "resources": [{"_type": "LocalFile", "name": "CHEP_PosterPresentation-70CMx100CM.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/484\/attachments\/578932\/797147\/CHEP_PosterPresentation-70CMx100CM.pdf", "fileName": "CHEP_PosterPresentation-70CMx100CM.pdf", "_fossil": "localFileMetadata", "id": "797147", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/484", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "10:45:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:00:00"}, "duration": 15, "session": "Track 8 Session", "keywords": [], "id": "483", "speakers": [{"_type": "ContributionParticipation", "emailHash": "ed0edbdf6017fcaf405ed6b95caa3c8b", "affiliation": "INFN Rome", "_fossil": "contributionParticipationMetadata", "fullName": "MARTINELLI, Michele", "id": "6"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "c8ea049d818017032cd8aa4ecc535554", "affiliation": "INFN Rome", "_fossil": "contributionParticipationMetadata", "fullName": "VICINI, Piero", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "eebe1e6285bea315b8bd658cd1f8889d", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "AMMENDOLA, Roberto", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "3a0fca133d8ea170304d87715732755c", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "BIAGIONI, Andrea", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "ab54bf6c33cf8df96aaaef89b30d0749", "affiliation": "INFN Rome", "_fossil": "contributionParticipationMetadata", "fullName": "FREZZA, Ottorino", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "139ee824930b1edbe0aea163b1f25b18", "affiliation": "INFN Rome", "_fossil": "contributionParticipationMetadata", "fullName": "LO CICERO, Francesca", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "a1b84ab782593aeb88af507dfc973dba", "affiliation": "Universita e INFN, Roma I (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "LONARDO, Alessandro", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "ed0edbdf6017fcaf405ed6b95caa3c8b", "affiliation": "INFN Rome", "_fossil": "contributionParticipationMetadata", "fullName": "MARTINELLI, Michele", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "05a185dabd0e4b43e71a916171504da6", "affiliation": "INFN Rome", "_fossil": "contributionParticipationMetadata", "fullName": "PAOLUCCI, Pier Stanislao", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "d331e29451858cbaa9c90c7c15f88030", "affiliation": "INFN Rome", "_fossil": "contributionParticipationMetadata", "fullName": "PASTORELLI, Elena", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "47793d0de56444a282ba12ac21ce8080", "affiliation": "INFN Rome and NVidia Corp. (USA)", "_fossil": "contributionParticipationMetadata", "fullName": "ROSSETTI, Davide", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "215d358e3546a0e19ff584d4bd95d5f7", "affiliation": "INFN Rome", "_fossil": "contributionParticipationMetadata", "fullName": "SIMULA, Francesco", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "255c31d15aa8b6b59e60bc27b46066f7", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "TOSORATTO, Laura", "id": "11"}], "title": "Hardware and Software Design of FPGA-based PCIe Gen3 interface for APENet+ network interconnect system", "note": {}, "location": "Village Center", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T06:37:41.298275+00:00", "description": "", "title": "CHEP2015_apenetV5_martinelli.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/483\/attachments\/578933\/797148\/CHEP2015_apenetV5_martinelli.pdf", "filename": "CHEP2015_apenetV5_martinelli.pdf", "content_type": "application\/pdf", "type": "file", "id": 797148, "size": 5700288}], "title": "Slides", "default_folder": false, "id": 578933, "description": ""}], "_type": "Contribution", "description": "The computing nodes of modern hybrid HPC systems are built using the CPU+GPU paradigm. \r\nWhen this class of systems is scaled to large size, the efficiency of the network connecting GPUs mesh and supporting the internode traffic is a critical factor. The adoption of a low latency, high performance dedicated network architecture, exploiting peculiar characteristics of CPU and GPU hardware, allows to guarantee scalability and a good level of sustained performances.   \r\n\r\nIn the attempt to develop a custom interconnection architecture optimized for scientific computing we designed APEnet+, a point-to-point, low-latency and high-performance 3D torus network controller which supports 6 fully bidirectional off-board links. \r\nThe first release of APEnet+ (named V4), was a board based on a high end 40nm Altera FPGA that integrates multiple (6) channels at 34Gbps of raw bandwidth per direction and a PCIe Gen2 x8 host interface. APEnet+ board was the first-of-its-kind to implement a Remote Direct Memory Access (RDMA) protocol to directly read\/write data from\/to Fermi and Kepler NVIDIA GPUs using the Nvidia \u201cpeer-to-peer\u201d and \u201cGPUDirect RDMA\u201d protocols, obtaining real zero-copy, low-latency GPU-to-GPU transfers over the network and reducing the performance bottleneck due to the costly copies of data from user to kernel space( and vice-versa).\r\n\r\nThe last generation of APEnet+ systems (V5), currently under development, is based on state-of-the-art high end FPGA, 28nm Altera Stratix V, offering a number of multi-standard fast transceivers (up to 14.4 Gbps), huge amount of configurable internal resources and hardware IP cores to support main interconnection standard protocols. \r\nAPEnet+ V5 implements a PCIe Gen3 x8 interface, the current standard protocol for high end system peripherals, in order to gain performance on the critical CPU\/GPU connection and mitigate the effect of the bottleneck represented by GPUs memory access.\r\nFurthermore the FPGA technology advancement, allowed us to integrate in V5, new off-board torus channels characterized by a target speed of 56 Gbps.\r\nBoth Linux Device Driver and the low-level libraries, have been redesigned to support the PCIe Gen3 protocol, introducing optimizations and solutions based on hardware\/software co-design.\r\n\r\nIn this paper we present the architecture of APEnet+ V5 and discuss the status of APEnet+ V5 PCIe Gen3 hardware and system software design.  Measures of performance in terms of latency and bandwidth, both for the local APEnet+ to CPU-GPU connection (with Kepler class GPU) and host-to-host via torus links, will also be provided.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578933", "resources": [{"_type": "LocalFile", "name": "CHEP2015_apenetV5_martinelli.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/483\/attachments\/578933\/797148\/CHEP2015_apenetV5_martinelli.pdf", "fileName": "CHEP2015_apenetV5_martinelli.pdf", "_fossil": "localFileMetadata", "id": "797148", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "Village Center", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/483", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:30:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "02:45:00"}, "duration": 15, "session": "Track 6 Session", "keywords": [], "id": "482", "speakers": [{"_type": "ContributionParticipation", "emailHash": "129b99ea1cf64f214e0ddc1c1ae55c96", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GABANCHO, Esteban", "id": "3"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "d77ba3ef219905889829b4c308d0ce82", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "KASIOUMIS, Nikos", "id": "0"}], "title": "Status report of the migration of the CERN Document Server to the invenio-next package", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-16T00:53:54.937338+00:00", "description": "", "title": "cds_status_report.key", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/482\/attachments\/578934\/797149\/cds_status_report.key", "filename": "cds_status_report.key", "content_type": "application\/pgp-keys", "type": "file", "id": 797149, "size": 9385240}, {"_type": "attachment", "modified_dt": "2015-04-16T00:53:54.937338+00:00", "description": "", "title": "cds_status_report.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/482\/attachments\/578934\/797150\/cds_status_report.pdf", "filename": "cds_status_report.pdf", "content_type": "application\/pdf", "type": "file", "id": 797150, "size": 3054479}], "title": "Slides", "default_folder": false, "id": 578934, "description": ""}], "_type": "Contribution", "description": "With the expected release of invenio next stable version, CDS is preparing a 'lab' service where users will have the opportunity to experience the powerful features of the new software.\r\n\r\nAfter a short introduction of Invenio next, the talk will explain the mechanisms that have been implemented to allow to run parallel services with the same content exposed from two different designs and the related constraints of this architecture. It will then focus on the main advantages of the new service for end-users.\r\n\r\nThe modern personalized collection home pages that have been designed after interviewing random users, will be detailed. Both the process to converge to a User Interface answering the major requirements and the result of the development (how a user can easily decide on the more interesting content to be displayed first) will be presented to the audience.\r\n\r\nMany other improvements to the current CDS will also be introduced, like the advanced search clusters, the new commenting scheme heavily used by the experiments, the new auto-suggest feature looking in multiple author databases and the move of the CDS service monitoring tool to a dedicated ElasticSearch\/Kibana instance.\r\n\r\nFinally, the on-going project of creating CERN Authors profile will be over-viewed, from the mandatory disambiguation process that uniquely identifies people to the design of the most appropriate profile pages.", "track": "Track6: Facilities, Infrastructure, Network", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578934", "resources": [{"_type": "LocalFile", "name": "cds_status_report.key", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/482\/attachments\/578934\/797149\/cds_status_report.key", "fileName": "cds_status_report.key", "_fossil": "localFileMetadata", "id": "797149", "_deprecated": true}, {"_type": "LocalFile", "name": "cds_status_report.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/482\/attachments\/578934\/797150\/cds_status_report.pdf", "fileName": "cds_status_report.pdf", "_fossil": "localFileMetadata", "id": "797150", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "129b99ea1cf64f214e0ddc1c1ae55c96", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GABANCHO, Esteban", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "bc903f9ff2bedaff4455c51cd4b712a4", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "WITOWSKI, Sebastian", "id": "2"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/6\/contribution\/482", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "481", "speakers": [{"_type": "ContributionParticipation", "emailHash": "3a0fca133d8ea170304d87715732755c", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "BIAGIONI, Andrea", "id": "1"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "eebe1e6285bea315b8bd658cd1f8889d", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "AMMENDOLA, Roberto", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "3a0fca133d8ea170304d87715732755c", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "BIAGIONI, Andrea", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "255c31d15aa8b6b59e60bc27b46066f7", "affiliation": "INFN", "_fossil": "contributionParticipationMetadata", "fullName": "TOSORATTO, Laura", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "c8ea049d818017032cd8aa4ecc535554", "affiliation": "INFN Rome Section", "_fossil": "contributionParticipationMetadata", "fullName": "VICINI, Piero", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "ab54bf6c33cf8df96aaaef89b30d0749", "affiliation": "INFN - Sezione di Roma", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. FREZZA, Ottorino", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "139ee824930b1edbe0aea163b1f25b18", "affiliation": "INFN - Sezione di Roma", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. LO CICERO, Francesca", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "7893046486614f3b54bdf2ad879109bc", "affiliation": "Universita e INFN, Roma I (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "LONARDO, Alessandro", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "ed0edbdf6017fcaf405ed6b95caa3c8b", "affiliation": "INFN - Sezione di Roma", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. MARTINELLI, Michele", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "05a185dabd0e4b43e71a916171504da6", "affiliation": "INFN - Sezione di Roma", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PAOLUCCI, Pier Stanislao", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "d331e29451858cbaa9c90c7c15f88030", "affiliation": "INFN - Sezione di Roma", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PASTORELLI, Elena", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "e7899fd4424241dd764766bd2a590ea3", "affiliation": "nVIDIA Corp.", "_fossil": "contributionParticipationMetadata", "fullName": "ROSSETTI, Davide", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "215d358e3546a0e19ff584d4bd95d5f7", "affiliation": "INFN - Sezione di Roma", "_fossil": "contributionParticipationMetadata", "fullName": "SIMULA, Francesco", "id": "14"}], "title": "A multi-port 10GbE PCIe NIC featuring UDP offload and GPUDirect capabilities.", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-10T16:07:52.619022+00:00", "description": "", "title": "poster_NaNet_chep2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/481\/attachments\/578935\/797151\/poster_NaNet_chep2015.pdf", "filename": "poster_NaNet_chep2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 797151, "size": 1997438}], "title": "Poster", "default_folder": false, "id": 578935, "description": ""}], "_type": "Contribution", "description": "NaNet-10 is a four-ports 10GbE PCIe Network Interface Card designed for low-latency real-time operations with GPU systems.\r\nFor this purpose the design includes a UDP offload module, for a fast and deterministic to clock-cyle handling of transport layer protocol, plus a GPUDirect P2P\/RDMA engine for low-latency communication with nVIDIA Tesla GPU devices.\r\nA dedicate module (Merger) can optionally process input UDP streams before data are delivered through PCIe DMA to their destination devices, e.g. coalescing\r\npayload data from different streams according to a reconfigurable algorithm.\r\nNaNet-10 is going to be integrated in the NA62 CERN experiment in order to assess the suitability of GPGPU systems as real-time triggers, we will report results and lessons learned in this activity.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578935", "resources": [{"_type": "LocalFile", "name": "poster_NaNet_chep2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/481\/attachments\/578935\/797151\/poster_NaNet_chep2015.pdf", "fileName": "poster_NaNet_chep2015.pdf", "_fossil": "localFileMetadata", "id": "797151", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "3cba000cd281d7e38d3e7b73625e60ea", "affiliation": "INFN LNF", "_fossil": "contributionParticipationMetadata", "fullName": "LAMANNA, Gianluca", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "d093e9cf0cee7f22485c76970bd9a7a9", "affiliation": "INFN - Sezione di Pisa", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PONTISSO, Luca", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "30e2fd549539a7a44a8e304a07d4d341", "affiliation": "Sezione di Pisa (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "SOZZI, Marco", "id": "6"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/481", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "480", "speakers": [{"_type": "ContributionParticipation", "emailHash": "7e8bcc2855b51c4e93b19e095a8b2ec9", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "WENZEL, Sandro Christian", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "7e8bcc2855b51c4e93b19e095a8b2ec9", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "WENZEL, Sandro Christian", "id": "0"}], "title": "VecGeom:\tA new vectorised geometry\tlibrary for particle-detector\tsimulation", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:36:15.556406+00:00", "description": "", "title": "CHEP15VecGeomPoster.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/480\/attachments\/578936\/797152\/CHEP15VecGeomPoster.pdf", "filename": "CHEP15VecGeomPoster.pdf", "content_type": "application\/pdf", "type": "file", "id": 797152, "size": 2600900}], "title": "Slides", "default_folder": false, "id": 578936, "description": ""}], "_type": "Contribution", "description": "A geometry modeller library is among the most important components of the\t\r\nsoftware\t simulating the\t passage of particles in a detector, and many experiment\t\r\nsimulations are currently based on the geometry implementations offered by Geant4 or ROOT.\r\nHere, we\t report on our effort to extend, re-engineer and evolve thes libraries\tin multiple directions in order to make them ready for the future challenges of computing in HEP.\r\nThis includes primarily\tan extended API\tas well\tas SIMD-vectorised algorithms able\t\r\nto efficiently handle geometry queries for multiple particles at the same time.\r\nSecondly, we aim\t for a native support of the geometry module on\tthe GPU or \t\t on mixed\t heterogeneous CPU\/GPU platforms.\r\nThis effort is one of the essential ingredients\tof the Geant-Vector project that is\t\r\nfocusing\ton a fine-grained multithreaded and vectorised design,\tpropagating\tmany\t\r\nparticles from different events\tat the same time.\r\n\r\nOur presentation\t will give an overview of the new vectorizsed, generic and\r\ntemplated geometry library\t\"VecGeom\" that accomplishes those primary goals\tand which also improves\tthe overall geometry performance for current simulation\t\r\nframeworks. Beyond discussing new performance numbers for elementary and\r\nhigher\tlevel\tgeometry algorithms, we\twill show a first global evaluation of the new\t\r\nlibrary\t on a realistic detector in form of the\tgeometry description from the CMS\t\r\nexperiment", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578936", "resources": [{"_type": "LocalFile", "name": "CHEP15VecGeomPoster.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/480\/attachments\/578936\/797152\/CHEP15VecGeomPoster.pdf", "fileName": "CHEP15VecGeomPoster.pdf", "_fossil": "localFileMetadata", "id": "797152", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "5b0402a9cd32d8915ceeb56718af64b6", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "CANAL, Philippe", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "73c47f432ac574bc76aba1a48761b1ff", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "JUN, Soon Yung", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "ddb4e4ec90b85b573b5dd09e3f0e05c2", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "APOSTOLAKIS, John", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "565043fdb94911d4ac35d97cfc629533", "affiliation": "University of Copenhagen (DK)", "_fossil": "contributionParticipationMetadata", "fullName": "DE FINE LICHT, Johannes Christof", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "362d1634ae1bc01cde64fb0881d042d4", "affiliation": "Bhabha Atomic Research Centre (IN)", "_fossil": "contributionParticipationMetadata", "fullName": "SEHGAL, Raman", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "6a2d92d7b45e7af553c5547d2f944431", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. BRUN, Rene", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "5a02abf7bf489401510b64b45e39a213", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "ELVIRA, Victor Daniel", "id": "7"}, {"_type": "ContributionParticipation", "emailHash": "f5e7378e16e9fee96ad835deb6454086", "affiliation": "National Technical Univ. of Ukraine \"Kyiv Polytechnic Institute", "_fossil": "contributionParticipationMetadata", "fullName": "SHADURA, Oksana", "id": "8"}, {"_type": "ContributionParticipation", "emailHash": "fc171e67d3d8e09687c21b9a42fdc39f", "affiliation": "National and Kapodistrian University of Athens (GR)", "_fossil": "contributionParticipationMetadata", "fullName": "BITZES, Georgios", "id": "9"}, {"_type": "ContributionParticipation", "emailHash": "8c01aed7220e755911911d2e59aa3676", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "NOVAK, Mihaly", "id": "10"}, {"_type": "ContributionParticipation", "emailHash": "f4b45dbfcfef5c90c1b484b28e777c71", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "BANDIERAMONTE, Marilena", "id": "11"}, {"_type": "ContributionParticipation", "emailHash": "5e451b3b7969b96f79563ef9500e6f3e", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "GHEATA, Andrei", "id": "12"}, {"_type": "ContributionParticipation", "emailHash": "94c96cb9abacdef3f192e4dff0f13905", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. CARMINATI, Federico", "id": "13"}, {"_type": "ContributionParticipation", "emailHash": "1f8a789a05dced65a9ebdcba127b01bf", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. COSMO, Gabriele", "id": "14"}, {"_type": "ContributionParticipation", "emailHash": "a4aed0af45e58b26250375a6c009955c", "affiliation": "Gangneung-Wonju National University (KR)", "_fossil": "contributionParticipationMetadata", "fullName": "NIKITINA, Tatiana", "id": "15"}, {"_type": "ContributionParticipation", "emailHash": "e7e207b4c5969bac3a9c7e859482b70a", "affiliation": "FermiLab (US)", "_fossil": "contributionParticipationMetadata", "fullName": "LIMA, Guilherme", "id": "16"}, {"_type": "ContributionParticipation", "emailHash": "50198189b9d7bc94a02bd81c85665ef9", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "DUHEM, Laurent", "id": "17"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/480", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "472", "speakers": [{"_type": "ContributionParticipation", "emailHash": "7813aea80d6855af936ef1f20f083779", "affiliation": "RIKEN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. REPONEN, Mikael", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "7813aea80d6855af936ef1f20f083779", "affiliation": "RIKEN", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. REPONEN, Mikael", "id": "0"}], "title": "Rate-Equation -based model for describing the resonance laser ionization process", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "The nucleus perturbs the atomic energy levels of atoms and ions at the ppm level and\r\nalthough this is a small absolute effect it is readily probed and measured by modern\r\nlaser spectroscopic methods. These methods are particularly suitable for the study of\r\nshort-lived radionuclides with lifetimes as short as a few milliseconds and production\r\nrates often only a few isotopes\/isomers per second in the case of resonance laser ionization\r\nspectroscopy (RIS)[1]. This ionization technique, which utilizes stepwise excitation of the\r\nunique atomic state of different elements, has been demonstrated to be a powerful tool\r\nfor probing nuclei far from stability[2, 3].\r\n\r\nThe analysis of the RIS hyperfine spectra is typically performed with the help of\r\nrelative intensities calculated using angular momentum coupling. The analysis yields the\r\nnuclear magnetic dipole and electric quadrupole moments and changes in mean-square\r\ncharge radii in a model-independent manner. However, recent studies have observed that\r\nthe relative intensities of the measured hyperfine transitions do not necessarily follow the\r\ncalculated intensities. This has partially been attributed to optical pumping effects which\r\nare not addressed by the typical procedures. Due to this new methods, based on modeling\r\nthe complete ionization process [4], are required to restrict the number of free parameters\r\nduring analysis.\r\n\r\nHere we present a model for resonance ionization process based on rate-equations as\r\na stepping stone towards a density matrix -based model. The software is written in C++\r\nutilizing Boost Odeint, Eigen and Sundials -libraries. In addition, the software facilitates\r\nparallel solving capabilities through the OpenMP library.\r\n\r\n[1]  T. Cocolios et al., Phys. Rev. Lett. 103,(2009), 102501\r\n\r\n[2]  R. Ferrer et al., Phys.Lett. B 728, (2014), 191\r\n\r\n[3]  S. Rothe et al., Nat.Commun.4, (2013), 1835.\r\n\r\n[4]  S. Gheysen et al., Phys Rev C 69, (2004), 064310", "track": "Track2: Offline software ", "material": [], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "dfd385a223ae043f0397c5ab448e51d1", "affiliation": "University of Jyv\u00e4skyl\u00e4", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. SONNENSCHEIN, Volker", "id": "1"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/472", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "473", "speakers": [{"_type": "ContributionParticipation", "emailHash": "85f863191188abed761d707cfc440a8a", "affiliation": "Max-Planck-Institut fuer Physik (Werner-Heisenberg-Institut) (D", "_fossil": "contributionParticipationMetadata", "fullName": "KLUTH, Stefan", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "a9db592454003edc4ee459a7c7da7af6", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PIA, Maria Grazia", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "1eed64fb1e5d9921a6493df3839d0d82", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SCHOERNER-SADENIUS, Thomas", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "59b13035bc48cc84e67515037be1fcec", "affiliation": "MPI-CBG", "_fossil": "contributionParticipationMetadata", "fullName": "STEINBACH, Peter", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "95f3c00bd72f8ba66949501017aa065e", "affiliation": "Universitaet Bonn (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "VELZ, Thomas", "id": "4"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "85f863191188abed761d707cfc440a8a", "affiliation": "Max-Planck-Institut fuer Physik (Werner-Heisenberg-Institut) (D", "_fossil": "contributionParticipationMetadata", "fullName": "KLUTH, Stefan", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "a9db592454003edc4ee459a7c7da7af6", "affiliation": "Universita e INFN (IT)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. PIA, Maria Grazia", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "1eed64fb1e5d9921a6493df3839d0d82", "affiliation": "DESY", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. SCHOERNER-SADENIUS, Thomas", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "59b13035bc48cc84e67515037be1fcec", "affiliation": "MPI-CBG", "_fossil": "contributionParticipationMetadata", "fullName": "STEINBACH, Peter", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "95f3c00bd72f8ba66949501017aa065e", "affiliation": "Universitaet Bonn (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "VELZ, Thomas", "id": "4"}], "title": "How do particle physicists learn the programming concepts they need?", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:26:38.810943+00:00", "description": "", "title": "poster_APC_V5.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/473\/attachments\/578937\/797153\/poster_APC_V5.pdf", "filename": "poster_APC_V5.pdf", "content_type": "application\/pdf", "type": "file", "id": 797153, "size": 782356}, {"_type": "attachment", "modified_dt": "2015-04-13T00:26:38.810943+00:00", "description": "", "title": "poster_APC_V5.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/473\/attachments\/578937\/797154\/poster_APC_V5.pptx", "filename": "poster_APC_V5.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797154, "size": 7582887}], "title": "Poster", "default_folder": false, "id": 578937, "description": ""}], "_type": "Contribution", "description": "The ability to judge, use and develop code efficiently and successfully is a key ingredient in modern particle physics. Software design plays a fundamental role in the software development process and is instrumental to many critical aspects in the life-cycle of an experiment: the transparency of software design enables the validation of physics results, contributes to the effective use of human and computational resources, facilitates the evolution and the maintainability of the software over the lifetime of an experiment. However, despite the wide consensus on this appraisal in the HEP community, many concepts and techniques that are essential to working effectively in a HEP experiment are not covered during university studies, and limited opportunities are available to students to learn them after that.\r\n\r\nWe report the experience of a training program, identified as \"Advanced Programming Concepts\", that introduces software concepts, methods and techniques to work effectively on a daily basis in a HEP experiment or other programming intensive fields. The program is targeted at students and young researchers involved in physics analysis and detector development, not only at core software developers of relevant scientific code bases. The presented workshop introduces basic and advanced programming techniques (principles of object-oriented design, design patterns, meta-template programming etc.), as well as elements of the software development process and project management skills. Emphasis is given to methods to work effectively with existing code and to improve it as well as to building a basis for further self-improvement in the field.\r\n\r\nThis presentation illustrates the principles and methods that shape the \"Advanced Computing Concepts\" training program, the knowledge base that it conveys, an analysis of the feedback received so far, and the integration of these concepts in the software development process of the experiments as well as its applicability to a wider audience. It intends to promote a discussion in the software-oriented particle physics community on the responsibility of better preparing our young people for their work in the experiments, and on how the experiments could profit from a wider knowledge of advanced software methods and techniques.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578937", "resources": [{"_type": "LocalFile", "name": "poster_APC_V5.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/473\/attachments\/578937\/797153\/poster_APC_V5.pdf", "fileName": "poster_APC_V5.pdf", "_fossil": "localFileMetadata", "id": "797153", "_deprecated": true}, {"_type": "LocalFile", "name": "poster_APC_V5.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/473\/attachments\/578937\/797154\/poster_APC_V5.pptx", "fileName": "poster_APC_V5.pptx", "_fossil": "localFileMetadata", "id": "797154", "_deprecated": true}], "_deprecated": true}], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/473", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "470", "speakers": [{"_type": "ContributionParticipation", "emailHash": "2463c98197fe73772aa1cd10b34131aa", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "COLLIER, Ian Peter", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "2463c98197fe73772aa1cd10b34131aa", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "COLLIER, Ian Peter", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "a4c2629976026a424cedc51ff4d288ef", "affiliation": "STFC", "_fossil": "contributionParticipationMetadata", "fullName": "RYALL, George", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "c481005af0f274d62299c100cf7585c2", "affiliation": "STFC RAL", "_fossil": "contributionParticipationMetadata", "fullName": "DIBBO, Alexander", "id": "4"}], "title": "A quantative evaluation of different methods for instantiating private cloud virtual machines", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-01T17:07:21.881309+00:00", "description": "", "title": "CHEP_2015_-_Ceph_Vs_Local_Storage_for_VMs.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/470\/attachments\/578938\/797155\/CHEP_2015_-_Ceph_Vs_Local_Storage_for_VMs.pdf", "filename": "CHEP_2015_-_Ceph_Vs_Local_Storage_for_VMs.pdf", "content_type": "application\/pdf", "type": "file", "id": 797155, "size": 1037759}], "title": "Poster", "default_folder": false, "id": 578938, "description": ""}], "_type": "Contribution", "description": "The RAL Tier-1 has been deploying production virtual machines for several years. As we move to providing a production private cloud, managed using OpenNebula, we have experimented with a range of different ways of deploying virtual machine images on hypervisors. We present a quantative comparison of a variety of virtual machine image and storage combinations, including monolithic Scientific Linux VMs copied and run locally on hy[pervisors, the same images run on Ceph shared storage, lean images with volatile filesystems created and run locally as required. We will also examine the same combinations using the micro-CernVM - where the bulk of the operating system is provided by the CernVM filesystem. We will compare performance, with regards to both instantiation time and computational performance for HEP workloads, and also discuss the virtual machine managememt implications of each combination.", "track": "Track7: Clouds and virtualization", "material": [{"_type": "Poster", "title": "Poster", "_fossil": "materialMetadata", "id": "578938", "resources": [{"_type": "LocalFile", "name": "CHEP_2015_-_Ceph_Vs_Local_Storage_for_VMs.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/470\/attachments\/578938\/797155\/CHEP_2015_-_Ceph_Vs_Local_Storage_for_VMs.pdf", "fileName": "CHEP_2015_-_Ceph_Vs_Local_Storage_for_VMs.pdf", "_fossil": "localFileMetadata", "id": "797155", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "fd06959b93c405340e9900abf962beb7", "affiliation": "STFC - Rutherford Appleton Lab. (GB)", "_fossil": "contributionParticipationMetadata", "fullName": "LAHIFF, Andrew David", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "27e591c48f3d8d36d8501eb3b2b88453", "affiliation": "STFC", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. BARNSLEY, Frazer", "id": "3"}], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/470", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "471", "speakers": [{"_type": "ContributionParticipation", "emailHash": "d3e2277ea45849205ed9ccb255a1a566", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "FARRELL, Steven Andrew", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "d3e2277ea45849205ed9ccb255a1a566", "affiliation": "Lawrence Berkeley National Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "FARRELL, Steven Andrew", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "80b3205f91c90701d8faedfc4456795c", "affiliation": "", "_fossil": "contributionParticipationMetadata", "fullName": "DOMINGUES, Luis", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "11ad2ef4d5f79d88093e41b55ab2c5b6", "affiliation": "Lawrence Berkeley National Laboratory", "_fossil": "contributionParticipationMetadata", "fullName": "MONNARD, Romain", "id": "2"}], "title": "Data-transfer optimization for accelerator coprocessors", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "Data flows between host and coprocessor memory have a significant impact on the\r\noverall performance of applications. Even so, most R&D tends to focus on developing and tuning algorithms for the target architectures only. Further,\r\nwhen algorithms are run from a multi-processing\/-threading framework, the design of data transfers can not be done in isolation.\r\nWe leverage the yampl and APE libraries to implement transfer strategies for\r\ntwo of the most common coprocessor architectures: Intel's Many Integrated Core\r\n(Xeon Phi) and General Purpose GPU. We collect communication patterns from our test codes (which include multi-threaded Geant4 and ATLAS' high-level trigger\r\ntrack reconstruction) and compare and contrast the use and performance of standard POSIX sockets, the Intel SCIF library, and Cuda streams, using different choices for data aggregation, buffer handling and transfer scheduling, for both serial as well as parallel executions of the test codes. We present our results and how they can be used as feedback to improve the data transfers for these applications.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/471", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session B", "keywords": [], "id": "476", "speakers": [{"_type": "ContributionParticipation", "emailHash": "f93807550591a4b3698b7ee42329e540", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MONETA, Lorenzo", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "f93807550591a4b3698b7ee42329e540", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MONETA, Lorenzo", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "28b03fed334cb75a99ee0a92ecd47530", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "VASILEV, Vasil Georgiev", "id": "1"}], "title": "Clad -\u00a0Automatic Differentiation Using Cling in ROOT", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "Differentiation is ubiquitous in high energy physics, for instance for minimization algorithms used for fitting and statistical analysis, detector alignment and calibration, and for theoretical physics.  Automatic differentiation (AD) avoids well-known limitations in round-offs and speed, which symbolic and numerical differentiation suffer from, by transforming the source code of functions.\r\nWe will present how AD can be used to compute the gradient of multi-variate functions and functor objects. We will explain approaches to implement an AD tool. We will show how LLVM, Clang and Cling (ROOT's C++11 interpreter) simplifies the creation of such a tool. We describe how the tool will be integrated within ROOT to be used by statistical tools such as RooFit and RooStats. We will demonstrate a simple proof-of-concept prototype, called clad, which is able to generate n-th order derivatives of C++ functions and other language constructs. We also demonstrate how clad can offload laborious computations from the CPU using OpenCL.\u00a0", "track": "Track2: Offline software ", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/10\/contribution\/476", "roomFullname": null}, {"startDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "10:45:00"}, "endDate": {"date": "2015-04-14", "tz": "Europe\/Zurich", "time": "11:00:00"}, "duration": 15, "session": "Track 8 Session", "keywords": [], "id": "477", "speakers": [{"_type": "ContributionParticipation", "emailHash": "b38f11fda25741459bd44e135fcb3cbd", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "FISCHER, Max", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "b38f11fda25741459bd44e135fcb3cbd", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "FISCHER, Max", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "d226af6c5b8949bfcd9de2e05d482260", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "KUHN, Eileen", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "b33ef76a03efd37e2298b4daeb119466", "affiliation": "KIT - Karlsruhe Institute of Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "METZLAFF, Christian", "id": "2"}], "title": "High performance data analysis via coordinated caches", "note": {}, "location": "B503", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:13:18.285760+00:00", "description": "", "title": "2015_CHEP_HTDA_MF_new.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/477\/attachments\/578939\/797156\/2015_CHEP.pdf", "filename": "2015_CHEP.pdf", "content_type": "application\/pdf", "type": "file", "id": 797156, "size": 11752947}], "title": "Slides", "default_folder": false, "id": 578939, "description": ""}], "_type": "Contribution", "description": "With the second run period of the LHC, high energy physics collaborations will have to face increasing computing infrastructural needs.\r\nOpportunistic resources are expected to absorb many computationally expensive tasks, such as Monte Carlo event simulation.\r\nThis leaves dedicated HEP infrastructure with an increased load of analysis tasks that in turn will need to process an increased volume of data.\r\nIn addition to storage capacities, a key factor for future computing infrastructure is therefore input bandwidth available per core.\r\n\r\nModern data analysis infrastructure relies on one of two paradigms:\r\ndata is kept on dedicated storage and accessed via network or distributed over all compute nodes and accessed locally.\r\nDedicated storage allows data volume to grow independently of processing capacities, whereas local access allows processing capacities to scale linearly.\r\nHowever, with the growing data volume and processing requirements, HEP will require both of these features.\r\n\r\nFor enabling adequate user analyses in the future, the KIT CMS group is merging both paradigms:\r\nHigh-throughput data is spread over a local disk layer on compute nodes,\r\nwhile any data is available from an arbitrarily sized background storage.\r\nThis concept is implemented as a pool of distributed caches, which are loosely coordinated by a central service.\r\nA Tier 3 prototype cluster is currently being set up for performant user analyses of both local and remote data.\r\n\r\nThe contribution will discuss the current topology of computing resources available for HEP user analyses.\r\nBased on this, an overview on the KIT CMS analysis cluster design and implementation is presented.\r\nFinally, operational experience in terms of performance and reliability is presented.", "track": "Track8: Performance increase and optimization exploiting hardware features", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578939", "resources": [{"_type": "LocalFile", "name": "2015_CHEP_HTDA_MF_new.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/477\/attachments\/578939\/797156\/2015_CHEP.pdf", "fileName": "2015_CHEP.pdf", "_fossil": "localFileMetadata", "id": "797156", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "7aba38cfa2a99bc621b6076d46ea0ab9", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "QUAST, Gunter", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "50a1799bdcddc4a0accf8871044dc30f", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "HAUTH, Thomas", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "4535f96fcde7dd5d7c121710acfa24cd", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "GIFFELS, Manuel", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "18f12d579cee5d8345191b00dae91d02", "affiliation": "KIT - Karlsruhe Institute of  Technology (DE)", "_fossil": "contributionParticipationMetadata", "fullName": "JUNG, Christopher", "id": "6"}], "subContributions": [], "room": "B503", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/8\/contribution\/477", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:00:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "04:15:00"}, "duration": 15, "session": "Track 2 Session", "keywords": [], "id": "474", "speakers": [{"_type": "ContributionParticipation", "emailHash": "f93807550591a4b3698b7ee42329e540", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MONETA, Lorenzo", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "f93807550591a4b3698b7ee42329e540", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MONETA, Lorenzo", "id": "0"}], "title": "Using R in ROOT with the  ROOT-R package", "note": {}, "location": "Auditorium", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T03:25:29.060985+00:00", "description": "", "title": "ROOTR_chep2015.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/474\/attachments\/578940\/797157\/ROOTR_chep2015.pdf", "filename": "ROOTR_chep2015.pdf", "content_type": "application\/pdf", "type": "file", "id": 797157, "size": 777241}], "title": "Slides", "default_folder": false, "id": 578940, "description": ""}], "_type": "Contribution", "description": "ROOT is \u00a0a C++ data analysis framework, providing advanced statistical methods needed by the HEP experiments for\u00a0analysing\u00a0their data.\u00a0R is a free software framework for statistical computing, which complements the functionality of ROOT, by including some of the latest tools developed by statistics and computing research groups. We will present the ROOT-R package, a module in ROOT, which allows to use from the ROOT environment R functions using the low-level R C++ API provided by R. This interface opens the\u00a0possibility to use in ROOT and with data stored in ROOT objects, the very large set\u00a0of statistical tools present in \u00a0R. We will describe how this interface works, by converting ROOT C++ objects in R\u2019s objects, which can be\u00a0passed to the \u00a0R functions and then by converting the result back in ROOT objects. \u00a0We will show as well examples how the R tools can be used inside ROOT, in particular by presenting a ROOT plug-in module based on the R\u00a0optimization package, which can be used to\u00a0minimise\u00a0functions in ROOT and also for fitting.\u00a0", "track": "Track2: Offline software ", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578940", "resources": [{"_type": "LocalFile", "name": "ROOTR_chep2015.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/474\/attachments\/578940\/797157\/ROOTR_chep2015.pdf", "fileName": "ROOTR_chep2015.pdf", "_fossil": "localFileMetadata", "id": "797157", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "2dd823761112a1676a86fdf28a9688ef", "affiliation": "Columbia", "_fossil": "contributionParticipationMetadata", "fullName": "ZAPATA MESA, Omar", "id": "1"}], "subContributions": [], "room": "Auditorium", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/2\/contribution\/474", "roomFullname": null}, {"startDate": null, "endDate": null, "duration": 15, "session": "Poster session A", "keywords": [], "id": "475", "speakers": [{"_type": "ContributionParticipation", "emailHash": "f93807550591a4b3698b7ee42329e540", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MONETA, Lorenzo", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "f93807550591a4b3698b7ee42329e540", "affiliation": "CERN", "_fossil": "contributionParticipationMetadata", "fullName": "MONETA, Lorenzo", "id": "0"}], "title": "New developments in the ROOT function and fitting classes", "note": {}, "location": "OIST", "_fossil": "contributionMetadataWithSubContribs", "type": "poster presentation", "folders": [], "_type": "Contribution", "description": "Several improvements have been introduced in the new version 6 of ROOT in the Math work package. We will report on the improvements in the ROOT function classes used for fitting data objects like histograms or trees. These include the usage of a new TFormula class, based on the capabilities Cling, which makes easier for the user to build complex expressions, which can be compiled on-the fly using the JIT provided by Cling. We will present also how the ROOT function classes have been now extended with the capabilities to auto-normalise, \u00a0making them usable as probability density functions in \u00a0fitting. We will show how the functions can be added together, to perform an extended likelihood fit of several normalised components, and also how it is possible to perform function convolution using the Fourier transforms, provided in ROOT by the FFTW package. Finally, we will present the changes in the function interfaces needed to use SIMD vectorisation to speed-up the function evaluation in fitting. \u00a0", "track": "Track2: Offline software ", "material": [], "coauthors": [], "subContributions": [], "room": "", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/9\/contribution\/475", "roomFullname": null}, {"startDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:15:00"}, "endDate": {"date": "2015-04-16", "tz": "Europe\/Zurich", "time": "03:30:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "478", "speakers": [{"_type": "ContributionParticipation", "emailHash": "7a6c8f49981d093b41b365a4b522378d", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. EULISSE, Giulio", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "7a6c8f49981d093b41b365a4b522378d", "affiliation": "Fermi National Accelerator Lab. (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. EULISSE, Giulio", "id": "0"}, {"_type": "ContributionParticipation", "emailHash": "b80cd961600a46852776599aee87a421", "affiliation": "Aalto University", "_fossil": "contributionParticipationMetadata", "fullName": "NYB\u00c4CK, Filip", "id": "1"}], "title": "IgProf profiler support for power efficient computing", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-15T17:55:54.432842+00:00", "description": "", "title": "chep2015-igprof.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/478\/attachments\/578941\/797158\/chep2015-igprof.pdf", "filename": "chep2015-igprof.pdf", "content_type": "application\/pdf", "type": "file", "id": 797158, "size": 570536}], "title": "Slides", "default_folder": false, "id": 578941, "description": ""}], "_type": "Contribution", "description": "In recent years the size and scale of scientific computing has grown\r\nsignificantly. Computing facilities have grown to the point where energy\r\navailability and costs have become important limiting factors\r\nfor data-center size and density. At the same time, power density \r\nlimitations in processors themselves are driving interest in more\r\nheterogeneous processor architectures. Optimizing application performance is\r\nno longer required merely to obtain results faster, but also to stay within\r\nthe economic limits and constraints imposed by power hungry datacenters.\r\n\r\nIgProf is an open-source, general purpose memory and performance\r\nprofiler suite. We present the improvements we have made to permit\r\noptimizing the power efficiency of an application. This functionality\r\nbuilds on direct measurements of power related quantities using\r\na newly developed IgProf module which exploits novel on-chip power monitoring\r\ncapabilities (such as RAPL on new Intel processors). We also explore\r\nindirect methods which extrapolate from other measured application\r\ncharacteristics and processor behaviors, using CPU performance counters\r\nand processor power states.\r\n\r\nWe demonstrate the use of our tools both on small micro-benchmarks,\r\ndeveloped to better understand problems and tuning measurements, and with\r\ncomplex, large-scale C++ applications, derived from millions of lines\r\nof code.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578941", "resources": [{"_type": "LocalFile", "name": "chep2015-igprof.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/478\/attachments\/578941\/797158\/chep2015-igprof.pdf", "fileName": "chep2015-igprof.pdf", "_fossil": "localFileMetadata", "id": "797158", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "9a8ce9aebaabdde7956acf3f73f3b7a5", "affiliation": "Princeton University (US)", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ELMER, Peter", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "5ed09727f43058232b24b8ad13f529a8", "affiliation": "Vilnius University (LT)", "_fossil": "contributionParticipationMetadata", "fullName": "ABDURACHMANOV, David", "id": "3"}, {"_type": "ContributionParticipation", "emailHash": "ec2d53415bd1d1a3f795de0beb712b78", "affiliation": "Helsinki Institute of Physics (FI)", "_fossil": "contributionParticipationMetadata", "fullName": "KHAN, Kashif Nizam", "id": "4"}, {"_type": "ContributionParticipation", "emailHash": "c371ac01e59978a8310c3164856cbf66", "affiliation": "Aalto University", "_fossil": "contributionParticipationMetadata", "fullName": "Prof. NURMINEN, Jukka", "id": "5"}, {"_type": "ContributionParticipation", "emailHash": "665ef6ba285e4397b8f25a6e52a5e614", "affiliation": "Helsinki Institute of Physics (FI)", "_fossil": "contributionParticipationMetadata", "fullName": "NIEMI, Tapio Petteri", "id": "6"}, {"_type": "ContributionParticipation", "emailHash": "b0f2b3146f3c6750086177d2befc37c2", "affiliation": "Aalto University", "_fossil": "contributionParticipationMetadata", "fullName": "ZHONGHONG, Ou", "id": "7"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/478", "roomFullname": null}, {"startDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:00:00"}, "endDate": {"date": "2015-04-13", "tz": "Europe\/Zurich", "time": "11:15:00"}, "duration": 15, "session": "Track 4 Session", "keywords": [], "id": "479", "speakers": [{"_type": "ContributionParticipation", "emailHash": "6932bbeb7bf05b8931ecb925f1435e00", "affiliation": "Institution of High Energy Physics, Chinese Academy of Science", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. YAN, Tian", "id": "0"}], "primaryauthors": [{"_type": "ContributionParticipation", "emailHash": "6932bbeb7bf05b8931ecb925f1435e00", "affiliation": "Institution of High Energy Physics, Chinese Academy of Science", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. YAN, Tian", "id": "0"}], "title": "Multi-VO Support in IHEP\u2019s Distributed Computing Environment", "note": {}, "location": "B250", "_fossil": "contributionMetadataWithSubContribs", "type": "oral presentation", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-11T16:30:14.515865+00:00", "description": "", "title": "chep2015_yant_v3.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/479\/attachments\/578942\/797160\/chep2015_yant_v3.pdf", "filename": "chep2015_yant_v3.pdf", "content_type": "application\/pdf", "type": "file", "id": 797160, "size": 1316551}, {"_type": "attachment", "modified_dt": "2015-04-11T16:30:14.515865+00:00", "description": "", "title": "chep2015_yant_v3.pptx", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/479\/attachments\/578942\/797159\/chep2015_yant_v3.pptx", "filename": "chep2015_yant_v3.pptx", "content_type": "application\/vnd.openxmlformats-officedocument.presentationml.presentation", "type": "file", "id": 797159, "size": 2188299}], "title": "Slides", "default_folder": false, "id": 578942, "description": ""}], "_type": "Contribution", "description": "For Beijing Spectrometer III (BESIII) experiment located at the Institute of High Energy Physics (IHEP),  China, the distributed computing environment (DCE) has been setup and been in production status since 2012. The basic framework or middleware is DIRAC (Distributed Infrastructure with Remote Agent Control) with BES-DIRAC extensions. About 2000 CPU cores and 400 TB storage contributed by BESIII collaboration members are integrated to serve MC simulation and reconstruction jobs, as well as data transfer between sites.\r\n\r\nRecently, this distributed computing environment has been extended to support other experiments operated by IHEP, such as Jiangmen Underground Neutrino Observatory (JUNO), Circular Electron Positron Collider (CEPC), Large High Altitude Air Shower Observatory (LHAASO), Hard X-ray Modulation Telescope (HXMT), etc. Each experiment\u2019s community naturally forms a virtual organization (VO). They have various heterogeneous computing and storage resources located at geographically distributed universities or institutions worldwide, with larger or smaller size. They wish to integrate those resources and supply to their VO\u2019s users. The already established BES-DIRAC DCE is chosen as platform to serve their requirements, with considerable multi-VO support technologies implemented. \r\n\r\nThe advantage of building DIRAC as a service is obvious in our case. First of all, DIRAC servers need dedicated hardware and expert manpower to maintain, some small VOs are not willing to afford that. Second, many universities and institutions in China are collaboration members of several experiments listed above, they would like to contribute resources to more than one experiment, a single DIRAC setup will be the easiest way to manage these resources. Therefore, Instead of setting up dedicated DCE for each experiment, operating a single setup for all the experiments is an effective way to make maximal use of shared resources and manpower while remaining flexible and open for new experiments to join. \r\n\r\nThe key point for this single setup is to design and develop a multi-VO support scheme based on BES-DIRAC framework. \r\n\r\nThe VO Management System (VOMS) plays a central role in multi-VO members and resources information management. Other services will retrieve user proxy information and resource per-VO configurations from VOMS server. \r\n\r\nIn data management subsystem, DIRAC file catalogs (DFC) with dynamic dataset functionality is extended to handle files and datasets belong to users from different VOs. StoRM SE is employed to manage the storage resources contributed by universities belong to more than two VOs. \r\n\r\nIn workload management subsystem, the pilot, job matcher and scheduler should take into consideration the VO properties of jobs, priorities of VOs in resources configurations, etc. \r\n\r\nIn frontend subsystem, a task based request submission and management system is developed. Different VO\u2019s users can specify their own workflows. It allows users from different VOs to submit jobs, manage data transferring, get monitoring and accounting information of their requests. \r\n\r\nThe experience of designing multi-VO support in IHEP\u2019s distributed computing environment may be interested and useful for other high energy physics center operating several experiments.", "track": "Track4: Middleware, software development and tools, experiment frameworks, tools for distributed computing", "material": [{"_type": "Slides", "title": "Slides", "_fossil": "materialMetadata", "id": "578942", "resources": [{"_type": "LocalFile", "name": "chep2015_yant_v3.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/479\/attachments\/578942\/797160\/chep2015_yant_v3.pdf", "fileName": "chep2015_yant_v3.pdf", "_fossil": "localFileMetadata", "id": "797160", "_deprecated": true}, {"_type": "LocalFile", "name": "chep2015_yant_v3.pptx", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/479\/attachments\/578942\/797159\/chep2015_yant_v3.pptx", "fileName": "chep2015_yant_v3.pptx", "_fossil": "localFileMetadata", "id": "797159", "_deprecated": true}], "_deprecated": true}], "coauthors": [{"_type": "ContributionParticipation", "emailHash": "2a22b12f3122ed9f50526fc1d94acdd8", "affiliation": "IHEP, CAS, China", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. SUO, Bing", "id": "1"}, {"_type": "ContributionParticipation", "emailHash": "ef679b9a5ef0fb7eeadebb1b038f6ef6", "affiliation": "IHEP, CAS, China", "_fossil": "contributionParticipationMetadata", "fullName": "Mr. ZHAO, Xianghu", "id": "2"}, {"_type": "ContributionParticipation", "emailHash": "865949cef918f0ff18b94a42afcaddf3", "affiliation": "IHEP, CAS, China", "_fossil": "contributionParticipationMetadata", "fullName": "Dr. ZHANG, Xiaomei", "id": "3"}], "subContributions": [], "room": "B250", "url": "https:\/\/indico.cern.ch\/event\/304944\/session\/4\/contribution\/479", "roomFullname": null}], "id": "304944", "category": "Conferences", "title": "21st International Conference on Computing in High Energy and Nuclear Physics (CHEP2015)", "note": {}, "location": "OIST", "_fossil": "conferenceMetadataWithSubContribs", "type": "conference", "categoryId": "6725", "folders": [{"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-04-13T00:00:00+00:00", "description": "", "title": "http:\/\/chep2015.kek.jp\/venue.html", "link_url": "http:\/\/chep2015.kek.jp\/venue.html", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/attachments\/578439\/796563\/go", "type": "link", "id": 796563}], "title": "Conference Venue", "default_folder": false, "id": 578439, "description": ""}, {"_type": "folder", "attachments": [{"_type": "attachment", "modified_dt": "2015-03-31T05:51:41.258754+00:00", "description": "", "title": "floormap.pdf", "download_url": "https:\/\/indico.cern.ch\/event\/304944\/attachments\/578440\/796564\/floormap.pdf", "filename": "floormap.pdf", "content_type": "application\/pdf", "type": "file", "id": 796564, "size": 3196704}], "title": "Map of Meeting Rooms", "default_folder": false, "id": 578440, "description": ""}], "_type": "Conference", "description": "\"Evolution of Software and Computing for Experiments\"\n\nIn 2015, the LHC will restart. Experimental groups at the LHC have reviewed their Run 1 experiences in detail, acquired the latest computing and software technologies, and constructed new computing models to prepare for Run 2. On the side of the intensity frontier, Super-KEKB will start commissioning in 2015, and fixed-target experiments at CERN, Fermilab and J-PARC are growing bigger in size. In nuclear physics field, FAIR is under construction and RHIC well engaged into its Phase-II research program facing increased datasets and new challenges with precision Physics. For the future, developments are progressing towards the construction of ILC. In all these projects, computing and software will be even more important than before. Beyond those examples, non-accelerator experiments are also seeking novel computing models as their apparatus and operation become larger and distributed. With much progress, thoughts and new directions made in recent years, we strongly feel we have reached the perfect time where meeting together and sharing our experiences will be highly beneficial. ", "roomMapURL": "", "material": [{"_type": "Material", "title": "Map of Meeting Rooms", "_fossil": "materialMetadata", "id": "578440", "resources": [{"_type": "LocalFile", "name": "floormap.pdf", "url": "https:\/\/indico.cern.ch\/event\/304944\/attachments\/578440\/796564\/floormap.pdf", "fileName": "floormap.pdf", "_fossil": "localFileMetadata", "id": "796564", "_deprecated": true}], "_deprecated": true}, {"_type": "Material", "title": "Conference Venue", "_fossil": "materialMetadata", "id": "578439", "resources": [{"_fossil": "linkMetadata", "url": "http:\/\/chep2015.kek.jp\/venue.html", "_type": "Link", "name": "http:\/\/chep2015.kek.jp\/venue.html", "_deprecated": true}], "_deprecated": true}], "visibility": {"id": "", "name": "Everywhere"}, "address": "1919-1 Tancha, Onna-son, Kunigami-gun Okinawa, Japan 904-0495", "creationDate": {"date": "2014-02-26", "tz": "UTC", "time": "15:31:36.278301"}, "room": "", "chairs": [{"_type": "ConferenceChair", "emailHash": "c0f60cc34b7b1742a9071b2a2c685d26", "affiliation": "University of Tokyo (JP)", "_fossil": "conferenceChairMetadata", "fullName": "Sakamoto, Hiroshi", "id": 0}], "url": "https:\/\/indico.cern.ch\/event\/304944\/"}], "ts": 1437925089}